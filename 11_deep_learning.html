<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>11_deep_learning</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Chapter-11-&#8211;-Deep-Learning"><strong>Chapter 11 &#8211; Deep Learning</strong><a class="anchor-link" href="#Chapter-11-&#8211;-Deep-Learning">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>This notebook contains all the sample code and solutions to the exercises in chapter 11.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Setup">Setup<a class="anchor-link" href="#Setup">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># To support both python 2 and python 3</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>

<span class="c1"># Common imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># to make this notebook&#39;s output stable across runs</span>
<span class="k">def</span> <span class="nf">reset_graph</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># To plot pretty figures</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;xtick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;ytick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>

<span class="c1"># Where to save the figures</span>
<span class="n">PROJECT_ROOT_DIR</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span>
<span class="n">CHAPTER_ID</span> <span class="o">=</span> <span class="s2">&quot;deep&quot;</span>

<span class="k">def</span> <span class="nf">save_fig</span><span class="p">(</span><span class="n">fig_id</span><span class="p">,</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">PROJECT_ROOT_DIR</span><span class="p">,</span> <span class="s2">&quot;images&quot;</span><span class="p">,</span> <span class="n">CHAPTER_ID</span><span class="p">,</span> <span class="n">fig_id</span> <span class="o">+</span> <span class="s2">&quot;.png&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Saving figure&quot;</span><span class="p">,</span> <span class="n">fig_id</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tight_layout</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Vanishing/Exploding-Gradients-Problem">Vanishing/Exploding Gradients Problem<a class="anchor-link" href="#Vanishing/Exploding-Gradients-Problem">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="o">/</span><span class="mi">4</span><span class="p">],</span> <span class="s1">&#39;g--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">logit</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">props</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Saturating&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">),</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">props</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Saturating&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">),</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">props</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Linear&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">props</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Sigmoid activation function&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>

<span class="n">save_fig</span><span class="p">(</span><span class="s2">&quot;sigmoid_saturation_plot&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving figure sigmoid_saturation_plot
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Xavier-and-He-Initialization">Xavier and He Initialization<a class="anchor-link" href="#Xavier-and-He-Initialization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: the book uses <code>tensorflow.contrib.layers.fully_connected()</code> rather than <code>tf.layers.dense()</code> (which did not exist when this chapter was written). It is now preferable to use <code>tf.layers.dense()</code>, because anything in the contrib module may change or be deleted without notice. The <code>dense()</code> function is almost identical to the <code>fully_connected()</code> function. The main differences relevant to this chapter are:</p>
<ul>
<li>several parameters are renamed: <code>scope</code> becomes <code>name</code>, <code>activation_fn</code> becomes <code>activation</code> (and similarly the <code>_fn</code> suffix is removed from other parameters such as <code>normalizer_fn</code>), <code>weights_initializer</code> becomes <code>kernel_initializer</code>, etc.</li>
<li>the default <code>activation</code> is now <code>None</code> rather than <code>tf.nn.relu</code>.</li>
<li>it does not support <code>tensorflow.contrib.framework.arg_scope()</code> (introduced later in chapter 11).</li>
<li>it does not support regularizer params (introduced later in chapter 11).</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">he_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">()</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                          <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Nonsaturating-Activation-Functions">Nonsaturating Activation Functions<a class="anchor-link" href="#Nonsaturating-Activation-Functions">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Leaky-ReLU">Leaky ReLU<a class="anchor-link" href="#Leaky-ReLU">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">),</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">props</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Leak&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">props</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Leaky ReLU activation function&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">])</span>

<span class="n">save_fig</span><span class="p">(</span><span class="s2">&quot;leaky_relu_plot&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving figure leaky_relu_plot
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Implementing Leaky ReLU in TensorFlow:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">leaky_relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">leaky_relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">leaky_relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's load the data:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Warning</strong>: <code>tf.examples.tutorials.mnist</code> is deprecated. We will use <code>tf.keras.datasets.mnist</code> instead.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span>
<span class="n">y_valid</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">shuffle_batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="n">n_batches</span><span class="p">):</span>
        <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">acc_batch</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
            <span class="n">acc_valid</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Batch accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_batch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_valid</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Batch accuracy: 0.86 Validation accuracy: 0.9044
5 Batch accuracy: 0.94 Validation accuracy: 0.9496
10 Batch accuracy: 0.92 Validation accuracy: 0.9654
15 Batch accuracy: 0.94 Validation accuracy: 0.971
20 Batch accuracy: 1.0 Validation accuracy: 0.9764
25 Batch accuracy: 1.0 Validation accuracy: 0.9778
30 Batch accuracy: 0.98 Validation accuracy: 0.978
35 Batch accuracy: 1.0 Validation accuracy: 0.9788
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ELU">ELU<a class="anchor-link" href="#ELU">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">z</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">z</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">elu</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;ELU activation function ($\alpha=1$)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">])</span>

<span class="n">save_fig</span><span class="p">(</span><span class="s2">&quot;elu_plot&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving figure elu_plot
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="SELU">SELU<a class="anchor-link" href="#SELU">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This activation function was proposed in this <a href="https://arxiv.org/pdf/1706.02515.pdf">great paper</a> by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017 (I added paragraph about SELU in the latest release of my book). During training, a neural network composed of a stack of dense layers using the SELU activation function will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span>
         <span class="n">scale</span><span class="o">=</span><span class="mf">1.0507009873554804934193349852946</span><span class="p">,</span>
         <span class="n">alpha</span><span class="o">=</span><span class="mf">1.6732632423543772848170429916717</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">elu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">selu</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.758</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.758</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;SELU activation function&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">])</span>

<span class="n">save_fig</span><span class="p">(</span><span class="s2">&quot;selu_plot&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving figure selu_plot
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By default, the SELU hyperparameters (<code>scale</code> and <code>alpha</code>) are tuned in such a way that the mean remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">100</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">selu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">layer</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer </span><span class="si">{}</span><span class="s2">: </span><span class="si">{:.2f}</span><span class="s2"> &lt; mean &lt; </span><span class="si">{:.2f}</span><span class="s2">, </span><span class="si">{:.2f}</span><span class="s2"> &lt; std deviation &lt; </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">layer</span><span class="p">,</span> <span class="n">means</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">means</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">stds</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">stds</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Layer 0: -0.26 &lt; mean &lt; 0.27, 0.74 &lt; std deviation &lt; 1.27
Layer 10: -0.24 &lt; mean &lt; 0.27, 0.74 &lt; std deviation &lt; 1.27
Layer 20: -0.17 &lt; mean &lt; 0.18, 0.74 &lt; std deviation &lt; 1.24
Layer 30: -0.27 &lt; mean &lt; 0.24, 0.78 &lt; std deviation &lt; 1.20
Layer 40: -0.38 &lt; mean &lt; 0.39, 0.74 &lt; std deviation &lt; 1.25
Layer 50: -0.27 &lt; mean &lt; 0.31, 0.73 &lt; std deviation &lt; 1.27
Layer 60: -0.26 &lt; mean &lt; 0.43, 0.74 &lt; std deviation &lt; 1.35
Layer 70: -0.19 &lt; mean &lt; 0.21, 0.75 &lt; std deviation &lt; 1.21
Layer 80: -0.18 &lt; mean &lt; 0.16, 0.72 &lt; std deviation &lt; 1.19
Layer 90: -0.19 &lt; mean &lt; 0.16, 0.75 &lt; std deviation &lt; 1.20
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>tf.nn.selu()</code> function was added in TensorFlow 1.4. For earlier versions, you can use the following implementation:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span>
         <span class="n">scale</span><span class="o">=</span><span class="mf">1.0507009873554804934193349852946</span><span class="p">,</span>
         <span class="n">alpha</span><span class="o">=</span><span class="mf">1.6732632423543772848170429916717</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, the SELU activation function cannot be used along with regular Dropout (this would cancel the SELU activation function's self-normalizing property). Fortunately, there is a Dropout variant called Alpha Dropout proposed in the same paper. It is available in <code>tf.contrib.nn.alpha_dropout()</code> since TF 1.4 (or check out <a href="https://github.com/bioinf-jku/SNNs/blob/master/selu.py">this implementation</a> by the Institute of Bioinformatics, Johannes Kepler University Linz).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's create a neural net for MNIST using the SELU activation function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">selu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">selu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">stds</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>
<span class="n">X_val_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_valid</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="n">stds</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_batch</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="n">stds</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">acc_batch</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
            <span class="n">acc_valid</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_val_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Batch accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_batch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_valid</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final_selu.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Batch accuracy: 0.88 Validation accuracy: 0.923
5 Batch accuracy: 0.98 Validation accuracy: 0.9578
10 Batch accuracy: 1.0 Validation accuracy: 0.9664
15 Batch accuracy: 0.96 Validation accuracy: 0.9682
20 Batch accuracy: 1.0 Validation accuracy: 0.9694
25 Batch accuracy: 1.0 Validation accuracy: 0.9688
30 Batch accuracy: 1.0 Validation accuracy: 0.9694
35 Batch accuracy: 1.0 Validation accuracy: 0.97
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Batch-Normalization">Batch Normalization<a class="anchor-link" href="#Batch-Normalization">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: the book uses <code>tensorflow.contrib.layers.batch_norm()</code> rather than <code>tf.layers.batch_normalization()</code> (which did not exist when this chapter was written). It is now preferable to use <code>tf.layers.batch_normalization()</code>, because anything in the contrib module may change or be deleted without notice. Instead of using the <code>batch_norm()</code> function as a regularizer parameter to the <code>fully_connected()</code> function, we now use <code>batch_normalization()</code> and we explicitly create a distinct layer. The parameters are a bit different, in particular:</p>
<ul>
<li><code>decay</code> is renamed to <code>momentum</code>,</li>
<li><code>is_training</code> is renamed to <code>training</code>,</li>
<li><code>updates_collections</code> is removed: the update operations needed by batch normalization are added to the <code>UPDATE_OPS</code> collection and you need to explicity run these operations during training (see the execution phase below),</li>
<li>we don't need to specify <code>scale=True</code>, as that is the default.</li>
</ul>
<p>Also note that in order to run batch norm just <em>before</em> each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.</p>
<p>Note: since the <code>tf.layers.dense()</code> function is incompatible with <code>tf.contrib.layers.arg_scope()</code> (which is used in the book), we now use python's <code>functools.partial()</code> function instead. It makes it easy to create a <code>my_dense_layer()</code> function that just calls <code>tf.layers.dense()</code> with the desired parameters automatically set (unless they are overridden when calling <code>my_dense_layer()</code>). As you can see, the code remains very similar.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>

<span class="n">training</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder_with_default</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>

<span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
<span class="n">bn1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">bn1_act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">bn1</span><span class="p">)</span>

<span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">bn1_act</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
<span class="n">bn2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">bn2_act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">bn2</span><span class="p">)</span>

<span class="n">logits_before_bn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">bn2_act</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">logits_before_bn</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
                                       <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">training</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder_with_default</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To avoid repeating the same parameters over and over again, we can use Python's <code>partial()</code> function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="k">import</span> <span class="n">partial</span>

<span class="n">my_batch_norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">,</span>
                              <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
<span class="n">bn1</span> <span class="o">=</span> <span class="n">my_batch_norm_layer</span><span class="p">(</span><span class="n">hidden1</span><span class="p">)</span>
<span class="n">bn1_act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">bn1</span><span class="p">)</span>
<span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">bn1_act</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
<span class="n">bn2</span> <span class="o">=</span> <span class="n">my_batch_norm_layer</span><span class="p">(</span><span class="n">hidden2</span><span class="p">)</span>
<span class="n">bn2_act</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">bn2</span><span class="p">)</span>
<span class="n">logits_before_bn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">bn2_act</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">my_batch_norm_layer</span><span class="p">(</span><span class="n">logits_before_bn</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">batch_norm_momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">training</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder_with_default</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">he_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">()</span>

    <span class="n">my_batch_norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">batch_norm_momentum</span><span class="p">)</span>

    <span class="n">my_dense_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">)</span>

    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">my_dense_layer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">bn1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">my_batch_norm_layer</span><span class="p">(</span><span class="n">hidden1</span><span class="p">))</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">my_dense_layer</span><span class="p">(</span><span class="n">bn1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">bn2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">my_batch_norm_layer</span><span class="p">(</span><span class="n">hidden2</span><span class="p">))</span>
    <span class="n">logits_before_bn</span> <span class="o">=</span> <span class="n">my_dense_layer</span><span class="p">(</span><span class="n">bn2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">my_batch_norm_layer</span><span class="p">(</span><span class="n">logits_before_bn</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: since we are using <code>tf.layers.batch_normalization()</code> rather than <code>tf.contrib.layers.batch_norm()</code> (as in the book), we need to explicitly run the extra update operations needed by batch normalization (<code>sess.run([training_op, extra_update_ops],...</code>).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">extra_update_ops</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">UPDATE_OPS</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">training_op</span><span class="p">,</span> <span class="n">extra_update_ops</span><span class="p">],</span>
                     <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">training</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.8952
1 Validation accuracy: 0.9202
2 Validation accuracy: 0.9318
3 Validation accuracy: 0.9422
4 Validation accuracy: 0.9468
5 Validation accuracy: 0.954
6 Validation accuracy: 0.9568
7 Validation accuracy: 0.96
8 Validation accuracy: 0.962
9 Validation accuracy: 0.9638
10 Validation accuracy: 0.9662
11 Validation accuracy: 0.9682
12 Validation accuracy: 0.9672
13 Validation accuracy: 0.9696
14 Validation accuracy: 0.9706
15 Validation accuracy: 0.9704
16 Validation accuracy: 0.9718
17 Validation accuracy: 0.9726
18 Validation accuracy: 0.9738
19 Validation accuracy: 0.9742
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What!? That's not a great accuracy for MNIST. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that you could also make the training operation depend on the update operations:</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">extra_update_ops</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">UPDATE_OPS</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">extra_update_ops</span><span class="p">):</span>
        <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
<p>This way, you would just have to evaluate the <code>training_op</code> during training, TensorFlow would automatically run the update operations as well:</p>
<div class="highlight"><pre><span></span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">training</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[37]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[&#39;hidden1/kernel:0&#39;,
 &#39;hidden1/bias:0&#39;,
 &#39;batch_normalization/gamma:0&#39;,
 &#39;batch_normalization/beta:0&#39;,
 &#39;hidden2/kernel:0&#39;,
 &#39;hidden2/bias:0&#39;,
 &#39;batch_normalization_1/gamma:0&#39;,
 &#39;batch_normalization_1/beta:0&#39;,
 &#39;outputs/kernel:0&#39;,
 &#39;outputs/bias:0&#39;,
 &#39;batch_normalization_2/gamma:0&#39;,
 &#39;batch_normalization_2/beta:0&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables</span><span class="p">()]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[38]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[&#39;hidden1/kernel:0&#39;,
 &#39;hidden1/bias:0&#39;,
 &#39;batch_normalization/gamma:0&#39;,
 &#39;batch_normalization/beta:0&#39;,
 &#39;batch_normalization/moving_mean:0&#39;,
 &#39;batch_normalization/moving_variance:0&#39;,
 &#39;hidden2/kernel:0&#39;,
 &#39;hidden2/bias:0&#39;,
 &#39;batch_normalization_1/gamma:0&#39;,
 &#39;batch_normalization_1/beta:0&#39;,
 &#39;batch_normalization_1/moving_mean:0&#39;,
 &#39;batch_normalization_1/moving_variance:0&#39;,
 &#39;outputs/kernel:0&#39;,
 &#39;outputs/bias:0&#39;,
 &#39;batch_normalization_2/gamma:0&#39;,
 &#39;batch_normalization_2/beta:0&#39;,
 &#39;batch_normalization_2/moving_mean:0&#39;,
 &#39;batch_normalization_2/moving_variance:0&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Clipping">Gradient Clipping<a class="anchor-link" href="#Gradient-Clipping">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_hidden3</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_hidden4</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_hidden5</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">hidden3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_hidden3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden3&quot;</span><span class="p">)</span>
    <span class="n">hidden4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden3</span><span class="p">,</span> <span class="n">n_hidden4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden4&quot;</span><span class="p">)</span>
    <span class="n">hidden5</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden4</span><span class="p">,</span> <span class="n">n_hidden5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden5&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden5</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we apply gradient clipping. For this, we need to get the gradients, use the <code>clip_by_value()</code> function to clip them, then apply them:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threshold</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">grads_and_vars</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">capped_gvs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="o">-</span><span class="n">threshold</span><span class="p">,</span> <span class="n">threshold</span><span class="p">),</span> <span class="n">var</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">]</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">capped_gvs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The rest is the same as usual:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.288
1 Validation accuracy: 0.7936
2 Validation accuracy: 0.8798
3 Validation accuracy: 0.906
4 Validation accuracy: 0.9164
5 Validation accuracy: 0.9218
6 Validation accuracy: 0.9296
7 Validation accuracy: 0.9358
8 Validation accuracy: 0.9382
9 Validation accuracy: 0.9414
10 Validation accuracy: 0.9456
11 Validation accuracy: 0.9474
12 Validation accuracy: 0.9478
13 Validation accuracy: 0.9534
14 Validation accuracy: 0.9568
15 Validation accuracy: 0.9566
16 Validation accuracy: 0.9574
17 Validation accuracy: 0.959
18 Validation accuracy: 0.9622
19 Validation accuracy: 0.9612
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reusing-Pretrained-Layers">Reusing Pretrained Layers<a class="anchor-link" href="#Reusing-Pretrained-Layers">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reusing-a-TensorFlow-Model">Reusing a TensorFlow Model<a class="anchor-link" href="#Reusing-a-TensorFlow-Model">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First you need to load the graph's structure. The <code>import_meta_graph()</code> function does just that, loading the graph's operations into the default graph, and returning a <code>Saver</code> that you can then use to restore the model's state. Note that by default, a <code>Saver</code> saves the structure of the graph into a <code>.meta</code> file, so that's the file you should load:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="s2">&quot;./my_model_final.ckpt.meta&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[48]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operations</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>X
y
hidden1/kernel/Initializer/random_uniform/shape
hidden1/kernel/Initializer/random_uniform/min
hidden1/kernel/Initializer/random_uniform/max
hidden1/kernel/Initializer/random_uniform/RandomUniform
hidden1/kernel/Initializer/random_uniform/sub
hidden1/kernel/Initializer/random_uniform/mul
hidden1/kernel/Initializer/random_uniform
hidden1/kernel
hidden1/kernel/Assign
hidden1/kernel/read
hidden1/bias/Initializer/zeros
hidden1/bias
hidden1/bias/Assign
hidden1/bias/read
dnn/hidden1/MatMul
dnn/hidden1/BiasAdd
dnn/hidden1/Relu
hidden2/kernel/Initializer/random_uniform/shape
hidden2/kernel/Initializer/random_uniform/min
hidden2/kernel/Initializer/random_uniform/max
hidden2/kernel/Initializer/random_uniform/RandomUniform
hidden2/kernel/Initializer/random_uniform/sub
hidden2/kernel/Initializer/random_uniform/mul
hidden2/kernel/Initializer/random_uniform
hidden2/kernel
hidden2/kernel/Assign
hidden2/kernel/read
hidden2/bias/Initializer/zeros
hidden2/bias
hidden2/bias/Assign
hidden2/bias/read
dnn/hidden2/MatMul
dnn/hidden2/BiasAdd
dnn/hidden2/Relu
hidden3/kernel/Initializer/random_uniform/shape
hidden3/kernel/Initializer/random_uniform/min
hidden3/kernel/Initializer/random_uniform/max
hidden3/kernel/Initializer/random_uniform/RandomUniform
hidden3/kernel/Initializer/random_uniform/sub
hidden3/kernel/Initializer/random_uniform/mul
hidden3/kernel/Initializer/random_uniform
hidden3/kernel
hidden3/kernel/Assign
hidden3/kernel/read
hidden3/bias/Initializer/zeros
hidden3/bias
hidden3/bias/Assign
hidden3/bias/read
dnn/hidden3/MatMul
dnn/hidden3/BiasAdd
dnn/hidden3/Relu
hidden4/kernel/Initializer/random_uniform/shape
hidden4/kernel/Initializer/random_uniform/min
hidden4/kernel/Initializer/random_uniform/max
hidden4/kernel/Initializer/random_uniform/RandomUniform
hidden4/kernel/Initializer/random_uniform/sub
hidden4/kernel/Initializer/random_uniform/mul
hidden4/kernel/Initializer/random_uniform
hidden4/kernel
hidden4/kernel/Assign
hidden4/kernel/read
hidden4/bias/Initializer/zeros
hidden4/bias
hidden4/bias/Assign
hidden4/bias/read
dnn/hidden4/MatMul
dnn/hidden4/BiasAdd
dnn/hidden4/Relu
hidden5/kernel/Initializer/random_uniform/shape
hidden5/kernel/Initializer/random_uniform/min
hidden5/kernel/Initializer/random_uniform/max
hidden5/kernel/Initializer/random_uniform/RandomUniform
hidden5/kernel/Initializer/random_uniform/sub
hidden5/kernel/Initializer/random_uniform/mul
hidden5/kernel/Initializer/random_uniform
hidden5/kernel
hidden5/kernel/Assign
hidden5/kernel/read
hidden5/bias/Initializer/zeros
hidden5/bias
hidden5/bias/Assign
hidden5/bias/read
dnn/hidden5/MatMul
dnn/hidden5/BiasAdd
dnn/hidden5/Relu
outputs/kernel/Initializer/random_uniform/shape
outputs/kernel/Initializer/random_uniform/min
outputs/kernel/Initializer/random_uniform/max
outputs/kernel/Initializer/random_uniform/RandomUniform
outputs/kernel/Initializer/random_uniform/sub
outputs/kernel/Initializer/random_uniform/mul
outputs/kernel/Initializer/random_uniform
outputs/kernel
outputs/kernel/Assign
outputs/kernel/read
outputs/bias/Initializer/zeros
outputs/bias
outputs/bias/Assign
outputs/bias/read
dnn/outputs/MatMul
dnn/outputs/BiasAdd
loss/SparseSoftmaxCrossEntropyWithLogits/Shape
loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits
loss/Const
loss/loss
gradients/Shape
gradients/grad_ys_0
gradients/Fill
gradients/loss/loss_grad/Reshape/shape
gradients/loss/loss_grad/Reshape
gradients/loss/loss_grad/Shape
gradients/loss/loss_grad/Tile
gradients/loss/loss_grad/Shape_1
gradients/loss/loss_grad/Shape_2
gradients/loss/loss_grad/Const
gradients/loss/loss_grad/Prod
gradients/loss/loss_grad/Const_1
gradients/loss/loss_grad/Prod_1
gradients/loss/loss_grad/Maximum/y
gradients/loss/loss_grad/Maximum
gradients/loss/loss_grad/floordiv
gradients/loss/loss_grad/Cast
gradients/loss/loss_grad/truediv
gradients/zeros_like
gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient
gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim
gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims
gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul
gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad
gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps
gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency
gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1
gradients/dnn/outputs/MatMul_grad/MatMul
gradients/dnn/outputs/MatMul_grad/MatMul_1
gradients/dnn/outputs/MatMul_grad/tuple/group_deps
gradients/dnn/outputs/MatMul_grad/tuple/control_dependency
gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1
gradients/dnn/hidden5/Relu_grad/ReluGrad
gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad
gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps
gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency
gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1
gradients/dnn/hidden5/MatMul_grad/MatMul
gradients/dnn/hidden5/MatMul_grad/MatMul_1
gradients/dnn/hidden5/MatMul_grad/tuple/group_deps
gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency
gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1
gradients/dnn/hidden4/Relu_grad/ReluGrad
gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad
gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps
gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency
gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1
gradients/dnn/hidden4/MatMul_grad/MatMul
gradients/dnn/hidden4/MatMul_grad/MatMul_1
gradients/dnn/hidden4/MatMul_grad/tuple/group_deps
gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency
gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1
gradients/dnn/hidden3/Relu_grad/ReluGrad
gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad
gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps
gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency
gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1
gradients/dnn/hidden3/MatMul_grad/MatMul
gradients/dnn/hidden3/MatMul_grad/MatMul_1
gradients/dnn/hidden3/MatMul_grad/tuple/group_deps
gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency
gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1
gradients/dnn/hidden2/Relu_grad/ReluGrad
gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad
gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps
gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency
gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1
gradients/dnn/hidden2/MatMul_grad/MatMul
gradients/dnn/hidden2/MatMul_grad/MatMul_1
gradients/dnn/hidden2/MatMul_grad/tuple/group_deps
gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency
gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1
gradients/dnn/hidden1/Relu_grad/ReluGrad
gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad
gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps
gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency
gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1
gradients/dnn/hidden1/MatMul_grad/MatMul
gradients/dnn/hidden1/MatMul_grad/MatMul_1
gradients/dnn/hidden1/MatMul_grad/tuple/group_deps
gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency
gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1
clip_by_value/Minimum/y
clip_by_value/Minimum
clip_by_value/y
clip_by_value
clip_by_value_1/Minimum/y
clip_by_value_1/Minimum
clip_by_value_1/y
clip_by_value_1
clip_by_value_2/Minimum/y
clip_by_value_2/Minimum
clip_by_value_2/y
clip_by_value_2
clip_by_value_3/Minimum/y
clip_by_value_3/Minimum
clip_by_value_3/y
clip_by_value_3
clip_by_value_4/Minimum/y
clip_by_value_4/Minimum
clip_by_value_4/y
clip_by_value_4
clip_by_value_5/Minimum/y
clip_by_value_5/Minimum
clip_by_value_5/y
clip_by_value_5
clip_by_value_6/Minimum/y
clip_by_value_6/Minimum
clip_by_value_6/y
clip_by_value_6
clip_by_value_7/Minimum/y
clip_by_value_7/Minimum
clip_by_value_7/y
clip_by_value_7
clip_by_value_8/Minimum/y
clip_by_value_8/Minimum
clip_by_value_8/y
clip_by_value_8
clip_by_value_9/Minimum/y
clip_by_value_9/Minimum
clip_by_value_9/y
clip_by_value_9
clip_by_value_10/Minimum/y
clip_by_value_10/Minimum
clip_by_value_10/y
clip_by_value_10
clip_by_value_11/Minimum/y
clip_by_value_11/Minimum
clip_by_value_11/y
clip_by_value_11
GradientDescent/learning_rate
GradientDescent/update_hidden1/kernel/ApplyGradientDescent
GradientDescent/update_hidden1/bias/ApplyGradientDescent
GradientDescent/update_hidden2/kernel/ApplyGradientDescent
GradientDescent/update_hidden2/bias/ApplyGradientDescent
GradientDescent/update_hidden3/kernel/ApplyGradientDescent
GradientDescent/update_hidden3/bias/ApplyGradientDescent
GradientDescent/update_hidden4/kernel/ApplyGradientDescent
GradientDescent/update_hidden4/bias/ApplyGradientDescent
GradientDescent/update_hidden5/kernel/ApplyGradientDescent
GradientDescent/update_hidden5/bias/ApplyGradientDescent
GradientDescent/update_outputs/kernel/ApplyGradientDescent
GradientDescent/update_outputs/bias/ApplyGradientDescent
GradientDescent
eval/in_top_k/InTopKV2/k
eval/in_top_k/InTopKV2
eval/Cast
eval/Const
eval/accuracy
init
save/Const
save/SaveV2/tensor_names
save/SaveV2/shape_and_slices
save/SaveV2
save/control_dependency
save/RestoreV2/tensor_names
save/RestoreV2/shape_and_slices
save/RestoreV2
save/Assign
save/Assign_1
save/Assign_2
save/Assign_3
save/Assign_4
save/Assign_5
save/Assign_6
save/Assign_7
save/Assign_8
save/Assign_9
save/Assign_10
save/Assign_11
save/restore_all
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Oops, that's a lot of operations! It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a <code>FileWriter</code> to save the graph and then visualize it in TensorBoard):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[49]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow_graph_in_jupyter</span> <span class="k">import</span> <span class="n">show_graph</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">show_graph</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">

        <iframe seamless style="width:1200px;height:620px;border:0" srcdoc="
        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>
        <script>
          function load() {
            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\n  name: &quot;X&quot;\n  op: &quot;Placeholder&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: -1\n        }\n        dim {\n          size: 784\n        }\n      }\n    }\n  }\n}\nnode {\n  name: &quot;y&quot;\n  op: &quot;Placeholder&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 2\n          }\n        }\n        tensor_content: &quot;\\020\\003\\000\\000,\\001\\000\\000&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -0.07439795136451721\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 0.07439795136451721\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\n  op: &quot;RandomUniform&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;seed&quot;\n    value {\n      i: 42\n    }\n  }\n  attr {\n    key: &quot;seed2&quot;\n    value {\n      i: 5\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\n  op: &quot;Sub&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\n  op: &quot;Mul&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\n  op: &quot;Add&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 784\n        }\n        dim {\n          size: 300\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden1/kernel&quot;\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/kernel/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden1/kernel&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n          dim {\n            size: 300\n          }\n        }\n        float_val: 0.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/bias&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 300\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/bias/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden1/bias&quot;\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden1/bias/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden1/bias&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/bias&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden1/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;X&quot;\n  input: &quot;hidden1/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden1/BiasAdd&quot;\n  op: &quot;BiasAdd&quot;\n  input: &quot;dnn/hidden1/MatMul&quot;\n  input: &quot;hidden1/bias/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden1/Relu&quot;\n  op: &quot;Relu&quot;\n  input: &quot;dnn/hidden1/BiasAdd&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 2\n          }\n        }\n        tensor_content: &quot;,\\001\\000\\0002\\000\\000\\000&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -0.13093073666095734\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 0.13093073666095734\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\n  op: &quot;RandomUniform&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;seed&quot;\n    value {\n      i: 42\n    }\n  }\n  attr {\n    key: &quot;seed2&quot;\n    value {\n      i: 22\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\n  op: &quot;Sub&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\n  op: &quot;Mul&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\n  op: &quot;Add&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 300\n        }\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden2/kernel&quot;\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/kernel/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden2/kernel&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n          dim {\n            size: 50\n          }\n        }\n        float_val: 0.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/bias&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/bias/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden2/bias&quot;\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden2/bias/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden2/bias&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/bias&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden2/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden1/Relu&quot;\n  input: &quot;hidden2/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden2/BiasAdd&quot;\n  op: &quot;BiasAdd&quot;\n  input: &quot;dnn/hidden2/MatMul&quot;\n  input: &quot;hidden2/bias/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden2/Relu&quot;\n  op: &quot;Relu&quot;\n  input: &quot;dnn/hidden2/BiasAdd&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 2\n          }\n        }\n        tensor_content: &quot;2\\000\\000\\0002\\000\\000\\000&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -0.24494896829128265\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 0.24494896829128265\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\n  op: &quot;RandomUniform&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;seed&quot;\n    value {\n      i: 42\n    }\n  }\n  attr {\n    key: &quot;seed2&quot;\n    value {\n      i: 39\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\n  op: &quot;Sub&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\n  op: &quot;Mul&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\n  op: &quot;Add&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden3/kernel&quot;\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/kernel/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden3/kernel&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n          dim {\n            size: 50\n          }\n        }\n        float_val: 0.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/bias&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/bias/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden3/bias&quot;\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden3/bias/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden3/bias&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/bias&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden3/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden2/Relu&quot;\n  input: &quot;hidden3/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden3/BiasAdd&quot;\n  op: &quot;BiasAdd&quot;\n  input: &quot;dnn/hidden3/MatMul&quot;\n  input: &quot;hidden3/bias/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden3/Relu&quot;\n  op: &quot;Relu&quot;\n  input: &quot;dnn/hidden3/BiasAdd&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 2\n          }\n        }\n        tensor_content: &quot;2\\000\\000\\0002\\000\\000\\000&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -0.24494896829128265\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 0.24494896829128265\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\n  op: &quot;RandomUniform&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;seed&quot;\n    value {\n      i: 42\n    }\n  }\n  attr {\n    key: &quot;seed2&quot;\n    value {\n      i: 56\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\n  op: &quot;Sub&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\n  op: &quot;Mul&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\n  op: &quot;Add&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden4/kernel&quot;\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/kernel/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden4/kernel&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n          dim {\n            size: 50\n          }\n        }\n        float_val: 0.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/bias&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/bias/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden4/bias&quot;\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden4/bias/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden4/bias&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/bias&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden4/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden3/Relu&quot;\n  input: &quot;hidden4/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden4/BiasAdd&quot;\n  op: &quot;BiasAdd&quot;\n  input: &quot;dnn/hidden4/MatMul&quot;\n  input: &quot;hidden4/bias/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden4/Relu&quot;\n  op: &quot;Relu&quot;\n  input: &quot;dnn/hidden4/BiasAdd&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 2\n          }\n        }\n        tensor_content: &quot;2\\000\\000\\0002\\000\\000\\000&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -0.24494896829128265\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 0.24494896829128265\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\n  op: &quot;RandomUniform&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;seed&quot;\n    value {\n      i: 42\n    }\n  }\n  attr {\n    key: &quot;seed2&quot;\n    value {\n      i: 73\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\n  op: &quot;Sub&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\n  op: &quot;Mul&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\n  op: &quot;Add&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden5/kernel&quot;\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/kernel/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden5/kernel&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n          dim {\n            size: 50\n          }\n        }\n        float_val: 0.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/bias&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/bias/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden5/bias&quot;\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;hidden5/bias/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;hidden5/bias&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/bias&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden5/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden4/Relu&quot;\n  input: &quot;hidden5/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden5/BiasAdd&quot;\n  op: &quot;BiasAdd&quot;\n  input: &quot;dnn/hidden5/MatMul&quot;\n  input: &quot;hidden5/bias/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;dnn/hidden5/Relu&quot;\n  op: &quot;Relu&quot;\n  input: &quot;dnn/hidden5/BiasAdd&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 2\n          }\n        }\n        tensor_content: &quot;2\\000\\000\\000\\n\\000\\000\\000&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -0.3162277638912201\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 0.3162277638912201\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\n  op: &quot;RandomUniform&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;seed&quot;\n    value {\n      i: 42\n    }\n  }\n  attr {\n    key: &quot;seed2&quot;\n    value {\n      i: 90\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\n  op: &quot;Sub&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\n  op: &quot;Mul&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\n  op: &quot;Add&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 50\n        }\n        dim {\n          size: 10\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;outputs/kernel&quot;\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;outputs/kernel/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;outputs/kernel&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/bias/Initializer/zeros&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n          dim {\n            size: 10\n          }\n        }\n        float_val: 0.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;outputs/bias&quot;\n  op: &quot;VariableV2&quot;\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;container&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;shape&quot;\n    value {\n      shape {\n        dim {\n          size: 10\n        }\n      }\n    }\n  }\n  attr {\n    key: &quot;shared_name&quot;\n    value {\n      s: &quot;&quot;\n    }\n  }\n}\nnode {\n  name: &quot;outputs/bias/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;outputs/bias&quot;\n  input: &quot;outputs/bias/Initializer/zeros&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;outputs/bias/read&quot;\n  op: &quot;Identity&quot;\n  input: &quot;outputs/bias&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/bias&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;dnn/outputs/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden5/Relu&quot;\n  input: &quot;outputs/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;dnn/outputs/BiasAdd&quot;\n  op: &quot;BiasAdd&quot;\n  input: &quot;dnn/outputs/MatMul&quot;\n  input: &quot;outputs/bias/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\n  op: &quot;Shape&quot;\n  input: &quot;y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;out_type&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\n  input: &quot;dnn/outputs/BiasAdd&quot;\n  input: &quot;y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;Tlabels&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;loss/Const&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 1\n          }\n        }\n        int_val: 0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;loss/loss&quot;\n  op: &quot;Mean&quot;\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\n  input: &quot;loss/Const&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;Tidx&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;keep_dims&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/Shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n          }\n        }\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/grad_ys_0&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/Fill&quot;\n  op: &quot;Fill&quot;\n  input: &quot;gradients/Shape&quot;\n  input: &quot;gradients/grad_ys_0&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;index_type&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 1\n          }\n        }\n        int_val: 1\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\n  op: &quot;Reshape&quot;\n  input: &quot;gradients/Fill&quot;\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;Tshape&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\n  op: &quot;Shape&quot;\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;out_type&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\n  op: &quot;Tile&quot;\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;Tmultiples&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\n  op: &quot;Shape&quot;\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;out_type&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n          }\n        }\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Const&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 1\n          }\n        }\n        int_val: 0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\n  op: &quot;Prod&quot;\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\n  input: &quot;gradients/loss/loss_grad/Const&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;Tidx&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;keep_dims&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 1\n          }\n        }\n        int_val: 0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\n  op: &quot;Prod&quot;\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;Tidx&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;keep_dims&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n        }\n        int_val: 1\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\n  op: &quot;FloorDiv&quot;\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\n  op: &quot;Cast&quot;\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\n  attr {\n    key: &quot;DstT&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;SrcT&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;Truncate&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\n  op: &quot;RealDiv&quot;\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/zeros_like&quot;\n  op: &quot;ZerosLike&quot;\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\n  op: &quot;PreventGradient&quot;\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;message&quot;\n    value {\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\'s interaction with tf.gradients()&quot;\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n        }\n        int_val: -1\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\n  op: &quot;ExpandDims&quot;\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;Tdim&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\n  op: &quot;Mul&quot;\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\n  op: &quot;BiasAddGrad&quot;\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\n}\nnode {\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\n  input: &quot;outputs/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden5/Relu&quot;\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\n}\nnode {\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\n  op: &quot;ReluGrad&quot;\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\n  input: &quot;dnn/hidden5/Relu&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\n  op: &quot;BiasAddGrad&quot;\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\n  input: &quot;hidden5/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden4/Relu&quot;\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\n  op: &quot;ReluGrad&quot;\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\n  input: &quot;dnn/hidden4/Relu&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\n  op: &quot;BiasAddGrad&quot;\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\n  input: &quot;hidden4/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden3/Relu&quot;\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\n  op: &quot;ReluGrad&quot;\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\n  input: &quot;dnn/hidden3/Relu&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\n  op: &quot;BiasAddGrad&quot;\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\n  input: &quot;hidden3/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden2/Relu&quot;\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\n  op: &quot;ReluGrad&quot;\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\n  input: &quot;dnn/hidden2/Relu&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\n  op: &quot;BiasAddGrad&quot;\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\n  input: &quot;hidden2/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;dnn/hidden1/Relu&quot;\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\n  op: &quot;ReluGrad&quot;\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\n  input: &quot;dnn/hidden1/Relu&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\n  op: &quot;BiasAddGrad&quot;\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;data_format&quot;\n    value {\n      s: &quot;NHWC&quot;\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\n  input: &quot;hidden1/kernel/read&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\n  op: &quot;MatMul&quot;\n  input: &quot;X&quot;\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;transpose_a&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;transpose_b&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\n  op: &quot;Identity&quot;\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value/Minimum&quot;\n  input: &quot;clip_by_value/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_1/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_1/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_1/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_1/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_1&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_1/Minimum&quot;\n  input: &quot;clip_by_value_1/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_2/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_2/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_2/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_2/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_2&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_2/Minimum&quot;\n  input: &quot;clip_by_value_2/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_3/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_3/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_3/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_3/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_3&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_3/Minimum&quot;\n  input: &quot;clip_by_value_3/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_4/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_4/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_4/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_4/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_4&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_4/Minimum&quot;\n  input: &quot;clip_by_value_4/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_5/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_5/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_5/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_5/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_5&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_5/Minimum&quot;\n  input: &quot;clip_by_value_5/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_6/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_6/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_6/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_6/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_6&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_6/Minimum&quot;\n  input: &quot;clip_by_value_6/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_7/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_7/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_7/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_7/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_7&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_7/Minimum&quot;\n  input: &quot;clip_by_value_7/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_8/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_8/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_8/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_8/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_8&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_8/Minimum&quot;\n  input: &quot;clip_by_value_8/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_9/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_9/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_9/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_9/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_9&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_9/Minimum&quot;\n  input: &quot;clip_by_value_9/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_10/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_10/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_10/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_10/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_10&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_10/Minimum&quot;\n  input: &quot;clip_by_value_10/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_11/Minimum/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_11/Minimum&quot;\n  op: &quot;Minimum&quot;\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\n  input: &quot;clip_by_value_11/Minimum/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_11/y&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: -1.0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;clip_by_value_11&quot;\n  op: &quot;Maximum&quot;\n  input: &quot;clip_by_value_11/Minimum&quot;\n  input: &quot;clip_by_value_11/y&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/learning_rate&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_FLOAT\n        tensor_shape {\n        }\n        float_val: 0.009999999776482582\n      }\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden1/kernel&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden1/bias&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_1&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden2/kernel&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_2&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden2/bias&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_3&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden3/kernel&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_4&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden3/bias&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_5&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden4/kernel&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_6&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden4/bias&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_7&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden5/kernel&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_8&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;hidden5/bias&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_9&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;outputs/kernel&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_10&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\n  op: &quot;ApplyGradientDescent&quot;\n  input: &quot;outputs/bias&quot;\n  input: &quot;GradientDescent/learning_rate&quot;\n  input: &quot;clip_by_value_11&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;GradientDescent&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\n}\nnode {\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n        }\n        int_val: 1\n      }\n    }\n  }\n}\nnode {\n  name: &quot;eval/in_top_k/InTopKV2&quot;\n  op: &quot;InTopKV2&quot;\n  input: &quot;dnn/outputs/BiasAdd&quot;\n  input: &quot;y&quot;\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n}\nnode {\n  name: &quot;eval/Cast&quot;\n  op: &quot;Cast&quot;\n  input: &quot;eval/in_top_k/InTopKV2&quot;\n  attr {\n    key: &quot;DstT&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;SrcT&quot;\n    value {\n      type: DT_BOOL\n    }\n  }\n  attr {\n    key: &quot;Truncate&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;eval/Const&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 1\n          }\n        }\n        int_val: 0\n      }\n    }\n  }\n}\nnode {\n  name: &quot;eval/accuracy&quot;\n  op: &quot;Mean&quot;\n  input: &quot;eval/Cast&quot;\n  input: &quot;eval/Const&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;Tidx&quot;\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: &quot;keep_dims&quot;\n    value {\n      b: false\n    }\n  }\n}\nnode {\n  name: &quot;init&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^hidden1/bias/Assign&quot;\n  input: &quot;^hidden1/kernel/Assign&quot;\n  input: &quot;^hidden2/bias/Assign&quot;\n  input: &quot;^hidden2/kernel/Assign&quot;\n  input: &quot;^hidden3/bias/Assign&quot;\n  input: &quot;^hidden3/kernel/Assign&quot;\n  input: &quot;^hidden4/bias/Assign&quot;\n  input: &quot;^hidden4/kernel/Assign&quot;\n  input: &quot;^hidden5/bias/Assign&quot;\n  input: &quot;^hidden5/kernel/Assign&quot;\n  input: &quot;^outputs/bias/Assign&quot;\n  input: &quot;^outputs/kernel/Assign&quot;\n}\nnode {\n  name: &quot;save/Const&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_STRING\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_STRING\n        tensor_shape {\n        }\n        string_val: &quot;model&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/SaveV2/tensor_names&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_STRING\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_STRING\n        tensor_shape {\n          dim {\n            size: 12\n          }\n        }\n        string_val: &quot;hidden1/bias&quot;\n        string_val: &quot;hidden1/kernel&quot;\n        string_val: &quot;hidden2/bias&quot;\n        string_val: &quot;hidden2/kernel&quot;\n        string_val: &quot;hidden3/bias&quot;\n        string_val: &quot;hidden3/kernel&quot;\n        string_val: &quot;hidden4/bias&quot;\n        string_val: &quot;hidden4/kernel&quot;\n        string_val: &quot;hidden5/bias&quot;\n        string_val: &quot;hidden5/kernel&quot;\n        string_val: &quot;outputs/bias&quot;\n        string_val: &quot;outputs/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/SaveV2/shape_and_slices&quot;\n  op: &quot;Const&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_STRING\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_STRING\n        tensor_shape {\n          dim {\n            size: 12\n          }\n        }\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/SaveV2&quot;\n  op: &quot;SaveV2&quot;\n  input: &quot;save/Const&quot;\n  input: &quot;save/SaveV2/tensor_names&quot;\n  input: &quot;save/SaveV2/shape_and_slices&quot;\n  input: &quot;hidden1/bias&quot;\n  input: &quot;hidden1/kernel&quot;\n  input: &quot;hidden2/bias&quot;\n  input: &quot;hidden2/kernel&quot;\n  input: &quot;hidden3/bias&quot;\n  input: &quot;hidden3/kernel&quot;\n  input: &quot;hidden4/bias&quot;\n  input: &quot;hidden4/kernel&quot;\n  input: &quot;hidden5/bias&quot;\n  input: &quot;hidden5/kernel&quot;\n  input: &quot;outputs/bias&quot;\n  input: &quot;outputs/kernel&quot;\n  attr {\n    key: &quot;dtypes&quot;\n    value {\n      list {\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/control_dependency&quot;\n  op: &quot;Identity&quot;\n  input: &quot;save/Const&quot;\n  input: &quot;^save/SaveV2&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_STRING\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@save/Const&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/RestoreV2/tensor_names&quot;\n  op: &quot;Const&quot;\n  device: &quot;/device:CPU:0&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_STRING\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_STRING\n        tensor_shape {\n          dim {\n            size: 12\n          }\n        }\n        string_val: &quot;hidden1/bias&quot;\n        string_val: &quot;hidden1/kernel&quot;\n        string_val: &quot;hidden2/bias&quot;\n        string_val: &quot;hidden2/kernel&quot;\n        string_val: &quot;hidden3/bias&quot;\n        string_val: &quot;hidden3/kernel&quot;\n        string_val: &quot;hidden4/bias&quot;\n        string_val: &quot;hidden4/kernel&quot;\n        string_val: &quot;hidden5/bias&quot;\n        string_val: &quot;hidden5/kernel&quot;\n        string_val: &quot;outputs/bias&quot;\n        string_val: &quot;outputs/kernel&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\n  op: &quot;Const&quot;\n  device: &quot;/device:CPU:0&quot;\n  attr {\n    key: &quot;dtype&quot;\n    value {\n      type: DT_STRING\n    }\n  }\n  attr {\n    key: &quot;value&quot;\n    value {\n      tensor {\n        dtype: DT_STRING\n        tensor_shape {\n          dim {\n            size: 12\n          }\n        }\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n        string_val: &quot;&quot;\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/RestoreV2&quot;\n  op: &quot;RestoreV2&quot;\n  input: &quot;save/Const&quot;\n  input: &quot;save/RestoreV2/tensor_names&quot;\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\n  device: &quot;/device:CPU:0&quot;\n  attr {\n    key: &quot;dtypes&quot;\n    value {\n      list {\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n        type: DT_FLOAT\n      }\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden1/bias&quot;\n  input: &quot;save/RestoreV2&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_1&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden1/kernel&quot;\n  input: &quot;save/RestoreV2:1&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden1/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_2&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden2/bias&quot;\n  input: &quot;save/RestoreV2:2&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_3&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden2/kernel&quot;\n  input: &quot;save/RestoreV2:3&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden2/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_4&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden3/bias&quot;\n  input: &quot;save/RestoreV2:4&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_5&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden3/kernel&quot;\n  input: &quot;save/RestoreV2:5&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden3/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_6&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden4/bias&quot;\n  input: &quot;save/RestoreV2:6&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_7&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden4/kernel&quot;\n  input: &quot;save/RestoreV2:7&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden4/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_8&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden5/bias&quot;\n  input: &quot;save/RestoreV2:8&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_9&quot;\n  op: &quot;Assign&quot;\n  input: &quot;hidden5/kernel&quot;\n  input: &quot;save/RestoreV2:9&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@hidden5/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_10&quot;\n  op: &quot;Assign&quot;\n  input: &quot;outputs/bias&quot;\n  input: &quot;save/RestoreV2:10&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/bias&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/Assign_11&quot;\n  op: &quot;Assign&quot;\n  input: &quot;outputs/kernel&quot;\n  input: &quot;save/RestoreV2:11&quot;\n  attr {\n    key: &quot;T&quot;\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: &quot;_class&quot;\n    value {\n      list {\n        s: &quot;loc:@outputs/kernel&quot;\n      }\n    }\n  }\n  attr {\n    key: &quot;use_locking&quot;\n    value {\n      b: true\n    }\n  }\n  attr {\n    key: &quot;validate_shape&quot;\n    value {\n      b: true\n    }\n  }\n}\nnode {\n  name: &quot;save/restore_all&quot;\n  op: &quot;NoOp&quot;\n  input: &quot;^save/Assign&quot;\n  input: &quot;^save/Assign_1&quot;\n  input: &quot;^save/Assign_10&quot;\n  input: &quot;^save/Assign_11&quot;\n  input: &quot;^save/Assign_2&quot;\n  input: &quot;^save/Assign_3&quot;\n  input: &quot;^save/Assign_4&quot;\n  input: &quot;^save/Assign_5&quot;\n  input: &quot;^save/Assign_6&quot;\n  input: &quot;^save/Assign_7&quot;\n  input: &quot;^save/Assign_8&quot;\n  input: &quot;^save/Assign_9&quot;\n}\n';
          }
        </script>
        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>
        <div style=&quot;height:600px&quot;>
          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>
        </div>
    "></iframe>
    
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once you know which operations you need, you can get a handle on them using the graph's <code>get_operation_by_name()</code> or <code>get_tensor_by_name()</code> methods:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[51]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;X:0&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;y:0&quot;</span><span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;eval/accuracy:0&quot;</span><span class="p">)</span>

<span class="n">training_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operation_by_name</span><span class="p">(</span><span class="s2">&quot;GradientDescent&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[52]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">training_op</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s2">&quot;my_important_ops&quot;</span><span class="p">,</span> <span class="n">op</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This way people who reuse your model will be able to simply write:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[53]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">training_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s2">&quot;my_important_ops&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now you can start a session, restore the model's state and continue training on your data:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[54]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
    <span class="c1"># continue training the model...</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Actually, let's test this for real!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[55]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_new_model_final.ckpt&quot;</span><span class="p">)</span>    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
0 Validation accuracy: 0.9636
1 Validation accuracy: 0.9632
2 Validation accuracy: 0.9658
3 Validation accuracy: 0.9652
4 Validation accuracy: 0.9646
5 Validation accuracy: 0.965
6 Validation accuracy: 0.969
7 Validation accuracy: 0.9682
8 Validation accuracy: 0.9682
9 Validation accuracy: 0.9684
10 Validation accuracy: 0.9704
11 Validation accuracy: 0.971
12 Validation accuracy: 0.9668
13 Validation accuracy: 0.97
14 Validation accuracy: 0.9712
15 Validation accuracy: 0.9726
16 Validation accuracy: 0.9718
17 Validation accuracy: 0.971
18 Validation accuracy: 0.9712
19 Validation accuracy: 0.9712
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Alternatively, if you have access to the Python code that built the original graph, you can use it instead of <code>import_meta_graph()</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[56]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_hidden3</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_hidden4</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_hidden5</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">hidden3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_hidden3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden3&quot;</span><span class="p">)</span>
    <span class="n">hidden4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden3</span><span class="p">,</span> <span class="n">n_hidden4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden4&quot;</span><span class="p">)</span>
    <span class="n">hidden5</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden4</span><span class="p">,</span> <span class="n">n_hidden5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden5&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden5</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">grads_and_vars</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">capped_gvs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="o">-</span><span class="n">threshold</span><span class="p">,</span> <span class="n">threshold</span><span class="p">),</span> <span class="n">var</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">]</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">capped_gvs</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And continue training:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[57]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_new_model_final.ckpt&quot;</span><span class="p">)</span>    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
0 Validation accuracy: 0.9642
1 Validation accuracy: 0.9632
2 Validation accuracy: 0.9656
3 Validation accuracy: 0.9652
4 Validation accuracy: 0.9646
5 Validation accuracy: 0.9652
6 Validation accuracy: 0.9688
7 Validation accuracy: 0.9686
8 Validation accuracy: 0.9682
9 Validation accuracy: 0.9686
10 Validation accuracy: 0.9704
11 Validation accuracy: 0.9712
12 Validation accuracy: 0.967
13 Validation accuracy: 0.9698
14 Validation accuracy: 0.9708
15 Validation accuracy: 0.9724
16 Validation accuracy: 0.9718
17 Validation accuracy: 0.9712
18 Validation accuracy: 0.9708
19 Validation accuracy: 0.9712
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In general you will want to reuse only the lower layers. If you are using <code>import_meta_graph()</code> it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[58]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_hidden4</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># new layer</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># new layer</span>

<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="s2">&quot;./my_model_final.ckpt.meta&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;X:0&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;y:0&quot;</span><span class="p">)</span>

<span class="n">hidden3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;dnn/hidden3/Relu:0&quot;</span><span class="p">)</span>

<span class="n">new_hidden4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden3</span><span class="p">,</span> <span class="n">n_hidden4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;new_hidden4&quot;</span><span class="p">)</span>
<span class="n">new_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">new_hidden4</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;new_outputs&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;new_loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">new_logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;new_eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">new_logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;new_train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">new_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can train this new model:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[59]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">new_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_new_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
0 Validation accuracy: 0.9126
1 Validation accuracy: 0.9374
2 Validation accuracy: 0.946
3 Validation accuracy: 0.9498
4 Validation accuracy: 0.953
5 Validation accuracy: 0.9528
6 Validation accuracy: 0.9564
7 Validation accuracy: 0.96
8 Validation accuracy: 0.9616
9 Validation accuracy: 0.9612
10 Validation accuracy: 0.9634
11 Validation accuracy: 0.9626
12 Validation accuracy: 0.9648
13 Validation accuracy: 0.9656
14 Validation accuracy: 0.9664
15 Validation accuracy: 0.967
16 Validation accuracy: 0.968
17 Validation accuracy: 0.9678
18 Validation accuracy: 0.9684
19 Validation accuracy: 0.9678
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[60]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1"># reused</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden3</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden4</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># new!</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># new!</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>       <span class="c1"># reused</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span> <span class="c1"># reused</span>
    <span class="n">hidden3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_hidden3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden3&quot;</span><span class="p">)</span> <span class="c1"># reused</span>
    <span class="n">hidden4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden3</span><span class="p">,</span> <span class="n">n_hidden4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden4&quot;</span><span class="p">)</span> <span class="c1"># new!</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden4</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>                         <span class="c1"># new!</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, you must create one <code>Saver</code> to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another <code>Saver</code> to save the new model, once it is trained:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[61]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reuse_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
                               <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden[123]&quot;</span><span class="p">)</span> <span class="c1"># regular expression</span>
<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">reuse_vars</span><span class="p">)</span> <span class="c1"># to restore layers 1-3</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>                                            <span class="c1"># not shown in the book</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span> <span class="c1"># not shown</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>        <span class="c1"># not shown</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>     <span class="c1"># not shown</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>                   <span class="c1"># not shown</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_new_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
0 Validation accuracy: 0.9024
1 Validation accuracy: 0.9332
2 Validation accuracy: 0.943
3 Validation accuracy: 0.947
4 Validation accuracy: 0.9516
5 Validation accuracy: 0.9532
6 Validation accuracy: 0.9558
7 Validation accuracy: 0.9592
8 Validation accuracy: 0.9586
9 Validation accuracy: 0.9608
10 Validation accuracy: 0.9626
11 Validation accuracy: 0.962
12 Validation accuracy: 0.964
13 Validation accuracy: 0.9662
14 Validation accuracy: 0.966
15 Validation accuracy: 0.9662
16 Validation accuracy: 0.9672
17 Validation accuracy: 0.9674
18 Validation accuracy: 0.9682
19 Validation accuracy: 0.9678
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reusing-Models-from-Other-Frameworks">Reusing Models from Other Frameworks<a class="anchor-link" href="#Reusing-Models-from-Other-Frameworks">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a <code>feed_dict</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[62]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[63]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">original_w</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]]</span> <span class="c1"># Load the weights from the other framework</span>
<span class="n">original_b</span> <span class="o">=</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]</span>                 <span class="c1"># Load the biases from the other framework</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
<span class="c1"># [...] Build the rest of the model</span>

<span class="c1"># Get a handle on the assignment nodes for the hidden1 variables</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
<span class="n">assign_kernel</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_operation_by_name</span><span class="p">(</span><span class="s2">&quot;hidden1/kernel/Assign&quot;</span><span class="p">)</span>
<span class="n">assign_bias</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_operation_by_name</span><span class="p">(</span><span class="s2">&quot;hidden1/bias/Assign&quot;</span><span class="p">)</span>
<span class="n">init_kernel</span> <span class="o">=</span> <span class="n">assign_kernel</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">init_bias</span> <span class="o">=</span> <span class="n">assign_bias</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">init_kernel</span><span class="p">:</span> <span class="n">original_w</span><span class="p">,</span> <span class="n">init_bias</span><span class="p">:</span> <span class="n">original_b</span><span class="p">})</span>
    <span class="c1"># [...] Train the model on your new task</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">hidden1</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="p">[[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">]]}))</span>  <span class="c1"># not shown in the book</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[[ 61.  83. 105.]]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: the weights variable created by the <code>tf.layers.dense()</code> function is called <code>"kernel"</code> (instead of <code>"weights"</code> when using the <code>tf.contrib.layers.fully_connected()</code>, as in the book), and the biases variable is called <code>bias</code> instead of <code>biases</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another approach (initially used in the book) would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but you may find this more explicit:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[64]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">original_w</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]]</span> <span class="c1"># Load the weights from the other framework</span>
<span class="n">original_b</span> <span class="o">=</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]</span>                 <span class="c1"># Load the biases from the other framework</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
<span class="c1"># [...] Build the rest of the model</span>

<span class="c1"># Get a handle on the variables of layer hidden1</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">default_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>  <span class="c1"># root scope</span>
    <span class="n">hidden1_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;hidden1/kernel&quot;</span><span class="p">)</span>
    <span class="n">hidden1_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;hidden1/bias&quot;</span><span class="p">)</span>

<span class="c1"># Create dedicated placeholders and assignment nodes</span>
<span class="n">original_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">))</span>
<span class="n">original_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n_hidden1</span><span class="p">)</span>
<span class="n">assign_hidden1_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">hidden1_weights</span><span class="p">,</span> <span class="n">original_weights</span><span class="p">)</span>
<span class="n">assign_hidden1_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">hidden1_biases</span><span class="p">,</span> <span class="n">original_biases</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">assign_hidden1_weights</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">original_weights</span><span class="p">:</span> <span class="n">original_w</span><span class="p">})</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">assign_hidden1_biases</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">original_biases</span><span class="p">:</span> <span class="n">original_b</span><span class="p">})</span>
    <span class="c1"># [...] Train the model on your new task</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">hidden1</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="p">[[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">]]}))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[[ 61.  83. 105.]]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that we could also get a handle on the variables using <code>get_collection()</code> and specifying the <code>scope</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[65]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[65]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[&lt;tf.Variable &#39;hidden1/kernel:0&#39; shape=(2, 3) dtype=float32_ref&gt;,
 &lt;tf.Variable &#39;hidden1/bias:0&#39; shape=(3,) dtype=float32_ref&gt;]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Or we could use the graph's <code>get_tensor_by_name()</code> method:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[66]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;hidden1/kernel:0&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[66]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor &#39;hidden1/kernel:0&#39; shape=(2, 3) dtype=float32_ref&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[67]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;hidden1/bias:0&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[67]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;tf.Tensor &#39;hidden1/bias:0&#39; shape=(3,) dtype=float32_ref&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Freezing-the-Lower-Layers">Freezing the Lower Layers<a class="anchor-link" href="#Freezing-the-Lower-Layers">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[68]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1"># reused</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden3</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden4</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># new!</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># new!</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>       <span class="c1"># reused</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span> <span class="c1"># reused</span>
    <span class="n">hidden3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_hidden3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden3&quot;</span><span class="p">)</span> <span class="c1"># reused</span>
    <span class="n">hidden4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden3</span><span class="p">,</span> <span class="n">n_hidden4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden4&quot;</span><span class="p">)</span> <span class="c1"># new!</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden4</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>                         <span class="c1"># new!</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[69]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>                                         <span class="c1"># not shown in the book</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>     <span class="c1"># not shown</span>
    <span class="n">train_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
                                   <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden[34]|outputs&quot;</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">train_vars</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[70]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">new_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[71]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reuse_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
                               <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden[123]&quot;</span><span class="p">)</span> <span class="c1"># regular expression</span>
<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">reuse_vars</span><span class="p">)</span> <span class="c1"># to restore layers 1-3</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_new_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
0 Validation accuracy: 0.8964
1 Validation accuracy: 0.9298
2 Validation accuracy: 0.94
3 Validation accuracy: 0.9442
4 Validation accuracy: 0.948
5 Validation accuracy: 0.951
6 Validation accuracy: 0.9508
7 Validation accuracy: 0.9538
8 Validation accuracy: 0.9554
9 Validation accuracy: 0.957
10 Validation accuracy: 0.9562
11 Validation accuracy: 0.9566
12 Validation accuracy: 0.9572
13 Validation accuracy: 0.9578
14 Validation accuracy: 0.959
15 Validation accuracy: 0.9576
16 Validation accuracy: 0.9574
17 Validation accuracy: 0.9602
18 Validation accuracy: 0.9592
19 Validation accuracy: 0.9602
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[72]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1"># reused</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden3</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden4</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># new!</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># new!</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[73]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span> <span class="c1"># reused frozen</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span> <span class="c1"># reused frozen</span>
    <span class="n">hidden2_stop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">hidden2</span><span class="p">)</span>
    <span class="n">hidden3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2_stop</span><span class="p">,</span> <span class="n">n_hidden3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden3&quot;</span><span class="p">)</span> <span class="c1"># reused, not frozen</span>
    <span class="n">hidden4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden3</span><span class="p">,</span> <span class="n">n_hidden4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden4&quot;</span><span class="p">)</span> <span class="c1"># new!</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden4</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span> <span class="c1"># new!</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[74]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The training code is exactly the same as earlier:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[75]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reuse_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
                               <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden[123]&quot;</span><span class="p">)</span> <span class="c1"># regular expression</span>
<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">reuse_vars</span><span class="p">)</span> <span class="c1"># to restore layers 1-3</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_new_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
0 Validation accuracy: 0.902
1 Validation accuracy: 0.9302
2 Validation accuracy: 0.9438
3 Validation accuracy: 0.9478
4 Validation accuracy: 0.9514
5 Validation accuracy: 0.9522
6 Validation accuracy: 0.9524
7 Validation accuracy: 0.9556
8 Validation accuracy: 0.9556
9 Validation accuracy: 0.9558
10 Validation accuracy: 0.957
11 Validation accuracy: 0.9552
12 Validation accuracy: 0.9572
13 Validation accuracy: 0.9582
14 Validation accuracy: 0.9582
15 Validation accuracy: 0.957
16 Validation accuracy: 0.9566
17 Validation accuracy: 0.9578
18 Validation accuracy: 0.9594
19 Validation accuracy: 0.958
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Caching-the-Frozen-Layers">Caching the Frozen Layers<a class="anchor-link" href="#Caching-the-Frozen-Layers">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[76]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1"># reused</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden3</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># reused</span>
<span class="n">n_hidden4</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># new!</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># new!</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span> <span class="c1"># reused frozen</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span> <span class="c1"># reused frozen &amp; cached</span>
    <span class="n">hidden2_stop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">hidden2</span><span class="p">)</span>
    <span class="n">hidden3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2_stop</span><span class="p">,</span> <span class="n">n_hidden3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden3&quot;</span><span class="p">)</span> <span class="c1"># reused, not frozen</span>
    <span class="n">hidden4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden3</span><span class="p">,</span> <span class="n">n_hidden4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden4&quot;</span><span class="p">)</span> <span class="c1"># new!</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden4</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span> <span class="c1"># new!</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[77]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reuse_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
                               <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden[123]&quot;</span><span class="p">)</span> <span class="c1"># regular expression</span>
<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">reuse_vars</span><span class="p">)</span> <span class="c1"># to restore layers 1-3</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[78]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
    
    <span class="n">h2_cache</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_train</span><span class="p">})</span>
    <span class="n">h2_cache_valid</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">})</span> <span class="c1"># not shown in the book</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">shuffled_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
        <span class="n">hidden2_batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">h2_cache</span><span class="p">[</span><span class="n">shuffled_idx</span><span class="p">],</span> <span class="n">n_batches</span><span class="p">)</span>
        <span class="n">y_batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">shuffled_idx</span><span class="p">],</span> <span class="n">n_batches</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">hidden2_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">hidden2_batches</span><span class="p">,</span> <span class="n">y_batches</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">hidden2</span><span class="p">:</span><span class="n">hidden2_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">y_batch</span><span class="p">})</span>

        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">hidden2</span><span class="p">:</span> <span class="n">h2_cache_valid</span><span class="p">,</span> <span class="c1"># not shown</span>
                                                <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>             <span class="c1"># not shown</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>               <span class="c1"># not shown</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_new_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt
0 Validation accuracy: 0.902
1 Validation accuracy: 0.9302
2 Validation accuracy: 0.9438
3 Validation accuracy: 0.9478
4 Validation accuracy: 0.9514
5 Validation accuracy: 0.9522
6 Validation accuracy: 0.9524
7 Validation accuracy: 0.9556
8 Validation accuracy: 0.9556
9 Validation accuracy: 0.9558
10 Validation accuracy: 0.957
11 Validation accuracy: 0.9552
12 Validation accuracy: 0.9572
13 Validation accuracy: 0.9582
14 Validation accuracy: 0.9582
15 Validation accuracy: 0.957
16 Validation accuracy: 0.9566
17 Validation accuracy: 0.9578
18 Validation accuracy: 0.9594
19 Validation accuracy: 0.958
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Faster-Optimizers">Faster Optimizers<a class="anchor-link" href="#Faster-Optimizers">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Momentum-optimization">Momentum optimization<a class="anchor-link" href="#Momentum-optimization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[79]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                       <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Nesterov-Accelerated-Gradient">Nesterov Accelerated Gradient<a class="anchor-link" href="#Nesterov-Accelerated-Gradient">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[80]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                       <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="AdaGrad">AdaGrad<a class="anchor-link" href="#AdaGrad">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[81]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="RMSProp">RMSProp<a class="anchor-link" href="#RMSProp">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[82]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                      <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Adam-Optimization">Adam Optimization<a class="anchor-link" href="#Adam-Optimization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[83]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-Rate-Scheduling">Learning Rate Scheduling<a class="anchor-link" href="#Learning-Rate-Scheduling">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[84]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[85]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>       <span class="c1"># not shown in the book</span>
    <span class="n">initial_learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">decay_steps</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">decay_rate</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span>
    <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;global_step&quot;</span><span class="p">)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">(</span><span class="n">initial_learning_rate</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span>
                                               <span class="n">decay_steps</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[86]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[87]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.959
1 Validation accuracy: 0.9688
2 Validation accuracy: 0.9726
3 Validation accuracy: 0.9804
4 Validation accuracy: 0.982
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Avoiding-Overfitting-Through-Regularization">Avoiding Overfitting Through Regularization<a class="anchor-link" href="#Avoiding-Overfitting-Through-Regularization">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="$\ell_1$-and-$\ell_2$-regularization">$\ell_1$ and $\ell_2$ regularization<a class="anchor-link" href="#$\ell_1$-and-$\ell_2$-regularization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's implement $\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[88]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\ell_1$ loss (i.e., the absolute values of the weights):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[89]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;hidden1/kernel:0&quot;</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;outputs/kernel:0&quot;</span><span class="p">)</span>

<span class="n">scale</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1"># l1 regularization hyperparameter</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                                              <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">base_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;avg_xentropy&quot;</span><span class="p">)</span>
    <span class="n">reg_losses</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">W1</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">W2</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">base_loss</span><span class="p">,</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">reg_losses</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The rest is just as usual:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[90]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[91]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.831
1 Validation accuracy: 0.871
2 Validation accuracy: 0.8838
3 Validation accuracy: 0.8934
4 Validation accuracy: 0.8966
5 Validation accuracy: 0.8988
6 Validation accuracy: 0.9016
7 Validation accuracy: 0.9044
8 Validation accuracy: 0.9058
9 Validation accuracy: 0.906
10 Validation accuracy: 0.9068
11 Validation accuracy: 0.9054
12 Validation accuracy: 0.907
13 Validation accuracy: 0.9084
14 Validation accuracy: 0.9088
15 Validation accuracy: 0.9064
16 Validation accuracy: 0.9066
17 Validation accuracy: 0.9066
18 Validation accuracy: 0.9066
19 Validation accuracy: 0.9052
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Alternatively, we can pass a regularization function to the <code>tf.layers.dense()</code> function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[92]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we will use Python's <code>partial()</code> function to avoid repeating the same arguments over and over again. Note that we set the <code>kernel_regularizer</code> argument:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[93]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">scale</span> <span class="o">=</span> <span class="mf">0.001</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[94]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_dense_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">l1_regularizer</span><span class="p">(</span><span class="n">scale</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">my_dense_layer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">my_dense_layer</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">my_dense_layer</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we must add the regularization losses to the base loss:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[95]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>                                     <span class="c1"># not shown in the book</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>  <span class="c1"># not shown</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>                                <span class="c1"># not shown</span>
    <span class="n">base_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;avg_xentropy&quot;</span><span class="p">)</span>   <span class="c1"># not shown</span>
    <span class="n">reg_losses</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">REGULARIZATION_LOSSES</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">([</span><span class="n">base_loss</span><span class="p">]</span> <span class="o">+</span> <span class="n">reg_losses</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And the rest is the same as usual:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[96]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[97]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.8274
1 Validation accuracy: 0.8766
2 Validation accuracy: 0.8952
3 Validation accuracy: 0.9016
4 Validation accuracy: 0.908
5 Validation accuracy: 0.9096
6 Validation accuracy: 0.9126
7 Validation accuracy: 0.9154
8 Validation accuracy: 0.9178
9 Validation accuracy: 0.919
10 Validation accuracy: 0.92
11 Validation accuracy: 0.9224
12 Validation accuracy: 0.9212
13 Validation accuracy: 0.9228
14 Validation accuracy: 0.9224
15 Validation accuracy: 0.9216
16 Validation accuracy: 0.9218
17 Validation accuracy: 0.9228
18 Validation accuracy: 0.9216
19 Validation accuracy: 0.9214
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dropout">Dropout<a class="anchor-link" href="#Dropout">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: the book uses <code>tf.contrib.layers.dropout()</code> rather than <code>tf.layers.dropout()</code> (which did not exist when this chapter was written). It is now preferable to use <code>tf.layers.dropout()</code>, because anything in the contrib module may change or be deleted without notice. The <code>tf.layers.dropout()</code> function is almost identical to the <code>tf.contrib.layers.dropout()</code> function, except for a few minor differences. Most importantly:</p>
<ul>
<li>you must specify the dropout rate (<code>rate</code>) rather than the keep probability (<code>keep_prob</code>), where <code>rate</code> is simply equal to <code>1 - keep_prob</code>,</li>
<li>the <code>is_training</code> parameter is renamed to <code>training</code>.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[98]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[99]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">training</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder_with_default</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>

<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># == 1 - keep_prob</span>
<span class="n">X_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X_drop</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden1_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1_drop</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">hidden2_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2_drop</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[100]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>    

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[101]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
        <span class="n">accuracy_val</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_val</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.9264
1 Validation accuracy: 0.9446
2 Validation accuracy: 0.9488
3 Validation accuracy: 0.9556
4 Validation accuracy: 0.9612
5 Validation accuracy: 0.9598
6 Validation accuracy: 0.9616
7 Validation accuracy: 0.9674
8 Validation accuracy: 0.967
9 Validation accuracy: 0.9706
10 Validation accuracy: 0.9674
11 Validation accuracy: 0.9678
12 Validation accuracy: 0.9698
13 Validation accuracy: 0.97
14 Validation accuracy: 0.971
15 Validation accuracy: 0.9702
16 Validation accuracy: 0.9718
17 Validation accuracy: 0.9716
18 Validation accuracy: 0.9734
19 Validation accuracy: 0.972
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Max-norm">Max norm<a class="anchor-link" href="#Max-norm">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[102]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>    

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the <code>clip_by_norm()</code> function. Then we create an assignment operation to assign the clipped weights to the weights variable:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[103]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threshold</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;hidden1/kernel:0&quot;</span><span class="p">)</span>
<span class="n">clipped_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clip_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clipped_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can do this as well for the second hidden layer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[104]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">weights2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;hidden2/kernel:0&quot;</span><span class="p">)</span>
<span class="n">clipped_weights2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">weights2</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clip_weights2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">weights2</span><span class="p">,</span> <span class="n">clipped_weights2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's add an initializer and a saver:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[105]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now we can train the model. It's pretty much as usual, except that right after running the <code>training_op</code>, we run the <code>clip_weights</code> and <code>clip_weights2</code> operations:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[106]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[107]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>                                              <span class="c1"># not shown in the book</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>                                                          <span class="c1"># not shown</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>                                       <span class="c1"># not shown</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span> <span class="c1"># not shown</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
            <span class="n">clip_weights</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="n">clip_weights2</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>                                        <span class="c1"># not shown</span>
        <span class="n">acc_valid</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>   <span class="c1"># not shown</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_valid</span><span class="p">)</span>                 <span class="c1"># not shown</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>               <span class="c1"># not shown</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.9568
1 Validation accuracy: 0.9696
2 Validation accuracy: 0.972
3 Validation accuracy: 0.9768
4 Validation accuracy: 0.9784
5 Validation accuracy: 0.9786
6 Validation accuracy: 0.9816
7 Validation accuracy: 0.9808
8 Validation accuracy: 0.981
9 Validation accuracy: 0.983
10 Validation accuracy: 0.9822
11 Validation accuracy: 0.9854
12 Validation accuracy: 0.9822
13 Validation accuracy: 0.9842
14 Validation accuracy: 0.984
15 Validation accuracy: 0.9852
16 Validation accuracy: 0.984
17 Validation accuracy: 0.9844
18 Validation accuracy: 0.9844
19 Validation accuracy: 0.9844
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a <code>max_norm_regularizer()</code> function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[108]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">max_norm_regularizer</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;max_norm&quot;</span><span class="p">,</span>
                         <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;max_norm&quot;</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">max_norm</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
        <span class="n">clipped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">)</span>
        <span class="n">clip_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clipped</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">clip_weights</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span> <span class="c1"># there is no regularization loss term</span>
    <span class="k">return</span> <span class="n">max_norm</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the <code>kernel_regularizer</code> argument:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[109]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>
<span class="n">n_hidden1</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">n_hidden2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[110]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_norm_reg</span> <span class="o">=</span> <span class="n">max_norm_regularizer</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">max_norm_reg</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                              <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">max_norm_reg</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[111]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
    <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>    

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Training is as usual, except you must run the weights clipping operations after each training operation:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[112]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[113]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">clip_all_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s2">&quot;max_norm&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">shuffle_batch</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">clip_all_weights</span><span class="p">)</span>
        <span class="n">acc_valid</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span> <span class="c1"># not shown</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Validation accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_valid</span><span class="p">)</span>               <span class="c1"># not shown</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>             <span class="c1"># not shown</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Validation accuracy: 0.9556
1 Validation accuracy: 0.9698
2 Validation accuracy: 0.9726
3 Validation accuracy: 0.9744
4 Validation accuracy: 0.9762
5 Validation accuracy: 0.9772
6 Validation accuracy: 0.979
7 Validation accuracy: 0.9816
8 Validation accuracy: 0.9814
9 Validation accuracy: 0.9812
10 Validation accuracy: 0.9818
11 Validation accuracy: 0.9816
12 Validation accuracy: 0.9802
13 Validation accuracy: 0.9822
14 Validation accuracy: 0.982
15 Validation accuracy: 0.9812
16 Validation accuracy: 0.9824
17 Validation accuracy: 0.9836
18 Validation accuracy: 0.9824
19 Validation accuracy: 0.9826
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Exercise-solutions">Exercise solutions<a class="anchor-link" href="#Exercise-solutions">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-to-7.">1. to 7.<a class="anchor-link" href="#1.-to-7.">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>See appendix A.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="8.-Deep-Learning">8. Deep Learning<a class="anchor-link" href="#8.-Deep-Learning">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="8.1.">8.1.<a class="anchor-link" href="#8.1.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will need similar DNNs in the next exercises, so let's create a function to build this DNN:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[114]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">he_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">variance_scaling_initializer</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">dnn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_hidden_layers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_neurons</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;dnn&quot;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden_layers</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                                     <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
                                     <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">inputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[115]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span> <span class="c1"># MNIST</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="n">dnn_outputs</span> <span class="o">=</span> <span class="n">dnn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">dnn_outputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;logits&quot;</span><span class="p">)</span>
<span class="n">Y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Y_proba&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="8.2.">8.2.<a class="anchor-link" href="#8.2.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's complete the graph with the cost function, the training op, and all the other usual components:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[116]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;training_op&quot;</span><span class="p">)</span>

<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's create the training set, validation and test set (we need the validation set to implement early stopping):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[117]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train1</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y_train1</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">X_valid1</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">[</span><span class="n">y_valid</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y_valid1</span> <span class="o">=</span> <span class="n">y_valid</span><span class="p">[</span><span class="n">y_valid</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">X_test1</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">y_test</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y_test1</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">y_test</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[118]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">max_checks_without_progress</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train1</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train1</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_train1</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train1</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid1</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid1</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_0_to_4.ckpt&quot;</span><span class="p">)</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
            <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">checks_without_progress</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">checks_without_progress</span> <span class="o">&gt;</span> <span class="n">max_checks_without_progress</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping!&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Validation loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Best loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_0_to_4.ckpt&quot;</span><span class="p">)</span>
    <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test1</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test1</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final test accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc_test</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.116407	Best loss: 0.116407	Accuracy: 97.58%
1	Validation loss: 0.180534	Best loss: 0.116407	Accuracy: 97.11%
2	Validation loss: 0.227535	Best loss: 0.116407	Accuracy: 93.86%
3	Validation loss: 0.107346	Best loss: 0.107346	Accuracy: 97.54%
4	Validation loss: 0.302668	Best loss: 0.107346	Accuracy: 95.35%
5	Validation loss: 1.631054	Best loss: 0.107346	Accuracy: 22.01%
6	Validation loss: 1.635262	Best loss: 0.107346	Accuracy: 18.73%
7	Validation loss: 1.671200	Best loss: 0.107346	Accuracy: 22.01%
8	Validation loss: 1.695277	Best loss: 0.107346	Accuracy: 19.27%
9	Validation loss: 1.744607	Best loss: 0.107346	Accuracy: 20.91%
10	Validation loss: 1.629857	Best loss: 0.107346	Accuracy: 22.01%
11	Validation loss: 1.810803	Best loss: 0.107346	Accuracy: 22.01%
12	Validation loss: 1.675703	Best loss: 0.107346	Accuracy: 18.73%
13	Validation loss: 1.633233	Best loss: 0.107346	Accuracy: 20.91%
14	Validation loss: 1.652905	Best loss: 0.107346	Accuracy: 20.91%
15	Validation loss: 1.635937	Best loss: 0.107346	Accuracy: 20.91%
16	Validation loss: 1.718919	Best loss: 0.107346	Accuracy: 19.08%
17	Validation loss: 1.682458	Best loss: 0.107346	Accuracy: 19.27%
18	Validation loss: 1.675366	Best loss: 0.107346	Accuracy: 18.73%
19	Validation loss: 1.645800	Best loss: 0.107346	Accuracy: 19.08%
20	Validation loss: 1.722334	Best loss: 0.107346	Accuracy: 22.01%
21	Validation loss: 1.656418	Best loss: 0.107346	Accuracy: 22.01%
22	Validation loss: 1.643529	Best loss: 0.107346	Accuracy: 18.73%
23	Validation loss: 1.644233	Best loss: 0.107346	Accuracy: 19.27%
Early stopping!
INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt
Final test accuracy: 97.26%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This test accuracy is not too bad, but let's see if we can do better by tuning the hyperparameters.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="8.3.">8.3.<a class="anchor-link" href="#8.3.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: Tune the hyperparameters using cross-validation and see what precision you can achieve.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's create a <code>DNNClassifier</code> class, compatible with Scikit-Learn's <code>RandomizedSearchCV</code> class, to perform hyperparameter tuning. Here are the key points of this implementation:</p>
<ul>
<li>the <code>__init__()</code> method (constructor) does nothing more than create instance variables for each of the hyperparameters.</li>
<li>the <code>fit()</code> method creates the graph, starts a session and trains the model:<ul>
<li>it calls the <code>_build_graph()</code> method to build the graph (much lile the graph we defined earlier). Once this method is done creating the graph, it saves all the important operations as instance variables for easy access by other methods.</li>
<li>the <code>_dnn()</code> method builds the hidden layers, just like the <code>dnn()</code> function above, but also with support for batch normalization and dropout (for the next exercises).</li>
<li>if the <code>fit()</code> method is given a validation set (<code>X_valid</code> and <code>y_valid</code>), then it implements early stopping. This implementation does not save the best model to disk, but rather to memory: it uses the <code>_get_model_params()</code> method to get all the graph's variables and their values, and the <code>_restore_model_params()</code> method to restore the variable values (of the best model found). This trick helps speed up training.</li>
<li>After the <code>fit()</code> method has finished training the model, it keeps the session open so that predictions can be made quickly, without having to save a model to disk and restore it for every prediction. You can close the session by calling the <code>close_session()</code> method.</li>
</ul>
</li>
<li>the <code>predict_proba()</code> method uses the trained model to predict the class probabilities.</li>
<li>the <code>predict()</code> method calls <code>predict_proba()</code> and returns the class with the highest probability, for each instance.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[119]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="k">import</span> <span class="n">NotFittedError</span>

<span class="k">class</span> <span class="nc">DNNClassifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_hidden_layers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_neurons</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">optimizer_class</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">,</span>
                 <span class="n">batch_norm_momentum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the DNNClassifier by simply storing all the hyperparameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden_layers</span> <span class="o">=</span> <span class="n">n_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_neurons</span> <span class="o">=</span> <span class="n">n_neurons</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_class</span> <span class="o">=</span> <span class="n">optimizer_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span> <span class="o">=</span> <span class="n">batch_norm_momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_dnn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build the hidden layers, with support for batch normalization and dropout.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_hidden_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_training</span><span class="p">)</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_neurons</span><span class="p">,</span>
                                     <span class="n">kernel_initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span>
                                     <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span><span class="p">,</span>
                                                       <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_training</span><span class="p">)</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden</span><span class="si">%d</span><span class="s2">_out&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">_build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build the same model as earlier&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_rate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder_with_default</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">dnn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dnn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">dnn_outputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;logits&quot;</span><span class="p">)</span>
        <span class="n">Y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Y_proba&quot;</span><span class="p">)</span>

        <span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                                                  <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_class</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

        <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
        <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>

        <span class="c1"># Make the important operations available easily through instance variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Y_proba</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span> <span class="o">=</span> <span class="n">Y_proba</span><span class="p">,</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_accuracy</span> <span class="o">=</span> <span class="n">training_op</span><span class="p">,</span> <span class="n">accuracy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_saver</span> <span class="o">=</span> <span class="n">init</span><span class="p">,</span> <span class="n">saver</span>

    <span class="k">def</span> <span class="nf">close_session</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get all variable values (used for early stopping, faster than saving to disk)&quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="n">gvars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">gvar</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">gvar</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gvars</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">gvars</span><span class="p">))}</span>

    <span class="k">def</span> <span class="nf">_restore_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_params</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set all variables to the given values (for early stopping, faster than loading from disk)&quot;&quot;&quot;</span>
        <span class="n">gvar_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model_params</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">assign_ops</span> <span class="o">=</span> <span class="p">{</span><span class="n">gvar_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">get_operation_by_name</span><span class="p">(</span><span class="n">gvar_name</span> <span class="o">+</span> <span class="s2">&quot;/Assign&quot;</span><span class="p">)</span>
                      <span class="k">for</span> <span class="n">gvar_name</span> <span class="ow">in</span> <span class="n">gvar_names</span><span class="p">}</span>
        <span class="n">init_values</span> <span class="o">=</span> <span class="p">{</span><span class="n">gvar_name</span><span class="p">:</span> <span class="n">assign_op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">gvar_name</span><span class="p">,</span> <span class="n">assign_op</span> <span class="ow">in</span> <span class="n">assign_ops</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">init_values</span><span class="p">[</span><span class="n">gvar_name</span><span class="p">]:</span> <span class="n">model_params</span><span class="p">[</span><span class="n">gvar_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">gvar_name</span> <span class="ow">in</span> <span class="n">gvar_names</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">assign_ops</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">close_session</span><span class="p">()</span>

        <span class="c1"># infer n_inputs and n_outputs from the training set.</span>
        <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">n_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
        
        <span class="c1"># Translate the labels vector to a vector of sorted class indices, containing</span>
        <span class="c1"># integers from 0 to n_outputs - 1.</span>
        <span class="c1"># For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class</span>
        <span class="c1"># labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector</span>
        <span class="c1"># will be translated to [3, 3, 4, 0, 2, 1, 1, 1]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_to_index_</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">index</span>
                                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)}</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">class_to_index_</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
                      <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">y</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_graph</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span>
            <span class="c1"># extra ops for batch normalization</span>
            <span class="n">extra_update_ops</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">UPDATE_OPS</span><span class="p">)</span>

        <span class="c1"># needed in case of early stopping</span>
        <span class="n">max_checks_without_progress</span> <span class="o">=</span> <span class="mi">20</span>
        <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>
        <span class="n">best_params</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="c1"># Now train the model!</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="p">)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
                <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
                    <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
                    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">}</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">feed_dict</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_training</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">extra_update_ops</span><span class="p">:</span>
                        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">extra_update_ops</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">X_valid</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y_valid</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">loss_val</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_accuracy</span><span class="p">],</span>
                                                 <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_X</span><span class="p">:</span> <span class="n">X_valid</span><span class="p">,</span>
                                                            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">:</span> <span class="n">y_valid</span><span class="p">})</span>
                    <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                        <span class="n">best_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_model_params</span><span class="p">()</span>
                        <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
                        <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">checks_without_progress</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Validation loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Best loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
                    <span class="k">if</span> <span class="n">checks_without_progress</span> <span class="o">&gt;</span> <span class="n">max_checks_without_progress</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping!&quot;</span><span class="p">)</span>
                        <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">loss_train</span><span class="p">,</span> <span class="n">acc_train</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_accuracy</span><span class="p">],</span>
                                                     <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span>
                                                                <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Last training batch loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_train</span><span class="p">,</span> <span class="n">acc_train</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
            <span class="c1"># If we used early stopping then rollback to the best model found</span>
            <span class="k">if</span> <span class="n">best_params</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_restore_model_params</span><span class="p">(</span><span class="n">best_params</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">NotFittedError</span><span class="p">(</span><span class="s2">&quot;This </span><span class="si">%s</span><span class="s2"> instance is not fitted yet&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Y_proba</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_X</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">class_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">class_index</span><span class="p">]]</span>
                         <span class="k">for</span> <span class="n">class_index</span> <span class="ow">in</span> <span class="n">class_indices</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_session</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[120]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn_clf</span> <span class="o">=</span> <span class="n">DNNClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dnn_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid1</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.116407	Best loss: 0.116407	Accuracy: 97.58%
1	Validation loss: 0.180534	Best loss: 0.116407	Accuracy: 97.11%
2	Validation loss: 0.227535	Best loss: 0.116407	Accuracy: 93.86%
3	Validation loss: 0.107346	Best loss: 0.107346	Accuracy: 97.54%
4	Validation loss: 0.302668	Best loss: 0.107346	Accuracy: 95.35%
5	Validation loss: 1.631054	Best loss: 0.107346	Accuracy: 22.01%
6	Validation loss: 1.635262	Best loss: 0.107346	Accuracy: 18.73%
7	Validation loss: 1.671200	Best loss: 0.107346	Accuracy: 22.01%
8	Validation loss: 1.695277	Best loss: 0.107346	Accuracy: 19.27%
9	Validation loss: 1.744607	Best loss: 0.107346	Accuracy: 20.91%
10	Validation loss: 1.629857	Best loss: 0.107346	Accuracy: 22.01%
11	Validation loss: 1.810803	Best loss: 0.107346	Accuracy: 22.01%
12	Validation loss: 1.675703	Best loss: 0.107346	Accuracy: 18.73%
13	Validation loss: 1.633233	Best loss: 0.107346	Accuracy: 20.91%
14	Validation loss: 1.652905	Best loss: 0.107346	Accuracy: 20.91%
15	Validation loss: 1.635937	Best loss: 0.107346	Accuracy: 20.91%
16	Validation loss: 1.718919	Best loss: 0.107346	Accuracy: 19.08%
17	Validation loss: 1.682458	Best loss: 0.107346	Accuracy: 19.27%
18	Validation loss: 1.675366	Best loss: 0.107346	Accuracy: 18.73%
19	Validation loss: 1.645800	Best loss: 0.107346	Accuracy: 19.08%
20	Validation loss: 1.722334	Best loss: 0.107346	Accuracy: 22.01%
21	Validation loss: 1.656418	Best loss: 0.107346	Accuracy: 22.01%
22	Validation loss: 1.643529	Best loss: 0.107346	Accuracy: 18.73%
23	Validation loss: 1.644233	Best loss: 0.107346	Accuracy: 19.27%
24	Validation loss: 1.690035	Best loss: 0.107346	Accuracy: 18.73%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[120]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>DNNClassifier(activation=&lt;function elu at 0x1243639d8&gt;,
       batch_norm_momentum=None, batch_size=20, dropout_rate=None,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The model is trained, let's see if it gets the same accuracy as earlier:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[121]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">accuracy_score</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">dnn_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[121]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9725627553998832</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Yep! Working fine. Now we can use Scikit-Learn's <code>RandomizedSearchCV</code> class to search for better hyperparameters (this may take over an hour, depending on your system):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[122]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">RandomizedSearchCV</span>

<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parametrized_leaky_relu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parametrized_leaky_relu</span>

<span class="n">param_distribs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_neurons&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="mi">160</span><span class="p">],</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)],</span>
    <span class="c1"># you could also try exploring different numbers of hidden layers, different optimizers, etc.</span>
    <span class="c1">#&quot;n_hidden_layers&quot;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],</span>
    <span class="c1">#&quot;optimizer_class&quot;: [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],</span>
<span class="p">}</span>

<span class="n">rnd_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">DNNClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">param_distribs</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                                <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rnd_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid1</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:</span>
<span class="c1"># fit_params = dict(X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)</span>
<span class="c1"># rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,</span>
<span class="c1">#                                 fit_params=fit_params, random_state=42, verbose=2)</span>
<span class="c1"># rnd_search.fit(X_train1, y_train1)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 3 folds for each of 50 candidates, totalling 150 fits
[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.143224	Best loss: 0.143224	Accuracy: 95.82%
1	Validation loss: 0.143304	Best loss: 0.143224	Accuracy: 96.60%
2	Validation loss: 0.106488	Best loss: 0.106488	Accuracy: 96.95%
3	Validation loss: 0.307107	Best loss: 0.106488	Accuracy: 92.34%
4	Validation loss: 0.157948	Best loss: 0.106488	Accuracy: 95.50%
5	Validation loss: 0.131002	Best loss: 0.106488	Accuracy: 96.40%
6	Validation loss: 0.931847	Best loss: 0.106488	Accuracy: 58.29%
7	Validation loss: 0.872748	Best loss: 0.106488	Accuracy: 57.97%
8	Validation loss: 0.699336	Best loss: 0.106488	Accuracy: 58.29%
9	Validation loss: 0.853343	Best loss: 0.106488	Accuracy: 57.27%
10	Validation loss: 0.738493	Best loss: 0.106488	Accuracy: 59.19%
11	Validation loss: 0.670431	Best loss: 0.106488	Accuracy: 59.23%
12	Validation loss: 0.717334	Best loss: 0.106488	Accuracy: 59.11%
13	Validation loss: 0.718714	Best loss: 0.106488	Accuracy: 56.57%
14	Validation loss: 0.679313	Best loss: 0.106488	Accuracy: 59.07%
15	Validation loss: 0.732966	Best loss: 0.106488	Accuracy: 58.41%
16	Validation loss: 0.666333	Best loss: 0.106488	Accuracy: 60.48%
17	Validation loss: 0.677045	Best loss: 0.106488	Accuracy: 61.18%
18	Validation loss: 0.666103	Best loss: 0.106488	Accuracy: 59.97%
19	Validation loss: 0.710005	Best loss: 0.106488	Accuracy: 63.21%
20	Validation loss: 1.037921	Best loss: 0.106488	Accuracy: 64.03%
21	Validation loss: 1.626959	Best loss: 0.106488	Accuracy: 19.27%
22	Validation loss: 1.615710	Best loss: 0.106488	Accuracy: 18.73%
23	Validation loss: 1.609028	Best loss: 0.106488	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   4.7s
[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.8s remaining:    0.0s
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.137274	Best loss: 0.137274	Accuracy: 96.33%
1	Validation loss: 0.145733	Best loss: 0.137274	Accuracy: 95.97%
2	Validation loss: 0.171077	Best loss: 0.137274	Accuracy: 95.90%
3	Validation loss: 0.139310	Best loss: 0.137274	Accuracy: 96.79%
4	Validation loss: 0.365713	Best loss: 0.137274	Accuracy: 92.73%
5	Validation loss: 0.514753	Best loss: 0.137274	Accuracy: 83.46%
6	Validation loss: 0.856815	Best loss: 0.137274	Accuracy: 57.08%
7	Validation loss: 0.897573	Best loss: 0.137274	Accuracy: 66.03%
8	Validation loss: 1.534284	Best loss: 0.137274	Accuracy: 37.02%
9	Validation loss: 0.974006	Best loss: 0.137274	Accuracy: 67.01%
10	Validation loss: 0.862646	Best loss: 0.137274	Accuracy: 71.85%
11	Validation loss: 0.817344	Best loss: 0.137274	Accuracy: 73.34%
12	Validation loss: 0.723583	Best loss: 0.137274	Accuracy: 79.75%
13	Validation loss: 0.646904	Best loss: 0.137274	Accuracy: 86.28%
14	Validation loss: 0.669650	Best loss: 0.137274	Accuracy: 87.57%
15	Validation loss: 0.557968	Best loss: 0.137274	Accuracy: 90.62%
16	Validation loss: 0.876380	Best loss: 0.137274	Accuracy: 71.07%
17	Validation loss: 1.092547	Best loss: 0.137274	Accuracy: 56.41%
18	Validation loss: 0.690035	Best loss: 0.137274	Accuracy: 81.63%
19	Validation loss: 0.707778	Best loss: 0.137274	Accuracy: 82.92%
20	Validation loss: 0.791751	Best loss: 0.137274	Accuracy: 73.85%
21	Validation loss: 2.051816	Best loss: 0.137274	Accuracy: 31.86%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   4.2s
[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.145602	Best loss: 0.145602	Accuracy: 96.17%
1	Validation loss: 0.182725	Best loss: 0.145602	Accuracy: 96.13%
2	Validation loss: 0.131560	Best loss: 0.131560	Accuracy: 96.76%
3	Validation loss: 0.164568	Best loss: 0.131560	Accuracy: 96.64%
4	Validation loss: 0.140317	Best loss: 0.131560	Accuracy: 97.11%
5	Validation loss: 0.155033	Best loss: 0.131560	Accuracy: 96.25%
6	Validation loss: 0.148936	Best loss: 0.131560	Accuracy: 96.68%
7	Validation loss: 0.126398	Best loss: 0.126398	Accuracy: 96.79%
8	Validation loss: 0.114643	Best loss: 0.114643	Accuracy: 97.30%
9	Validation loss: 0.158117	Best loss: 0.114643	Accuracy: 96.25%
10	Validation loss: 0.152243	Best loss: 0.114643	Accuracy: 95.86%
11	Validation loss: 0.136021	Best loss: 0.114643	Accuracy: 97.19%
12	Validation loss: 0.138771	Best loss: 0.114643	Accuracy: 96.44%
13	Validation loss: 1.160713	Best loss: 0.114643	Accuracy: 38.66%
14	Validation loss: 1.126040	Best loss: 0.114643	Accuracy: 41.83%
15	Validation loss: 1.128232	Best loss: 0.114643	Accuracy: 42.10%
16	Validation loss: 1.151199	Best loss: 0.114643	Accuracy: 40.30%
17	Validation loss: 1.105671	Best loss: 0.114643	Accuracy: 35.69%
18	Validation loss: 1.057996	Best loss: 0.114643	Accuracy: 41.95%
19	Validation loss: 1.087900	Best loss: 0.114643	Accuracy: 39.21%
20	Validation loss: 1.102349	Best loss: 0.114643	Accuracy: 42.34%
21	Validation loss: 1.152924	Best loss: 0.114643	Accuracy: 36.47%
22	Validation loss: 1.099882	Best loss: 0.114643	Accuracy: 41.99%
23	Validation loss: 1.109928	Best loss: 0.114643	Accuracy: 42.61%
24	Validation loss: 1.112383	Best loss: 0.114643	Accuracy: 42.69%
25	Validation loss: 1.060540	Best loss: 0.114643	Accuracy: 36.36%
26	Validation loss: 1.166065	Best loss: 0.114643	Accuracy: 42.30%
27	Validation loss: 1.186971	Best loss: 0.114643	Accuracy: 40.85%
28	Validation loss: 1.202409	Best loss: 0.114643	Accuracy: 37.69%
29	Validation loss: 1.082845	Best loss: 0.114643	Accuracy: 36.90%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   5.7s
[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.110739	Best loss: 0.110739	Accuracy: 96.36%
1	Validation loss: 0.083676	Best loss: 0.083676	Accuracy: 97.50%
2	Validation loss: 0.085564	Best loss: 0.083676	Accuracy: 97.81%
3	Validation loss: 0.070906	Best loss: 0.070906	Accuracy: 98.05%
4	Validation loss: 0.066968	Best loss: 0.066968	Accuracy: 98.36%
5	Validation loss: 0.081629	Best loss: 0.066968	Accuracy: 97.85%
6	Validation loss: 0.061419	Best loss: 0.061419	Accuracy: 98.40%
7	Validation loss: 0.062254	Best loss: 0.061419	Accuracy: 98.55%
8	Validation loss: 0.059769	Best loss: 0.059769	Accuracy: 98.36%
9	Validation loss: 0.057321	Best loss: 0.057321	Accuracy: 98.40%
10	Validation loss: 0.066610	Best loss: 0.057321	Accuracy: 98.48%
11	Validation loss: 0.086045	Best loss: 0.057321	Accuracy: 98.16%
12	Validation loss: 0.089807	Best loss: 0.057321	Accuracy: 98.08%
13	Validation loss: 0.066828	Best loss: 0.057321	Accuracy: 98.71%
14	Validation loss: 0.062486	Best loss: 0.057321	Accuracy: 98.40%
15	Validation loss: 0.063937	Best loss: 0.057321	Accuracy: 98.44%
16	Validation loss: 0.083851	Best loss: 0.057321	Accuracy: 98.20%
17	Validation loss: 0.070227	Best loss: 0.057321	Accuracy: 98.55%
18	Validation loss: 0.070609	Best loss: 0.057321	Accuracy: 98.59%
19	Validation loss: 0.069561	Best loss: 0.057321	Accuracy: 98.71%
20	Validation loss: 0.069609	Best loss: 0.057321	Accuracy: 98.48%
21	Validation loss: 0.086223	Best loss: 0.057321	Accuracy: 98.05%
22	Validation loss: 0.075333	Best loss: 0.057321	Accuracy: 98.63%
23	Validation loss: 0.083690	Best loss: 0.057321	Accuracy: 98.83%
24	Validation loss: 0.085811	Best loss: 0.057321	Accuracy: 98.75%
25	Validation loss: 0.090140	Best loss: 0.057321	Accuracy: 98.28%
26	Validation loss: 0.080301	Best loss: 0.057321	Accuracy: 98.51%
27	Validation loss: 0.085537	Best loss: 0.057321	Accuracy: 98.24%
28	Validation loss: 0.074002	Best loss: 0.057321	Accuracy: 98.71%
29	Validation loss: 0.100095	Best loss: 0.057321	Accuracy: 98.05%
30	Validation loss: 0.107288	Best loss: 0.057321	Accuracy: 98.51%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=&lt;function relu at 0x124366d08&gt;, total=   3.7s
[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.150243	Best loss: 0.150243	Accuracy: 95.93%
1	Validation loss: 0.074734	Best loss: 0.074734	Accuracy: 97.85%
2	Validation loss: 0.064674	Best loss: 0.064674	Accuracy: 98.28%
3	Validation loss: 0.059260	Best loss: 0.059260	Accuracy: 98.20%
4	Validation loss: 0.070795	Best loss: 0.059260	Accuracy: 97.93%
5	Validation loss: 0.064104	Best loss: 0.059260	Accuracy: 98.12%
6	Validation loss: 0.058989	Best loss: 0.058989	Accuracy: 98.51%
7	Validation loss: 0.074514	Best loss: 0.058989	Accuracy: 98.01%
8	Validation loss: 0.055630	Best loss: 0.055630	Accuracy: 98.51%
9	Validation loss: 0.057264	Best loss: 0.055630	Accuracy: 98.55%
10	Validation loss: 0.066092	Best loss: 0.055630	Accuracy: 98.55%
11	Validation loss: 0.054832	Best loss: 0.054832	Accuracy: 98.75%
12	Validation loss: 0.067625	Best loss: 0.054832	Accuracy: 98.48%
13	Validation loss: 0.071580	Best loss: 0.054832	Accuracy: 98.05%
14	Validation loss: 0.060259	Best loss: 0.054832	Accuracy: 98.44%
15	Validation loss: 0.074533	Best loss: 0.054832	Accuracy: 98.32%
16	Validation loss: 0.063384	Best loss: 0.054832	Accuracy: 98.28%
17	Validation loss: 0.077577	Best loss: 0.054832	Accuracy: 98.75%
18	Validation loss: 0.066334	Best loss: 0.054832	Accuracy: 98.67%
19	Validation loss: 0.067524	Best loss: 0.054832	Accuracy: 98.75%
20	Validation loss: 0.072858	Best loss: 0.054832	Accuracy: 98.44%
21	Validation loss: 0.070549	Best loss: 0.054832	Accuracy: 98.67%
22	Validation loss: 0.081628	Best loss: 0.054832	Accuracy: 98.51%
23	Validation loss: 0.092745	Best loss: 0.054832	Accuracy: 98.36%
24	Validation loss: 0.089131	Best loss: 0.054832	Accuracy: 98.48%
25	Validation loss: 0.070926	Best loss: 0.054832	Accuracy: 98.63%
26	Validation loss: 0.076760	Best loss: 0.054832	Accuracy: 98.40%
27	Validation loss: 0.079344	Best loss: 0.054832	Accuracy: 98.59%
28	Validation loss: 0.072136	Best loss: 0.054832	Accuracy: 98.51%
29	Validation loss: 0.083934	Best loss: 0.054832	Accuracy: 98.44%
30	Validation loss: 0.081027	Best loss: 0.054832	Accuracy: 98.48%
31	Validation loss: 0.076559	Best loss: 0.054832	Accuracy: 98.63%
32	Validation loss: 0.082805	Best loss: 0.054832	Accuracy: 98.63%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=&lt;function relu at 0x124366d08&gt;, total=   3.6s
[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.101571	Best loss: 0.101571	Accuracy: 96.79%
1	Validation loss: 0.077615	Best loss: 0.077615	Accuracy: 97.97%
2	Validation loss: 0.060394	Best loss: 0.060394	Accuracy: 98.05%
3	Validation loss: 0.069809	Best loss: 0.060394	Accuracy: 97.97%
4	Validation loss: 0.047249	Best loss: 0.047249	Accuracy: 98.59%
5	Validation loss: 0.063866	Best loss: 0.047249	Accuracy: 97.81%
6	Validation loss: 0.054860	Best loss: 0.047249	Accuracy: 98.24%
7	Validation loss: 0.070164	Best loss: 0.047249	Accuracy: 98.40%
8	Validation loss: 0.050729	Best loss: 0.047249	Accuracy: 98.63%
9	Validation loss: 0.052778	Best loss: 0.047249	Accuracy: 98.67%
10	Validation loss: 0.051625	Best loss: 0.047249	Accuracy: 98.71%
11	Validation loss: 0.046354	Best loss: 0.046354	Accuracy: 98.67%
12	Validation loss: 0.060452	Best loss: 0.046354	Accuracy: 98.48%
13	Validation loss: 0.087543	Best loss: 0.046354	Accuracy: 98.32%
14	Validation loss: 0.078309	Best loss: 0.046354	Accuracy: 98.48%
15	Validation loss: 0.067486	Best loss: 0.046354	Accuracy: 98.40%
16	Validation loss: 0.055117	Best loss: 0.046354	Accuracy: 98.71%
17	Validation loss: 0.062640	Best loss: 0.046354	Accuracy: 98.94%
18	Validation loss: 0.063772	Best loss: 0.046354	Accuracy: 98.71%
19	Validation loss: 0.065355	Best loss: 0.046354	Accuracy: 98.55%
20	Validation loss: 0.079718	Best loss: 0.046354	Accuracy: 98.48%
21	Validation loss: 0.072222	Best loss: 0.046354	Accuracy: 98.59%
22	Validation loss: 0.097125	Best loss: 0.046354	Accuracy: 98.36%
23	Validation loss: 0.091628	Best loss: 0.046354	Accuracy: 98.36%
24	Validation loss: 0.071661	Best loss: 0.046354	Accuracy: 98.48%
25	Validation loss: 0.119163	Best loss: 0.046354	Accuracy: 98.16%
26	Validation loss: 0.118342	Best loss: 0.046354	Accuracy: 98.24%
27	Validation loss: 0.113409	Best loss: 0.046354	Accuracy: 97.73%
28	Validation loss: 0.063301	Best loss: 0.046354	Accuracy: 98.71%
29	Validation loss: 0.073191	Best loss: 0.046354	Accuracy: 98.55%
30	Validation loss: 0.085522	Best loss: 0.046354	Accuracy: 98.44%
31	Validation loss: 0.092242	Best loss: 0.046354	Accuracy: 98.44%
32	Validation loss: 0.086612	Best loss: 0.046354	Accuracy: 98.36%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=&lt;function relu at 0x124366d08&gt;, total=   3.7s
[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 1550.464722	Best loss: 1550.464722	Accuracy: 18.37%
1	Validation loss: 3.543767	Best loss: 3.543767	Accuracy: 83.82%
2	Validation loss: 110.237564	Best loss: 3.543767	Accuracy: 68.37%
3	Validation loss: 4.845516	Best loss: 3.543767	Accuracy: 86.16%
4	Validation loss: 5.187873	Best loss: 3.543767	Accuracy: 79.83%
5	Validation loss: 3.867887	Best loss: 3.543767	Accuracy: 86.08%
6	Validation loss: 74.955330	Best loss: 3.543767	Accuracy: 57.74%
7	Validation loss: 10.644798	Best loss: 3.543767	Accuracy: 64.86%
8	Validation loss: 3.318047	Best loss: 3.318047	Accuracy: 80.65%
9	Validation loss: 18850.718750	Best loss: 3.318047	Accuracy: 23.46%
10	Validation loss: 566.612061	Best loss: 3.318047	Accuracy: 52.85%
11	Validation loss: 536.679932	Best loss: 3.318047	Accuracy: 70.76%
12	Validation loss: 860.014954	Best loss: 3.318047	Accuracy: 62.98%
13	Validation loss: 358.129242	Best loss: 3.318047	Accuracy: 74.04%
14	Validation loss: 174.931961	Best loss: 3.318047	Accuracy: 86.67%
15	Validation loss: 213.551010	Best loss: 3.318047	Accuracy: 79.87%
16	Validation loss: 258.406891	Best loss: 3.318047	Accuracy: 85.26%
17	Validation loss: 1923.203735	Best loss: 3.318047	Accuracy: 45.47%
18	Validation loss: 224.166092	Best loss: 3.318047	Accuracy: 88.04%
19	Validation loss: 339.692871	Best loss: 3.318047	Accuracy: 68.73%
20	Validation loss: 477.542114	Best loss: 3.318047	Accuracy: 72.87%
21	Validation loss: 180.150482	Best loss: 3.318047	Accuracy: 91.28%
22	Validation loss: 271.070770	Best loss: 3.318047	Accuracy: 90.07%
23	Validation loss: 200.239182	Best loss: 3.318047	Accuracy: 93.59%
24	Validation loss: 1509.065308	Best loss: 3.318047	Accuracy: 72.09%
25	Validation loss: 112.731544	Best loss: 3.318047	Accuracy: 79.91%
26	Validation loss: 186.466751	Best loss: 3.318047	Accuracy: 91.83%
27	Validation loss: 172.139267	Best loss: 3.318047	Accuracy: 88.70%
28	Validation loss: 93.079277	Best loss: 3.318047	Accuracy: 91.71%
29	Validation loss: 105.408752	Best loss: 3.318047	Accuracy: 92.18%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  20.4s
[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 621.882202	Best loss: 621.882202	Accuracy: 20.91%
1	Validation loss: 4.700475	Best loss: 4.700475	Accuracy: 55.00%
2	Validation loss: 4.871758	Best loss: 4.700475	Accuracy: 56.33%
3	Validation loss: 2.304079	Best loss: 2.304079	Accuracy: 68.80%
4	Validation loss: 1.001438	Best loss: 1.001438	Accuracy: 84.28%
5	Validation loss: 1.761082	Best loss: 1.001438	Accuracy: 78.19%
6	Validation loss: 1.113568	Best loss: 1.001438	Accuracy: 80.49%
7	Validation loss: 0.607927	Best loss: 0.607927	Accuracy: 88.15%
8	Validation loss: 1.285415	Best loss: 0.607927	Accuracy: 82.64%
9	Validation loss: 0.442559	Best loss: 0.442559	Accuracy: 90.85%
10	Validation loss: 1.371850	Best loss: 0.442559	Accuracy: 76.70%
11	Validation loss: 0.803230	Best loss: 0.442559	Accuracy: 87.76%
12	Validation loss: 10770.880859	Best loss: 0.442559	Accuracy: 18.73%
13	Validation loss: 185.091644	Best loss: 0.442559	Accuracy: 63.68%
14	Validation loss: 301.453125	Best loss: 0.442559	Accuracy: 55.32%
15	Validation loss: 178.694153	Best loss: 0.442559	Accuracy: 45.50%
16	Validation loss: 144.673111	Best loss: 0.442559	Accuracy: 64.66%
17	Validation loss: 143.865906	Best loss: 0.442559	Accuracy: 72.52%
18	Validation loss: 119.270966	Best loss: 0.442559	Accuracy: 78.30%
19	Validation loss: 50.850903	Best loss: 0.442559	Accuracy: 86.20%
20	Validation loss: 66.464035	Best loss: 0.442559	Accuracy: 83.93%
21	Validation loss: 46854.062500	Best loss: 0.442559	Accuracy: 21.11%
22	Validation loss: 8904.892578	Best loss: 0.442559	Accuracy: 69.98%
23	Validation loss: 1024.563965	Best loss: 0.442559	Accuracy: 68.14%
24	Validation loss: 3176.281982	Best loss: 0.442559	Accuracy: 58.01%
25	Validation loss: 737.650940	Best loss: 0.442559	Accuracy: 79.09%
26	Validation loss: 785.814270	Best loss: 0.442559	Accuracy: 62.31%
27	Validation loss: 486.644257	Best loss: 0.442559	Accuracy: 87.37%
28	Validation loss: 560.122314	Best loss: 0.442559	Accuracy: 90.07%
29	Validation loss: 577.399231	Best loss: 0.442559	Accuracy: 79.91%
30	Validation loss: 212.916000	Best loss: 0.442559	Accuracy: 91.48%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  21.2s
[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.946387	Best loss: 0.946387	Accuracy: 93.20%
1	Validation loss: 12.100128	Best loss: 0.946387	Accuracy: 40.70%
2	Validation loss: 13.804142	Best loss: 0.946387	Accuracy: 37.72%
3	Validation loss: 17280.560547	Best loss: 0.946387	Accuracy: 30.81%
4	Validation loss: 127.023079	Best loss: 0.946387	Accuracy: 79.40%
5	Validation loss: 66.456802	Best loss: 0.946387	Accuracy: 82.25%
6	Validation loss: 43.608822	Best loss: 0.946387	Accuracy: 89.56%
7	Validation loss: 30.854092	Best loss: 0.946387	Accuracy: 88.98%
8	Validation loss: 477.349701	Best loss: 0.946387	Accuracy: 77.21%
9	Validation loss: 88.112274	Best loss: 0.946387	Accuracy: 92.26%
10	Validation loss: 61.352814	Best loss: 0.946387	Accuracy: 93.67%
11	Validation loss: 43.495338	Best loss: 0.946387	Accuracy: 93.32%
12	Validation loss: 46.072357	Best loss: 0.946387	Accuracy: 93.86%
13	Validation loss: 70.133774	Best loss: 0.946387	Accuracy: 91.63%
14	Validation loss: 53.063385	Best loss: 0.946387	Accuracy: 91.99%
15	Validation loss: 47.811386	Best loss: 0.946387	Accuracy: 92.34%
16	Validation loss: 25.837633	Best loss: 0.946387	Accuracy: 93.82%
17	Validation loss: 27.841801	Best loss: 0.946387	Accuracy: 95.07%
18	Validation loss: 211.868423	Best loss: 0.946387	Accuracy: 84.95%
19	Validation loss: 22290.859375	Best loss: 0.946387	Accuracy: 51.49%
20	Validation loss: 936.696655	Best loss: 0.946387	Accuracy: 82.02%
21	Validation loss: 693.945862	Best loss: 0.946387	Accuracy: 78.42%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  14.3s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 1789.889160	Best loss: 1789.889160	Accuracy: 70.48%
1	Validation loss: 242.268845	Best loss: 242.268845	Accuracy: 89.91%
2	Validation loss: 197.313705	Best loss: 197.313705	Accuracy: 87.88%
3	Validation loss: 273.397003	Best loss: 197.313705	Accuracy: 89.09%
4	Validation loss: 128.020111	Best loss: 128.020111	Accuracy: 91.87%
5	Validation loss: 134.973755	Best loss: 128.020111	Accuracy: 92.85%
6	Validation loss: 140.270203	Best loss: 128.020111	Accuracy: 91.13%
7	Validation loss: 2483301.250000	Best loss: 128.020111	Accuracy: 22.01%
8	Validation loss: 103920.015625	Best loss: 128.020111	Accuracy: 85.77%
9	Validation loss: 53551.222656	Best loss: 128.020111	Accuracy: 90.97%
10	Validation loss: 27988.152344	Best loss: 128.020111	Accuracy: 92.30%
11	Validation loss: 26809.955078	Best loss: 128.020111	Accuracy: 92.92%
12	Validation loss: 28686.222656	Best loss: 128.020111	Accuracy: 87.96%
13	Validation loss: 26549.455078	Best loss: 128.020111	Accuracy: 92.26%
14	Validation loss: 12639.249023	Best loss: 128.020111	Accuracy: 95.04%
15	Validation loss: 16398.583984	Best loss: 128.020111	Accuracy: 94.21%
16	Validation loss: 11684.181641	Best loss: 128.020111	Accuracy: 96.13%
17	Validation loss: 8643.539062	Best loss: 128.020111	Accuracy: 94.10%
18	Validation loss: 15076.775391	Best loss: 128.020111	Accuracy: 93.04%
19	Validation loss: 15883.515625	Best loss: 128.020111	Accuracy: 95.43%
20	Validation loss: 15323.984375	Best loss: 128.020111	Accuracy: 94.68%
21	Validation loss: 9787.508789	Best loss: 128.020111	Accuracy: 96.56%
22	Validation loss: 8069.798340	Best loss: 128.020111	Accuracy: 96.56%
23	Validation loss: 7801.803711	Best loss: 128.020111	Accuracy: 96.76%
24	Validation loss: 10910.854492	Best loss: 128.020111	Accuracy: 90.50%
25	Validation loss: 4924.967285	Best loss: 128.020111	Accuracy: 95.15%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  15.8s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.466725	Best loss: 0.466725	Accuracy: 88.19%
1	Validation loss: 2033812.500000	Best loss: 0.466725	Accuracy: 39.44%
2	Validation loss: 236975.968750	Best loss: 0.466725	Accuracy: 75.76%
3	Validation loss: 29789.376953	Best loss: 0.466725	Accuracy: 89.91%
4	Validation loss: 30450.886719	Best loss: 0.466725	Accuracy: 90.93%
5	Validation loss: 18224.691406	Best loss: 0.466725	Accuracy: 91.40%
6	Validation loss: 22133.652344	Best loss: 0.466725	Accuracy: 89.64%
7	Validation loss: 41642.316406	Best loss: 0.466725	Accuracy: 92.65%
8	Validation loss: 9787.914062	Best loss: 0.466725	Accuracy: 95.35%
9	Validation loss: 11031.457031	Best loss: 0.466725	Accuracy: 95.47%
10	Validation loss: 30170.615234	Best loss: 0.466725	Accuracy: 93.08%
11	Validation loss: 9628.743164	Best loss: 0.466725	Accuracy: 94.14%
12	Validation loss: 39060.000000	Best loss: 0.466725	Accuracy: 91.16%
13	Validation loss: 15631.790039	Best loss: 0.466725	Accuracy: 95.47%
14	Validation loss: 293540.343750	Best loss: 0.466725	Accuracy: 94.64%
15	Validation loss: 39858.792969	Best loss: 0.466725	Accuracy: 95.93%
16	Validation loss: 15974.951172	Best loss: 0.466725	Accuracy: 96.60%
17	Validation loss: 10339.972656	Best loss: 0.466725	Accuracy: 96.87%
18	Validation loss: 8368.265625	Best loss: 0.466725	Accuracy: 97.03%
19	Validation loss: 5688.387695	Best loss: 0.466725	Accuracy: 96.40%
20	Validation loss: 4295.638184	Best loss: 0.466725	Accuracy: 97.89%
21	Validation loss: 81016.671875	Best loss: 0.466725	Accuracy: 97.42%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  13.9s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.287232	Best loss: 0.287232	Accuracy: 92.96%
1	Validation loss: 0.478906	Best loss: 0.287232	Accuracy: 92.96%
2	Validation loss: 2372.589844	Best loss: 0.287232	Accuracy: 87.22%
3	Validation loss: 1015.821045	Best loss: 0.287232	Accuracy: 90.38%
4	Validation loss: 649.598999	Best loss: 0.287232	Accuracy: 93.16%
5	Validation loss: 941.223999	Best loss: 0.287232	Accuracy: 91.87%
6	Validation loss: 452.865875	Best loss: 0.287232	Accuracy: 93.32%
7	Validation loss: 275.014526	Best loss: 0.287232	Accuracy: 95.54%
8	Validation loss: 2093.019043	Best loss: 0.287232	Accuracy: 82.02%
9	Validation loss: 370.321625	Best loss: 0.287232	Accuracy: 95.43%
10	Validation loss: 450.765350	Best loss: 0.287232	Accuracy: 94.64%
11	Validation loss: 240.273895	Best loss: 0.287232	Accuracy: 95.74%
12	Validation loss: 166241.656250	Best loss: 0.287232	Accuracy: 94.45%
13	Validation loss: 120045.546875	Best loss: 0.287232	Accuracy: 90.81%
14	Validation loss: 95318.468750	Best loss: 0.287232	Accuracy: 94.57%
15	Validation loss: 35744.273438	Best loss: 0.287232	Accuracy: 95.19%
16	Validation loss: 17772.041016	Best loss: 0.287232	Accuracy: 94.96%
17	Validation loss: 36768.390625	Best loss: 0.287232	Accuracy: 94.64%
18	Validation loss: 16274.967773	Best loss: 0.287232	Accuracy: 95.58%
19	Validation loss: 25718.509766	Best loss: 0.287232	Accuracy: 96.72%
20	Validation loss: 47776.636719	Best loss: 0.287232	Accuracy: 95.93%
21	Validation loss: 65078.355469	Best loss: 0.287232	Accuracy: 94.68%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  12.8s
[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.116430	Best loss: 0.116430	Accuracy: 96.52%
1	Validation loss: 0.058841	Best loss: 0.058841	Accuracy: 98.20%
2	Validation loss: 0.058809	Best loss: 0.058809	Accuracy: 98.20%
3	Validation loss: 0.051221	Best loss: 0.051221	Accuracy: 98.48%
4	Validation loss: 0.050770	Best loss: 0.050770	Accuracy: 98.48%
5	Validation loss: 0.046502	Best loss: 0.046502	Accuracy: 98.63%
6	Validation loss: 0.047975	Best loss: 0.046502	Accuracy: 98.79%
7	Validation loss: 0.043331	Best loss: 0.043331	Accuracy: 98.75%
8	Validation loss: 0.040555	Best loss: 0.040555	Accuracy: 98.94%
9	Validation loss: 0.069098	Best loss: 0.040555	Accuracy: 98.55%
10	Validation loss: 0.054209	Best loss: 0.040555	Accuracy: 98.83%
11	Validation loss: 0.055467	Best loss: 0.040555	Accuracy: 98.87%
12	Validation loss: 0.050334	Best loss: 0.040555	Accuracy: 98.91%
13	Validation loss: 0.056235	Best loss: 0.040555	Accuracy: 98.83%
14	Validation loss: 0.060593	Best loss: 0.040555	Accuracy: 98.51%
15	Validation loss: 0.090569	Best loss: 0.040555	Accuracy: 98.01%
16	Validation loss: 0.069206	Best loss: 0.040555	Accuracy: 98.55%
17	Validation loss: 0.057612	Best loss: 0.040555	Accuracy: 99.02%
18	Validation loss: 0.068602	Best loss: 0.040555	Accuracy: 98.87%
19	Validation loss: 0.068288	Best loss: 0.040555	Accuracy: 98.94%
20	Validation loss: 0.054578	Best loss: 0.040555	Accuracy: 98.91%
21	Validation loss: 0.068164	Best loss: 0.040555	Accuracy: 98.94%
22	Validation loss: 0.064385	Best loss: 0.040555	Accuracy: 99.18%
23	Validation loss: 0.095468	Best loss: 0.040555	Accuracy: 99.02%
24	Validation loss: 0.055795	Best loss: 0.040555	Accuracy: 98.83%
25	Validation loss: 0.058368	Best loss: 0.040555	Accuracy: 99.02%
26	Validation loss: 0.067411	Best loss: 0.040555	Accuracy: 98.98%
27	Validation loss: 0.066987	Best loss: 0.040555	Accuracy: 98.71%
28	Validation loss: 0.064438	Best loss: 0.040555	Accuracy: 98.63%
29	Validation loss: 0.072174	Best loss: 0.040555	Accuracy: 99.02%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   8.2s
[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.099178	Best loss: 0.099178	Accuracy: 97.15%
1	Validation loss: 0.068436	Best loss: 0.068436	Accuracy: 98.05%
2	Validation loss: 0.058424	Best loss: 0.058424	Accuracy: 98.24%
3	Validation loss: 0.056367	Best loss: 0.056367	Accuracy: 98.51%
4	Validation loss: 0.069450	Best loss: 0.056367	Accuracy: 98.32%
5	Validation loss: 0.047193	Best loss: 0.047193	Accuracy: 98.63%
6	Validation loss: 0.061327	Best loss: 0.047193	Accuracy: 98.40%
7	Validation loss: 0.046580	Best loss: 0.046580	Accuracy: 98.59%
8	Validation loss: 0.048273	Best loss: 0.046580	Accuracy: 98.71%
9	Validation loss: 0.057993	Best loss: 0.046580	Accuracy: 98.40%
10	Validation loss: 0.053794	Best loss: 0.046580	Accuracy: 98.40%
11	Validation loss: 0.052939	Best loss: 0.046580	Accuracy: 98.55%
12	Validation loss: 0.057744	Best loss: 0.046580	Accuracy: 98.71%
13	Validation loss: 0.067038	Best loss: 0.046580	Accuracy: 98.67%
14	Validation loss: 0.064506	Best loss: 0.046580	Accuracy: 98.67%
15	Validation loss: 0.048939	Best loss: 0.046580	Accuracy: 99.14%
16	Validation loss: 0.056095	Best loss: 0.046580	Accuracy: 98.71%
17	Validation loss: 0.049879	Best loss: 0.046580	Accuracy: 98.98%
18	Validation loss: 0.050285	Best loss: 0.046580	Accuracy: 98.83%
19	Validation loss: 0.061907	Best loss: 0.046580	Accuracy: 98.71%
20	Validation loss: 0.075889	Best loss: 0.046580	Accuracy: 98.63%
21	Validation loss: 0.059141	Best loss: 0.046580	Accuracy: 98.67%
22	Validation loss: 0.088861	Best loss: 0.046580	Accuracy: 98.87%
23	Validation loss: 0.049113	Best loss: 0.046580	Accuracy: 98.91%
24	Validation loss: 0.094168	Best loss: 0.046580	Accuracy: 98.20%
25	Validation loss: 0.057257	Best loss: 0.046580	Accuracy: 98.75%
26	Validation loss: 0.102022	Best loss: 0.046580	Accuracy: 98.79%
27	Validation loss: 0.084049	Best loss: 0.046580	Accuracy: 98.67%
28	Validation loss: 0.078676	Best loss: 0.046580	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   7.7s
[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.102935	Best loss: 0.102935	Accuracy: 97.38%
1	Validation loss: 0.070595	Best loss: 0.070595	Accuracy: 98.08%
2	Validation loss: 0.055720	Best loss: 0.055720	Accuracy: 98.55%
3	Validation loss: 0.049335	Best loss: 0.049335	Accuracy: 98.67%
4	Validation loss: 0.057408	Best loss: 0.049335	Accuracy: 98.59%
5	Validation loss: 0.069351	Best loss: 0.049335	Accuracy: 98.32%
6	Validation loss: 0.046165	Best loss: 0.046165	Accuracy: 98.79%
7	Validation loss: 0.077345	Best loss: 0.046165	Accuracy: 98.28%
8	Validation loss: 0.049518	Best loss: 0.046165	Accuracy: 98.71%
9	Validation loss: 0.057047	Best loss: 0.046165	Accuracy: 98.51%
10	Validation loss: 0.064146	Best loss: 0.046165	Accuracy: 98.48%
11	Validation loss: 0.057623	Best loss: 0.046165	Accuracy: 98.75%
12	Validation loss: 0.095035	Best loss: 0.046165	Accuracy: 98.51%
13	Validation loss: 0.060855	Best loss: 0.046165	Accuracy: 98.75%
14	Validation loss: 0.066942	Best loss: 0.046165	Accuracy: 98.94%
15	Validation loss: 0.065466	Best loss: 0.046165	Accuracy: 98.63%
16	Validation loss: 0.086311	Best loss: 0.046165	Accuracy: 98.40%
17	Validation loss: 0.055745	Best loss: 0.046165	Accuracy: 98.94%
18	Validation loss: 0.061926	Best loss: 0.046165	Accuracy: 98.91%
19	Validation loss: 0.055294	Best loss: 0.046165	Accuracy: 99.22%
20	Validation loss: 0.061392	Best loss: 0.046165	Accuracy: 98.94%
21	Validation loss: 0.078003	Best loss: 0.046165	Accuracy: 98.63%
22	Validation loss: 0.061102	Best loss: 0.046165	Accuracy: 98.79%
23	Validation loss: 0.063233	Best loss: 0.046165	Accuracy: 98.87%
24	Validation loss: 0.074755	Best loss: 0.046165	Accuracy: 98.94%
25	Validation loss: 0.071048	Best loss: 0.046165	Accuracy: 99.06%
26	Validation loss: 0.070057	Best loss: 0.046165	Accuracy: 98.71%
27	Validation loss: 0.076987	Best loss: 0.046165	Accuracy: 98.51%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   7.6s
[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.125644	Best loss: 0.125644	Accuracy: 96.99%
1	Validation loss: 0.061403	Best loss: 0.061403	Accuracy: 98.24%
2	Validation loss: 0.066687	Best loss: 0.061403	Accuracy: 97.77%
3	Validation loss: 0.054367	Best loss: 0.054367	Accuracy: 98.20%
4	Validation loss: 0.053543	Best loss: 0.053543	Accuracy: 98.36%
5	Validation loss: 0.085092	Best loss: 0.053543	Accuracy: 97.77%
6	Validation loss: 0.043427	Best loss: 0.043427	Accuracy: 98.75%
7	Validation loss: 0.053855	Best loss: 0.043427	Accuracy: 98.24%
8	Validation loss: 0.071230	Best loss: 0.043427	Accuracy: 98.28%
9	Validation loss: 0.045863	Best loss: 0.043427	Accuracy: 98.83%
10	Validation loss: 0.052570	Best loss: 0.043427	Accuracy: 98.75%
11	Validation loss: 0.044042	Best loss: 0.043427	Accuracy: 99.06%
12	Validation loss: 0.043857	Best loss: 0.043427	Accuracy: 98.79%
13	Validation loss: 0.059595	Best loss: 0.043427	Accuracy: 98.40%
14	Validation loss: 0.046081	Best loss: 0.043427	Accuracy: 99.06%
15	Validation loss: 0.045468	Best loss: 0.043427	Accuracy: 99.06%
16	Validation loss: 0.054471	Best loss: 0.043427	Accuracy: 98.75%
17	Validation loss: 0.047426	Best loss: 0.043427	Accuracy: 98.83%
18	Validation loss: 0.045058	Best loss: 0.043427	Accuracy: 98.91%
19	Validation loss: 0.038145	Best loss: 0.038145	Accuracy: 99.30%
20	Validation loss: 0.050162	Best loss: 0.038145	Accuracy: 98.98%
21	Validation loss: 0.047995	Best loss: 0.038145	Accuracy: 98.83%
22	Validation loss: 0.069844	Best loss: 0.038145	Accuracy: 98.63%
23	Validation loss: 0.079210	Best loss: 0.038145	Accuracy: 98.71%
24	Validation loss: 0.039782	Best loss: 0.038145	Accuracy: 98.83%
25	Validation loss: 0.058421	Best loss: 0.038145	Accuracy: 98.83%
26	Validation loss: 0.090251	Best loss: 0.038145	Accuracy: 98.40%
27	Validation loss: 0.072953	Best loss: 0.038145	Accuracy: 98.83%
28	Validation loss: 0.077514	Best loss: 0.038145	Accuracy: 98.94%
29	Validation loss: 0.109877	Best loss: 0.038145	Accuracy: 97.73%
30	Validation loss: 1.963847	Best loss: 0.038145	Accuracy: 79.87%
31	Validation loss: 0.137070	Best loss: 0.038145	Accuracy: 95.66%
32	Validation loss: 0.163969	Best loss: 0.038145	Accuracy: 96.95%
33	Validation loss: 0.085918	Best loss: 0.038145	Accuracy: 98.20%
34	Validation loss: 0.096633	Best loss: 0.038145	Accuracy: 98.59%
35	Validation loss: 0.088312	Best loss: 0.038145	Accuracy: 98.94%
36	Validation loss: 0.113396	Best loss: 0.038145	Accuracy: 98.67%
37	Validation loss: 0.080893	Best loss: 0.038145	Accuracy: 98.79%
38	Validation loss: 0.089838	Best loss: 0.038145	Accuracy: 98.94%
39	Validation loss: 0.099905	Best loss: 0.038145	Accuracy: 98.83%
40	Validation loss: 0.097144	Best loss: 0.038145	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   9.1s
[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.086824	Best loss: 0.086824	Accuracy: 97.42%
1	Validation loss: 0.062750	Best loss: 0.062750	Accuracy: 98.01%
2	Validation loss: 0.058109	Best loss: 0.058109	Accuracy: 98.12%
3	Validation loss: 0.068417	Best loss: 0.058109	Accuracy: 98.05%
4	Validation loss: 0.055424	Best loss: 0.055424	Accuracy: 98.12%
5	Validation loss: 0.051893	Best loss: 0.051893	Accuracy: 98.24%
6	Validation loss: 0.046882	Best loss: 0.046882	Accuracy: 98.51%
7	Validation loss: 0.042989	Best loss: 0.042989	Accuracy: 98.75%
8	Validation loss: 0.043665	Best loss: 0.042989	Accuracy: 98.67%
9	Validation loss: 0.047032	Best loss: 0.042989	Accuracy: 98.48%
10	Validation loss: 0.063107	Best loss: 0.042989	Accuracy: 98.63%
11	Validation loss: 0.049539	Best loss: 0.042989	Accuracy: 98.51%
12	Validation loss: 0.053372	Best loss: 0.042989	Accuracy: 98.91%
13	Validation loss: 0.058846	Best loss: 0.042989	Accuracy: 98.75%
14	Validation loss: 0.044186	Best loss: 0.042989	Accuracy: 98.79%
15	Validation loss: 0.043688	Best loss: 0.042989	Accuracy: 98.91%
16	Validation loss: 0.072271	Best loss: 0.042989	Accuracy: 98.83%
17	Validation loss: 0.052348	Best loss: 0.042989	Accuracy: 98.79%
18	Validation loss: 0.061667	Best loss: 0.042989	Accuracy: 98.63%
19	Validation loss: 0.047948	Best loss: 0.042989	Accuracy: 98.83%
20	Validation loss: 0.049384	Best loss: 0.042989	Accuracy: 98.91%
21	Validation loss: 0.073082	Best loss: 0.042989	Accuracy: 98.79%
22	Validation loss: 0.058249	Best loss: 0.042989	Accuracy: 98.55%
23	Validation loss: 0.059458	Best loss: 0.042989	Accuracy: 98.63%
24	Validation loss: 0.071491	Best loss: 0.042989	Accuracy: 99.02%
25	Validation loss: 0.070081	Best loss: 0.042989	Accuracy: 99.02%
26	Validation loss: 0.061678	Best loss: 0.042989	Accuracy: 98.75%
27	Validation loss: 0.081200	Best loss: 0.042989	Accuracy: 98.83%
28	Validation loss: 0.082826	Best loss: 0.042989	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   6.7s
[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.093040	Best loss: 0.093040	Accuracy: 97.42%
1	Validation loss: 0.064318	Best loss: 0.064318	Accuracy: 98.20%
2	Validation loss: 0.054942	Best loss: 0.054942	Accuracy: 98.32%
3	Validation loss: 0.045607	Best loss: 0.045607	Accuracy: 98.40%
4	Validation loss: 0.044180	Best loss: 0.044180	Accuracy: 98.51%
5	Validation loss: 0.075201	Best loss: 0.044180	Accuracy: 97.89%
6	Validation loss: 0.045445	Best loss: 0.044180	Accuracy: 98.51%
7	Validation loss: 0.049712	Best loss: 0.044180	Accuracy: 98.63%
8	Validation loss: 0.050711	Best loss: 0.044180	Accuracy: 98.87%
9	Validation loss: 0.048439	Best loss: 0.044180	Accuracy: 98.91%
10	Validation loss: 0.059090	Best loss: 0.044180	Accuracy: 98.44%
11	Validation loss: 0.051300	Best loss: 0.044180	Accuracy: 98.87%
12	Validation loss: 0.062008	Best loss: 0.044180	Accuracy: 98.67%
13	Validation loss: 0.069208	Best loss: 0.044180	Accuracy: 98.40%
14	Validation loss: 0.053334	Best loss: 0.044180	Accuracy: 98.79%
15	Validation loss: 0.065371	Best loss: 0.044180	Accuracy: 98.55%
16	Validation loss: 0.053195	Best loss: 0.044180	Accuracy: 98.75%
17	Validation loss: 0.058298	Best loss: 0.044180	Accuracy: 98.87%
18	Validation loss: 0.055120	Best loss: 0.044180	Accuracy: 98.98%
19	Validation loss: 0.054835	Best loss: 0.044180	Accuracy: 98.91%
20	Validation loss: 0.045153	Best loss: 0.044180	Accuracy: 99.02%
21	Validation loss: 0.077403	Best loss: 0.044180	Accuracy: 98.40%
22	Validation loss: 0.042720	Best loss: 0.042720	Accuracy: 98.87%
23	Validation loss: 0.041272	Best loss: 0.041272	Accuracy: 98.75%
24	Validation loss: 0.052007	Best loss: 0.041272	Accuracy: 98.98%
25	Validation loss: 0.048275	Best loss: 0.041272	Accuracy: 98.98%
26	Validation loss: 0.048148	Best loss: 0.041272	Accuracy: 99.02%
27	Validation loss: 0.048752	Best loss: 0.041272	Accuracy: 99.02%
28	Validation loss: 0.048551	Best loss: 0.041272	Accuracy: 99.06%
29	Validation loss: 0.049233	Best loss: 0.041272	Accuracy: 99.10%
30	Validation loss: 0.049932	Best loss: 0.041272	Accuracy: 99.10%
31	Validation loss: 0.050627	Best loss: 0.041272	Accuracy: 99.10%
32	Validation loss: 0.051301	Best loss: 0.041272	Accuracy: 99.10%
33	Validation loss: 0.051988	Best loss: 0.041272	Accuracy: 99.10%
34	Validation loss: 0.052638	Best loss: 0.041272	Accuracy: 99.10%
35	Validation loss: 0.053546	Best loss: 0.041272	Accuracy: 99.06%
36	Validation loss: 0.054142	Best loss: 0.041272	Accuracy: 99.10%
37	Validation loss: 0.055024	Best loss: 0.041272	Accuracy: 99.10%
38	Validation loss: 0.056008	Best loss: 0.041272	Accuracy: 99.14%
39	Validation loss: 0.058413	Best loss: 0.041272	Accuracy: 99.14%
40	Validation loss: 0.057780	Best loss: 0.041272	Accuracy: 99.14%
41	Validation loss: 0.060285	Best loss: 0.041272	Accuracy: 99.10%
42	Validation loss: 0.063786	Best loss: 0.041272	Accuracy: 99.10%
43	Validation loss: 0.055702	Best loss: 0.041272	Accuracy: 99.10%
44	Validation loss: 0.056195	Best loss: 0.041272	Accuracy: 99.02%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  11.5s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.131317	Best loss: 0.131317	Accuracy: 95.70%
1	Validation loss: 0.076757	Best loss: 0.076757	Accuracy: 97.58%
2	Validation loss: 0.071406	Best loss: 0.071406	Accuracy: 97.69%
3	Validation loss: 0.085052	Best loss: 0.071406	Accuracy: 97.50%
4	Validation loss: 0.065436	Best loss: 0.065436	Accuracy: 97.89%
5	Validation loss: 0.070238	Best loss: 0.065436	Accuracy: 98.16%
6	Validation loss: 0.058229	Best loss: 0.058229	Accuracy: 98.40%
7	Validation loss: 0.043609	Best loss: 0.043609	Accuracy: 98.67%
8	Validation loss: 0.047669	Best loss: 0.043609	Accuracy: 98.63%
9	Validation loss: 0.050012	Best loss: 0.043609	Accuracy: 98.75%
10	Validation loss: 0.045480	Best loss: 0.043609	Accuracy: 98.94%
11	Validation loss: 0.064467	Best loss: 0.043609	Accuracy: 98.51%
12	Validation loss: 0.052846	Best loss: 0.043609	Accuracy: 98.63%
13	Validation loss: 0.050390	Best loss: 0.043609	Accuracy: 98.71%
14	Validation loss: 0.062753	Best loss: 0.043609	Accuracy: 98.67%
15	Validation loss: 0.065671	Best loss: 0.043609	Accuracy: 98.87%
16	Validation loss: 0.063353	Best loss: 0.043609	Accuracy: 98.75%
17	Validation loss: 0.056318	Best loss: 0.043609	Accuracy: 99.02%
18	Validation loss: 0.052571	Best loss: 0.043609	Accuracy: 99.02%
19	Validation loss: 0.046458	Best loss: 0.043609	Accuracy: 99.06%
20	Validation loss: 0.118436	Best loss: 0.043609	Accuracy: 97.65%
21	Validation loss: 0.061281	Best loss: 0.043609	Accuracy: 98.75%
22	Validation loss: 0.051521	Best loss: 0.043609	Accuracy: 98.87%
23	Validation loss: 0.059192	Best loss: 0.043609	Accuracy: 98.63%
24	Validation loss: 0.057845	Best loss: 0.043609	Accuracy: 98.79%
25	Validation loss: 0.060894	Best loss: 0.043609	Accuracy: 98.79%
26	Validation loss: 0.053508	Best loss: 0.043609	Accuracy: 98.98%
27	Validation loss: 0.062864	Best loss: 0.043609	Accuracy: 98.87%
28	Validation loss: 0.055089	Best loss: 0.043609	Accuracy: 98.98%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.3s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.123218	Best loss: 0.123218	Accuracy: 95.93%
1	Validation loss: 0.083363	Best loss: 0.083363	Accuracy: 97.30%
2	Validation loss: 0.065928	Best loss: 0.065928	Accuracy: 97.77%
3	Validation loss: 0.059005	Best loss: 0.059005	Accuracy: 97.89%
4	Validation loss: 0.071270	Best loss: 0.059005	Accuracy: 97.58%
5	Validation loss: 0.065771	Best loss: 0.059005	Accuracy: 97.93%
6	Validation loss: 0.048410	Best loss: 0.048410	Accuracy: 98.51%
7	Validation loss: 0.057415	Best loss: 0.048410	Accuracy: 98.28%
8	Validation loss: 0.052836	Best loss: 0.048410	Accuracy: 98.44%
9	Validation loss: 0.054508	Best loss: 0.048410	Accuracy: 98.67%
10	Validation loss: 0.055758	Best loss: 0.048410	Accuracy: 98.44%
11	Validation loss: 0.059304	Best loss: 0.048410	Accuracy: 98.48%
12	Validation loss: 0.063566	Best loss: 0.048410	Accuracy: 98.55%
13	Validation loss: 0.057075	Best loss: 0.048410	Accuracy: 98.44%
14	Validation loss: 0.052350	Best loss: 0.048410	Accuracy: 98.87%
15	Validation loss: 0.059708	Best loss: 0.048410	Accuracy: 98.67%
16	Validation loss: 0.063762	Best loss: 0.048410	Accuracy: 98.55%
17	Validation loss: 0.054331	Best loss: 0.048410	Accuracy: 98.71%
18	Validation loss: 0.073180	Best loss: 0.048410	Accuracy: 98.75%
19	Validation loss: 0.077151	Best loss: 0.048410	Accuracy: 98.48%
20	Validation loss: 0.062613	Best loss: 0.048410	Accuracy: 98.48%
21	Validation loss: 0.057950	Best loss: 0.048410	Accuracy: 98.91%
22	Validation loss: 0.062222	Best loss: 0.048410	Accuracy: 98.79%
23	Validation loss: 0.037418	Best loss: 0.037418	Accuracy: 99.06%
24	Validation loss: 0.050106	Best loss: 0.037418	Accuracy: 98.98%
25	Validation loss: 0.044183	Best loss: 0.037418	Accuracy: 98.91%
26	Validation loss: 0.059713	Best loss: 0.037418	Accuracy: 98.71%
27	Validation loss: 0.051249	Best loss: 0.037418	Accuracy: 98.91%
28	Validation loss: 0.063620	Best loss: 0.037418	Accuracy: 98.83%
29	Validation loss: 0.079643	Best loss: 0.037418	Accuracy: 98.87%
30	Validation loss: 0.047468	Best loss: 0.037418	Accuracy: 98.94%
31	Validation loss: 0.057355	Best loss: 0.037418	Accuracy: 98.79%
32	Validation loss: 0.057004	Best loss: 0.037418	Accuracy: 98.79%
33	Validation loss: 0.081491	Best loss: 0.037418	Accuracy: 98.59%
34	Validation loss: 0.061995	Best loss: 0.037418	Accuracy: 98.87%
35	Validation loss: 0.120450	Best loss: 0.037418	Accuracy: 98.51%
36	Validation loss: 2.646602	Best loss: 0.037418	Accuracy: 91.44%
37	Validation loss: 0.944654	Best loss: 0.037418	Accuracy: 84.68%
38	Validation loss: 0.313351	Best loss: 0.037418	Accuracy: 95.82%
39	Validation loss: 0.192025	Best loss: 0.037418	Accuracy: 96.36%
40	Validation loss: 0.235635	Best loss: 0.037418	Accuracy: 97.11%
41	Validation loss: 0.112230	Best loss: 0.037418	Accuracy: 97.38%
42	Validation loss: 0.114586	Best loss: 0.037418	Accuracy: 97.81%
43	Validation loss: 0.106075	Best loss: 0.037418	Accuracy: 97.77%
44	Validation loss: 0.083530	Best loss: 0.037418	Accuracy: 98.24%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  16.4s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.108754	Best loss: 0.108754	Accuracy: 96.68%
1	Validation loss: 0.076621	Best loss: 0.076621	Accuracy: 97.54%
2	Validation loss: 0.066869	Best loss: 0.066869	Accuracy: 98.08%
3	Validation loss: 0.059805	Best loss: 0.059805	Accuracy: 98.20%
4	Validation loss: 0.050830	Best loss: 0.050830	Accuracy: 98.36%
5	Validation loss: 0.050915	Best loss: 0.050830	Accuracy: 98.32%
6	Validation loss: 0.054090	Best loss: 0.050830	Accuracy: 98.44%
7	Validation loss: 0.041230	Best loss: 0.041230	Accuracy: 98.71%
8	Validation loss: 0.049021	Best loss: 0.041230	Accuracy: 98.51%
9	Validation loss: 0.056147	Best loss: 0.041230	Accuracy: 98.79%
10	Validation loss: 0.073793	Best loss: 0.041230	Accuracy: 98.28%
11	Validation loss: 0.063423	Best loss: 0.041230	Accuracy: 98.59%
12	Validation loss: 0.058882	Best loss: 0.041230	Accuracy: 98.51%
13	Validation loss: 0.063429	Best loss: 0.041230	Accuracy: 98.59%
14	Validation loss: 0.075903	Best loss: 0.041230	Accuracy: 98.48%
15	Validation loss: 0.049413	Best loss: 0.041230	Accuracy: 98.91%
16	Validation loss: 0.048683	Best loss: 0.041230	Accuracy: 98.83%
17	Validation loss: 0.063263	Best loss: 0.041230	Accuracy: 98.63%
18	Validation loss: 0.057600	Best loss: 0.041230	Accuracy: 98.79%
19	Validation loss: 0.060078	Best loss: 0.041230	Accuracy: 98.91%
20	Validation loss: 0.064916	Best loss: 0.041230	Accuracy: 98.71%
21	Validation loss: 0.063562	Best loss: 0.041230	Accuracy: 98.75%
22	Validation loss: 0.057318	Best loss: 0.041230	Accuracy: 98.79%
23	Validation loss: 0.066298	Best loss: 0.041230	Accuracy: 98.83%
24	Validation loss: 0.064556	Best loss: 0.041230	Accuracy: 98.79%
25	Validation loss: 0.069466	Best loss: 0.041230	Accuracy: 98.71%
26	Validation loss: 0.103293	Best loss: 0.041230	Accuracy: 98.24%
27	Validation loss: 0.067639	Best loss: 0.041230	Accuracy: 98.71%
28	Validation loss: 0.088912	Best loss: 0.041230	Accuracy: 98.55%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.4s
[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.326134	Best loss: 1.326134	Accuracy: 40.03%
1	Validation loss: 1.637048	Best loss: 1.326134	Accuracy: 19.27%
2	Validation loss: 1.619814	Best loss: 1.326134	Accuracy: 19.27%
3	Validation loss: 1.616517	Best loss: 1.326134	Accuracy: 19.27%
4	Validation loss: 1.654968	Best loss: 1.326134	Accuracy: 22.01%
5	Validation loss: 1.621476	Best loss: 1.326134	Accuracy: 22.01%
6	Validation loss: 1.632695	Best loss: 1.326134	Accuracy: 19.27%
7	Validation loss: 1.614866	Best loss: 1.326134	Accuracy: 19.27%
8	Validation loss: 1.642610	Best loss: 1.326134	Accuracy: 19.27%
9	Validation loss: 1.639823	Best loss: 1.326134	Accuracy: 19.27%
10	Validation loss: 1.615378	Best loss: 1.326134	Accuracy: 22.01%
11	Validation loss: 1.630569	Best loss: 1.326134	Accuracy: 22.01%
12	Validation loss: 1.636738	Best loss: 1.326134	Accuracy: 19.08%
13	Validation loss: 1.613106	Best loss: 1.326134	Accuracy: 19.27%
14	Validation loss: 1.629267	Best loss: 1.326134	Accuracy: 19.08%
15	Validation loss: 1.617142	Best loss: 1.326134	Accuracy: 22.01%
16	Validation loss: 1.626923	Best loss: 1.326134	Accuracy: 18.73%
17	Validation loss: 1.608595	Best loss: 1.326134	Accuracy: 22.01%
18	Validation loss: 1.624731	Best loss: 1.326134	Accuracy: 19.27%
19	Validation loss: 1.624030	Best loss: 1.326134	Accuracy: 20.91%
20	Validation loss: 1.615449	Best loss: 1.326134	Accuracy: 19.27%
21	Validation loss: 1.629645	Best loss: 1.326134	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total=  33.8s
[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.631946	Best loss: 1.631946	Accuracy: 22.01%
1	Validation loss: 1.644833	Best loss: 1.631946	Accuracy: 19.08%
2	Validation loss: 1.611661	Best loss: 1.611661	Accuracy: 22.01%
3	Validation loss: 1.614180	Best loss: 1.611661	Accuracy: 22.01%
4	Validation loss: 1.617815	Best loss: 1.611661	Accuracy: 22.01%
5	Validation loss: 1.624095	Best loss: 1.611661	Accuracy: 22.01%
6	Validation loss: 1.635101	Best loss: 1.611661	Accuracy: 19.27%
7	Validation loss: 1.630914	Best loss: 1.611661	Accuracy: 18.73%
8	Validation loss: 1.639863	Best loss: 1.611661	Accuracy: 22.01%
9	Validation loss: 1.611894	Best loss: 1.611661	Accuracy: 20.91%
10	Validation loss: 1.613292	Best loss: 1.611661	Accuracy: 22.01%
11	Validation loss: 1.614465	Best loss: 1.611661	Accuracy: 22.01%
12	Validation loss: 1.615519	Best loss: 1.611661	Accuracy: 20.91%
13	Validation loss: 1.609593	Best loss: 1.609593	Accuracy: 22.01%
14	Validation loss: 1.628134	Best loss: 1.609593	Accuracy: 22.01%
15	Validation loss: 1.613723	Best loss: 1.609593	Accuracy: 19.08%
16	Validation loss: 1.621646	Best loss: 1.609593	Accuracy: 22.01%
17	Validation loss: 1.612428	Best loss: 1.609593	Accuracy: 19.08%
18	Validation loss: 1.622170	Best loss: 1.609593	Accuracy: 22.01%
19	Validation loss: 1.611825	Best loss: 1.609593	Accuracy: 22.01%
20	Validation loss: 1.612158	Best loss: 1.609593	Accuracy: 20.91%
21	Validation loss: 1.657981	Best loss: 1.609593	Accuracy: 22.01%
22	Validation loss: 1.643383	Best loss: 1.609593	Accuracy: 19.27%
23	Validation loss: 1.647554	Best loss: 1.609593	Accuracy: 22.01%
24	Validation loss: 1.652203	Best loss: 1.609593	Accuracy: 18.73%
25	Validation loss: 1.610976	Best loss: 1.609593	Accuracy: 20.91%
26	Validation loss: 1.614098	Best loss: 1.609593	Accuracy: 20.91%
27	Validation loss: 1.614371	Best loss: 1.609593	Accuracy: 20.91%
28	Validation loss: 1.642984	Best loss: 1.609593	Accuracy: 19.08%
29	Validation loss: 1.614041	Best loss: 1.609593	Accuracy: 19.08%
30	Validation loss: 1.609285	Best loss: 1.609285	Accuracy: 22.01%
31	Validation loss: 1.634894	Best loss: 1.609285	Accuracy: 18.73%
32	Validation loss: 1.614006	Best loss: 1.609285	Accuracy: 19.08%
33	Validation loss: 1.619314	Best loss: 1.609285	Accuracy: 19.08%
34	Validation loss: 1.611677	Best loss: 1.609285	Accuracy: 22.01%
35	Validation loss: 1.638075	Best loss: 1.609285	Accuracy: 20.91%
36	Validation loss: 1.621402	Best loss: 1.609285	Accuracy: 22.01%
37	Validation loss: 1.617570	Best loss: 1.609285	Accuracy: 22.01%
38	Validation loss: 1.620770	Best loss: 1.609285	Accuracy: 20.91%
39	Validation loss: 1.627340	Best loss: 1.609285	Accuracy: 19.27%
40	Validation loss: 1.607887	Best loss: 1.607887	Accuracy: 22.01%
41	Validation loss: 1.634036	Best loss: 1.607887	Accuracy: 22.01%
42	Validation loss: 1.618335	Best loss: 1.607887	Accuracy: 20.91%
43	Validation loss: 1.618744	Best loss: 1.607887	Accuracy: 22.01%
44	Validation loss: 1.621666	Best loss: 1.607887	Accuracy: 19.08%
45	Validation loss: 1.629387	Best loss: 1.607887	Accuracy: 22.01%
46	Validation loss: 1.637660	Best loss: 1.607887	Accuracy: 18.73%
47	Validation loss: 1.609609	Best loss: 1.607887	Accuracy: 22.01%
48	Validation loss: 1.612161	Best loss: 1.607887	Accuracy: 22.01%
49	Validation loss: 1.643130	Best loss: 1.607887	Accuracy: 19.27%
50	Validation loss: 1.611989	Best loss: 1.607887	Accuracy: 22.01%
51	Validation loss: 1.619973	Best loss: 1.607887	Accuracy: 19.08%
52	Validation loss: 1.627019	Best loss: 1.607887	Accuracy: 19.08%
53	Validation loss: 1.618522	Best loss: 1.607887	Accuracy: 22.01%
54	Validation loss: 1.654810	Best loss: 1.607887	Accuracy: 18.73%
55	Validation loss: 1.645115	Best loss: 1.607887	Accuracy: 18.73%
56	Validation loss: 1.669473	Best loss: 1.607887	Accuracy: 19.08%
57	Validation loss: 1.619665	Best loss: 1.607887	Accuracy: 18.73%
58	Validation loss: 1.612546	Best loss: 1.607887	Accuracy: 22.01%
59	Validation loss: 1.619512	Best loss: 1.607887	Accuracy: 22.01%
60	Validation loss: 1.635963	Best loss: 1.607887	Accuracy: 18.73%
61	Validation loss: 1.627398	Best loss: 1.607887	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total= 1.7min
[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.644702	Best loss: 1.644702	Accuracy: 19.27%
1	Validation loss: 1.624363	Best loss: 1.624363	Accuracy: 19.27%
2	Validation loss: 1.614131	Best loss: 1.614131	Accuracy: 19.27%
3	Validation loss: 1.616997	Best loss: 1.614131	Accuracy: 19.27%
4	Validation loss: 1.614503	Best loss: 1.614131	Accuracy: 22.01%
5	Validation loss: 1.627699	Best loss: 1.614131	Accuracy: 22.01%
6	Validation loss: 1.648675	Best loss: 1.614131	Accuracy: 19.27%
7	Validation loss: 1.627600	Best loss: 1.614131	Accuracy: 18.73%
8	Validation loss: 1.625684	Best loss: 1.614131	Accuracy: 22.01%
9	Validation loss: 1.615554	Best loss: 1.614131	Accuracy: 20.91%
10	Validation loss: 1.619121	Best loss: 1.614131	Accuracy: 22.01%
11	Validation loss: 1.639597	Best loss: 1.614131	Accuracy: 22.01%
12	Validation loss: 1.627611	Best loss: 1.614131	Accuracy: 19.08%
13	Validation loss: 1.615012	Best loss: 1.614131	Accuracy: 19.08%
14	Validation loss: 1.624332	Best loss: 1.614131	Accuracy: 19.08%
15	Validation loss: 1.612443	Best loss: 1.612443	Accuracy: 22.01%
16	Validation loss: 1.627838	Best loss: 1.612443	Accuracy: 22.01%
17	Validation loss: 1.613231	Best loss: 1.612443	Accuracy: 18.73%
18	Validation loss: 1.615750	Best loss: 1.612443	Accuracy: 22.01%
19	Validation loss: 1.619094	Best loss: 1.612443	Accuracy: 22.01%
20	Validation loss: 1.609112	Best loss: 1.609112	Accuracy: 22.01%
21	Validation loss: 1.689015	Best loss: 1.609112	Accuracy: 22.01%
22	Validation loss: 1.618887	Best loss: 1.609112	Accuracy: 18.73%
23	Validation loss: 1.638923	Best loss: 1.609112	Accuracy: 22.01%
24	Validation loss: 1.660611	Best loss: 1.609112	Accuracy: 20.91%
25	Validation loss: 1.619327	Best loss: 1.609112	Accuracy: 19.27%
26	Validation loss: 1.611328	Best loss: 1.609112	Accuracy: 22.01%
27	Validation loss: 1.614297	Best loss: 1.609112	Accuracy: 20.91%
28	Validation loss: 1.628115	Best loss: 1.609112	Accuracy: 19.08%
29	Validation loss: 1.610351	Best loss: 1.609112	Accuracy: 20.91%
30	Validation loss: 1.626940	Best loss: 1.609112	Accuracy: 20.91%
31	Validation loss: 1.633271	Best loss: 1.609112	Accuracy: 18.73%
32	Validation loss: 1.609059	Best loss: 1.609059	Accuracy: 22.01%
33	Validation loss: 1.610649	Best loss: 1.609059	Accuracy: 19.08%
34	Validation loss: 1.616985	Best loss: 1.609059	Accuracy: 22.01%
35	Validation loss: 1.627558	Best loss: 1.609059	Accuracy: 19.27%
36	Validation loss: 1.609164	Best loss: 1.609059	Accuracy: 20.91%
37	Validation loss: 1.626446	Best loss: 1.609059	Accuracy: 19.27%
38	Validation loss: 1.614500	Best loss: 1.609059	Accuracy: 22.01%
39	Validation loss: 1.617748	Best loss: 1.609059	Accuracy: 19.27%
40	Validation loss: 1.623124	Best loss: 1.609059	Accuracy: 22.01%
41	Validation loss: 1.615785	Best loss: 1.609059	Accuracy: 19.27%
42	Validation loss: 1.617481	Best loss: 1.609059	Accuracy: 19.27%
43	Validation loss: 1.617776	Best loss: 1.609059	Accuracy: 19.08%
44	Validation loss: 1.626857	Best loss: 1.609059	Accuracy: 22.01%
45	Validation loss: 1.613781	Best loss: 1.609059	Accuracy: 22.01%
46	Validation loss: 1.618688	Best loss: 1.609059	Accuracy: 22.01%
47	Validation loss: 1.610628	Best loss: 1.609059	Accuracy: 22.01%
48	Validation loss: 1.614394	Best loss: 1.609059	Accuracy: 18.73%
49	Validation loss: 1.612177	Best loss: 1.609059	Accuracy: 19.27%
50	Validation loss: 1.623884	Best loss: 1.609059	Accuracy: 19.27%
51	Validation loss: 1.616774	Best loss: 1.609059	Accuracy: 22.01%
52	Validation loss: 1.620593	Best loss: 1.609059	Accuracy: 18.73%
53	Validation loss: 1.617683	Best loss: 1.609059	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total= 1.5min
[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.078550	Best loss: 0.078550	Accuracy: 97.73%
1	Validation loss: 0.078343	Best loss: 0.078343	Accuracy: 97.62%
2	Validation loss: 0.090554	Best loss: 0.078343	Accuracy: 97.73%
3	Validation loss: 0.081384	Best loss: 0.078343	Accuracy: 97.77%
4	Validation loss: 0.073455	Best loss: 0.073455	Accuracy: 98.16%
5	Validation loss: 0.082281	Best loss: 0.073455	Accuracy: 97.97%
6	Validation loss: 0.078110	Best loss: 0.073455	Accuracy: 98.05%
7	Validation loss: 0.166926	Best loss: 0.073455	Accuracy: 96.13%
8	Validation loss: 0.079902	Best loss: 0.073455	Accuracy: 97.77%
9	Validation loss: 0.117198	Best loss: 0.073455	Accuracy: 97.46%
10	Validation loss: 0.100702	Best loss: 0.073455	Accuracy: 97.89%
11	Validation loss: 0.135164	Best loss: 0.073455	Accuracy: 96.25%
12	Validation loss: 0.117619	Best loss: 0.073455	Accuracy: 96.91%
13	Validation loss: 0.104963	Best loss: 0.073455	Accuracy: 97.26%
14	Validation loss: 0.091707	Best loss: 0.073455	Accuracy: 97.93%
15	Validation loss: 0.100199	Best loss: 0.073455	Accuracy: 97.38%
16	Validation loss: 0.081302	Best loss: 0.073455	Accuracy: 98.08%
17	Validation loss: 0.095125	Best loss: 0.073455	Accuracy: 97.62%
18	Validation loss: 0.108007	Best loss: 0.073455	Accuracy: 97.93%
19	Validation loss: 0.082095	Best loss: 0.073455	Accuracy: 97.81%
20	Validation loss: 0.094520	Best loss: 0.073455	Accuracy: 98.24%
21	Validation loss: 0.089198	Best loss: 0.073455	Accuracy: 98.20%
22	Validation loss: 0.067610	Best loss: 0.067610	Accuracy: 98.40%
23	Validation loss: 0.093824	Best loss: 0.067610	Accuracy: 98.28%
24	Validation loss: 0.080660	Best loss: 0.067610	Accuracy: 98.20%
25	Validation loss: 0.105736	Best loss: 0.067610	Accuracy: 97.97%
26	Validation loss: 0.094569	Best loss: 0.067610	Accuracy: 98.05%
27	Validation loss: 0.088745	Best loss: 0.067610	Accuracy: 98.05%
28	Validation loss: 0.100566	Best loss: 0.067610	Accuracy: 98.12%
29	Validation loss: 0.100837	Best loss: 0.067610	Accuracy: 97.69%
30	Validation loss: 0.089960	Best loss: 0.067610	Accuracy: 98.32%
31	Validation loss: 0.080266	Best loss: 0.067610	Accuracy: 98.24%
32	Validation loss: 0.086318	Best loss: 0.067610	Accuracy: 98.05%
33	Validation loss: 0.087818	Best loss: 0.067610	Accuracy: 98.12%
34	Validation loss: 0.107289	Best loss: 0.067610	Accuracy: 97.89%
35	Validation loss: 0.133042	Best loss: 0.067610	Accuracy: 98.36%
36	Validation loss: 0.078993	Best loss: 0.067610	Accuracy: 98.16%
37	Validation loss: 0.080012	Best loss: 0.067610	Accuracy: 98.44%
38	Validation loss: 0.086776	Best loss: 0.067610	Accuracy: 98.48%
39	Validation loss: 0.128176	Best loss: 0.067610	Accuracy: 97.62%
40	Validation loss: 0.200972	Best loss: 0.067610	Accuracy: 98.01%
41	Validation loss: 0.109124	Best loss: 0.067610	Accuracy: 98.20%
42	Validation loss: 0.089580	Best loss: 0.067610	Accuracy: 98.36%
43	Validation loss: 0.084378	Best loss: 0.067610	Accuracy: 98.28%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  12.4s
[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.129494	Best loss: 0.129494	Accuracy: 96.33%
1	Validation loss: 0.082325	Best loss: 0.082325	Accuracy: 97.73%
2	Validation loss: 0.079789	Best loss: 0.079789	Accuracy: 97.54%
3	Validation loss: 0.106100	Best loss: 0.079789	Accuracy: 97.38%
4	Validation loss: 0.114749	Best loss: 0.079789	Accuracy: 97.34%
5	Validation loss: 0.095024	Best loss: 0.079789	Accuracy: 97.58%
6	Validation loss: 0.083856	Best loss: 0.079789	Accuracy: 98.08%
7	Validation loss: 0.079762	Best loss: 0.079762	Accuracy: 98.05%
8	Validation loss: 0.092993	Best loss: 0.079762	Accuracy: 98.16%
9	Validation loss: 0.111594	Best loss: 0.079762	Accuracy: 97.85%
10	Validation loss: 0.072535	Best loss: 0.072535	Accuracy: 98.44%
11	Validation loss: 0.105637	Best loss: 0.072535	Accuracy: 97.11%
12	Validation loss: 0.070952	Best loss: 0.070952	Accuracy: 97.69%
13	Validation loss: 0.077082	Best loss: 0.070952	Accuracy: 98.01%
14	Validation loss: 0.079732	Best loss: 0.070952	Accuracy: 98.28%
15	Validation loss: 0.093847	Best loss: 0.070952	Accuracy: 98.16%
16	Validation loss: 0.084058	Best loss: 0.070952	Accuracy: 98.36%
17	Validation loss: 0.116975	Best loss: 0.070952	Accuracy: 97.69%
18	Validation loss: 0.082882	Best loss: 0.070952	Accuracy: 97.89%
19	Validation loss: 0.119219	Best loss: 0.070952	Accuracy: 97.85%
20	Validation loss: 0.133494	Best loss: 0.070952	Accuracy: 97.58%
21	Validation loss: 0.118746	Best loss: 0.070952	Accuracy: 97.15%
22	Validation loss: 0.144742	Best loss: 0.070952	Accuracy: 95.90%
23	Validation loss: 0.091479	Best loss: 0.070952	Accuracy: 97.85%
24	Validation loss: 0.104808	Best loss: 0.070952	Accuracy: 97.77%
25	Validation loss: 0.146654	Best loss: 0.070952	Accuracy: 96.48%
26	Validation loss: 0.130551	Best loss: 0.070952	Accuracy: 96.99%
27	Validation loss: 0.166807	Best loss: 0.070952	Accuracy: 96.95%
28	Validation loss: 0.163758	Best loss: 0.070952	Accuracy: 95.70%
29	Validation loss: 0.119599	Best loss: 0.070952	Accuracy: 97.11%
30	Validation loss: 0.146869	Best loss: 0.070952	Accuracy: 97.38%
31	Validation loss: 0.211143	Best loss: 0.070952	Accuracy: 94.76%
32	Validation loss: 0.270048	Best loss: 0.070952	Accuracy: 93.82%
33	Validation loss: 0.260073	Best loss: 0.070952	Accuracy: 93.16%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   9.2s
[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.085739	Best loss: 0.085739	Accuracy: 98.01%
1	Validation loss: 0.083467	Best loss: 0.083467	Accuracy: 97.89%
2	Validation loss: 0.110032	Best loss: 0.083467	Accuracy: 97.19%
3	Validation loss: 0.070999	Best loss: 0.070999	Accuracy: 97.81%
4	Validation loss: 0.080186	Best loss: 0.070999	Accuracy: 98.24%
5	Validation loss: 0.069961	Best loss: 0.069961	Accuracy: 98.32%
6	Validation loss: 0.066667	Best loss: 0.066667	Accuracy: 98.16%
7	Validation loss: 0.069066	Best loss: 0.066667	Accuracy: 98.40%
8	Validation loss: 0.100259	Best loss: 0.066667	Accuracy: 97.97%
9	Validation loss: 0.109054	Best loss: 0.066667	Accuracy: 97.19%
10	Validation loss: 0.095701	Best loss: 0.066667	Accuracy: 97.54%
11	Validation loss: 0.099834	Best loss: 0.066667	Accuracy: 97.50%
12	Validation loss: 0.090669	Best loss: 0.066667	Accuracy: 97.89%
13	Validation loss: 0.089493	Best loss: 0.066667	Accuracy: 98.01%
14	Validation loss: 0.079547	Best loss: 0.066667	Accuracy: 98.12%
15	Validation loss: 0.099282	Best loss: 0.066667	Accuracy: 97.85%
16	Validation loss: 0.095696	Best loss: 0.066667	Accuracy: 97.58%
17	Validation loss: 0.073424	Best loss: 0.066667	Accuracy: 98.40%
18	Validation loss: 0.078910	Best loss: 0.066667	Accuracy: 98.40%
19	Validation loss: 0.114018	Best loss: 0.066667	Accuracy: 98.51%
20	Validation loss: 0.116281	Best loss: 0.066667	Accuracy: 97.65%
21	Validation loss: 0.098496	Best loss: 0.066667	Accuracy: 97.89%
22	Validation loss: 0.094655	Best loss: 0.066667	Accuracy: 98.01%
23	Validation loss: 0.098031	Best loss: 0.066667	Accuracy: 98.01%
24	Validation loss: 0.117206	Best loss: 0.066667	Accuracy: 98.05%
25	Validation loss: 0.087987	Best loss: 0.066667	Accuracy: 98.12%
26	Validation loss: 0.192290	Best loss: 0.066667	Accuracy: 96.25%
27	Validation loss: 0.102447	Best loss: 0.066667	Accuracy: 97.03%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   6.7s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.171932	Best loss: 0.171932	Accuracy: 96.09%
1	Validation loss: 1.038886	Best loss: 0.171932	Accuracy: 53.17%
2	Validation loss: 0.680028	Best loss: 0.171932	Accuracy: 73.42%
3	Validation loss: 0.438730	Best loss: 0.171932	Accuracy: 83.78%
4	Validation loss: 0.355981	Best loss: 0.171932	Accuracy: 88.15%
5	Validation loss: 0.384783	Best loss: 0.171932	Accuracy: 88.94%
6	Validation loss: 117.547462	Best loss: 0.171932	Accuracy: 19.27%
7	Validation loss: 119.098175	Best loss: 0.171932	Accuracy: 29.79%
8	Validation loss: 3.429434	Best loss: 0.171932	Accuracy: 57.43%
9	Validation loss: 211.094406	Best loss: 0.171932	Accuracy: 24.75%
10	Validation loss: 15.742632	Best loss: 0.171932	Accuracy: 48.75%
11	Validation loss: 7.858672	Best loss: 0.171932	Accuracy: 52.42%
12	Validation loss: 18.902439	Best loss: 0.171932	Accuracy: 37.14%
13	Validation loss: 10.846861	Best loss: 0.171932	Accuracy: 56.06%
14	Validation loss: 3.999559	Best loss: 0.171932	Accuracy: 63.64%
15	Validation loss: 6.790777	Best loss: 0.171932	Accuracy: 46.83%
16	Validation loss: 5.565273	Best loss: 0.171932	Accuracy: 57.19%
17	Validation loss: 3.148133	Best loss: 0.171932	Accuracy: 64.97%
18	Validation loss: 2.817183	Best loss: 0.171932	Accuracy: 65.72%
19	Validation loss: 3.476157	Best loss: 0.171932	Accuracy: 62.35%
20	Validation loss: 5.238483	Best loss: 0.171932	Accuracy: 50.20%
21	Validation loss: 2.675043	Best loss: 0.171932	Accuracy: 66.38%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   8.0s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.180804	Best loss: 0.180804	Accuracy: 95.07%
1	Validation loss: 0.107859	Best loss: 0.107859	Accuracy: 96.99%
2	Validation loss: 0.137847	Best loss: 0.107859	Accuracy: 95.97%
3	Validation loss: 82.405571	Best loss: 0.107859	Accuracy: 52.93%
4	Validation loss: 101.892464	Best loss: 0.107859	Accuracy: 19.66%
5	Validation loss: 17.118782	Best loss: 0.107859	Accuracy: 24.43%
6	Validation loss: 1.953578	Best loss: 0.107859	Accuracy: 58.84%
7	Validation loss: 2.356493	Best loss: 0.107859	Accuracy: 42.69%
8	Validation loss: 1.258095	Best loss: 0.107859	Accuracy: 60.83%
9	Validation loss: 2.109886	Best loss: 0.107859	Accuracy: 62.24%
10	Validation loss: 0.936943	Best loss: 0.107859	Accuracy: 62.20%
11	Validation loss: 0.929063	Best loss: 0.107859	Accuracy: 61.53%
12	Validation loss: 0.974701	Best loss: 0.107859	Accuracy: 64.97%
13	Validation loss: 1.108398	Best loss: 0.107859	Accuracy: 63.96%
14	Validation loss: 0.756348	Best loss: 0.107859	Accuracy: 72.05%
15	Validation loss: 0.742958	Best loss: 0.107859	Accuracy: 74.20%
16	Validation loss: 0.753458	Best loss: 0.107859	Accuracy: 70.64%
17	Validation loss: 0.634644	Best loss: 0.107859	Accuracy: 74.71%
18	Validation loss: 1.051074	Best loss: 0.107859	Accuracy: 71.31%
19	Validation loss: 0.469943	Best loss: 0.107859	Accuracy: 81.59%
20	Validation loss: 0.430562	Best loss: 0.107859	Accuracy: 84.48%
21	Validation loss: 0.383880	Best loss: 0.107859	Accuracy: 85.22%
22	Validation loss: 0.498660	Best loss: 0.107859	Accuracy: 82.41%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   7.6s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.164369	Best loss: 0.164369	Accuracy: 95.43%
1	Validation loss: 0.129233	Best loss: 0.129233	Accuracy: 96.56%
2	Validation loss: 135.679459	Best loss: 0.129233	Accuracy: 20.68%
3	Validation loss: 4.386415	Best loss: 0.129233	Accuracy: 43.32%
4	Validation loss: 1.554753	Best loss: 0.129233	Accuracy: 54.57%
5	Validation loss: 0.711188	Best loss: 0.129233	Accuracy: 65.32%
6	Validation loss: 0.756422	Best loss: 0.129233	Accuracy: 69.43%
7	Validation loss: 0.683508	Best loss: 0.129233	Accuracy: 66.81%
8	Validation loss: 0.666013	Best loss: 0.129233	Accuracy: 67.20%
9	Validation loss: 0.640476	Best loss: 0.129233	Accuracy: 73.85%
10	Validation loss: 0.578091	Best loss: 0.129233	Accuracy: 77.68%
11	Validation loss: 1.003106	Best loss: 0.129233	Accuracy: 66.65%
12	Validation loss: 0.755255	Best loss: 0.129233	Accuracy: 68.22%
13	Validation loss: 0.705561	Best loss: 0.129233	Accuracy: 76.82%
14	Validation loss: 0.487373	Best loss: 0.129233	Accuracy: 83.19%
15	Validation loss: 0.424385	Best loss: 0.129233	Accuracy: 86.59%
16	Validation loss: 0.380428	Best loss: 0.129233	Accuracy: 87.45%
17	Validation loss: 133.669022	Best loss: 0.129233	Accuracy: 19.27%
18	Validation loss: 1.399158	Best loss: 0.129233	Accuracy: 56.92%
19	Validation loss: 1.688156	Best loss: 0.129233	Accuracy: 57.94%
20	Validation loss: 1.115087	Best loss: 0.129233	Accuracy: 63.45%
21	Validation loss: 1.121054	Best loss: 0.129233	Accuracy: 63.06%
22	Validation loss: 0.829359	Best loss: 0.129233	Accuracy: 71.50%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   7.7s
[CV] n_neurons=50, learning_rate=0.01, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.097395	Best loss: 0.097395	Accuracy: 97.15%
1	Validation loss: 0.082138	Best loss: 0.082138	Accuracy: 97.73%
2	Validation loss: 0.065578	Best loss: 0.065578	Accuracy: 98.44%
3	Validation loss: 0.141438	Best loss: 0.065578	Accuracy: 97.03%
4	Validation loss: 0.066911	Best loss: 0.065578	Accuracy: 98.48%
5	Validation loss: 0.081286	Best loss: 0.065578	Accuracy: 98.05%
6	Validation loss: 0.156696	Best loss: 0.065578	Accuracy: 95.39%
7	Validation loss: 0.149376	Best loss: 0.065578	Accuracy: 98.20%
8	Validation loss: 0.087414	Best loss: 0.065578	Accuracy: 98.24%
9	Validation loss: 0.064247	Best loss: 0.064247	Accuracy: 98.59%
10	Validation loss: 0.088529	Best loss: 0.064247	Accuracy: 98.20%
11	Validation loss: 0.118210	Best loss: 0.064247	Accuracy: 97.81%
12	Validation loss: 0.103483	Best loss: 0.064247	Accuracy: 98.67%
13	Validation loss: 0.152536	Best loss: 0.064247	Accuracy: 97.30%
14	Validation loss: 0.135260	Best loss: 0.064247	Accuracy: 97.81%
15	Validation loss: 0.144709	Best loss: 0.064247	Accuracy: 98.63%
16	Validation loss: 0.092648	Best loss: 0.064247	Accuracy: 98.40%
17	Validation loss: 0.166917	Best loss: 0.064247	Accuracy: 96.44%
18	Validation loss: 8.939317	Best loss: 0.064247	Accuracy: 94.84%
19	Validation loss: 0.475701	Best loss: 0.064247	Accuracy: 96.44%
20	Validation loss: 0.325683	Best loss: 0.064247	Accuracy: 98.01%
21	Validation loss: 0.276909	Best loss: 0.064247	Accuracy: 98.01%
22	Validation loss: 0.399500	Best loss: 0.064247	Accuracy: 97.58%
23	Validation loss: 0.533994	Best loss: 0.064247	Accuracy: 97.30%
24	Validation loss: 1.214290	Best loss: 0.064247	Accuracy: 97.42%
25	Validation loss: 0.335489	Best loss: 0.064247	Accuracy: 97.93%
26	Validation loss: 0.283033	Best loss: 0.064247	Accuracy: 97.93%
27	Validation loss: 0.181035	Best loss: 0.064247	Accuracy: 97.77%
28	Validation loss: 0.228483	Best loss: 0.064247	Accuracy: 98.36%
29	Validation loss: 0.249921	Best loss: 0.064247	Accuracy: 98.44%
30	Validation loss: 0.955154	Best loss: 0.064247	Accuracy: 94.80%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  17.0s
[CV] n_neurons=50, learning_rate=0.01, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.104407	Best loss: 0.104407	Accuracy: 96.99%
1	Validation loss: 0.075187	Best loss: 0.075187	Accuracy: 98.05%
2	Validation loss: 0.062755	Best loss: 0.062755	Accuracy: 98.24%
3	Validation loss: 0.075989	Best loss: 0.062755	Accuracy: 98.44%
4	Validation loss: 0.739471	Best loss: 0.062755	Accuracy: 93.67%
5	Validation loss: 0.073977	Best loss: 0.062755	Accuracy: 98.08%
6	Validation loss: 0.055494	Best loss: 0.055494	Accuracy: 98.28%
7	Validation loss: 0.089304	Best loss: 0.055494	Accuracy: 98.12%
8	Validation loss: 0.079445	Best loss: 0.055494	Accuracy: 98.24%
9	Validation loss: 0.096176	Best loss: 0.055494	Accuracy: 98.01%
10	Validation loss: 0.089028	Best loss: 0.055494	Accuracy: 98.24%
11	Validation loss: 0.093192	Best loss: 0.055494	Accuracy: 98.51%
12	Validation loss: 2.956207	Best loss: 0.055494	Accuracy: 91.52%
13	Validation loss: 0.178766	Best loss: 0.055494	Accuracy: 96.83%
14	Validation loss: 0.099225	Best loss: 0.055494	Accuracy: 98.36%
15	Validation loss: 0.095734	Best loss: 0.055494	Accuracy: 98.48%
16	Validation loss: 0.118818	Best loss: 0.055494	Accuracy: 98.24%
17	Validation loss: 0.121784	Best loss: 0.055494	Accuracy: 98.51%
18	Validation loss: 0.376779	Best loss: 0.055494	Accuracy: 96.25%
19	Validation loss: 0.109155	Best loss: 0.055494	Accuracy: 98.24%
20	Validation loss: 0.140088	Best loss: 0.055494	Accuracy: 98.67%
21	Validation loss: 0.110600	Best loss: 0.055494	Accuracy: 98.71%
22	Validation loss: 0.091074	Best loss: 0.055494	Accuracy: 98.67%
23	Validation loss: 0.130935	Best loss: 0.055494	Accuracy: 98.87%
24	Validation loss: 0.623751	Best loss: 0.055494	Accuracy: 95.70%
25	Validation loss: 0.265153	Best loss: 0.055494	Accuracy: 98.16%
26	Validation loss: 0.303658	Best loss: 0.055494	Accuracy: 98.05%
27	Validation loss: 0.311030	Best loss: 0.055494	Accuracy: 98.40%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  15.2s
[CV] n_neurons=50, learning_rate=0.01, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.113774	Best loss: 0.113774	Accuracy: 96.52%
1	Validation loss: 0.081518	Best loss: 0.081518	Accuracy: 97.73%
2	Validation loss: 0.068409	Best loss: 0.068409	Accuracy: 98.12%
3	Validation loss: 0.171678	Best loss: 0.068409	Accuracy: 96.76%
4	Validation loss: 0.189425	Best loss: 0.068409	Accuracy: 96.87%
5	Validation loss: 0.093827	Best loss: 0.068409	Accuracy: 97.81%
6	Validation loss: 0.117298	Best loss: 0.068409	Accuracy: 97.85%
7	Validation loss: 0.079396	Best loss: 0.068409	Accuracy: 98.24%
8	Validation loss: 0.074437	Best loss: 0.068409	Accuracy: 98.44%
9	Validation loss: 0.197279	Best loss: 0.068409	Accuracy: 98.51%
10	Validation loss: 0.254823	Best loss: 0.068409	Accuracy: 97.77%
11	Validation loss: 0.094301	Best loss: 0.068409	Accuracy: 97.85%
12	Validation loss: 3.873534	Best loss: 0.068409	Accuracy: 94.57%
13	Validation loss: 0.109859	Best loss: 0.068409	Accuracy: 98.12%
14	Validation loss: 0.141363	Best loss: 0.068409	Accuracy: 97.30%
15	Validation loss: 0.086170	Best loss: 0.068409	Accuracy: 98.48%
16	Validation loss: 0.098948	Best loss: 0.068409	Accuracy: 98.44%
17	Validation loss: 0.105253	Best loss: 0.068409	Accuracy: 98.44%
18	Validation loss: 0.101155	Best loss: 0.068409	Accuracy: 98.55%
19	Validation loss: 0.169691	Best loss: 0.068409	Accuracy: 98.05%
20	Validation loss: 0.106316	Best loss: 0.068409	Accuracy: 98.16%
21	Validation loss: 0.176682	Best loss: 0.068409	Accuracy: 98.36%
22	Validation loss: 0.102204	Best loss: 0.068409	Accuracy: 98.51%
23	Validation loss: 0.130347	Best loss: 0.068409	Accuracy: 98.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  12.1s
[CV] n_neurons=120, learning_rate=0.02, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 13.252987	Best loss: 13.252987	Accuracy: 90.23%
1	Validation loss: 30.856510	Best loss: 13.252987	Accuracy: 88.62%
2	Validation loss: 477.997498	Best loss: 13.252987	Accuracy: 93.55%
3	Validation loss: 2986.771240	Best loss: 13.252987	Accuracy: 90.19%
4	Validation loss: 830.171204	Best loss: 13.252987	Accuracy: 86.71%
5	Validation loss: 196.935577	Best loss: 13.252987	Accuracy: 96.25%
6	Validation loss: 803.824646	Best loss: 13.252987	Accuracy: 95.78%
7	Validation loss: 344.679535	Best loss: 13.252987	Accuracy: 96.60%
8	Validation loss: 331.148315	Best loss: 13.252987	Accuracy: 95.78%
9	Validation loss: 10138.730469	Best loss: 13.252987	Accuracy: 96.95%
10	Validation loss: 3498.079346	Best loss: 13.252987	Accuracy: 95.27%
11	Validation loss: 1146.324707	Best loss: 13.252987	Accuracy: 95.39%
12	Validation loss: 4479.070312	Best loss: 13.252987	Accuracy: 93.82%
13	Validation loss: 1762.231201	Best loss: 13.252987	Accuracy: 96.09%
14	Validation loss: 1204.038818	Best loss: 13.252987	Accuracy: 97.42%
15	Validation loss: 1455.648193	Best loss: 13.252987	Accuracy: 94.10%
16	Validation loss: 2959.973145	Best loss: 13.252987	Accuracy: 96.56%
17	Validation loss: 2230.365967	Best loss: 13.252987	Accuracy: 96.72%
18	Validation loss: 4546.692871	Best loss: 13.252987	Accuracy: 97.85%
19	Validation loss: 3013.716309	Best loss: 13.252987	Accuracy: 96.25%
20	Validation loss: 79526.898438	Best loss: 13.252987	Accuracy: 94.88%
21	Validation loss: 15440.260742	Best loss: 13.252987	Accuracy: 97.77%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.02, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  57.1s
[CV] n_neurons=120, learning_rate=0.02, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.275512	Best loss: 0.275512	Accuracy: 94.57%
1	Validation loss: 2458.208740	Best loss: 0.275512	Accuracy: 90.27%
2	Validation loss: 1160.022827	Best loss: 0.275512	Accuracy: 89.84%
3	Validation loss: 90.559357	Best loss: 0.275512	Accuracy: 94.49%
4	Validation loss: 1647.384644	Best loss: 0.275512	Accuracy: 94.61%
5	Validation loss: 355.847260	Best loss: 0.275512	Accuracy: 96.60%
6	Validation loss: 470.135742	Best loss: 0.275512	Accuracy: 96.79%
7	Validation loss: 3147.360840	Best loss: 0.275512	Accuracy: 95.19%
8	Validation loss: 606.044983	Best loss: 0.275512	Accuracy: 95.35%
9	Validation loss: 535.478333	Best loss: 0.275512	Accuracy: 95.74%
10	Validation loss: 4918.682617	Best loss: 0.275512	Accuracy: 84.52%
11	Validation loss: 400.342285	Best loss: 0.275512	Accuracy: 96.13%
12	Validation loss: 1782.666504	Best loss: 0.275512	Accuracy: 96.25%
13	Validation loss: 3425.582520	Best loss: 0.275512	Accuracy: 94.45%
14	Validation loss: 2106.623047	Best loss: 0.275512	Accuracy: 95.39%
15	Validation loss: 1759.347534	Best loss: 0.275512	Accuracy: 96.91%
16	Validation loss: 74986.953125	Best loss: 0.275512	Accuracy: 84.17%
17	Validation loss: 7008.572266	Best loss: 0.275512	Accuracy: 96.91%
18	Validation loss: 7867.869629	Best loss: 0.275512	Accuracy: 97.65%
19	Validation loss: 5067.838867	Best loss: 0.275512	Accuracy: 97.50%
20	Validation loss: 10995.084961	Best loss: 0.275512	Accuracy: 93.16%
21	Validation loss: 3285.040283	Best loss: 0.275512	Accuracy: 97.65%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.02, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  54.8s
[CV] n_neurons=120, learning_rate=0.02, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 3.562732	Best loss: 3.562732	Accuracy: 73.81%
1	Validation loss: 77.857300	Best loss: 3.562732	Accuracy: 94.25%
2	Validation loss: 147.026993	Best loss: 3.562732	Accuracy: 94.64%
3	Validation loss: 51.536034	Best loss: 3.562732	Accuracy: 94.96%
4	Validation loss: 41.498760	Best loss: 3.562732	Accuracy: 94.25%
5	Validation loss: 1022.556763	Best loss: 3.562732	Accuracy: 95.66%
6	Validation loss: 335.832001	Best loss: 3.562732	Accuracy: 96.48%
7	Validation loss: 600.618530	Best loss: 3.562732	Accuracy: 96.40%
8	Validation loss: 2771.998535	Best loss: 3.562732	Accuracy: 87.41%
9	Validation loss: 472.547882	Best loss: 3.562732	Accuracy: 96.48%
10	Validation loss: 3690.787354	Best loss: 3.562732	Accuracy: 95.90%
11	Validation loss: 3017.259033	Best loss: 3.562732	Accuracy: 94.76%
12	Validation loss: 1449.037109	Best loss: 3.562732	Accuracy: 96.72%
13	Validation loss: 18061.894531	Best loss: 3.562732	Accuracy: 96.72%
14	Validation loss: 1819.477905	Best loss: 3.562732	Accuracy: 96.68%
15	Validation loss: 5281.603027	Best loss: 3.562732	Accuracy: 96.52%
16	Validation loss: 4043.333008	Best loss: 3.562732	Accuracy: 92.81%
17	Validation loss: 13227.490234	Best loss: 3.562732	Accuracy: 86.83%
18	Validation loss: 1446.238037	Best loss: 3.562732	Accuracy: 96.56%
19	Validation loss: 3757.952393	Best loss: 3.562732	Accuracy: 97.07%
20	Validation loss: 123484.773438	Best loss: 3.562732	Accuracy: 88.27%
21	Validation loss: 3108.786865	Best loss: 3.562732	Accuracy: 97.34%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.02, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  54.1s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.471161	Best loss: 1.471161	Accuracy: 31.55%
1	Validation loss: 0.911543	Best loss: 0.911543	Accuracy: 57.11%
2	Validation loss: 0.686715	Best loss: 0.686715	Accuracy: 74.86%
3	Validation loss: 0.594151	Best loss: 0.594151	Accuracy: 75.14%
4	Validation loss: 0.572818	Best loss: 0.572818	Accuracy: 76.31%
5	Validation loss: 0.501630	Best loss: 0.501630	Accuracy: 78.15%
6	Validation loss: 2.142648	Best loss: 0.501630	Accuracy: 22.01%
7	Validation loss: 1.638075	Best loss: 0.501630	Accuracy: 22.01%
8	Validation loss: 1.623386	Best loss: 0.501630	Accuracy: 22.01%
9	Validation loss: 1.616580	Best loss: 0.501630	Accuracy: 19.27%
10	Validation loss: 1.611831	Best loss: 0.501630	Accuracy: 22.01%
11	Validation loss: 1.611434	Best loss: 0.501630	Accuracy: 19.27%
12	Validation loss: 1.608670	Best loss: 0.501630	Accuracy: 20.91%
13	Validation loss: 1.611635	Best loss: 0.501630	Accuracy: 22.01%
14	Validation loss: 1.609822	Best loss: 0.501630	Accuracy: 22.01%
15	Validation loss: 1.619921	Best loss: 0.501630	Accuracy: 22.01%
16	Validation loss: 1.616171	Best loss: 0.501630	Accuracy: 19.08%
17	Validation loss: 1.613617	Best loss: 0.501630	Accuracy: 22.01%
18	Validation loss: 1.623765	Best loss: 0.501630	Accuracy: 19.27%
19	Validation loss: 1.619030	Best loss: 0.501630	Accuracy: 19.08%
20	Validation loss: 1.622882	Best loss: 0.501630	Accuracy: 20.91%
21	Validation loss: 1.608486	Best loss: 0.501630	Accuracy: 22.01%
22	Validation loss: 1.611815	Best loss: 0.501630	Accuracy: 19.08%
23	Validation loss: 1.612531	Best loss: 0.501630	Accuracy: 22.01%
24	Validation loss: 1.624218	Best loss: 0.501630	Accuracy: 22.01%
25	Validation loss: 1.616368	Best loss: 0.501630	Accuracy: 19.27%
26	Validation loss: 1.617670	Best loss: 0.501630	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   4.6s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.328480	Best loss: 1.328480	Accuracy: 43.71%
1	Validation loss: 0.614470	Best loss: 0.614470	Accuracy: 79.79%
2	Validation loss: 0.393559	Best loss: 0.393559	Accuracy: 84.52%
3	Validation loss: 0.274669	Best loss: 0.274669	Accuracy: 90.81%
4	Validation loss: 0.269803	Best loss: 0.269803	Accuracy: 91.16%
5	Validation loss: 0.293372	Best loss: 0.269803	Accuracy: 90.42%
6	Validation loss: 0.235249	Best loss: 0.235249	Accuracy: 92.69%
7	Validation loss: 0.199580	Best loss: 0.199580	Accuracy: 93.98%
8	Validation loss: 0.204801	Best loss: 0.199580	Accuracy: 93.71%
9	Validation loss: 0.210236	Best loss: 0.199580	Accuracy: 93.67%
10	Validation loss: 0.200994	Best loss: 0.199580	Accuracy: 93.75%
11	Validation loss: 0.168679	Best loss: 0.168679	Accuracy: 95.00%
12	Validation loss: 0.148922	Best loss: 0.148922	Accuracy: 95.50%
13	Validation loss: 0.197888	Best loss: 0.148922	Accuracy: 94.21%
14	Validation loss: 0.145279	Best loss: 0.145279	Accuracy: 96.21%
15	Validation loss: 0.125818	Best loss: 0.125818	Accuracy: 96.60%
16	Validation loss: 0.161170	Best loss: 0.125818	Accuracy: 95.62%
17	Validation loss: 0.145746	Best loss: 0.125818	Accuracy: 96.13%
18	Validation loss: 0.132611	Best loss: 0.125818	Accuracy: 96.79%
19	Validation loss: 0.147601	Best loss: 0.125818	Accuracy: 96.25%
20	Validation loss: 0.141092	Best loss: 0.125818	Accuracy: 96.83%
21	Validation loss: 0.131162	Best loss: 0.125818	Accuracy: 96.52%
22	Validation loss: 0.117126	Best loss: 0.117126	Accuracy: 96.95%
23	Validation loss: 0.149578	Best loss: 0.117126	Accuracy: 96.36%
24	Validation loss: 0.117559	Best loss: 0.117126	Accuracy: 96.95%
25	Validation loss: 0.130193	Best loss: 0.117126	Accuracy: 96.83%
26	Validation loss: 0.133021	Best loss: 0.117126	Accuracy: 96.99%
27	Validation loss: 0.123906	Best loss: 0.117126	Accuracy: 96.87%
28	Validation loss: 0.151778	Best loss: 0.117126	Accuracy: 96.72%
29	Validation loss: 0.175073	Best loss: 0.117126	Accuracy: 96.33%
30	Validation loss: 0.132625	Best loss: 0.117126	Accuracy: 96.95%
31	Validation loss: 0.147731	Best loss: 0.117126	Accuracy: 96.33%
32	Validation loss: 0.172049	Best loss: 0.117126	Accuracy: 95.11%
33	Validation loss: 0.158813	Best loss: 0.117126	Accuracy: 96.09%
34	Validation loss: 0.140673	Best loss: 0.117126	Accuracy: 96.83%
35	Validation loss: 0.153276	Best loss: 0.117126	Accuracy: 95.82%
36	Validation loss: 0.127205	Best loss: 0.117126	Accuracy: 96.87%
37	Validation loss: 4.061659	Best loss: 0.117126	Accuracy: 20.91%
38	Validation loss: 1.669226	Best loss: 0.117126	Accuracy: 22.01%
39	Validation loss: 1.610390	Best loss: 0.117126	Accuracy: 20.91%
40	Validation loss: 1.619733	Best loss: 0.117126	Accuracy: 22.01%
41	Validation loss: 1.613513	Best loss: 0.117126	Accuracy: 22.01%
42	Validation loss: 1.618589	Best loss: 0.117126	Accuracy: 22.01%
43	Validation loss: 1.622345	Best loss: 0.117126	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   6.5s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.719831	Best loss: 1.719831	Accuracy: 22.01%
1	Validation loss: 1.623656	Best loss: 1.623656	Accuracy: 19.08%
2	Validation loss: 1.617378	Best loss: 1.617378	Accuracy: 19.08%
3	Validation loss: 1.626063	Best loss: 1.617378	Accuracy: 19.27%
4	Validation loss: 1.616276	Best loss: 1.616276	Accuracy: 22.01%
5	Validation loss: 1.613683	Best loss: 1.613683	Accuracy: 19.27%
6	Validation loss: 1.615639	Best loss: 1.613683	Accuracy: 19.27%
7	Validation loss: 1.631293	Best loss: 1.613683	Accuracy: 22.01%
8	Validation loss: 1.612280	Best loss: 1.612280	Accuracy: 19.27%
9	Validation loss: 1.613432	Best loss: 1.612280	Accuracy: 18.73%
10	Validation loss: 1.617714	Best loss: 1.612280	Accuracy: 19.27%
11	Validation loss: 1.646665	Best loss: 1.612280	Accuracy: 18.73%
12	Validation loss: 1.614816	Best loss: 1.612280	Accuracy: 20.91%
13	Validation loss: 1.610558	Best loss: 1.610558	Accuracy: 22.01%
14	Validation loss: 1.619380	Best loss: 1.610558	Accuracy: 18.73%
15	Validation loss: 1.635474	Best loss: 1.610558	Accuracy: 19.08%
16	Validation loss: 1.629787	Best loss: 1.610558	Accuracy: 18.73%
17	Validation loss: 1.612355	Best loss: 1.610558	Accuracy: 22.01%
18	Validation loss: 1.614929	Best loss: 1.610558	Accuracy: 22.01%
19	Validation loss: 1.615354	Best loss: 1.610558	Accuracy: 19.08%
20	Validation loss: 1.638465	Best loss: 1.610558	Accuracy: 19.27%
21	Validation loss: 1.638183	Best loss: 1.610558	Accuracy: 19.27%
22	Validation loss: 1.645104	Best loss: 1.610558	Accuracy: 18.73%
23	Validation loss: 1.621162	Best loss: 1.610558	Accuracy: 18.73%
24	Validation loss: 1.644720	Best loss: 1.610558	Accuracy: 19.27%
25	Validation loss: 1.623364	Best loss: 1.610558	Accuracy: 22.01%
26	Validation loss: 1.613383	Best loss: 1.610558	Accuracy: 22.01%
27	Validation loss: 1.636157	Best loss: 1.610558	Accuracy: 19.27%
28	Validation loss: 1.647456	Best loss: 1.610558	Accuracy: 22.01%
29	Validation loss: 1.622686	Best loss: 1.610558	Accuracy: 18.73%
30	Validation loss: 1.634940	Best loss: 1.610558	Accuracy: 19.08%
31	Validation loss: 1.627924	Best loss: 1.610558	Accuracy: 18.73%
32	Validation loss: 1.639020	Best loss: 1.610558	Accuracy: 18.73%
33	Validation loss: 1.621580	Best loss: 1.610558	Accuracy: 19.08%
34	Validation loss: 1.624100	Best loss: 1.610558	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   5.7s
[CV] n_neurons=100, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 8637.908203	Best loss: 8637.908203	Accuracy: 28.85%
1	Validation loss: 82964.664062	Best loss: 8637.908203	Accuracy: 23.69%
2	Validation loss: 2315.544189	Best loss: 2315.544189	Accuracy: 72.24%
3	Validation loss: 882.594482	Best loss: 882.594482	Accuracy: 76.70%
4	Validation loss: 2233.500488	Best loss: 882.594482	Accuracy: 62.78%
5	Validation loss: 516.881714	Best loss: 516.881714	Accuracy: 85.46%
6	Validation loss: 458.069489	Best loss: 458.069489	Accuracy: 81.51%
7	Validation loss: 636.897156	Best loss: 458.069489	Accuracy: 84.68%
8	Validation loss: 239711.843750	Best loss: 458.069489	Accuracy: 41.09%
9	Validation loss: 72437.187500	Best loss: 458.069489	Accuracy: 67.16%
10	Validation loss: 5884.095215	Best loss: 458.069489	Accuracy: 74.98%
11	Validation loss: 189946.390625	Best loss: 458.069489	Accuracy: 62.78%
12	Validation loss: 34526.296875	Best loss: 458.069489	Accuracy: 59.19%
13	Validation loss: 12218.773438	Best loss: 458.069489	Accuracy: 85.14%
14	Validation loss: 25830.705078	Best loss: 458.069489	Accuracy: 66.22%
15	Validation loss: 3445.154785	Best loss: 458.069489	Accuracy: 93.51%
16	Validation loss: 4267.619629	Best loss: 458.069489	Accuracy: 86.79%
17	Validation loss: 5654.661133	Best loss: 458.069489	Accuracy: 90.50%
18	Validation loss: 11092.227539	Best loss: 458.069489	Accuracy: 93.94%
19	Validation loss: 19301334.000000	Best loss: 458.069489	Accuracy: 20.91%
20	Validation loss: 104082600.000000	Best loss: 458.069489	Accuracy: 86.43%
21	Validation loss: 175706.671875	Best loss: 458.069489	Accuracy: 87.02%
22	Validation loss: 101410.937500	Best loss: 458.069489	Accuracy: 91.28%
23	Validation loss: 60790.230469	Best loss: 458.069489	Accuracy: 94.53%
24	Validation loss: 69749.648438	Best loss: 458.069489	Accuracy: 92.81%
25	Validation loss: 53525.898438	Best loss: 458.069489	Accuracy: 94.49%
26	Validation loss: 92709.875000	Best loss: 458.069489	Accuracy: 89.80%
27	Validation loss: 60201.019531	Best loss: 458.069489	Accuracy: 94.14%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  19.1s
[CV] n_neurons=100, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.512231	Best loss: 0.512231	Accuracy: 81.47%
1	Validation loss: 1.452414	Best loss: 0.512231	Accuracy: 90.42%
2	Validation loss: 6927.644043	Best loss: 0.512231	Accuracy: 20.48%
3	Validation loss: 471.879303	Best loss: 0.512231	Accuracy: 57.23%
4	Validation loss: 192.132751	Best loss: 0.512231	Accuracy: 66.93%
5	Validation loss: 567.799866	Best loss: 0.512231	Accuracy: 58.87%
6	Validation loss: 150.234589	Best loss: 0.512231	Accuracy: 71.15%
7	Validation loss: 145.653458	Best loss: 0.512231	Accuracy: 68.41%
8	Validation loss: 81.165398	Best loss: 0.512231	Accuracy: 79.87%
9	Validation loss: 145.883987	Best loss: 0.512231	Accuracy: 61.49%
10	Validation loss: 228.557877	Best loss: 0.512231	Accuracy: 37.06%
11	Validation loss: 26.550442	Best loss: 0.512231	Accuracy: 83.85%
12	Validation loss: 26.499146	Best loss: 0.512231	Accuracy: 81.86%
13	Validation loss: 115940.617188	Best loss: 0.512231	Accuracy: 20.52%
14	Validation loss: 3679.880371	Best loss: 0.512231	Accuracy: 52.46%
15	Validation loss: 6176.601074	Best loss: 0.512231	Accuracy: 44.57%
16	Validation loss: 25691.734375	Best loss: 0.512231	Accuracy: 47.89%
17	Validation loss: 3168.093750	Best loss: 0.512231	Accuracy: 65.91%
18	Validation loss: 7767.674805	Best loss: 0.512231	Accuracy: 62.74%
19	Validation loss: 11861.402344	Best loss: 0.512231	Accuracy: 50.20%
20	Validation loss: 4087.705566	Best loss: 0.512231	Accuracy: 62.78%
21	Validation loss: 2716.279785	Best loss: 0.512231	Accuracy: 70.09%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  15.1s
[CV] n_neurons=100, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 53353.343750	Best loss: 53353.343750	Accuracy: 21.58%
1	Validation loss: 2522.142822	Best loss: 2522.142822	Accuracy: 18.73%
2	Validation loss: 444.671173	Best loss: 444.671173	Accuracy: 35.18%
3	Validation loss: 289.759888	Best loss: 289.759888	Accuracy: 20.56%
4	Validation loss: 348.952148	Best loss: 289.759888	Accuracy: 33.46%
5	Validation loss: 126.257721	Best loss: 126.257721	Accuracy: 45.31%
6	Validation loss: 105.549515	Best loss: 105.549515	Accuracy: 55.16%
7	Validation loss: 197.734436	Best loss: 105.549515	Accuracy: 49.65%
8	Validation loss: 74.332794	Best loss: 74.332794	Accuracy: 56.61%
9	Validation loss: 98.882805	Best loss: 74.332794	Accuracy: 62.90%
10	Validation loss: 37.446243	Best loss: 37.446243	Accuracy: 69.35%
11	Validation loss: 40.559872	Best loss: 37.446243	Accuracy: 64.93%
12	Validation loss: 36.824947	Best loss: 36.824947	Accuracy: 70.84%
13	Validation loss: 2590717.000000	Best loss: 36.824947	Accuracy: 34.05%
14	Validation loss: 118182.781250	Best loss: 36.824947	Accuracy: 64.00%
15	Validation loss: 61102.656250	Best loss: 36.824947	Accuracy: 67.94%
16	Validation loss: 45593.949219	Best loss: 36.824947	Accuracy: 77.44%
17	Validation loss: 105106.765625	Best loss: 36.824947	Accuracy: 57.94%
18	Validation loss: 94656.546875	Best loss: 36.824947	Accuracy: 61.34%
19	Validation loss: 33225.007812	Best loss: 36.824947	Accuracy: 82.45%
20	Validation loss: 31648.529297	Best loss: 36.824947	Accuracy: 88.98%
21	Validation loss: 43226.503906	Best loss: 36.824947	Accuracy: 83.23%
22	Validation loss: 28700.230469	Best loss: 36.824947	Accuracy: 90.54%
23	Validation loss: 313566.937500	Best loss: 36.824947	Accuracy: 82.17%
24	Validation loss: 467834.281250	Best loss: 36.824947	Accuracy: 73.81%
25	Validation loss: 381093.968750	Best loss: 36.824947	Accuracy: 87.37%
26	Validation loss: 121639.359375	Best loss: 36.824947	Accuracy: 88.31%
27	Validation loss: 57471.550781	Best loss: 36.824947	Accuracy: 93.51%
28	Validation loss: 35673.464844	Best loss: 36.824947	Accuracy: 94.21%
29	Validation loss: 32101.603516	Best loss: 36.824947	Accuracy: 95.00%
30	Validation loss: 55017.351562	Best loss: 36.824947	Accuracy: 92.26%
31	Validation loss: 28675.652344	Best loss: 36.824947	Accuracy: 95.47%
32	Validation loss: 23630.435547	Best loss: 36.824947	Accuracy: 95.82%
33	Validation loss: 24275.894531	Best loss: 36.824947	Accuracy: 95.27%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  22.9s
[CV] n_neurons=70, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 7776.632324	Best loss: 7776.632324	Accuracy: 34.56%
1	Validation loss: 57.224480	Best loss: 57.224480	Accuracy: 83.58%
2	Validation loss: 33.969543	Best loss: 33.969543	Accuracy: 90.23%
3	Validation loss: 23.630808	Best loss: 23.630808	Accuracy: 88.31%
4	Validation loss: 25.135756	Best loss: 23.630808	Accuracy: 89.01%
5	Validation loss: 43.799129	Best loss: 23.630808	Accuracy: 93.32%
6	Validation loss: 7.554410	Best loss: 7.554410	Accuracy: 92.85%
7	Validation loss: 8.793777	Best loss: 7.554410	Accuracy: 94.61%
8	Validation loss: 23.255089	Best loss: 7.554410	Accuracy: 91.09%
9	Validation loss: 13.388994	Best loss: 7.554410	Accuracy: 94.21%
10	Validation loss: 19.856628	Best loss: 7.554410	Accuracy: 95.04%
11	Validation loss: 50.149605	Best loss: 7.554410	Accuracy: 83.89%
12	Validation loss: 244254.218750	Best loss: 7.554410	Accuracy: 80.30%
13	Validation loss: 55088.507812	Best loss: 7.554410	Accuracy: 88.82%
14	Validation loss: 24463.738281	Best loss: 7.554410	Accuracy: 89.25%
15	Validation loss: 22539.423828	Best loss: 7.554410	Accuracy: 91.79%
16	Validation loss: 13023.830078	Best loss: 7.554410	Accuracy: 92.69%
17	Validation loss: 14039.358398	Best loss: 7.554410	Accuracy: 93.67%
18	Validation loss: 37722.687500	Best loss: 7.554410	Accuracy: 88.12%
19	Validation loss: 14776.075195	Best loss: 7.554410	Accuracy: 94.33%
20	Validation loss: 13343.121094	Best loss: 7.554410	Accuracy: 92.38%
21	Validation loss: 8600.383789	Best loss: 7.554410	Accuracy: 93.98%
22	Validation loss: 18034.798828	Best loss: 7.554410	Accuracy: 88.35%
23	Validation loss: 5494.707031	Best loss: 7.554410	Accuracy: 95.15%
24	Validation loss: 5488.125977	Best loss: 7.554410	Accuracy: 95.62%
25	Validation loss: 5231.083496	Best loss: 7.554410	Accuracy: 95.19%
26	Validation loss: 5588.979980	Best loss: 7.554410	Accuracy: 94.25%
27	Validation loss: 4360.971680	Best loss: 7.554410	Accuracy: 95.47%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  10.5s
[CV] n_neurons=70, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 15.681207	Best loss: 15.681207	Accuracy: 92.85%
1	Validation loss: 3.288811	Best loss: 3.288811	Accuracy: 95.31%
2	Validation loss: 3.094400	Best loss: 3.094400	Accuracy: 93.71%
3	Validation loss: 2.742948	Best loss: 2.742948	Accuracy: 92.73%
4	Validation loss: 0.952018	Best loss: 0.952018	Accuracy: 96.56%
5	Validation loss: 0.925839	Best loss: 0.925839	Accuracy: 96.83%
6	Validation loss: 6.699744	Best loss: 0.925839	Accuracy: 87.61%
7	Validation loss: 4.235938	Best loss: 0.925839	Accuracy: 88.31%
8	Validation loss: 2.179403	Best loss: 0.925839	Accuracy: 95.66%
9	Validation loss: 1.226505	Best loss: 0.925839	Accuracy: 95.78%
10	Validation loss: 1.055946	Best loss: 0.925839	Accuracy: 95.54%
11	Validation loss: 0.666319	Best loss: 0.666319	Accuracy: 96.79%
12	Validation loss: 1.020967	Best loss: 0.666319	Accuracy: 97.07%
13	Validation loss: 0.663534	Best loss: 0.663534	Accuracy: 97.07%
14	Validation loss: 725857.437500	Best loss: 0.663534	Accuracy: 47.42%
15	Validation loss: 28678.386719	Best loss: 0.663534	Accuracy: 86.12%
16	Validation loss: 41554.035156	Best loss: 0.663534	Accuracy: 83.74%
17	Validation loss: 25662.591797	Best loss: 0.663534	Accuracy: 78.85%
18	Validation loss: 16587.292969	Best loss: 0.663534	Accuracy: 91.59%
19	Validation loss: 45446.753906	Best loss: 0.663534	Accuracy: 79.87%
20	Validation loss: 13509.989258	Best loss: 0.663534	Accuracy: 92.49%
21	Validation loss: 12298.182617	Best loss: 0.663534	Accuracy: 91.63%
22	Validation loss: 10465.880859	Best loss: 0.663534	Accuracy: 93.71%
23	Validation loss: 18367.736328	Best loss: 0.663534	Accuracy: 92.26%
24	Validation loss: 14488.790039	Best loss: 0.663534	Accuracy: 90.77%
25	Validation loss: 8508.434570	Best loss: 0.663534	Accuracy: 94.45%
26	Validation loss: 6659.347168	Best loss: 0.663534	Accuracy: 94.61%
27	Validation loss: 7886.392578	Best loss: 0.663534	Accuracy: 93.28%
28	Validation loss: 11588.251953	Best loss: 0.663534	Accuracy: 93.47%
29	Validation loss: 8104.395020	Best loss: 0.663534	Accuracy: 92.49%
30	Validation loss: 11553.480469	Best loss: 0.663534	Accuracy: 94.57%
31	Validation loss: 8503.565430	Best loss: 0.663534	Accuracy: 94.21%
32	Validation loss: 9673.297852	Best loss: 0.663534	Accuracy: 94.25%
33	Validation loss: 18215.628906	Best loss: 0.663534	Accuracy: 91.05%
34	Validation loss: 10773.076172	Best loss: 0.663534	Accuracy: 93.24%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  13.0s
[CV] n_neurons=70, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 23.194595	Best loss: 23.194595	Accuracy: 79.48%
1	Validation loss: 12.975707	Best loss: 12.975707	Accuracy: 84.95%
2	Validation loss: 4.305013	Best loss: 4.305013	Accuracy: 91.67%
3	Validation loss: 2.776266	Best loss: 2.776266	Accuracy: 92.92%
4	Validation loss: 2.287157	Best loss: 2.287157	Accuracy: 94.53%
5	Validation loss: 2.332558	Best loss: 2.287157	Accuracy: 92.96%
6	Validation loss: 1.427269	Best loss: 1.427269	Accuracy: 94.33%
7	Validation loss: 1.576847	Best loss: 1.427269	Accuracy: 93.39%
8	Validation loss: 2.445789	Best loss: 1.427269	Accuracy: 94.06%
9	Validation loss: 1.352688	Best loss: 1.352688	Accuracy: 96.09%
10	Validation loss: 1.385194	Best loss: 1.352688	Accuracy: 95.58%
11	Validation loss: 4.720071	Best loss: 1.352688	Accuracy: 94.72%
12	Validation loss: 3.317804	Best loss: 1.352688	Accuracy: 96.25%
13	Validation loss: 1.846429	Best loss: 1.352688	Accuracy: 96.68%
14	Validation loss: 5370311.500000	Best loss: 1.352688	Accuracy: 32.68%
15	Validation loss: 23836.755859	Best loss: 1.352688	Accuracy: 87.41%
16	Validation loss: 11308.788086	Best loss: 1.352688	Accuracy: 90.19%
17	Validation loss: 8052.075195	Best loss: 1.352688	Accuracy: 91.44%
18	Validation loss: 15315.836914	Best loss: 1.352688	Accuracy: 85.03%
19	Validation loss: 8723.285156	Best loss: 1.352688	Accuracy: 89.68%
20	Validation loss: 7542.917969	Best loss: 1.352688	Accuracy: 92.30%
21	Validation loss: 14734.782227	Best loss: 1.352688	Accuracy: 92.26%
22	Validation loss: 9722.098633	Best loss: 1.352688	Accuracy: 91.40%
23	Validation loss: 15308.541016	Best loss: 1.352688	Accuracy: 87.37%
24	Validation loss: 6411.029785	Best loss: 1.352688	Accuracy: 93.00%
25	Validation loss: 5245.630371	Best loss: 1.352688	Accuracy: 93.55%
26	Validation loss: 3896.140625	Best loss: 1.352688	Accuracy: 93.82%
27	Validation loss: 6062.370117	Best loss: 1.352688	Accuracy: 93.75%
28	Validation loss: 3425.580566	Best loss: 1.352688	Accuracy: 94.14%
29	Validation loss: 3441.984375	Best loss: 1.352688	Accuracy: 95.54%
30	Validation loss: 9662.421875	Best loss: 1.352688	Accuracy: 89.48%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  11.5s
[CV] n_neurons=100, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.998116	Best loss: 1.998116	Accuracy: 18.73%
1	Validation loss: 2.409117	Best loss: 1.998116	Accuracy: 18.73%
2	Validation loss: 2.103438	Best loss: 1.998116	Accuracy: 19.27%
3	Validation loss: 2.146087	Best loss: 1.998116	Accuracy: 19.27%
4	Validation loss: 2.041547	Best loss: 1.998116	Accuracy: 19.27%
5	Validation loss: 1.694722	Best loss: 1.694722	Accuracy: 20.91%
6	Validation loss: 2.098616	Best loss: 1.694722	Accuracy: 18.73%
7	Validation loss: 1.991297	Best loss: 1.694722	Accuracy: 19.08%
8	Validation loss: 1.886122	Best loss: 1.694722	Accuracy: 19.08%
9	Validation loss: 2.482885	Best loss: 1.694722	Accuracy: 19.27%
10	Validation loss: 1.726890	Best loss: 1.694722	Accuracy: 20.91%
11	Validation loss: 1.677544	Best loss: 1.677544	Accuracy: 22.01%
12	Validation loss: 1.999843	Best loss: 1.677544	Accuracy: 19.08%
13	Validation loss: 1.756564	Best loss: 1.677544	Accuracy: 19.08%
14	Validation loss: 2.995764	Best loss: 1.677544	Accuracy: 20.91%
15	Validation loss: 1.696041	Best loss: 1.677544	Accuracy: 19.27%
16	Validation loss: 2.644184	Best loss: 1.677544	Accuracy: 20.91%
17	Validation loss: 2.019515	Best loss: 1.677544	Accuracy: 20.91%
18	Validation loss: 2.568083	Best loss: 1.677544	Accuracy: 22.01%
19	Validation loss: 2.833769	Best loss: 1.677544	Accuracy: 22.01%
20	Validation loss: 2.283544	Best loss: 1.677544	Accuracy: 22.01%
21	Validation loss: 2.377197	Best loss: 1.677544	Accuracy: 20.91%
22	Validation loss: 1.983326	Best loss: 1.677544	Accuracy: 18.73%
23	Validation loss: 2.143944	Best loss: 1.677544	Accuracy: 19.27%
24	Validation loss: 2.032362	Best loss: 1.677544	Accuracy: 20.91%
25	Validation loss: 1.827985	Best loss: 1.677544	Accuracy: 20.91%
26	Validation loss: 1.893213	Best loss: 1.677544	Accuracy: 19.08%
27	Validation loss: 2.163728	Best loss: 1.677544	Accuracy: 22.01%
28	Validation loss: 2.621584	Best loss: 1.677544	Accuracy: 19.08%
29	Validation loss: 2.281030	Best loss: 1.677544	Accuracy: 20.91%
30	Validation loss: 1.913920	Best loss: 1.677544	Accuracy: 19.27%
31	Validation loss: 2.141627	Best loss: 1.677544	Accuracy: 19.27%
32	Validation loss: 2.017694	Best loss: 1.677544	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.1min
[CV] n_neurons=100, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.006631	Best loss: 2.006631	Accuracy: 19.27%
1	Validation loss: 2.026161	Best loss: 2.006631	Accuracy: 22.01%
2	Validation loss: 2.136949	Best loss: 2.006631	Accuracy: 22.01%
3	Validation loss: 2.180786	Best loss: 2.006631	Accuracy: 22.01%
4	Validation loss: 2.010551	Best loss: 2.006631	Accuracy: 20.91%
5	Validation loss: 2.124357	Best loss: 2.006631	Accuracy: 19.27%
6	Validation loss: 1.798514	Best loss: 1.798514	Accuracy: 19.27%
7	Validation loss: 2.029064	Best loss: 1.798514	Accuracy: 19.08%
8	Validation loss: 2.358265	Best loss: 1.798514	Accuracy: 18.73%
9	Validation loss: 1.772398	Best loss: 1.772398	Accuracy: 19.27%
10	Validation loss: 2.146667	Best loss: 1.772398	Accuracy: 19.08%
11	Validation loss: 2.742454	Best loss: 1.772398	Accuracy: 22.01%
12	Validation loss: 2.148697	Best loss: 1.772398	Accuracy: 19.27%
13	Validation loss: 2.504802	Best loss: 1.772398	Accuracy: 19.27%
14	Validation loss: 2.295739	Best loss: 1.772398	Accuracy: 19.27%
15	Validation loss: 2.060830	Best loss: 1.772398	Accuracy: 18.73%
16	Validation loss: 1.657125	Best loss: 1.657125	Accuracy: 19.08%
17	Validation loss: 1.926824	Best loss: 1.657125	Accuracy: 18.73%
18	Validation loss: 1.868723	Best loss: 1.657125	Accuracy: 19.08%
19	Validation loss: 2.003774	Best loss: 1.657125	Accuracy: 18.73%
20	Validation loss: 2.407035	Best loss: 1.657125	Accuracy: 19.27%
21	Validation loss: 1.857629	Best loss: 1.657125	Accuracy: 19.27%
22	Validation loss: 2.096537	Best loss: 1.657125	Accuracy: 19.27%
23	Validation loss: 2.520478	Best loss: 1.657125	Accuracy: 22.01%
24	Validation loss: 1.893649	Best loss: 1.657125	Accuracy: 19.08%
25	Validation loss: 2.078131	Best loss: 1.657125	Accuracy: 22.01%
26	Validation loss: 2.804227	Best loss: 1.657125	Accuracy: 19.08%
27	Validation loss: 2.394251	Best loss: 1.657125	Accuracy: 20.91%
28	Validation loss: 1.977026	Best loss: 1.657125	Accuracy: 20.91%
29	Validation loss: 1.721668	Best loss: 1.657125	Accuracy: 19.27%
30	Validation loss: 2.203953	Best loss: 1.657125	Accuracy: 19.08%
31	Validation loss: 2.605186	Best loss: 1.657125	Accuracy: 22.01%
32	Validation loss: 2.066092	Best loss: 1.657125	Accuracy: 18.73%
33	Validation loss: 2.527797	Best loss: 1.657125	Accuracy: 19.08%
34	Validation loss: 2.730656	Best loss: 1.657125	Accuracy: 18.73%
35	Validation loss: 2.582781	Best loss: 1.657125	Accuracy: 22.01%
36	Validation loss: 2.845672	Best loss: 1.657125	Accuracy: 19.27%
37	Validation loss: 1.998367	Best loss: 1.657125	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.3min
[CV] n_neurons=100, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.852486	Best loss: 1.852486	Accuracy: 18.73%
1	Validation loss: 2.186056	Best loss: 1.852486	Accuracy: 22.01%
2	Validation loss: 2.193672	Best loss: 1.852486	Accuracy: 19.08%
3	Validation loss: 2.144101	Best loss: 1.852486	Accuracy: 19.27%
4	Validation loss: 3.100767	Best loss: 1.852486	Accuracy: 20.91%
5	Validation loss: 2.464265	Best loss: 1.852486	Accuracy: 20.91%
6	Validation loss: 2.003222	Best loss: 1.852486	Accuracy: 22.01%
7	Validation loss: 1.763754	Best loss: 1.763754	Accuracy: 22.01%
8	Validation loss: 1.700600	Best loss: 1.700600	Accuracy: 19.27%
9	Validation loss: 2.334469	Best loss: 1.700600	Accuracy: 22.01%
10	Validation loss: 1.853945	Best loss: 1.700600	Accuracy: 18.73%
11	Validation loss: 1.842178	Best loss: 1.700600	Accuracy: 19.27%
12	Validation loss: 2.091015	Best loss: 1.700600	Accuracy: 20.91%
13	Validation loss: 3.280345	Best loss: 1.700600	Accuracy: 19.08%
14	Validation loss: 2.147487	Best loss: 1.700600	Accuracy: 19.08%
15	Validation loss: 1.816309	Best loss: 1.700600	Accuracy: 19.27%
16	Validation loss: 1.802173	Best loss: 1.700600	Accuracy: 18.73%
17	Validation loss: 2.435386	Best loss: 1.700600	Accuracy: 20.91%
18	Validation loss: 1.970514	Best loss: 1.700600	Accuracy: 19.27%
19	Validation loss: 2.368726	Best loss: 1.700600	Accuracy: 22.01%
20	Validation loss: 2.211067	Best loss: 1.700600	Accuracy: 19.27%
21	Validation loss: 2.549936	Best loss: 1.700600	Accuracy: 22.01%
22	Validation loss: 1.926571	Best loss: 1.700600	Accuracy: 19.27%
23	Validation loss: 2.674015	Best loss: 1.700600	Accuracy: 19.08%
24	Validation loss: 2.478701	Best loss: 1.700600	Accuracy: 19.27%
25	Validation loss: 1.703399	Best loss: 1.700600	Accuracy: 22.01%
26	Validation loss: 2.281172	Best loss: 1.700600	Accuracy: 18.73%
27	Validation loss: 1.997704	Best loss: 1.700600	Accuracy: 18.73%
28	Validation loss: 2.451618	Best loss: 1.700600	Accuracy: 19.08%
29	Validation loss: 2.781829	Best loss: 1.700600	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.0min
[CV] n_neurons=90, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.153382	Best loss: 0.153382	Accuracy: 94.80%
1	Validation loss: 0.087583	Best loss: 0.087583	Accuracy: 97.50%
2	Validation loss: 0.076679	Best loss: 0.076679	Accuracy: 97.77%
3	Validation loss: 0.081569	Best loss: 0.076679	Accuracy: 97.69%
4	Validation loss: 0.075921	Best loss: 0.075921	Accuracy: 97.93%
5	Validation loss: 0.065014	Best loss: 0.065014	Accuracy: 98.44%
6	Validation loss: 0.065708	Best loss: 0.065014	Accuracy: 98.36%
7	Validation loss: 0.071029	Best loss: 0.065014	Accuracy: 98.32%
8	Validation loss: 0.070514	Best loss: 0.065014	Accuracy: 98.32%
9	Validation loss: 0.073578	Best loss: 0.065014	Accuracy: 98.28%
10	Validation loss: 0.048636	Best loss: 0.048636	Accuracy: 98.91%
11	Validation loss: 0.064015	Best loss: 0.048636	Accuracy: 98.40%
12	Validation loss: 0.074224	Best loss: 0.048636	Accuracy: 98.20%
13	Validation loss: 0.087422	Best loss: 0.048636	Accuracy: 98.05%
14	Validation loss: 0.077789	Best loss: 0.048636	Accuracy: 98.32%
15	Validation loss: 0.079970	Best loss: 0.048636	Accuracy: 98.36%
16	Validation loss: 0.095411	Best loss: 0.048636	Accuracy: 98.71%
17	Validation loss: 0.070337	Best loss: 0.048636	Accuracy: 98.51%
18	Validation loss: 0.079868	Best loss: 0.048636	Accuracy: 98.28%
19	Validation loss: 0.052126	Best loss: 0.048636	Accuracy: 98.79%
20	Validation loss: 0.112095	Best loss: 0.048636	Accuracy: 98.40%
21	Validation loss: 0.074458	Best loss: 0.048636	Accuracy: 98.55%
22	Validation loss: 0.083636	Best loss: 0.048636	Accuracy: 98.40%
23	Validation loss: 0.089375	Best loss: 0.048636	Accuracy: 98.59%
24	Validation loss: 0.061038	Best loss: 0.048636	Accuracy: 98.63%
25	Validation loss: 0.069667	Best loss: 0.048636	Accuracy: 98.75%
26	Validation loss: 0.084384	Best loss: 0.048636	Accuracy: 98.83%
27	Validation loss: 0.085436	Best loss: 0.048636	Accuracy: 98.20%
28	Validation loss: 0.101631	Best loss: 0.048636	Accuracy: 98.24%
29	Validation loss: 0.065817	Best loss: 0.048636	Accuracy: 98.79%
30	Validation loss: 0.069274	Best loss: 0.048636	Accuracy: 98.87%
31	Validation loss: 0.070389	Best loss: 0.048636	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   7.2s
[CV] n_neurons=90, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.142722	Best loss: 0.142722	Accuracy: 96.09%
1	Validation loss: 0.077564	Best loss: 0.077564	Accuracy: 97.54%
2	Validation loss: 0.068071	Best loss: 0.068071	Accuracy: 97.81%
3	Validation loss: 0.056796	Best loss: 0.056796	Accuracy: 98.24%
4	Validation loss: 0.057227	Best loss: 0.056796	Accuracy: 98.32%
5	Validation loss: 0.047565	Best loss: 0.047565	Accuracy: 98.51%
6	Validation loss: 0.064648	Best loss: 0.047565	Accuracy: 97.97%
7	Validation loss: 0.063277	Best loss: 0.047565	Accuracy: 98.12%
8	Validation loss: 0.088097	Best loss: 0.047565	Accuracy: 97.58%
9	Validation loss: 0.055489	Best loss: 0.047565	Accuracy: 98.36%
10	Validation loss: 0.053590	Best loss: 0.047565	Accuracy: 98.83%
11	Validation loss: 0.089331	Best loss: 0.047565	Accuracy: 97.93%
12	Validation loss: 0.047922	Best loss: 0.047565	Accuracy: 98.67%
13	Validation loss: 0.055789	Best loss: 0.047565	Accuracy: 98.67%
14	Validation loss: 0.060776	Best loss: 0.047565	Accuracy: 98.63%
15	Validation loss: 0.069278	Best loss: 0.047565	Accuracy: 98.59%
16	Validation loss: 0.079920	Best loss: 0.047565	Accuracy: 98.48%
17	Validation loss: 0.053722	Best loss: 0.047565	Accuracy: 98.79%
18	Validation loss: 0.066007	Best loss: 0.047565	Accuracy: 98.79%
19	Validation loss: 0.067654	Best loss: 0.047565	Accuracy: 98.55%
20	Validation loss: 0.059452	Best loss: 0.047565	Accuracy: 98.59%
21	Validation loss: 0.063603	Best loss: 0.047565	Accuracy: 98.51%
22	Validation loss: 0.069212	Best loss: 0.047565	Accuracy: 98.67%
23	Validation loss: 0.086959	Best loss: 0.047565	Accuracy: 98.59%
24	Validation loss: 0.064216	Best loss: 0.047565	Accuracy: 98.75%
25	Validation loss: 0.062887	Best loss: 0.047565	Accuracy: 98.83%
26	Validation loss: 0.067499	Best loss: 0.047565	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   5.9s
[CV] n_neurons=90, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.140275	Best loss: 0.140275	Accuracy: 95.31%
1	Validation loss: 0.078177	Best loss: 0.078177	Accuracy: 97.30%
2	Validation loss: 0.067005	Best loss: 0.067005	Accuracy: 97.77%
3	Validation loss: 0.056298	Best loss: 0.056298	Accuracy: 98.24%
4	Validation loss: 0.059301	Best loss: 0.056298	Accuracy: 98.28%
5	Validation loss: 0.053600	Best loss: 0.053600	Accuracy: 98.28%
6	Validation loss: 0.081684	Best loss: 0.053600	Accuracy: 97.93%
7	Validation loss: 0.063679	Best loss: 0.053600	Accuracy: 98.16%
8	Validation loss: 0.056795	Best loss: 0.053600	Accuracy: 98.28%
9	Validation loss: 0.051927	Best loss: 0.051927	Accuracy: 98.51%
10	Validation loss: 0.072561	Best loss: 0.051927	Accuracy: 98.44%
11	Validation loss: 0.074705	Best loss: 0.051927	Accuracy: 98.32%
12	Validation loss: 0.071259	Best loss: 0.051927	Accuracy: 98.48%
13	Validation loss: 0.059811	Best loss: 0.051927	Accuracy: 98.44%
14	Validation loss: 0.064046	Best loss: 0.051927	Accuracy: 98.55%
15	Validation loss: 0.058110	Best loss: 0.051927	Accuracy: 98.75%
16	Validation loss: 0.067559	Best loss: 0.051927	Accuracy: 98.94%
17	Validation loss: 0.072451	Best loss: 0.051927	Accuracy: 98.48%
18	Validation loss: 0.060583	Best loss: 0.051927	Accuracy: 98.67%
19	Validation loss: 0.070337	Best loss: 0.051927	Accuracy: 98.91%
20	Validation loss: 0.065303	Best loss: 0.051927	Accuracy: 98.79%
21	Validation loss: 0.089947	Best loss: 0.051927	Accuracy: 98.32%
22	Validation loss: 0.060776	Best loss: 0.051927	Accuracy: 98.71%
23	Validation loss: 0.075793	Best loss: 0.051927	Accuracy: 98.79%
24	Validation loss: 0.055325	Best loss: 0.051927	Accuracy: 98.71%
25	Validation loss: 0.067039	Best loss: 0.051927	Accuracy: 98.71%
26	Validation loss: 0.065968	Best loss: 0.051927	Accuracy: 98.87%
27	Validation loss: 0.099620	Best loss: 0.051927	Accuracy: 98.51%
28	Validation loss: 0.057779	Best loss: 0.051927	Accuracy: 99.02%
29	Validation loss: 0.064403	Best loss: 0.051927	Accuracy: 98.91%
30	Validation loss: 0.078111	Best loss: 0.051927	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   6.5s
[CV] n_neurons=30, learning_rate=0.05, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.208889	Best loss: 0.208889	Accuracy: 94.68%
1	Validation loss: 0.196880	Best loss: 0.196880	Accuracy: 94.33%
2	Validation loss: 0.846685	Best loss: 0.196880	Accuracy: 85.11%
3	Validation loss: 0.381405	Best loss: 0.196880	Accuracy: 90.03%
4	Validation loss: 0.196067	Best loss: 0.196067	Accuracy: 94.14%
5	Validation loss: 0.248567	Best loss: 0.196067	Accuracy: 93.98%
6	Validation loss: 0.354902	Best loss: 0.196067	Accuracy: 92.14%
7	Validation loss: 0.587415	Best loss: 0.196067	Accuracy: 75.72%
8	Validation loss: 0.779964	Best loss: 0.196067	Accuracy: 59.77%
9	Validation loss: 0.762669	Best loss: 0.196067	Accuracy: 60.36%
10	Validation loss: 1.100663	Best loss: 0.196067	Accuracy: 40.77%
11	Validation loss: 1.268158	Best loss: 0.196067	Accuracy: 42.42%
12	Validation loss: 1.205222	Best loss: 0.196067	Accuracy: 42.81%
13	Validation loss: 1.200612	Best loss: 0.196067	Accuracy: 39.64%
14	Validation loss: 1.218175	Best loss: 0.196067	Accuracy: 39.29%
15	Validation loss: 1.611601	Best loss: 0.196067	Accuracy: 22.01%
16	Validation loss: 1.609522	Best loss: 0.196067	Accuracy: 19.08%
17	Validation loss: 1.610101	Best loss: 0.196067	Accuracy: 19.27%
18	Validation loss: 1.615307	Best loss: 0.196067	Accuracy: 18.73%
19	Validation loss: 1.608972	Best loss: 0.196067	Accuracy: 22.01%
20	Validation loss: 1.609100	Best loss: 0.196067	Accuracy: 20.91%
21	Validation loss: 1.615682	Best loss: 0.196067	Accuracy: 22.01%
22	Validation loss: 1.613700	Best loss: 0.196067	Accuracy: 19.27%
23	Validation loss: 1.617172	Best loss: 0.196067	Accuracy: 19.27%
24	Validation loss: 1.614723	Best loss: 0.196067	Accuracy: 22.01%
25	Validation loss: 1.609878	Best loss: 0.196067	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.05, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  11.0s
[CV] n_neurons=30, learning_rate=0.05, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.204108	Best loss: 0.204108	Accuracy: 94.92%
1	Validation loss: 0.535567	Best loss: 0.204108	Accuracy: 72.56%
2	Validation loss: 0.647109	Best loss: 0.204108	Accuracy: 73.96%
3	Validation loss: 0.565037	Best loss: 0.204108	Accuracy: 74.32%
4	Validation loss: 0.600417	Best loss: 0.204108	Accuracy: 73.92%
5	Validation loss: 1.325181	Best loss: 0.204108	Accuracy: 39.21%
6	Validation loss: 1.072954	Best loss: 0.204108	Accuracy: 54.89%
7	Validation loss: 0.886973	Best loss: 0.204108	Accuracy: 56.53%
8	Validation loss: 0.894407	Best loss: 0.204108	Accuracy: 59.62%
9	Validation loss: 0.830660	Best loss: 0.204108	Accuracy: 59.07%
10	Validation loss: 1.156962	Best loss: 0.204108	Accuracy: 43.63%
11	Validation loss: 0.978975	Best loss: 0.204108	Accuracy: 51.21%
12	Validation loss: 1.352465	Best loss: 0.204108	Accuracy: 39.52%
13	Validation loss: 1.390018	Best loss: 0.204108	Accuracy: 38.23%
14	Validation loss: 1.558584	Best loss: 0.204108	Accuracy: 26.43%
15	Validation loss: 1.425663	Best loss: 0.204108	Accuracy: 32.84%
16	Validation loss: 1.449399	Best loss: 0.204108	Accuracy: 30.18%
17	Validation loss: 1.250049	Best loss: 0.204108	Accuracy: 39.91%
18	Validation loss: 1.404004	Best loss: 0.204108	Accuracy: 39.99%
19	Validation loss: 1.324953	Best loss: 0.204108	Accuracy: 39.99%
20	Validation loss: 1.327070	Best loss: 0.204108	Accuracy: 39.99%
21	Validation loss: 1.340566	Best loss: 0.204108	Accuracy: 39.99%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.05, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=   9.8s
[CV] n_neurons=30, learning_rate=0.05, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.155778	Best loss: 0.155778	Accuracy: 95.54%
1	Validation loss: 0.565385	Best loss: 0.155778	Accuracy: 81.08%
2	Validation loss: 0.625947	Best loss: 0.155778	Accuracy: 75.45%
3	Validation loss: 1.621723	Best loss: 0.155778	Accuracy: 19.27%
4	Validation loss: 1.606711	Best loss: 0.155778	Accuracy: 22.13%
5	Validation loss: 1.609844	Best loss: 0.155778	Accuracy: 22.13%
6	Validation loss: 1.612115	Best loss: 0.155778	Accuracy: 22.13%
7	Validation loss: 1.614434	Best loss: 0.155778	Accuracy: 22.05%
8	Validation loss: 1.608141	Best loss: 0.155778	Accuracy: 22.05%
9	Validation loss: 1.610325	Best loss: 0.155778	Accuracy: 22.05%
10	Validation loss: 1.611391	Best loss: 0.155778	Accuracy: 18.76%
11	Validation loss: 1.615734	Best loss: 0.155778	Accuracy: 22.05%
12	Validation loss: 1.606610	Best loss: 0.155778	Accuracy: 22.05%
13	Validation loss: 1.612772	Best loss: 0.155778	Accuracy: 19.19%
14	Validation loss: 1.608262	Best loss: 0.155778	Accuracy: 20.91%
15	Validation loss: 1.611867	Best loss: 0.155778	Accuracy: 22.05%
16	Validation loss: 1.619117	Best loss: 0.155778	Accuracy: 18.76%
17	Validation loss: 1.614075	Best loss: 0.155778	Accuracy: 19.12%
18	Validation loss: 1.607158	Best loss: 0.155778	Accuracy: 20.91%
19	Validation loss: 1.609069	Best loss: 0.155778	Accuracy: 22.05%
20	Validation loss: 1.615657	Best loss: 0.155778	Accuracy: 19.27%
21	Validation loss: 1.611873	Best loss: 0.155778	Accuracy: 22.05%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.05, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=   9.2s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.173533	Best loss: 0.173533	Accuracy: 95.58%
1	Validation loss: 0.136189	Best loss: 0.136189	Accuracy: 96.68%
2	Validation loss: 1172.923828	Best loss: 0.136189	Accuracy: 27.48%
3	Validation loss: 31.471066	Best loss: 0.136189	Accuracy: 95.19%
4	Validation loss: 7.091805	Best loss: 0.136189	Accuracy: 94.92%
5	Validation loss: 4.040255	Best loss: 0.136189	Accuracy: 95.97%
6	Validation loss: 5.318775	Best loss: 0.136189	Accuracy: 94.49%
7	Validation loss: 6.093581	Best loss: 0.136189	Accuracy: 91.79%
8	Validation loss: 3.691033	Best loss: 0.136189	Accuracy: 93.94%
9	Validation loss: 2.590963	Best loss: 0.136189	Accuracy: 96.21%
10	Validation loss: 1.881721	Best loss: 0.136189	Accuracy: 95.19%
11	Validation loss: 1.832517	Best loss: 0.136189	Accuracy: 95.58%
12	Validation loss: 1.329144	Best loss: 0.136189	Accuracy: 96.36%
13	Validation loss: 1.421219	Best loss: 0.136189	Accuracy: 94.61%
14	Validation loss: 1.293307	Best loss: 0.136189	Accuracy: 96.05%
15	Validation loss: 1.154145	Best loss: 0.136189	Accuracy: 95.23%
16	Validation loss: 0.775225	Best loss: 0.136189	Accuracy: 96.56%
17	Validation loss: 1.563391	Best loss: 0.136189	Accuracy: 95.19%
18	Validation loss: 0.928625	Best loss: 0.136189	Accuracy: 96.60%
19	Validation loss: 1.110330	Best loss: 0.136189	Accuracy: 95.31%
20	Validation loss: 1.810596	Best loss: 0.136189	Accuracy: 93.67%
21	Validation loss: 1.000906	Best loss: 0.136189	Accuracy: 95.58%
22	Validation loss: 1.699700	Best loss: 0.136189	Accuracy: 95.93%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   7.8s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.152045	Best loss: 0.152045	Accuracy: 96.48%
1	Validation loss: 0.113022	Best loss: 0.113022	Accuracy: 96.48%
2	Validation loss: 71.919838	Best loss: 0.113022	Accuracy: 61.69%
3	Validation loss: 10.932671	Best loss: 0.113022	Accuracy: 79.95%
4	Validation loss: 4.743956	Best loss: 0.113022	Accuracy: 89.95%
5	Validation loss: 3.371500	Best loss: 0.113022	Accuracy: 91.83%
6	Validation loss: 16.913937	Best loss: 0.113022	Accuracy: 76.43%
7	Validation loss: 1.610712	Best loss: 0.113022	Accuracy: 96.01%
8	Validation loss: 1.699082	Best loss: 0.113022	Accuracy: 94.45%
9	Validation loss: 1.285351	Best loss: 0.113022	Accuracy: 95.23%
10	Validation loss: 1.741202	Best loss: 0.113022	Accuracy: 94.41%
11	Validation loss: 1.041083	Best loss: 0.113022	Accuracy: 95.97%
12	Validation loss: 1.227441	Best loss: 0.113022	Accuracy: 94.84%
13	Validation loss: 1.381479	Best loss: 0.113022	Accuracy: 94.88%
14	Validation loss: 1.357859	Best loss: 0.113022	Accuracy: 92.10%
15	Validation loss: 1.712307	Best loss: 0.113022	Accuracy: 93.39%
16	Validation loss: 0.776625	Best loss: 0.113022	Accuracy: 95.27%
17	Validation loss: 11.476986	Best loss: 0.113022	Accuracy: 93.51%
18	Validation loss: 18.676138	Best loss: 0.113022	Accuracy: 73.57%
19	Validation loss: 6.512959	Best loss: 0.113022	Accuracy: 86.94%
20	Validation loss: 1.691948	Best loss: 0.113022	Accuracy: 93.75%
21	Validation loss: 2.830635	Best loss: 0.113022	Accuracy: 94.88%
22	Validation loss: 1.017452	Best loss: 0.113022	Accuracy: 96.05%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   7.7s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.121566	Best loss: 0.121566	Accuracy: 96.91%
1	Validation loss: 0.149015	Best loss: 0.121566	Accuracy: 95.86%
2	Validation loss: 0.120651	Best loss: 0.120651	Accuracy: 96.95%
3	Validation loss: 10722.568359	Best loss: 0.120651	Accuracy: 20.91%
4	Validation loss: 38.127628	Best loss: 0.120651	Accuracy: 86.55%
5	Validation loss: 23.077528	Best loss: 0.120651	Accuracy: 89.01%
6	Validation loss: 19.225563	Best loss: 0.120651	Accuracy: 87.65%
7	Validation loss: 6.812086	Best loss: 0.120651	Accuracy: 94.02%
8	Validation loss: 8.739277	Best loss: 0.120651	Accuracy: 91.40%
9	Validation loss: 5.743387	Best loss: 0.120651	Accuracy: 93.55%
10	Validation loss: 5.866643	Best loss: 0.120651	Accuracy: 92.06%
11	Validation loss: 6.345101	Best loss: 0.120651	Accuracy: 91.87%
12	Validation loss: 4.756159	Best loss: 0.120651	Accuracy: 93.35%
13	Validation loss: 6.136342	Best loss: 0.120651	Accuracy: 89.56%
14	Validation loss: 5.295972	Best loss: 0.120651	Accuracy: 91.95%
15	Validation loss: 9.234484	Best loss: 0.120651	Accuracy: 86.20%
16	Validation loss: 10.922627	Best loss: 0.120651	Accuracy: 93.24%
17	Validation loss: 2.116156	Best loss: 0.120651	Accuracy: 95.86%
18	Validation loss: 3.189718	Best loss: 0.120651	Accuracy: 95.00%
19	Validation loss: 1.976016	Best loss: 0.120651	Accuracy: 95.78%
20	Validation loss: 1.789881	Best loss: 0.120651	Accuracy: 96.01%
21	Validation loss: 2.705747	Best loss: 0.120651	Accuracy: 95.15%
22	Validation loss: 1.304972	Best loss: 0.120651	Accuracy: 96.33%
23	Validation loss: 2.290079	Best loss: 0.120651	Accuracy: 95.58%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   8.0s
[CV] n_neurons=10, learning_rate=0.02, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.155508	Best loss: 0.155508	Accuracy: 95.19%
1	Validation loss: 0.172070	Best loss: 0.155508	Accuracy: 94.84%
2	Validation loss: 0.211869	Best loss: 0.155508	Accuracy: 95.39%
3	Validation loss: 0.166934	Best loss: 0.155508	Accuracy: 95.90%
4	Validation loss: 0.138085	Best loss: 0.138085	Accuracy: 96.48%
5	Validation loss: 0.523770	Best loss: 0.138085	Accuracy: 73.38%
6	Validation loss: 0.464819	Best loss: 0.138085	Accuracy: 81.98%
7	Validation loss: 0.228719	Best loss: 0.138085	Accuracy: 93.94%
8	Validation loss: 0.345822	Best loss: 0.138085	Accuracy: 89.21%
9	Validation loss: 0.198381	Best loss: 0.138085	Accuracy: 94.41%
10	Validation loss: 0.205222	Best loss: 0.138085	Accuracy: 95.15%
11	Validation loss: 0.190186	Best loss: 0.138085	Accuracy: 96.56%
12	Validation loss: 0.205403	Best loss: 0.138085	Accuracy: 95.93%
13	Validation loss: 0.167345	Best loss: 0.138085	Accuracy: 95.43%
14	Validation loss: 0.510426	Best loss: 0.138085	Accuracy: 75.45%
15	Validation loss: 0.296162	Best loss: 0.138085	Accuracy: 92.18%
16	Validation loss: 0.332997	Best loss: 0.138085	Accuracy: 91.28%
17	Validation loss: 0.283011	Best loss: 0.138085	Accuracy: 93.55%
18	Validation loss: 0.328170	Best loss: 0.138085	Accuracy: 92.42%
19	Validation loss: 0.294710	Best loss: 0.138085	Accuracy: 92.85%
20	Validation loss: 0.246017	Best loss: 0.138085	Accuracy: 95.35%
21	Validation loss: 0.328338	Best loss: 0.138085	Accuracy: 91.91%
22	Validation loss: 1.038355	Best loss: 0.138085	Accuracy: 76.51%
23	Validation loss: 0.538462	Best loss: 0.138085	Accuracy: 86.79%
24	Validation loss: 0.352565	Best loss: 0.138085	Accuracy: 90.07%
25	Validation loss: 0.535684	Best loss: 0.138085	Accuracy: 76.58%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  33.3s
[CV] n_neurons=10, learning_rate=0.02, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.161620	Best loss: 0.161620	Accuracy: 96.09%
1	Validation loss: 0.187821	Best loss: 0.161620	Accuracy: 95.54%
2	Validation loss: 0.183055	Best loss: 0.161620	Accuracy: 95.35%
3	Validation loss: 0.189675	Best loss: 0.161620	Accuracy: 94.80%
4	Validation loss: 0.192879	Best loss: 0.161620	Accuracy: 94.84%
5	Validation loss: 0.430944	Best loss: 0.161620	Accuracy: 89.37%
6	Validation loss: 0.312362	Best loss: 0.161620	Accuracy: 93.43%
7	Validation loss: 0.377621	Best loss: 0.161620	Accuracy: 91.20%
8	Validation loss: 0.185464	Best loss: 0.161620	Accuracy: 96.01%
9	Validation loss: 0.186855	Best loss: 0.161620	Accuracy: 95.23%
10	Validation loss: 0.243033	Best loss: 0.161620	Accuracy: 94.41%
11	Validation loss: 0.417186	Best loss: 0.161620	Accuracy: 90.46%
12	Validation loss: 0.240051	Best loss: 0.161620	Accuracy: 93.59%
13	Validation loss: 0.261962	Best loss: 0.161620	Accuracy: 93.24%
14	Validation loss: 0.432068	Best loss: 0.161620	Accuracy: 92.61%
15	Validation loss: 0.955792	Best loss: 0.161620	Accuracy: 55.43%
16	Validation loss: 0.763639	Best loss: 0.161620	Accuracy: 60.05%
17	Validation loss: 0.826536	Best loss: 0.161620	Accuracy: 58.80%
18	Validation loss: 0.893267	Best loss: 0.161620	Accuracy: 56.61%
19	Validation loss: 0.789824	Best loss: 0.161620	Accuracy: 60.05%
20	Validation loss: 0.807277	Best loss: 0.161620	Accuracy: 59.58%
21	Validation loss: 0.830346	Best loss: 0.161620	Accuracy: 57.66%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  28.1s
[CV] n_neurons=10, learning_rate=0.02, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.139355	Best loss: 0.139355	Accuracy: 96.91%
1	Validation loss: 0.312400	Best loss: 0.139355	Accuracy: 91.99%
2	Validation loss: 0.171563	Best loss: 0.139355	Accuracy: 96.36%
3	Validation loss: 0.139697	Best loss: 0.139355	Accuracy: 96.56%
4	Validation loss: 0.159594	Best loss: 0.139355	Accuracy: 96.52%
5	Validation loss: 0.127609	Best loss: 0.127609	Accuracy: 96.36%
6	Validation loss: 0.199721	Best loss: 0.127609	Accuracy: 95.86%
7	Validation loss: 0.241060	Best loss: 0.127609	Accuracy: 97.22%
8	Validation loss: 0.177888	Best loss: 0.127609	Accuracy: 96.40%
9	Validation loss: 0.167159	Best loss: 0.127609	Accuracy: 95.82%
10	Validation loss: 0.204242	Best loss: 0.127609	Accuracy: 95.74%
11	Validation loss: 0.287856	Best loss: 0.127609	Accuracy: 90.27%
12	Validation loss: 0.765289	Best loss: 0.127609	Accuracy: 64.78%
13	Validation loss: 0.522110	Best loss: 0.127609	Accuracy: 77.80%
14	Validation loss: 0.485750	Best loss: 0.127609	Accuracy: 76.35%
15	Validation loss: 0.200613	Best loss: 0.127609	Accuracy: 96.09%
16	Validation loss: 0.291575	Best loss: 0.127609	Accuracy: 95.86%
17	Validation loss: 0.151815	Best loss: 0.127609	Accuracy: 96.95%
18	Validation loss: 0.384590	Best loss: 0.127609	Accuracy: 88.15%
19	Validation loss: 0.239808	Best loss: 0.127609	Accuracy: 94.14%
20	Validation loss: 0.280598	Best loss: 0.127609	Accuracy: 90.30%
21	Validation loss: 0.624851	Best loss: 0.127609	Accuracy: 77.91%
22	Validation loss: 0.518005	Best loss: 0.127609	Accuracy: 82.37%
23	Validation loss: 0.818115	Best loss: 0.127609	Accuracy: 74.90%
24	Validation loss: 0.540981	Best loss: 0.127609	Accuracy: 77.72%
25	Validation loss: 0.538322	Best loss: 0.127609	Accuracy: 80.81%
26	Validation loss: 0.746314	Best loss: 0.127609	Accuracy: 76.43%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  34.5s
[CV] n_neurons=140, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 152.370255	Best loss: 152.370255	Accuracy: 30.06%
1	Validation loss: 1631.797363	Best loss: 152.370255	Accuracy: 38.27%
2	Validation loss: 42.100266	Best loss: 42.100266	Accuracy: 80.30%
3	Validation loss: 8.333238	Best loss: 8.333238	Accuracy: 88.78%
4	Validation loss: 2.811370	Best loss: 2.811370	Accuracy: 93.47%
5	Validation loss: 3.195494	Best loss: 2.811370	Accuracy: 92.77%
6	Validation loss: 1.822216	Best loss: 1.822216	Accuracy: 94.02%
7	Validation loss: 1.340225	Best loss: 1.340225	Accuracy: 94.57%
8	Validation loss: 1.521645	Best loss: 1.340225	Accuracy: 93.51%
9	Validation loss: 2.184760	Best loss: 1.340225	Accuracy: 94.57%
10	Validation loss: 1.093879	Best loss: 1.093879	Accuracy: 95.54%
11	Validation loss: 1.042484	Best loss: 1.042484	Accuracy: 95.62%
12	Validation loss: 0.841740	Best loss: 0.841740	Accuracy: 95.47%
13	Validation loss: 0.707454	Best loss: 0.707454	Accuracy: 95.97%
14	Validation loss: 0.973012	Best loss: 0.707454	Accuracy: 92.96%
15	Validation loss: 0.823143	Best loss: 0.707454	Accuracy: 95.19%
16	Validation loss: 1.374965	Best loss: 0.707454	Accuracy: 91.44%
17	Validation loss: 0.710963	Best loss: 0.707454	Accuracy: 94.80%
18	Validation loss: 0.555652	Best loss: 0.555652	Accuracy: 95.35%
19	Validation loss: 0.576319	Best loss: 0.555652	Accuracy: 95.31%
20	Validation loss: 0.632696	Best loss: 0.555652	Accuracy: 95.35%
21	Validation loss: 1.396068	Best loss: 0.555652	Accuracy: 93.00%
22	Validation loss: 1.017286	Best loss: 0.555652	Accuracy: 96.29%
23	Validation loss: 0.858361	Best loss: 0.555652	Accuracy: 96.48%
24	Validation loss: 0.598668	Best loss: 0.555652	Accuracy: 97.15%
25	Validation loss: 0.514869	Best loss: 0.514869	Accuracy: 96.21%
26	Validation loss: 0.475902	Best loss: 0.475902	Accuracy: 96.72%
27	Validation loss: 0.953610	Best loss: 0.475902	Accuracy: 94.68%
28	Validation loss: 0.440536	Best loss: 0.440536	Accuracy: 96.60%
29	Validation loss: 2.972693	Best loss: 0.440536	Accuracy: 95.82%
30	Validation loss: 4.561710	Best loss: 0.440536	Accuracy: 94.45%
31	Validation loss: 1.228685	Best loss: 0.440536	Accuracy: 96.48%
32	Validation loss: 1.112278	Best loss: 0.440536	Accuracy: 96.56%
33	Validation loss: 0.779291	Best loss: 0.440536	Accuracy: 96.83%
34	Validation loss: 0.898217	Best loss: 0.440536	Accuracy: 96.29%
35	Validation loss: 0.741009	Best loss: 0.440536	Accuracy: 97.11%
36	Validation loss: 0.928390	Best loss: 0.440536	Accuracy: 95.04%
37	Validation loss: 0.715220	Best loss: 0.440536	Accuracy: 96.72%
38	Validation loss: 0.933809	Best loss: 0.440536	Accuracy: 96.17%
39	Validation loss: 0.477783	Best loss: 0.440536	Accuracy: 97.46%
40	Validation loss: 0.460213	Best loss: 0.440536	Accuracy: 97.07%
41	Validation loss: 0.531350	Best loss: 0.440536	Accuracy: 96.72%
42	Validation loss: 0.485408	Best loss: 0.440536	Accuracy: 96.95%
43	Validation loss: 0.458669	Best loss: 0.440536	Accuracy: 96.99%
44	Validation loss: 0.411550	Best loss: 0.411550	Accuracy: 97.26%
45	Validation loss: 0.410574	Best loss: 0.410574	Accuracy: 97.07%
46	Validation loss: 0.389480	Best loss: 0.389480	Accuracy: 97.11%
47	Validation loss: 0.377742	Best loss: 0.377742	Accuracy: 97.26%
48	Validation loss: 0.401779	Best loss: 0.377742	Accuracy: 97.03%
49	Validation loss: 0.428220	Best loss: 0.377742	Accuracy: 96.95%
50	Validation loss: 0.455424	Best loss: 0.377742	Accuracy: 97.50%
51	Validation loss: 0.352408	Best loss: 0.352408	Accuracy: 97.11%
52	Validation loss: 0.316857	Best loss: 0.316857	Accuracy: 97.38%
53	Validation loss: 0.391387	Best loss: 0.316857	Accuracy: 97.30%
54	Validation loss: 0.330179	Best loss: 0.316857	Accuracy: 97.34%
55	Validation loss: 0.362316	Best loss: 0.316857	Accuracy: 96.60%
56	Validation loss: 0.311253	Best loss: 0.311253	Accuracy: 97.81%
57	Validation loss: 0.388706	Best loss: 0.311253	Accuracy: 97.07%
58	Validation loss: 0.404465	Best loss: 0.311253	Accuracy: 97.22%
59	Validation loss: 0.277069	Best loss: 0.277069	Accuracy: 97.93%
60	Validation loss: 0.270600	Best loss: 0.270600	Accuracy: 97.93%
61	Validation loss: 0.297091	Best loss: 0.270600	Accuracy: 97.62%
62	Validation loss: 0.302148	Best loss: 0.270600	Accuracy: 98.01%
63	Validation loss: 0.248933	Best loss: 0.248933	Accuracy: 97.89%
64	Validation loss: 0.312293	Best loss: 0.248933	Accuracy: 97.54%
65	Validation loss: 0.278509	Best loss: 0.248933	Accuracy: 97.73%
66	Validation loss: 0.296778	Best loss: 0.248933	Accuracy: 97.58%
67	Validation loss: 0.315736	Best loss: 0.248933	Accuracy: 97.89%
68	Validation loss: 0.306143	Best loss: 0.248933	Accuracy: 97.54%
69	Validation loss: 0.341059	Best loss: 0.248933	Accuracy: 97.77%
70	Validation loss: 0.246285	Best loss: 0.246285	Accuracy: 98.08%
71	Validation loss: 0.265977	Best loss: 0.246285	Accuracy: 97.81%
72	Validation loss: 0.258144	Best loss: 0.246285	Accuracy: 97.77%
73	Validation loss: 0.284498	Best loss: 0.246285	Accuracy: 97.58%
74	Validation loss: 0.257833	Best loss: 0.246285	Accuracy: 98.08%
75	Validation loss: 0.242996	Best loss: 0.242996	Accuracy: 97.89%
76	Validation loss: 0.333282	Best loss: 0.242996	Accuracy: 97.62%
77	Validation loss: 0.273158	Best loss: 0.242996	Accuracy: 97.54%
78	Validation loss: 0.424917	Best loss: 0.242996	Accuracy: 96.95%
79	Validation loss: 0.302496	Best loss: 0.242996	Accuracy: 97.73%
80	Validation loss: 0.297338	Best loss: 0.242996	Accuracy: 97.93%
81	Validation loss: 0.352303	Best loss: 0.242996	Accuracy: 97.26%
82	Validation loss: 0.345814	Best loss: 0.242996	Accuracy: 97.19%
83	Validation loss: 0.306648	Best loss: 0.242996	Accuracy: 97.85%
84	Validation loss: 0.452081	Best loss: 0.242996	Accuracy: 97.07%
85	Validation loss: 0.410783	Best loss: 0.242996	Accuracy: 97.46%
86	Validation loss: 0.419103	Best loss: 0.242996	Accuracy: 97.50%
87	Validation loss: 0.322185	Best loss: 0.242996	Accuracy: 98.08%
88	Validation loss: 0.262740	Best loss: 0.242996	Accuracy: 98.12%
89	Validation loss: 0.371747	Best loss: 0.242996	Accuracy: 97.69%
90	Validation loss: 0.252841	Best loss: 0.242996	Accuracy: 98.05%
91	Validation loss: 0.305028	Best loss: 0.242996	Accuracy: 97.62%
92	Validation loss: 0.478015	Best loss: 0.242996	Accuracy: 97.69%
93	Validation loss: 0.403924	Best loss: 0.242996	Accuracy: 98.12%
94	Validation loss: 0.738513	Best loss: 0.242996	Accuracy: 97.34%
95	Validation loss: 0.341484	Best loss: 0.242996	Accuracy: 97.54%
96	Validation loss: 0.688301	Best loss: 0.242996	Accuracy: 96.91%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  29.6s
[CV] n_neurons=140, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 286.783661	Best loss: 286.783661	Accuracy: 12.86%
1	Validation loss: 354.127045	Best loss: 286.783661	Accuracy: 36.98%
2	Validation loss: 6.097324	Best loss: 6.097324	Accuracy: 85.03%
3	Validation loss: 2.367486	Best loss: 2.367486	Accuracy: 91.99%
4	Validation loss: 1.448885	Best loss: 1.448885	Accuracy: 92.57%
5	Validation loss: 0.994597	Best loss: 0.994597	Accuracy: 94.33%
6	Validation loss: 0.821487	Best loss: 0.821487	Accuracy: 94.14%
7	Validation loss: 0.875826	Best loss: 0.821487	Accuracy: 92.10%
8	Validation loss: 1.058989	Best loss: 0.821487	Accuracy: 92.14%
9	Validation loss: 0.683541	Best loss: 0.683541	Accuracy: 94.37%
10	Validation loss: 0.715271	Best loss: 0.683541	Accuracy: 95.97%
11	Validation loss: 0.820618	Best loss: 0.683541	Accuracy: 95.19%
12	Validation loss: 1.134477	Best loss: 0.683541	Accuracy: 94.25%
13	Validation loss: 1.321205	Best loss: 0.683541	Accuracy: 96.76%
14	Validation loss: 1.730227	Best loss: 0.683541	Accuracy: 96.48%
15	Validation loss: 2.580812	Best loss: 0.683541	Accuracy: 92.77%
16	Validation loss: 0.650325	Best loss: 0.650325	Accuracy: 92.14%
17	Validation loss: 0.309950	Best loss: 0.309950	Accuracy: 96.72%
18	Validation loss: 1.297456	Best loss: 0.309950	Accuracy: 80.57%
19	Validation loss: 0.506580	Best loss: 0.309950	Accuracy: 96.05%
20	Validation loss: 0.362785	Best loss: 0.309950	Accuracy: 96.87%
21	Validation loss: 0.290221	Best loss: 0.290221	Accuracy: 96.33%
22	Validation loss: 0.358446	Best loss: 0.290221	Accuracy: 95.04%
23	Validation loss: 0.228791	Best loss: 0.228791	Accuracy: 96.60%
24	Validation loss: 0.329328	Best loss: 0.228791	Accuracy: 96.13%
25	Validation loss: 0.206457	Best loss: 0.206457	Accuracy: 97.26%
26	Validation loss: 0.241002	Best loss: 0.206457	Accuracy: 95.54%
27	Validation loss: 0.262814	Best loss: 0.206457	Accuracy: 96.40%
28	Validation loss: 0.260296	Best loss: 0.206457	Accuracy: 96.40%
29	Validation loss: 0.327665	Best loss: 0.206457	Accuracy: 95.27%
30	Validation loss: 0.388840	Best loss: 0.206457	Accuracy: 93.86%
31	Validation loss: 0.246937	Best loss: 0.206457	Accuracy: 96.17%
32	Validation loss: 0.201623	Best loss: 0.201623	Accuracy: 96.76%
33	Validation loss: 0.197513	Best loss: 0.197513	Accuracy: 96.33%
34	Validation loss: 0.158994	Best loss: 0.158994	Accuracy: 97.58%
35	Validation loss: 0.177252	Best loss: 0.158994	Accuracy: 96.83%
36	Validation loss: 0.185149	Best loss: 0.158994	Accuracy: 97.03%
37	Validation loss: 0.158045	Best loss: 0.158045	Accuracy: 97.46%
38	Validation loss: 0.265935	Best loss: 0.158045	Accuracy: 95.15%
39	Validation loss: 0.199555	Best loss: 0.158045	Accuracy: 97.03%
40	Validation loss: 0.165183	Best loss: 0.158045	Accuracy: 97.93%
41	Validation loss: 0.176074	Best loss: 0.158045	Accuracy: 97.58%
42	Validation loss: 0.220411	Best loss: 0.158045	Accuracy: 97.50%
43	Validation loss: 0.227304	Best loss: 0.158045	Accuracy: 97.22%
44	Validation loss: 0.196049	Best loss: 0.158045	Accuracy: 97.73%
45	Validation loss: 0.333284	Best loss: 0.158045	Accuracy: 96.40%
46	Validation loss: 0.285324	Best loss: 0.158045	Accuracy: 97.38%
47	Validation loss: 0.337597	Best loss: 0.158045	Accuracy: 97.30%
48	Validation loss: 0.905938	Best loss: 0.158045	Accuracy: 95.97%
49	Validation loss: 0.559338	Best loss: 0.158045	Accuracy: 96.56%
50	Validation loss: 0.234997	Best loss: 0.158045	Accuracy: 97.07%
51	Validation loss: 0.229551	Best loss: 0.158045	Accuracy: 97.11%
52	Validation loss: 0.200782	Best loss: 0.158045	Accuracy: 97.54%
53	Validation loss: 0.215983	Best loss: 0.158045	Accuracy: 97.65%
54	Validation loss: 0.199858	Best loss: 0.158045	Accuracy: 97.54%
55	Validation loss: 0.198671	Best loss: 0.158045	Accuracy: 97.73%
56	Validation loss: 0.194869	Best loss: 0.158045	Accuracy: 97.89%
57	Validation loss: 0.201563	Best loss: 0.158045	Accuracy: 97.93%
58	Validation loss: 0.244536	Best loss: 0.158045	Accuracy: 97.26%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  20.2s
[CV] n_neurons=140, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 12.303310	Best loss: 12.303310	Accuracy: 67.01%
1	Validation loss: 0.346506	Best loss: 0.346506	Accuracy: 94.10%
2	Validation loss: 0.245067	Best loss: 0.245067	Accuracy: 94.14%
3	Validation loss: 0.256230	Best loss: 0.245067	Accuracy: 93.35%
4	Validation loss: 0.187726	Best loss: 0.187726	Accuracy: 96.13%
5	Validation loss: 0.245541	Best loss: 0.187726	Accuracy: 94.53%
6	Validation loss: 0.179336	Best loss: 0.179336	Accuracy: 95.70%
7	Validation loss: 0.148672	Best loss: 0.148672	Accuracy: 95.82%
8	Validation loss: 0.141818	Best loss: 0.141818	Accuracy: 96.48%
9	Validation loss: 0.152969	Best loss: 0.141818	Accuracy: 95.74%
10	Validation loss: 0.111446	Best loss: 0.111446	Accuracy: 96.79%
11	Validation loss: 0.107044	Best loss: 0.107044	Accuracy: 97.03%
12	Validation loss: 0.117242	Best loss: 0.107044	Accuracy: 97.03%
13	Validation loss: 0.101835	Best loss: 0.101835	Accuracy: 97.30%
14	Validation loss: 0.097481	Best loss: 0.097481	Accuracy: 96.95%
15	Validation loss: 0.097007	Best loss: 0.097007	Accuracy: 97.46%
16	Validation loss: 0.094475	Best loss: 0.094475	Accuracy: 97.22%
17	Validation loss: 0.089408	Best loss: 0.089408	Accuracy: 97.38%
18	Validation loss: 0.086676	Best loss: 0.086676	Accuracy: 97.85%
19	Validation loss: 0.088550	Best loss: 0.086676	Accuracy: 97.81%
20	Validation loss: 0.087464	Best loss: 0.086676	Accuracy: 97.65%
21	Validation loss: 0.103469	Best loss: 0.086676	Accuracy: 97.69%
22	Validation loss: 0.077897	Best loss: 0.077897	Accuracy: 98.01%
23	Validation loss: 0.071677	Best loss: 0.071677	Accuracy: 97.85%
24	Validation loss: 0.083012	Best loss: 0.071677	Accuracy: 98.12%
25	Validation loss: 0.098855	Best loss: 0.071677	Accuracy: 97.81%
26	Validation loss: 0.071173	Best loss: 0.071173	Accuracy: 97.89%
27	Validation loss: 0.075030	Best loss: 0.071173	Accuracy: 98.40%
28	Validation loss: 0.072881	Best loss: 0.071173	Accuracy: 98.08%
29	Validation loss: 0.079524	Best loss: 0.071173	Accuracy: 98.24%
30	Validation loss: 0.101098	Best loss: 0.071173	Accuracy: 97.54%
31	Validation loss: 0.076047	Best loss: 0.071173	Accuracy: 98.63%
32	Validation loss: 0.075898	Best loss: 0.071173	Accuracy: 98.44%
33	Validation loss: 0.081967	Best loss: 0.071173	Accuracy: 98.59%
34	Validation loss: 0.089672	Best loss: 0.071173	Accuracy: 98.12%
35	Validation loss: 0.088549	Best loss: 0.071173	Accuracy: 98.28%
36	Validation loss: 0.114074	Best loss: 0.071173	Accuracy: 97.81%
37	Validation loss: 0.118452	Best loss: 0.071173	Accuracy: 98.01%
38	Validation loss: 0.114593	Best loss: 0.071173	Accuracy: 97.97%
39	Validation loss: 0.136675	Best loss: 0.071173	Accuracy: 98.05%
40	Validation loss: 0.111146	Best loss: 0.071173	Accuracy: 98.08%
41	Validation loss: 0.105812	Best loss: 0.071173	Accuracy: 98.28%
42	Validation loss: 0.094273	Best loss: 0.071173	Accuracy: 98.71%
43	Validation loss: 0.093487	Best loss: 0.071173	Accuracy: 98.36%
44	Validation loss: 0.094387	Best loss: 0.071173	Accuracy: 98.51%
45	Validation loss: 0.111084	Best loss: 0.071173	Accuracy: 98.71%
46	Validation loss: 0.106598	Best loss: 0.071173	Accuracy: 98.32%
47	Validation loss: 0.110840	Best loss: 0.071173	Accuracy: 98.40%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  16.1s
[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.178654	Best loss: 0.178654	Accuracy: 96.17%
1	Validation loss: 0.146276	Best loss: 0.146276	Accuracy: 96.13%
2	Validation loss: 0.147183	Best loss: 0.146276	Accuracy: 96.17%
3	Validation loss: 0.131057	Best loss: 0.131057	Accuracy: 96.44%
4	Validation loss: 0.170015	Best loss: 0.131057	Accuracy: 95.82%
5	Validation loss: 0.135399	Best loss: 0.131057	Accuracy: 96.64%
6	Validation loss: 0.143707	Best loss: 0.131057	Accuracy: 96.25%
7	Validation loss: 0.135335	Best loss: 0.131057	Accuracy: 96.52%
8	Validation loss: 0.165761	Best loss: 0.131057	Accuracy: 95.62%
9	Validation loss: 0.207927	Best loss: 0.131057	Accuracy: 93.98%
10	Validation loss: 0.188575	Best loss: 0.131057	Accuracy: 96.01%
11	Validation loss: 0.158964	Best loss: 0.131057	Accuracy: 96.13%
12	Validation loss: 0.241362	Best loss: 0.131057	Accuracy: 94.80%
13	Validation loss: 0.176953	Best loss: 0.131057	Accuracy: 95.47%
14	Validation loss: 0.200894	Best loss: 0.131057	Accuracy: 94.92%
15	Validation loss: 0.233800	Best loss: 0.131057	Accuracy: 95.82%
16	Validation loss: 0.215006	Best loss: 0.131057	Accuracy: 95.54%
17	Validation loss: 0.458718	Best loss: 0.131057	Accuracy: 78.97%
18	Validation loss: 0.413928	Best loss: 0.131057	Accuracy: 79.28%
19	Validation loss: 0.560573	Best loss: 0.131057	Accuracy: 72.63%
20	Validation loss: 0.433849	Best loss: 0.131057	Accuracy: 80.45%
21	Validation loss: 0.482278	Best loss: 0.131057	Accuracy: 78.15%
22	Validation loss: 0.545422	Best loss: 0.131057	Accuracy: 74.47%
23	Validation loss: 0.533977	Best loss: 0.131057	Accuracy: 76.97%
24	Validation loss: 0.917948	Best loss: 0.131057	Accuracy: 58.37%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   5.0s
[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.566326	Best loss: 0.566326	Accuracy: 74.59%
1	Validation loss: 0.544273	Best loss: 0.544273	Accuracy: 78.81%
2	Validation loss: 0.528807	Best loss: 0.528807	Accuracy: 76.08%
3	Validation loss: 0.509245	Best loss: 0.509245	Accuracy: 80.92%
4	Validation loss: 0.547789	Best loss: 0.509245	Accuracy: 79.09%
5	Validation loss: 0.502002	Best loss: 0.502002	Accuracy: 80.30%
6	Validation loss: 0.485137	Best loss: 0.485137	Accuracy: 81.24%
7	Validation loss: 0.506554	Best loss: 0.485137	Accuracy: 73.30%
8	Validation loss: 0.451554	Best loss: 0.451554	Accuracy: 83.62%
9	Validation loss: 0.468964	Best loss: 0.451554	Accuracy: 83.54%
10	Validation loss: 0.423287	Best loss: 0.423287	Accuracy: 85.57%
11	Validation loss: 0.402609	Best loss: 0.402609	Accuracy: 86.36%
12	Validation loss: 0.369873	Best loss: 0.369873	Accuracy: 88.70%
13	Validation loss: 0.396968	Best loss: 0.369873	Accuracy: 88.55%
14	Validation loss: 0.377162	Best loss: 0.369873	Accuracy: 89.68%
15	Validation loss: 0.353063	Best loss: 0.353063	Accuracy: 89.41%
16	Validation loss: 0.337188	Best loss: 0.337188	Accuracy: 89.95%
17	Validation loss: 0.331109	Best loss: 0.331109	Accuracy: 90.97%
18	Validation loss: 0.304207	Best loss: 0.304207	Accuracy: 92.26%
19	Validation loss: 0.425449	Best loss: 0.304207	Accuracy: 87.33%
20	Validation loss: 0.392361	Best loss: 0.304207	Accuracy: 88.12%
21	Validation loss: 0.509973	Best loss: 0.304207	Accuracy: 83.35%
22	Validation loss: 0.323481	Best loss: 0.304207	Accuracy: 90.15%
23	Validation loss: 0.372600	Best loss: 0.304207	Accuracy: 88.51%
24	Validation loss: 0.473344	Best loss: 0.304207	Accuracy: 85.38%
25	Validation loss: 0.333342	Best loss: 0.304207	Accuracy: 91.24%
26	Validation loss: 0.328909	Best loss: 0.304207	Accuracy: 90.77%
27	Validation loss: 0.437306	Best loss: 0.304207	Accuracy: 91.05%
28	Validation loss: 0.438796	Best loss: 0.304207	Accuracy: 88.04%
29	Validation loss: 0.393294	Best loss: 0.304207	Accuracy: 89.68%
30	Validation loss: 0.619243	Best loss: 0.304207	Accuracy: 70.21%
31	Validation loss: 0.601636	Best loss: 0.304207	Accuracy: 71.19%
32	Validation loss: 0.579141	Best loss: 0.304207	Accuracy: 71.19%
33	Validation loss: 0.580195	Best loss: 0.304207	Accuracy: 71.74%
34	Validation loss: 0.612437	Best loss: 0.304207	Accuracy: 71.58%
35	Validation loss: 0.638040	Best loss: 0.304207	Accuracy: 69.78%
36	Validation loss: 0.622924	Best loss: 0.304207	Accuracy: 68.37%
37	Validation loss: 0.610070	Best loss: 0.304207	Accuracy: 71.62%
38	Validation loss: 0.551005	Best loss: 0.304207	Accuracy: 81.82%
39	Validation loss: 0.358885	Best loss: 0.304207	Accuracy: 90.19%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   7.6s
[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.600484	Best loss: 0.600484	Accuracy: 69.94%
1	Validation loss: 0.583824	Best loss: 0.583824	Accuracy: 71.77%
2	Validation loss: 0.583143	Best loss: 0.583143	Accuracy: 70.09%
3	Validation loss: 0.626965	Best loss: 0.583143	Accuracy: 70.68%
4	Validation loss: 0.574527	Best loss: 0.574527	Accuracy: 72.60%
5	Validation loss: 0.561407	Best loss: 0.561407	Accuracy: 75.25%
6	Validation loss: 0.569099	Best loss: 0.561407	Accuracy: 74.32%
7	Validation loss: 0.629491	Best loss: 0.561407	Accuracy: 73.69%
8	Validation loss: 0.600609	Best loss: 0.561407	Accuracy: 71.66%
9	Validation loss: 0.541790	Best loss: 0.541790	Accuracy: 79.20%
10	Validation loss: 0.567158	Best loss: 0.541790	Accuracy: 83.97%
11	Validation loss: 0.726722	Best loss: 0.541790	Accuracy: 69.00%
12	Validation loss: 0.692018	Best loss: 0.541790	Accuracy: 68.06%
13	Validation loss: 0.738344	Best loss: 0.541790	Accuracy: 68.41%
14	Validation loss: 0.729945	Best loss: 0.541790	Accuracy: 68.57%
15	Validation loss: 0.863641	Best loss: 0.541790	Accuracy: 64.11%
16	Validation loss: 1.632048	Best loss: 0.541790	Accuracy: 18.80%
17	Validation loss: 1.513750	Best loss: 0.541790	Accuracy: 24.82%
18	Validation loss: 1.525382	Best loss: 0.541790	Accuracy: 25.72%
19	Validation loss: 1.337246	Best loss: 0.541790	Accuracy: 34.40%
20	Validation loss: 1.206082	Best loss: 0.541790	Accuracy: 43.75%
21	Validation loss: 1.046625	Best loss: 0.541790	Accuracy: 51.45%
22	Validation loss: 0.976512	Best loss: 0.541790	Accuracy: 57.15%
23	Validation loss: 0.972727	Best loss: 0.541790	Accuracy: 57.31%
24	Validation loss: 0.887570	Best loss: 0.541790	Accuracy: 61.14%
25	Validation loss: 0.885624	Best loss: 0.541790	Accuracy: 61.53%
26	Validation loss: 0.848716	Best loss: 0.541790	Accuracy: 62.47%
27	Validation loss: 0.854931	Best loss: 0.541790	Accuracy: 66.38%
28	Validation loss: 0.811165	Best loss: 0.541790	Accuracy: 60.87%
29	Validation loss: 0.838484	Best loss: 0.541790	Accuracy: 61.77%
30	Validation loss: 0.807819	Best loss: 0.541790	Accuracy: 62.71%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   6.0s
[CV] n_neurons=10, learning_rate=0.02, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.207486	Best loss: 0.207486	Accuracy: 94.53%
1	Validation loss: 0.130800	Best loss: 0.130800	Accuracy: 96.01%
2	Validation loss: 0.107869	Best loss: 0.107869	Accuracy: 97.22%
3	Validation loss: 0.092574	Best loss: 0.092574	Accuracy: 97.54%
4	Validation loss: 0.100518	Best loss: 0.092574	Accuracy: 96.83%
5	Validation loss: 0.092414	Best loss: 0.092414	Accuracy: 97.34%
6	Validation loss: 0.077367	Best loss: 0.077367	Accuracy: 97.89%
7	Validation loss: 0.074508	Best loss: 0.074508	Accuracy: 97.73%
8	Validation loss: 0.074833	Best loss: 0.074508	Accuracy: 97.85%
9	Validation loss: 0.075632	Best loss: 0.074508	Accuracy: 98.12%
10	Validation loss: 0.084214	Best loss: 0.074508	Accuracy: 97.81%
11	Validation loss: 0.080151	Best loss: 0.074508	Accuracy: 97.89%
12	Validation loss: 0.083634	Best loss: 0.074508	Accuracy: 97.73%
13	Validation loss: 0.089605	Best loss: 0.074508	Accuracy: 97.54%
14	Validation loss: 0.082191	Best loss: 0.074508	Accuracy: 97.97%
15	Validation loss: 0.080143	Best loss: 0.074508	Accuracy: 98.01%
16	Validation loss: 0.082523	Best loss: 0.074508	Accuracy: 97.97%
17	Validation loss: 0.104101	Best loss: 0.074508	Accuracy: 97.46%
18	Validation loss: 0.091626	Best loss: 0.074508	Accuracy: 97.73%
19	Validation loss: 0.091752	Best loss: 0.074508	Accuracy: 97.69%
20	Validation loss: 0.102569	Best loss: 0.074508	Accuracy: 97.85%
21	Validation loss: 0.103210	Best loss: 0.074508	Accuracy: 97.89%
22	Validation loss: 0.079039	Best loss: 0.074508	Accuracy: 98.12%
23	Validation loss: 0.104948	Best loss: 0.074508	Accuracy: 98.05%
24	Validation loss: 0.096223	Best loss: 0.074508	Accuracy: 97.77%
25	Validation loss: 0.094507	Best loss: 0.074508	Accuracy: 97.89%
26	Validation loss: 0.098282	Best loss: 0.074508	Accuracy: 97.73%
27	Validation loss: 0.098845	Best loss: 0.074508	Accuracy: 97.85%
28	Validation loss: 0.119070	Best loss: 0.074508	Accuracy: 97.46%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   3.8s
[CV] n_neurons=10, learning_rate=0.02, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.213791	Best loss: 0.213791	Accuracy: 93.98%
1	Validation loss: 0.134713	Best loss: 0.134713	Accuracy: 96.36%
2	Validation loss: 0.122060	Best loss: 0.122060	Accuracy: 96.44%
3	Validation loss: 0.105862	Best loss: 0.105862	Accuracy: 97.34%
4	Validation loss: 0.097588	Best loss: 0.097588	Accuracy: 97.34%
5	Validation loss: 0.094758	Best loss: 0.094758	Accuracy: 97.26%
6	Validation loss: 0.101506	Best loss: 0.094758	Accuracy: 96.99%
7	Validation loss: 0.105163	Best loss: 0.094758	Accuracy: 96.95%
8	Validation loss: 0.093930	Best loss: 0.093930	Accuracy: 97.62%
9	Validation loss: 0.099865	Best loss: 0.093930	Accuracy: 96.87%
10	Validation loss: 0.118037	Best loss: 0.093930	Accuracy: 96.60%
11	Validation loss: 0.086666	Best loss: 0.086666	Accuracy: 97.65%
12	Validation loss: 0.082490	Best loss: 0.082490	Accuracy: 97.69%
13	Validation loss: 0.087297	Best loss: 0.082490	Accuracy: 97.62%
14	Validation loss: 0.095151	Best loss: 0.082490	Accuracy: 97.19%
15	Validation loss: 0.099684	Best loss: 0.082490	Accuracy: 97.42%
16	Validation loss: 0.120457	Best loss: 0.082490	Accuracy: 97.19%
17	Validation loss: 0.084895	Best loss: 0.082490	Accuracy: 97.54%
18	Validation loss: 0.096523	Best loss: 0.082490	Accuracy: 97.62%
19	Validation loss: 0.100872	Best loss: 0.082490	Accuracy: 97.69%
20	Validation loss: 0.102067	Best loss: 0.082490	Accuracy: 97.34%
21	Validation loss: 0.102798	Best loss: 0.082490	Accuracy: 97.69%
22	Validation loss: 0.085588	Best loss: 0.082490	Accuracy: 97.65%
23	Validation loss: 0.099083	Best loss: 0.082490	Accuracy: 97.54%
24	Validation loss: 0.087378	Best loss: 0.082490	Accuracy: 97.97%
25	Validation loss: 0.098460	Best loss: 0.082490	Accuracy: 97.93%
26	Validation loss: 0.113241	Best loss: 0.082490	Accuracy: 97.30%
27	Validation loss: 0.111281	Best loss: 0.082490	Accuracy: 97.34%
28	Validation loss: 0.094299	Best loss: 0.082490	Accuracy: 97.73%
29	Validation loss: 0.104083	Best loss: 0.082490	Accuracy: 97.89%
30	Validation loss: 0.099387	Best loss: 0.082490	Accuracy: 97.69%
31	Validation loss: 0.089369	Best loss: 0.082490	Accuracy: 97.97%
32	Validation loss: 0.118494	Best loss: 0.082490	Accuracy: 97.69%
33	Validation loss: 0.107422	Best loss: 0.082490	Accuracy: 97.81%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   3.3s
[CV] n_neurons=10, learning_rate=0.02, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.394790	Best loss: 0.394790	Accuracy: 92.77%
1	Validation loss: 0.149463	Best loss: 0.149463	Accuracy: 95.86%
2	Validation loss: 0.117006	Best loss: 0.117006	Accuracy: 96.52%
3	Validation loss: 0.110410	Best loss: 0.110410	Accuracy: 96.60%
4	Validation loss: 0.101255	Best loss: 0.101255	Accuracy: 96.87%
5	Validation loss: 0.093460	Best loss: 0.093460	Accuracy: 97.42%
6	Validation loss: 0.092671	Best loss: 0.092671	Accuracy: 97.03%
7	Validation loss: 0.085061	Best loss: 0.085061	Accuracy: 97.50%
8	Validation loss: 0.092156	Best loss: 0.085061	Accuracy: 97.34%
9	Validation loss: 0.097953	Best loss: 0.085061	Accuracy: 97.11%
10	Validation loss: 0.087984	Best loss: 0.085061	Accuracy: 97.46%
11	Validation loss: 0.082237	Best loss: 0.082237	Accuracy: 97.62%
12	Validation loss: 0.080251	Best loss: 0.080251	Accuracy: 97.69%
13	Validation loss: 0.088765	Best loss: 0.080251	Accuracy: 97.54%
14	Validation loss: 0.096297	Best loss: 0.080251	Accuracy: 97.42%
15	Validation loss: 0.082582	Best loss: 0.080251	Accuracy: 97.77%
16	Validation loss: 0.099556	Best loss: 0.080251	Accuracy: 97.19%
17	Validation loss: 0.093599	Best loss: 0.080251	Accuracy: 97.54%
18	Validation loss: 0.097314	Best loss: 0.080251	Accuracy: 97.50%
19	Validation loss: 0.082903	Best loss: 0.080251	Accuracy: 97.77%
20	Validation loss: 0.082038	Best loss: 0.080251	Accuracy: 97.73%
21	Validation loss: 0.091673	Best loss: 0.080251	Accuracy: 97.65%
22	Validation loss: 0.083849	Best loss: 0.080251	Accuracy: 97.93%
23	Validation loss: 0.091656	Best loss: 0.080251	Accuracy: 97.62%
24	Validation loss: 0.090152	Best loss: 0.080251	Accuracy: 97.58%
25	Validation loss: 0.130457	Best loss: 0.080251	Accuracy: 97.22%
26	Validation loss: 0.115599	Best loss: 0.080251	Accuracy: 96.83%
27	Validation loss: 0.106129	Best loss: 0.080251	Accuracy: 97.50%
28	Validation loss: 0.086481	Best loss: 0.080251	Accuracy: 97.77%
29	Validation loss: 0.100005	Best loss: 0.080251	Accuracy: 97.62%
30	Validation loss: 0.098412	Best loss: 0.080251	Accuracy: 97.62%
31	Validation loss: 0.096594	Best loss: 0.080251	Accuracy: 97.89%
32	Validation loss: 0.114362	Best loss: 0.080251	Accuracy: 97.50%
33	Validation loss: 0.112146	Best loss: 0.080251	Accuracy: 97.73%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   3.3s
[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.098576	Best loss: 0.098576	Accuracy: 97.46%
1	Validation loss: 0.109577	Best loss: 0.098576	Accuracy: 96.48%
2	Validation loss: 0.104401	Best loss: 0.098576	Accuracy: 97.22%
3	Validation loss: 0.165897	Best loss: 0.098576	Accuracy: 94.53%
4	Validation loss: 0.100991	Best loss: 0.098576	Accuracy: 97.11%
5	Validation loss: 0.065385	Best loss: 0.065385	Accuracy: 98.12%
6	Validation loss: 0.069665	Best loss: 0.065385	Accuracy: 98.05%
7	Validation loss: 0.081934	Best loss: 0.065385	Accuracy: 98.08%
8	Validation loss: 0.122111	Best loss: 0.065385	Accuracy: 96.79%
9	Validation loss: 0.408159	Best loss: 0.065385	Accuracy: 95.54%
10	Validation loss: 0.430802	Best loss: 0.065385	Accuracy: 85.22%
11	Validation loss: 0.162833	Best loss: 0.065385	Accuracy: 97.22%
12	Validation loss: 0.148286	Best loss: 0.065385	Accuracy: 96.87%
13	Validation loss: 0.100783	Best loss: 0.065385	Accuracy: 97.97%
14	Validation loss: 0.088627	Best loss: 0.065385	Accuracy: 98.05%
15	Validation loss: 0.138482	Best loss: 0.065385	Accuracy: 97.85%
16	Validation loss: 0.210588	Best loss: 0.065385	Accuracy: 97.62%
17	Validation loss: 0.200730	Best loss: 0.065385	Accuracy: 97.62%
18	Validation loss: 0.737146	Best loss: 0.065385	Accuracy: 95.93%
19	Validation loss: 0.915009	Best loss: 0.065385	Accuracy: 75.41%
20	Validation loss: 0.527974	Best loss: 0.065385	Accuracy: 77.87%
21	Validation loss: 0.206539	Best loss: 0.065385	Accuracy: 97.34%
22	Validation loss: 0.323650	Best loss: 0.065385	Accuracy: 95.00%
23	Validation loss: 0.277662	Best loss: 0.065385	Accuracy: 96.40%
24	Validation loss: 0.263050	Best loss: 0.065385	Accuracy: 96.76%
25	Validation loss: 0.292013	Best loss: 0.065385	Accuracy: 96.29%
26	Validation loss: 0.244727	Best loss: 0.065385	Accuracy: 96.44%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.1s
[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.117713	Best loss: 0.117713	Accuracy: 97.22%
1	Validation loss: 0.094400	Best loss: 0.094400	Accuracy: 97.69%
2	Validation loss: 0.065969	Best loss: 0.065969	Accuracy: 98.16%
3	Validation loss: 0.094659	Best loss: 0.065969	Accuracy: 97.15%
4	Validation loss: 0.096006	Best loss: 0.065969	Accuracy: 97.89%
5	Validation loss: 0.890320	Best loss: 0.065969	Accuracy: 82.17%
6	Validation loss: 0.221224	Best loss: 0.065969	Accuracy: 94.72%
7	Validation loss: 0.142926	Best loss: 0.065969	Accuracy: 97.50%
8	Validation loss: 0.239876	Best loss: 0.065969	Accuracy: 96.56%
9	Validation loss: 0.115036	Best loss: 0.065969	Accuracy: 98.28%
10	Validation loss: 0.089214	Best loss: 0.065969	Accuracy: 98.12%
11	Validation loss: 0.121801	Best loss: 0.065969	Accuracy: 98.40%
12	Validation loss: 0.141299	Best loss: 0.065969	Accuracy: 98.32%
13	Validation loss: 0.100573	Best loss: 0.065969	Accuracy: 98.20%
14	Validation loss: 0.108493	Best loss: 0.065969	Accuracy: 98.28%
15	Validation loss: 0.088625	Best loss: 0.065969	Accuracy: 97.97%
16	Validation loss: 0.085419	Best loss: 0.065969	Accuracy: 98.48%
17	Validation loss: 0.254387	Best loss: 0.065969	Accuracy: 97.19%
18	Validation loss: 1.655165	Best loss: 0.065969	Accuracy: 19.27%
19	Validation loss: 1.627377	Best loss: 0.065969	Accuracy: 19.27%
20	Validation loss: 1.667500	Best loss: 0.065969	Accuracy: 20.91%
21	Validation loss: 1.679644	Best loss: 0.065969	Accuracy: 18.73%
22	Validation loss: 1.615968	Best loss: 0.065969	Accuracy: 22.01%
23	Validation loss: 1.705366	Best loss: 0.065969	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   8.7s
[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.122131	Best loss: 0.122131	Accuracy: 96.21%
1	Validation loss: 0.113646	Best loss: 0.113646	Accuracy: 97.54%
2	Validation loss: 1.522864	Best loss: 0.113646	Accuracy: 35.46%
3	Validation loss: 0.748457	Best loss: 0.113646	Accuracy: 73.22%
4	Validation loss: 0.196577	Best loss: 0.113646	Accuracy: 95.31%
5	Validation loss: 0.127123	Best loss: 0.113646	Accuracy: 97.19%
6	Validation loss: 0.115110	Best loss: 0.113646	Accuracy: 97.54%
7	Validation loss: 0.168400	Best loss: 0.113646	Accuracy: 97.42%
8	Validation loss: 0.170906	Best loss: 0.113646	Accuracy: 97.30%
9	Validation loss: 0.107348	Best loss: 0.107348	Accuracy: 98.12%
10	Validation loss: 1.222679	Best loss: 0.107348	Accuracy: 39.87%
11	Validation loss: 1.213918	Best loss: 0.107348	Accuracy: 37.45%
12	Validation loss: 1.192050	Best loss: 0.107348	Accuracy: 39.25%
13	Validation loss: 1.228508	Best loss: 0.107348	Accuracy: 37.06%
14	Validation loss: 1.191023	Best loss: 0.107348	Accuracy: 37.45%
15	Validation loss: 1.155447	Best loss: 0.107348	Accuracy: 39.44%
16	Validation loss: 1.177388	Best loss: 0.107348	Accuracy: 37.53%
17	Validation loss: 1.197146	Best loss: 0.107348	Accuracy: 39.44%
18	Validation loss: 1.161949	Best loss: 0.107348	Accuracy: 41.99%
19	Validation loss: 1.156646	Best loss: 0.107348	Accuracy: 37.88%
20	Validation loss: 1.136484	Best loss: 0.107348	Accuracy: 41.01%
21	Validation loss: 1.261032	Best loss: 0.107348	Accuracy: 40.42%
22	Validation loss: 1.157738	Best loss: 0.107348	Accuracy: 37.49%
23	Validation loss: 1.263299	Best loss: 0.107348	Accuracy: 40.70%
24	Validation loss: 0.791872	Best loss: 0.107348	Accuracy: 61.10%
25	Validation loss: 0.747293	Best loss: 0.107348	Accuracy: 58.21%
26	Validation loss: 0.799284	Best loss: 0.107348	Accuracy: 57.47%
27	Validation loss: 0.809419	Best loss: 0.107348	Accuracy: 57.86%
28	Validation loss: 0.767365	Best loss: 0.107348	Accuracy: 57.54%
29	Validation loss: 1.722662	Best loss: 0.107348	Accuracy: 35.50%
30	Validation loss: 1.008404	Best loss: 0.107348	Accuracy: 40.03%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  10.3s
[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.124031	Best loss: 0.124031	Accuracy: 96.99%
1	Validation loss: 0.109387	Best loss: 0.109387	Accuracy: 97.22%
2	Validation loss: 0.096057	Best loss: 0.096057	Accuracy: 97.07%
3	Validation loss: 0.094867	Best loss: 0.094867	Accuracy: 97.85%
4	Validation loss: 0.124788	Best loss: 0.094867	Accuracy: 96.87%
5	Validation loss: 0.101062	Best loss: 0.094867	Accuracy: 97.69%
6	Validation loss: 0.372596	Best loss: 0.094867	Accuracy: 95.11%
7	Validation loss: 0.118188	Best loss: 0.094867	Accuracy: 96.76%
8	Validation loss: 0.115300	Best loss: 0.094867	Accuracy: 96.79%
9	Validation loss: 0.090794	Best loss: 0.090794	Accuracy: 97.93%
10	Validation loss: 0.084682	Best loss: 0.084682	Accuracy: 98.20%
11	Validation loss: 0.080577	Best loss: 0.080577	Accuracy: 97.93%
12	Validation loss: 0.086808	Best loss: 0.080577	Accuracy: 97.58%
13	Validation loss: 0.088385	Best loss: 0.080577	Accuracy: 98.05%
14	Validation loss: 0.093950	Best loss: 0.080577	Accuracy: 97.65%
15	Validation loss: 0.077412	Best loss: 0.077412	Accuracy: 97.93%
16	Validation loss: 0.162574	Best loss: 0.077412	Accuracy: 97.93%
17	Validation loss: 0.072786	Best loss: 0.072786	Accuracy: 98.05%
18	Validation loss: 0.084302	Best loss: 0.072786	Accuracy: 97.97%
19	Validation loss: 0.086607	Best loss: 0.072786	Accuracy: 98.01%
20	Validation loss: 0.171010	Best loss: 0.072786	Accuracy: 97.65%
21	Validation loss: 0.081593	Best loss: 0.072786	Accuracy: 98.01%
22	Validation loss: 1.478996	Best loss: 0.072786	Accuracy: 93.55%
23	Validation loss: 0.553737	Best loss: 0.072786	Accuracy: 95.90%
24	Validation loss: 0.482796	Best loss: 0.072786	Accuracy: 95.47%
25	Validation loss: 0.312746	Best loss: 0.072786	Accuracy: 96.87%
26	Validation loss: 0.641606	Best loss: 0.072786	Accuracy: 94.49%
27	Validation loss: 0.464669	Best loss: 0.072786	Accuracy: 95.93%
28	Validation loss: 0.278866	Best loss: 0.072786	Accuracy: 97.22%
29	Validation loss: 0.439674	Best loss: 0.072786	Accuracy: 94.49%
30	Validation loss: 0.212626	Best loss: 0.072786	Accuracy: 97.22%
31	Validation loss: 0.187699	Best loss: 0.072786	Accuracy: 97.22%
32	Validation loss: 0.250207	Best loss: 0.072786	Accuracy: 96.79%
33	Validation loss: 0.294034	Best loss: 0.072786	Accuracy: 95.70%
34	Validation loss: 0.237576	Best loss: 0.072786	Accuracy: 97.42%
35	Validation loss: 0.186174	Best loss: 0.072786	Accuracy: 97.46%
36	Validation loss: 0.247089	Best loss: 0.072786	Accuracy: 97.30%
37	Validation loss: 0.276806	Best loss: 0.072786	Accuracy: 95.86%
38	Validation loss: 0.179723	Best loss: 0.072786	Accuracy: 97.34%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  10.5s
[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.124410	Best loss: 0.124410	Accuracy: 97.15%
1	Validation loss: 0.111566	Best loss: 0.111566	Accuracy: 96.64%
2	Validation loss: 0.087142	Best loss: 0.087142	Accuracy: 97.62%
3	Validation loss: 0.145317	Best loss: 0.087142	Accuracy: 96.99%
4	Validation loss: 0.062343	Best loss: 0.062343	Accuracy: 98.36%
5	Validation loss: 0.087214	Best loss: 0.062343	Accuracy: 97.77%
6	Validation loss: 3.245079	Best loss: 0.062343	Accuracy: 81.00%
7	Validation loss: 0.136732	Best loss: 0.062343	Accuracy: 96.87%
8	Validation loss: 0.119195	Best loss: 0.062343	Accuracy: 97.26%
9	Validation loss: 0.165915	Best loss: 0.062343	Accuracy: 96.33%
10	Validation loss: 0.164220	Best loss: 0.062343	Accuracy: 96.95%
11	Validation loss: 0.120173	Best loss: 0.062343	Accuracy: 97.77%
12	Validation loss: 0.084389	Best loss: 0.062343	Accuracy: 97.85%
13	Validation loss: 0.085428	Best loss: 0.062343	Accuracy: 98.16%
14	Validation loss: 0.079955	Best loss: 0.062343	Accuracy: 98.12%
15	Validation loss: 0.108905	Best loss: 0.062343	Accuracy: 97.89%
16	Validation loss: 0.114831	Best loss: 0.062343	Accuracy: 97.77%
17	Validation loss: 0.131415	Best loss: 0.062343	Accuracy: 97.81%
18	Validation loss: 0.093874	Best loss: 0.062343	Accuracy: 97.77%
19	Validation loss: 0.114136	Best loss: 0.062343	Accuracy: 97.89%
20	Validation loss: 0.098332	Best loss: 0.062343	Accuracy: 98.05%
21	Validation loss: 0.094333	Best loss: 0.062343	Accuracy: 97.89%
22	Validation loss: 0.126372	Best loss: 0.062343	Accuracy: 97.97%
23	Validation loss: 0.145349	Best loss: 0.062343	Accuracy: 97.93%
24	Validation loss: 0.126429	Best loss: 0.062343	Accuracy: 98.01%
25	Validation loss: 0.265013	Best loss: 0.062343	Accuracy: 97.77%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   7.4s
[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.104337	Best loss: 0.104337	Accuracy: 97.50%
1	Validation loss: 0.084283	Best loss: 0.084283	Accuracy: 97.34%
2	Validation loss: 0.075232	Best loss: 0.075232	Accuracy: 97.97%
3	Validation loss: 0.104202	Best loss: 0.075232	Accuracy: 97.46%
4	Validation loss: 0.166353	Best loss: 0.075232	Accuracy: 96.95%
5	Validation loss: 0.512761	Best loss: 0.075232	Accuracy: 90.70%
6	Validation loss: 0.171127	Best loss: 0.075232	Accuracy: 96.29%
7	Validation loss: 0.148642	Best loss: 0.075232	Accuracy: 96.79%
8	Validation loss: 0.142347	Best loss: 0.075232	Accuracy: 97.42%
9	Validation loss: 0.155604	Best loss: 0.075232	Accuracy: 97.34%
10	Validation loss: 0.154422	Best loss: 0.075232	Accuracy: 97.77%
11	Validation loss: 0.163809	Best loss: 0.075232	Accuracy: 97.81%
12	Validation loss: 0.144619	Best loss: 0.075232	Accuracy: 97.77%
13	Validation loss: 0.146264	Best loss: 0.075232	Accuracy: 98.01%
14	Validation loss: 0.167427	Best loss: 0.075232	Accuracy: 97.58%
15	Validation loss: 0.168090	Best loss: 0.075232	Accuracy: 97.93%
16	Validation loss: 0.115891	Best loss: 0.075232	Accuracy: 97.85%
17	Validation loss: 0.119827	Best loss: 0.075232	Accuracy: 98.24%
18	Validation loss: 0.123248	Best loss: 0.075232	Accuracy: 98.05%
19	Validation loss: 0.141931	Best loss: 0.075232	Accuracy: 98.12%
20	Validation loss: 0.080278	Best loss: 0.075232	Accuracy: 98.20%
21	Validation loss: 0.139633	Best loss: 0.075232	Accuracy: 97.93%
22	Validation loss: 0.122557	Best loss: 0.075232	Accuracy: 98.36%
23	Validation loss: 0.131697	Best loss: 0.075232	Accuracy: 98.24%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   6.9s
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.105370	Best loss: 2.105370	Accuracy: 19.27%
1	Validation loss: 1.792641	Best loss: 1.792641	Accuracy: 22.01%
2	Validation loss: 2.225835	Best loss: 1.792641	Accuracy: 19.27%
3	Validation loss: 3.221144	Best loss: 1.792641	Accuracy: 18.73%
4	Validation loss: 2.254824	Best loss: 1.792641	Accuracy: 18.73%
5	Validation loss: 3.244872	Best loss: 1.792641	Accuracy: 20.91%
6	Validation loss: 2.319155	Best loss: 1.792641	Accuracy: 19.27%
7	Validation loss: 2.237682	Best loss: 1.792641	Accuracy: 20.91%
8	Validation loss: 3.369040	Best loss: 1.792641	Accuracy: 19.08%
9	Validation loss: 2.729949	Best loss: 1.792641	Accuracy: 22.01%
10	Validation loss: 2.272411	Best loss: 1.792641	Accuracy: 20.91%
11	Validation loss: 4.206909	Best loss: 1.792641	Accuracy: 19.08%
12	Validation loss: 2.241374	Best loss: 1.792641	Accuracy: 22.01%
13	Validation loss: 2.446312	Best loss: 1.792641	Accuracy: 22.01%
14	Validation loss: 2.820485	Best loss: 1.792641	Accuracy: 22.01%
15	Validation loss: 2.841007	Best loss: 1.792641	Accuracy: 19.08%
16	Validation loss: 3.978535	Best loss: 1.792641	Accuracy: 18.73%
17	Validation loss: 2.961750	Best loss: 1.792641	Accuracy: 19.08%
18	Validation loss: 3.258812	Best loss: 1.792641	Accuracy: 22.01%
19	Validation loss: 4.359087	Best loss: 1.792641	Accuracy: 22.01%
20	Validation loss: 3.578790	Best loss: 1.792641	Accuracy: 22.01%
21	Validation loss: 3.364505	Best loss: 1.792641	Accuracy: 20.91%
22	Validation loss: 2.003894	Best loss: 1.792641	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  46.4s
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.626807	Best loss: 1.626807	Accuracy: 19.08%
1	Validation loss: 2.016086	Best loss: 1.626807	Accuracy: 19.27%
2	Validation loss: 1.730734	Best loss: 1.626807	Accuracy: 20.91%
3	Validation loss: 2.506221	Best loss: 1.626807	Accuracy: 22.01%
4	Validation loss: 2.564813	Best loss: 1.626807	Accuracy: 19.08%
5	Validation loss: 2.373041	Best loss: 1.626807	Accuracy: 18.73%
6	Validation loss: 2.495036	Best loss: 1.626807	Accuracy: 19.27%
7	Validation loss: 3.045451	Best loss: 1.626807	Accuracy: 19.08%
8	Validation loss: 3.277675	Best loss: 1.626807	Accuracy: 18.73%
9	Validation loss: 2.673789	Best loss: 1.626807	Accuracy: 22.01%
10	Validation loss: 2.647234	Best loss: 1.626807	Accuracy: 20.91%
11	Validation loss: 4.397942	Best loss: 1.626807	Accuracy: 19.08%
12	Validation loss: 2.239443	Best loss: 1.626807	Accuracy: 19.08%
13	Validation loss: 2.290069	Best loss: 1.626807	Accuracy: 22.01%
14	Validation loss: 2.716570	Best loss: 1.626807	Accuracy: 20.91%
15	Validation loss: 2.944774	Best loss: 1.626807	Accuracy: 20.91%
16	Validation loss: 2.316695	Best loss: 1.626807	Accuracy: 22.01%
17	Validation loss: 3.568805	Best loss: 1.626807	Accuracy: 18.73%
18	Validation loss: 2.146409	Best loss: 1.626807	Accuracy: 19.27%
19	Validation loss: 2.572818	Best loss: 1.626807	Accuracy: 18.73%
20	Validation loss: 1.871783	Best loss: 1.626807	Accuracy: 18.73%
21	Validation loss: 2.151021	Best loss: 1.626807	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  44.4s
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.747268	Best loss: 1.747268	Accuracy: 20.91%
1	Validation loss: 2.225343	Best loss: 1.747268	Accuracy: 22.01%
2	Validation loss: 2.564789	Best loss: 1.747268	Accuracy: 19.08%
3	Validation loss: 2.893422	Best loss: 1.747268	Accuracy: 19.27%
4	Validation loss: 2.342296	Best loss: 1.747268	Accuracy: 20.91%
5	Validation loss: 2.412631	Best loss: 1.747268	Accuracy: 20.91%
6	Validation loss: 2.488507	Best loss: 1.747268	Accuracy: 20.91%
7	Validation loss: 2.537169	Best loss: 1.747268	Accuracy: 19.08%
8	Validation loss: 1.855671	Best loss: 1.747268	Accuracy: 22.01%
9	Validation loss: 3.472439	Best loss: 1.747268	Accuracy: 19.27%
10	Validation loss: 3.425062	Best loss: 1.747268	Accuracy: 18.73%
11	Validation loss: 2.849223	Best loss: 1.747268	Accuracy: 19.08%
12	Validation loss: 2.780649	Best loss: 1.747268	Accuracy: 19.08%
13	Validation loss: 6.415426	Best loss: 1.747268	Accuracy: 22.01%
14	Validation loss: 4.538351	Best loss: 1.747268	Accuracy: 19.27%
15	Validation loss: 1.910851	Best loss: 1.747268	Accuracy: 19.27%
16	Validation loss: 2.299468	Best loss: 1.747268	Accuracy: 19.08%
17	Validation loss: 2.972358	Best loss: 1.747268	Accuracy: 18.73%
18	Validation loss: 2.482542	Best loss: 1.747268	Accuracy: 19.27%
19	Validation loss: 2.255617	Best loss: 1.747268	Accuracy: 22.01%
20	Validation loss: 2.188776	Best loss: 1.747268	Accuracy: 19.27%
21	Validation loss: 3.428161	Best loss: 1.747268	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  45.2s
[CV] n_neurons=90, learning_rate=0.1, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.612701	Best loss: 1.612701	Accuracy: 18.73%
1	Validation loss: 1.628481	Best loss: 1.612701	Accuracy: 19.08%
2	Validation loss: 1.627636	Best loss: 1.612701	Accuracy: 19.08%
3	Validation loss: 1.654221	Best loss: 1.612701	Accuracy: 19.08%
4	Validation loss: 1.698263	Best loss: 1.612701	Accuracy: 19.27%
5	Validation loss: 1.644175	Best loss: 1.612701	Accuracy: 18.73%
6	Validation loss: 1.655063	Best loss: 1.612701	Accuracy: 19.27%
7	Validation loss: 1.636059	Best loss: 1.612701	Accuracy: 19.08%
8	Validation loss: 1.721399	Best loss: 1.612701	Accuracy: 19.27%
9	Validation loss: 1.809415	Best loss: 1.612701	Accuracy: 19.08%
10	Validation loss: 1.682375	Best loss: 1.612701	Accuracy: 22.01%
11	Validation loss: 1.685104	Best loss: 1.612701	Accuracy: 18.73%
12	Validation loss: 1.703506	Best loss: 1.612701	Accuracy: 19.08%
13	Validation loss: 1.674670	Best loss: 1.612701	Accuracy: 22.01%
14	Validation loss: 1.720536	Best loss: 1.612701	Accuracy: 19.08%
15	Validation loss: 1.658070	Best loss: 1.612701	Accuracy: 22.01%
16	Validation loss: 1.705758	Best loss: 1.612701	Accuracy: 18.73%
17	Validation loss: 1.714344	Best loss: 1.612701	Accuracy: 18.73%
18	Validation loss: 1.746365	Best loss: 1.612701	Accuracy: 20.91%
19	Validation loss: 1.823398	Best loss: 1.612701	Accuracy: 20.91%
20	Validation loss: 1.622464	Best loss: 1.612701	Accuracy: 19.08%
21	Validation loss: 1.691369	Best loss: 1.612701	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.5s
[CV] n_neurons=90, learning_rate=0.1, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.359327	Best loss: 0.359327	Accuracy: 90.46%
1	Validation loss: 0.415580	Best loss: 0.359327	Accuracy: 91.52%
2	Validation loss: 0.182378	Best loss: 0.182378	Accuracy: 95.90%
3	Validation loss: 0.176038	Best loss: 0.176038	Accuracy: 95.43%
4	Validation loss: 6.454424	Best loss: 0.176038	Accuracy: 54.61%
5	Validation loss: 1.669825	Best loss: 0.176038	Accuracy: 22.01%
6	Validation loss: 1.637594	Best loss: 0.176038	Accuracy: 19.08%
7	Validation loss: 1.624218	Best loss: 0.176038	Accuracy: 18.73%
8	Validation loss: 1.621284	Best loss: 0.176038	Accuracy: 19.27%
9	Validation loss: 1.682289	Best loss: 0.176038	Accuracy: 22.01%
10	Validation loss: 1.628525	Best loss: 0.176038	Accuracy: 19.27%
11	Validation loss: 1.635774	Best loss: 0.176038	Accuracy: 19.27%
12	Validation loss: 1.627792	Best loss: 0.176038	Accuracy: 20.91%
13	Validation loss: 1.634672	Best loss: 0.176038	Accuracy: 18.73%
14	Validation loss: 1.726843	Best loss: 0.176038	Accuracy: 22.01%
15	Validation loss: 1.705196	Best loss: 0.176038	Accuracy: 18.73%
16	Validation loss: 1.644885	Best loss: 0.176038	Accuracy: 19.27%
17	Validation loss: 1.678849	Best loss: 0.176038	Accuracy: 20.91%
18	Validation loss: 1.704342	Best loss: 0.176038	Accuracy: 22.01%
19	Validation loss: 1.663776	Best loss: 0.176038	Accuracy: 22.01%
20	Validation loss: 1.802348	Best loss: 0.176038	Accuracy: 19.27%
21	Validation loss: 1.845189	Best loss: 0.176038	Accuracy: 22.01%
22	Validation loss: 1.716254	Best loss: 0.176038	Accuracy: 19.27%
23	Validation loss: 1.712694	Best loss: 0.176038	Accuracy: 22.01%
24	Validation loss: 1.877658	Best loss: 0.176038	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  11.1s
[CV] n_neurons=90, learning_rate=0.1, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.371520	Best loss: 1.371520	Accuracy: 54.81%
1	Validation loss: 0.504213	Best loss: 0.504213	Accuracy: 77.44%
2	Validation loss: 0.565278	Best loss: 0.504213	Accuracy: 75.45%
3	Validation loss: 0.319430	Best loss: 0.319430	Accuracy: 92.06%
4	Validation loss: 0.313614	Best loss: 0.313614	Accuracy: 91.13%
5	Validation loss: 0.334844	Best loss: 0.313614	Accuracy: 91.99%
6	Validation loss: 0.358455	Best loss: 0.313614	Accuracy: 90.54%
7	Validation loss: 1.618367	Best loss: 0.313614	Accuracy: 22.01%
8	Validation loss: 1.629803	Best loss: 0.313614	Accuracy: 19.27%
9	Validation loss: 1.668949	Best loss: 0.313614	Accuracy: 22.01%
10	Validation loss: 1.634287	Best loss: 0.313614	Accuracy: 22.01%
11	Validation loss: 1.636450	Best loss: 0.313614	Accuracy: 19.27%
12	Validation loss: 1.647250	Best loss: 0.313614	Accuracy: 19.08%
13	Validation loss: 1.625385	Best loss: 0.313614	Accuracy: 19.08%
14	Validation loss: 1.656526	Best loss: 0.313614	Accuracy: 19.08%
15	Validation loss: 1.631290	Best loss: 0.313614	Accuracy: 20.91%
16	Validation loss: 1.716754	Best loss: 0.313614	Accuracy: 19.27%
17	Validation loss: 1.669007	Best loss: 0.313614	Accuracy: 20.91%
18	Validation loss: 1.637150	Best loss: 0.313614	Accuracy: 18.73%
19	Validation loss: 1.681008	Best loss: 0.313614	Accuracy: 18.73%
20	Validation loss: 1.661242	Best loss: 0.313614	Accuracy: 19.08%
21	Validation loss: 1.860466	Best loss: 0.313614	Accuracy: 22.01%
22	Validation loss: 1.722477	Best loss: 0.313614	Accuracy: 18.73%
23	Validation loss: 1.701664	Best loss: 0.313614	Accuracy: 22.01%
24	Validation loss: 1.617530	Best loss: 0.313614	Accuracy: 18.73%
25	Validation loss: 1.790974	Best loss: 0.313614	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.9s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 1.414469	Best loss: 1.414469	Accuracy: 39.01%
1	Validation loss: 0.971469	Best loss: 0.971469	Accuracy: 60.75%
2	Validation loss: 0.662494	Best loss: 0.662494	Accuracy: 75.72%
3	Validation loss: 0.610591	Best loss: 0.610591	Accuracy: 75.33%
4	Validation loss: 0.454052	Best loss: 0.454052	Accuracy: 84.95%
5	Validation loss: 0.475226	Best loss: 0.454052	Accuracy: 77.83%
6	Validation loss: 129.066193	Best loss: 0.454052	Accuracy: 22.44%
7	Validation loss: 1139.646362	Best loss: 0.454052	Accuracy: 19.27%
8	Validation loss: 982.544495	Best loss: 0.454052	Accuracy: 20.91%
9	Validation loss: 402.371246	Best loss: 0.454052	Accuracy: 20.91%
10	Validation loss: 155.374359	Best loss: 0.454052	Accuracy: 20.95%
11	Validation loss: 120.516502	Best loss: 0.454052	Accuracy: 32.99%
12	Validation loss: 71.686569	Best loss: 0.454052	Accuracy: 19.51%
13	Validation loss: 92.486191	Best loss: 0.454052	Accuracy: 19.08%
14	Validation loss: 4.758350	Best loss: 0.454052	Accuracy: 25.80%
15	Validation loss: 12.766411	Best loss: 0.454052	Accuracy: 37.14%
16	Validation loss: 28.881765	Best loss: 0.454052	Accuracy: 31.08%
17	Validation loss: 5.388309	Best loss: 0.454052	Accuracy: 43.24%
18	Validation loss: 30.043100	Best loss: 0.454052	Accuracy: 21.70%
19	Validation loss: 5.271226	Best loss: 0.454052	Accuracy: 37.76%
20	Validation loss: 4.753526	Best loss: 0.454052	Accuracy: 42.92%
21	Validation loss: 12.851851	Best loss: 0.454052	Accuracy: 36.08%
22	Validation loss: 6.638640	Best loss: 0.454052	Accuracy: 37.02%
23	Validation loss: 4.053299	Best loss: 0.454052	Accuracy: 44.80%
24	Validation loss: 1.690434	Best loss: 0.454052	Accuracy: 46.01%
25	Validation loss: 1.618438	Best loss: 0.454052	Accuracy: 48.32%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   4.3s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 1.331102	Best loss: 1.331102	Accuracy: 53.09%
1	Validation loss: 0.521213	Best loss: 0.521213	Accuracy: 86.20%
2	Validation loss: 0.299926	Best loss: 0.299926	Accuracy: 90.03%
3	Validation loss: 0.263144	Best loss: 0.263144	Accuracy: 92.30%
4	Validation loss: 0.148634	Best loss: 0.148634	Accuracy: 95.74%
5	Validation loss: 0.149212	Best loss: 0.148634	Accuracy: 95.54%
6	Validation loss: 22.653978	Best loss: 0.148634	Accuracy: 19.35%
7	Validation loss: 1024.520996	Best loss: 0.148634	Accuracy: 19.27%
8	Validation loss: 1234.848755	Best loss: 0.148634	Accuracy: 19.08%
9	Validation loss: 1032.739014	Best loss: 0.148634	Accuracy: 18.73%
10	Validation loss: 1821.194702	Best loss: 0.148634	Accuracy: 19.27%
11	Validation loss: 4108.279785	Best loss: 0.148634	Accuracy: 19.31%
12	Validation loss: 6980.005371	Best loss: 0.148634	Accuracy: 18.73%
13	Validation loss: 406.276611	Best loss: 0.148634	Accuracy: 19.35%
14	Validation loss: 181.438690	Best loss: 0.148634	Accuracy: 20.72%
15	Validation loss: 127.127701	Best loss: 0.148634	Accuracy: 16.81%
16	Validation loss: 104.221306	Best loss: 0.148634	Accuracy: 21.03%
17	Validation loss: 91.561287	Best loss: 0.148634	Accuracy: 17.87%
18	Validation loss: 92.720085	Best loss: 0.148634	Accuracy: 17.59%
19	Validation loss: 106.311546	Best loss: 0.148634	Accuracy: 18.10%
20	Validation loss: 62.373569	Best loss: 0.148634	Accuracy: 17.71%
21	Validation loss: 43.266510	Best loss: 0.148634	Accuracy: 25.92%
22	Validation loss: 27.779711	Best loss: 0.148634	Accuracy: 21.54%
23	Validation loss: 28.797630	Best loss: 0.148634	Accuracy: 25.61%
24	Validation loss: 22.275381	Best loss: 0.148634	Accuracy: 16.69%
25	Validation loss: 64.497231	Best loss: 0.148634	Accuracy: 24.90%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   4.5s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 1.347215	Best loss: 1.347215	Accuracy: 32.25%
1	Validation loss: 0.435853	Best loss: 0.435853	Accuracy: 88.12%
2	Validation loss: 0.202064	Best loss: 0.202064	Accuracy: 94.53%
3	Validation loss: 0.356782	Best loss: 0.202064	Accuracy: 93.16%
4	Validation loss: 8.308368	Best loss: 0.202064	Accuracy: 53.75%
5	Validation loss: 89.517143	Best loss: 0.202064	Accuracy: 19.70%
6	Validation loss: 159.847458	Best loss: 0.202064	Accuracy: 32.99%
7	Validation loss: 23.016571	Best loss: 0.202064	Accuracy: 31.47%
8	Validation loss: 117.867607	Best loss: 0.202064	Accuracy: 42.38%
9	Validation loss: 13.977825	Best loss: 0.202064	Accuracy: 50.12%
10	Validation loss: 15.568913	Best loss: 0.202064	Accuracy: 46.91%
11	Validation loss: 2.935148	Best loss: 0.202064	Accuracy: 58.52%
12	Validation loss: 2.571046	Best loss: 0.202064	Accuracy: 54.46%
13	Validation loss: 0.976211	Best loss: 0.202064	Accuracy: 65.09%
14	Validation loss: 0.844333	Best loss: 0.202064	Accuracy: 68.80%
15	Validation loss: 1.053507	Best loss: 0.202064	Accuracy: 66.34%
16	Validation loss: 0.864558	Best loss: 0.202064	Accuracy: 69.43%
17	Validation loss: 0.694817	Best loss: 0.202064	Accuracy: 74.04%
18	Validation loss: 0.837045	Best loss: 0.202064	Accuracy: 68.30%
19	Validation loss: 1.054492	Best loss: 0.202064	Accuracy: 67.90%
20	Validation loss: 0.654476	Best loss: 0.202064	Accuracy: 77.72%
21	Validation loss: 0.747184	Best loss: 0.202064	Accuracy: 73.61%
22	Validation loss: 0.630725	Best loss: 0.202064	Accuracy: 77.76%
23	Validation loss: 0.589785	Best loss: 0.202064	Accuracy: 79.91%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   3.8s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.258340	Best loss: 0.258340	Accuracy: 92.92%
1	Validation loss: 0.115569	Best loss: 0.115569	Accuracy: 96.13%
2	Validation loss: 0.093406	Best loss: 0.093406	Accuracy: 96.83%
3	Validation loss: 0.082859	Best loss: 0.082859	Accuracy: 97.62%
4	Validation loss: 0.082384	Best loss: 0.082384	Accuracy: 97.58%
5	Validation loss: 0.098454	Best loss: 0.082384	Accuracy: 97.38%
6	Validation loss: 0.080288	Best loss: 0.080288	Accuracy: 97.69%
7	Validation loss: 0.060731	Best loss: 0.060731	Accuracy: 98.24%
8	Validation loss: 0.076581	Best loss: 0.060731	Accuracy: 97.81%
9	Validation loss: 0.050582	Best loss: 0.050582	Accuracy: 98.28%
10	Validation loss: 0.064212	Best loss: 0.050582	Accuracy: 98.40%
11	Validation loss: 0.041620	Best loss: 0.041620	Accuracy: 98.63%
12	Validation loss: 0.055166	Best loss: 0.041620	Accuracy: 98.59%
13	Validation loss: 0.073887	Best loss: 0.041620	Accuracy: 98.16%
14	Validation loss: 0.050183	Best loss: 0.041620	Accuracy: 98.55%
15	Validation loss: 0.062054	Best loss: 0.041620	Accuracy: 98.59%
16	Validation loss: 0.050397	Best loss: 0.041620	Accuracy: 98.63%
17	Validation loss: 0.077734	Best loss: 0.041620	Accuracy: 98.32%
18	Validation loss: 0.072404	Best loss: 0.041620	Accuracy: 98.71%
19	Validation loss: 0.058754	Best loss: 0.041620	Accuracy: 98.55%
20	Validation loss: 0.082845	Best loss: 0.041620	Accuracy: 98.32%
21	Validation loss: 0.062110	Best loss: 0.041620	Accuracy: 98.48%
22	Validation loss: 0.050153	Best loss: 0.041620	Accuracy: 98.67%
23	Validation loss: 0.071242	Best loss: 0.041620	Accuracy: 98.55%
24	Validation loss: 0.068520	Best loss: 0.041620	Accuracy: 98.51%
25	Validation loss: 0.054088	Best loss: 0.041620	Accuracy: 98.91%
26	Validation loss: 0.061320	Best loss: 0.041620	Accuracy: 98.67%
27	Validation loss: 0.073719	Best loss: 0.041620	Accuracy: 98.48%
28	Validation loss: 0.064418	Best loss: 0.041620	Accuracy: 98.48%
29	Validation loss: 0.076555	Best loss: 0.041620	Accuracy: 98.32%
30	Validation loss: 0.048874	Best loss: 0.041620	Accuracy: 98.94%
31	Validation loss: 0.077968	Best loss: 0.041620	Accuracy: 98.36%
32	Validation loss: 0.084945	Best loss: 0.041620	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.6s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.195919	Best loss: 0.195919	Accuracy: 94.41%
1	Validation loss: 0.101576	Best loss: 0.101576	Accuracy: 96.87%
2	Validation loss: 0.073400	Best loss: 0.073400	Accuracy: 97.62%
3	Validation loss: 0.072323	Best loss: 0.072323	Accuracy: 97.77%
4	Validation loss: 0.082016	Best loss: 0.072323	Accuracy: 97.65%
5	Validation loss: 0.068769	Best loss: 0.068769	Accuracy: 98.01%
6	Validation loss: 0.055265	Best loss: 0.055265	Accuracy: 98.20%
7	Validation loss: 0.053377	Best loss: 0.053377	Accuracy: 98.44%
8	Validation loss: 0.074022	Best loss: 0.053377	Accuracy: 98.01%
9	Validation loss: 0.057653	Best loss: 0.053377	Accuracy: 98.55%
10	Validation loss: 0.069203	Best loss: 0.053377	Accuracy: 98.28%
11	Validation loss: 0.073844	Best loss: 0.053377	Accuracy: 98.16%
12	Validation loss: 0.065818	Best loss: 0.053377	Accuracy: 98.48%
13	Validation loss: 0.051381	Best loss: 0.051381	Accuracy: 98.71%
14	Validation loss: 0.056184	Best loss: 0.051381	Accuracy: 98.71%
15	Validation loss: 0.058119	Best loss: 0.051381	Accuracy: 98.71%
16	Validation loss: 0.078480	Best loss: 0.051381	Accuracy: 98.59%
17	Validation loss: 0.081927	Best loss: 0.051381	Accuracy: 98.08%
18	Validation loss: 0.078240	Best loss: 0.051381	Accuracy: 98.55%
19	Validation loss: 0.063731	Best loss: 0.051381	Accuracy: 98.75%
20	Validation loss: 0.053653	Best loss: 0.051381	Accuracy: 98.75%
21	Validation loss: 0.078513	Best loss: 0.051381	Accuracy: 98.59%
22	Validation loss: 0.061059	Best loss: 0.051381	Accuracy: 98.83%
23	Validation loss: 0.079295	Best loss: 0.051381	Accuracy: 98.67%
24	Validation loss: 0.065652	Best loss: 0.051381	Accuracy: 98.91%
25	Validation loss: 0.068640	Best loss: 0.051381	Accuracy: 98.59%
26	Validation loss: 0.078065	Best loss: 0.051381	Accuracy: 98.51%
27	Validation loss: 0.091825	Best loss: 0.051381	Accuracy: 98.59%
28	Validation loss: 0.124278	Best loss: 0.051381	Accuracy: 98.12%
29	Validation loss: 0.094669	Best loss: 0.051381	Accuracy: 98.63%
30	Validation loss: 0.062744	Best loss: 0.051381	Accuracy: 98.87%
31	Validation loss: 0.075470	Best loss: 0.051381	Accuracy: 98.83%
32	Validation loss: 0.069903	Best loss: 0.051381	Accuracy: 98.67%
33	Validation loss: 0.062418	Best loss: 0.051381	Accuracy: 98.91%
34	Validation loss: 0.078725	Best loss: 0.051381	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  10.6s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.196207	Best loss: 0.196207	Accuracy: 95.04%
1	Validation loss: 0.106164	Best loss: 0.106164	Accuracy: 96.44%
2	Validation loss: 0.080493	Best loss: 0.080493	Accuracy: 97.34%
3	Validation loss: 0.070034	Best loss: 0.070034	Accuracy: 97.93%
4	Validation loss: 0.060364	Best loss: 0.060364	Accuracy: 98.24%
5	Validation loss: 0.058853	Best loss: 0.058853	Accuracy: 98.28%
6	Validation loss: 0.084910	Best loss: 0.058853	Accuracy: 97.89%
7	Validation loss: 0.083243	Best loss: 0.058853	Accuracy: 97.62%
8	Validation loss: 0.073156	Best loss: 0.058853	Accuracy: 97.93%
9	Validation loss: 0.068721	Best loss: 0.058853	Accuracy: 98.12%
10	Validation loss: 0.088060	Best loss: 0.058853	Accuracy: 98.36%
11	Validation loss: 0.061755	Best loss: 0.058853	Accuracy: 98.40%
12	Validation loss: 0.057419	Best loss: 0.057419	Accuracy: 98.83%
13	Validation loss: 0.125483	Best loss: 0.057419	Accuracy: 98.79%
14	Validation loss: 0.142434	Best loss: 0.057419	Accuracy: 98.59%
15	Validation loss: 0.076428	Best loss: 0.057419	Accuracy: 98.91%
16	Validation loss: 0.189391	Best loss: 0.057419	Accuracy: 98.75%
17	Validation loss: 0.134430	Best loss: 0.057419	Accuracy: 98.40%
18	Validation loss: 0.105059	Best loss: 0.057419	Accuracy: 98.71%
19	Validation loss: 0.081367	Best loss: 0.057419	Accuracy: 98.59%
20	Validation loss: 0.177855	Best loss: 0.057419	Accuracy: 98.98%
21	Validation loss: 0.121275	Best loss: 0.057419	Accuracy: 98.63%
22	Validation loss: 0.150440	Best loss: 0.057419	Accuracy: 98.79%
23	Validation loss: 0.078703	Best loss: 0.057419	Accuracy: 98.63%
24	Validation loss: 0.089935	Best loss: 0.057419	Accuracy: 98.75%
25	Validation loss: 0.066308	Best loss: 0.057419	Accuracy: 98.71%
26	Validation loss: 0.072230	Best loss: 0.057419	Accuracy: 98.75%
27	Validation loss: 0.067487	Best loss: 0.057419	Accuracy: 98.71%
28	Validation loss: 0.058618	Best loss: 0.057419	Accuracy: 98.94%
29	Validation loss: 0.085906	Best loss: 0.057419	Accuracy: 98.51%
30	Validation loss: 0.085821	Best loss: 0.057419	Accuracy: 98.59%
31	Validation loss: 0.078337	Best loss: 0.057419	Accuracy: 98.79%
32	Validation loss: 0.091019	Best loss: 0.057419	Accuracy: 98.98%
33	Validation loss: 0.101394	Best loss: 0.057419	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  11.3s
[CV] n_neurons=30, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.101546	Best loss: 0.101546	Accuracy: 97.03%
1	Validation loss: 0.082136	Best loss: 0.082136	Accuracy: 97.97%
2	Validation loss: 0.064446	Best loss: 0.064446	Accuracy: 98.12%
3	Validation loss: 0.057286	Best loss: 0.057286	Accuracy: 98.40%
4	Validation loss: 0.051642	Best loss: 0.051642	Accuracy: 98.63%
5	Validation loss: 0.066847	Best loss: 0.051642	Accuracy: 98.24%
6	Validation loss: 0.057508	Best loss: 0.051642	Accuracy: 98.24%
7	Validation loss: 0.059458	Best loss: 0.051642	Accuracy: 98.36%
8	Validation loss: 0.063016	Best loss: 0.051642	Accuracy: 98.32%
9	Validation loss: 0.054459	Best loss: 0.051642	Accuracy: 98.67%
10	Validation loss: 0.080018	Best loss: 0.051642	Accuracy: 98.12%
11	Validation loss: 0.054092	Best loss: 0.051642	Accuracy: 98.63%
12	Validation loss: 0.048308	Best loss: 0.048308	Accuracy: 98.75%
13	Validation loss: 0.071646	Best loss: 0.048308	Accuracy: 98.44%
14	Validation loss: 0.073093	Best loss: 0.048308	Accuracy: 98.40%
15	Validation loss: 0.079270	Best loss: 0.048308	Accuracy: 98.16%
16	Validation loss: 0.066251	Best loss: 0.048308	Accuracy: 98.67%
17	Validation loss: 0.054391	Best loss: 0.048308	Accuracy: 98.83%
18	Validation loss: 0.082131	Best loss: 0.048308	Accuracy: 98.32%
19	Validation loss: 0.054707	Best loss: 0.048308	Accuracy: 99.02%
20	Validation loss: 0.057996	Best loss: 0.048308	Accuracy: 99.06%
21	Validation loss: 0.062516	Best loss: 0.048308	Accuracy: 98.83%
22	Validation loss: 0.074774	Best loss: 0.048308	Accuracy: 98.40%
23	Validation loss: 0.073156	Best loss: 0.048308	Accuracy: 98.83%
24	Validation loss: 0.075554	Best loss: 0.048308	Accuracy: 98.55%
25	Validation loss: 0.070182	Best loss: 0.048308	Accuracy: 97.97%
26	Validation loss: 0.077794	Best loss: 0.048308	Accuracy: 98.55%
27	Validation loss: 0.095381	Best loss: 0.048308	Accuracy: 98.75%
28	Validation loss: 0.091249	Best loss: 0.048308	Accuracy: 98.63%
29	Validation loss: 0.087929	Best loss: 0.048308	Accuracy: 98.32%
30	Validation loss: 0.095704	Best loss: 0.048308	Accuracy: 98.24%
31	Validation loss: 0.081100	Best loss: 0.048308	Accuracy: 98.36%
32	Validation loss: 0.090418	Best loss: 0.048308	Accuracy: 98.44%
33	Validation loss: 0.080309	Best loss: 0.048308	Accuracy: 98.59%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   8.1s
[CV] n_neurons=30, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.117540	Best loss: 0.117540	Accuracy: 96.29%
1	Validation loss: 0.087592	Best loss: 0.087592	Accuracy: 97.46%
2	Validation loss: 0.068490	Best loss: 0.068490	Accuracy: 98.01%
3	Validation loss: 0.071824	Best loss: 0.068490	Accuracy: 97.85%
4	Validation loss: 0.078805	Best loss: 0.068490	Accuracy: 97.97%
5	Validation loss: 0.058387	Best loss: 0.058387	Accuracy: 98.20%
6	Validation loss: 0.064575	Best loss: 0.058387	Accuracy: 98.08%
7	Validation loss: 0.068040	Best loss: 0.058387	Accuracy: 98.05%
8	Validation loss: 0.067466	Best loss: 0.058387	Accuracy: 98.48%
9	Validation loss: 0.076764	Best loss: 0.058387	Accuracy: 97.97%
10	Validation loss: 0.085981	Best loss: 0.058387	Accuracy: 97.93%
11	Validation loss: 0.099940	Best loss: 0.058387	Accuracy: 97.85%
12	Validation loss: 0.071132	Best loss: 0.058387	Accuracy: 98.24%
13	Validation loss: 0.055397	Best loss: 0.055397	Accuracy: 98.71%
14	Validation loss: 0.067567	Best loss: 0.055397	Accuracy: 98.55%
15	Validation loss: 0.054587	Best loss: 0.054587	Accuracy: 98.67%
16	Validation loss: 0.095611	Best loss: 0.054587	Accuracy: 98.28%
17	Validation loss: 0.053537	Best loss: 0.053537	Accuracy: 98.71%
18	Validation loss: 0.055657	Best loss: 0.053537	Accuracy: 98.63%
19	Validation loss: 0.066688	Best loss: 0.053537	Accuracy: 98.71%
20	Validation loss: 0.074925	Best loss: 0.053537	Accuracy: 98.55%
21	Validation loss: 0.090380	Best loss: 0.053537	Accuracy: 98.32%
22	Validation loss: 0.113896	Best loss: 0.053537	Accuracy: 98.20%
23	Validation loss: 0.087666	Best loss: 0.053537	Accuracy: 98.67%
24	Validation loss: 0.062637	Best loss: 0.053537	Accuracy: 98.71%
25	Validation loss: 0.081145	Best loss: 0.053537	Accuracy: 98.71%
26	Validation loss: 0.071176	Best loss: 0.053537	Accuracy: 98.63%
27	Validation loss: 0.082752	Best loss: 0.053537	Accuracy: 98.59%
28	Validation loss: 0.094544	Best loss: 0.053537	Accuracy: 98.40%
29	Validation loss: 0.096859	Best loss: 0.053537	Accuracy: 98.67%
30	Validation loss: 0.067748	Best loss: 0.053537	Accuracy: 98.79%
31	Validation loss: 0.064254	Best loss: 0.053537	Accuracy: 98.98%
32	Validation loss: 0.063506	Best loss: 0.053537	Accuracy: 98.83%
33	Validation loss: 0.072465	Best loss: 0.053537	Accuracy: 98.40%
34	Validation loss: 0.062435	Best loss: 0.053537	Accuracy: 98.59%
35	Validation loss: 0.075934	Best loss: 0.053537	Accuracy: 98.75%
36	Validation loss: 0.100622	Best loss: 0.053537	Accuracy: 98.71%
37	Validation loss: 0.090904	Best loss: 0.053537	Accuracy: 98.40%
38	Validation loss: 0.053348	Best loss: 0.053348	Accuracy: 98.91%
39	Validation loss: 0.079260	Best loss: 0.053348	Accuracy: 98.79%
40	Validation loss: 0.071847	Best loss: 0.053348	Accuracy: 98.87%
41	Validation loss: 0.085352	Best loss: 0.053348	Accuracy: 98.44%
42	Validation loss: 0.103555	Best loss: 0.053348	Accuracy: 98.24%
43	Validation loss: 0.077692	Best loss: 0.053348	Accuracy: 98.63%
44	Validation loss: 0.070862	Best loss: 0.053348	Accuracy: 98.40%
45	Validation loss: 0.065101	Best loss: 0.053348	Accuracy: 98.48%
46	Validation loss: 0.065490	Best loss: 0.053348	Accuracy: 98.67%
47	Validation loss: 0.074382	Best loss: 0.053348	Accuracy: 98.36%
48	Validation loss: 0.088091	Best loss: 0.053348	Accuracy: 98.75%
49	Validation loss: 0.089190	Best loss: 0.053348	Accuracy: 98.08%
50	Validation loss: 0.123582	Best loss: 0.053348	Accuracy: 98.51%
51	Validation loss: 0.121455	Best loss: 0.053348	Accuracy: 98.32%
52	Validation loss: 0.087018	Best loss: 0.053348	Accuracy: 98.59%
53	Validation loss: 0.106865	Best loss: 0.053348	Accuracy: 98.05%
54	Validation loss: 0.090702	Best loss: 0.053348	Accuracy: 98.16%
55	Validation loss: 0.091998	Best loss: 0.053348	Accuracy: 98.48%
56	Validation loss: 0.068694	Best loss: 0.053348	Accuracy: 98.67%
57	Validation loss: 0.100902	Best loss: 0.053348	Accuracy: 98.63%
58	Validation loss: 0.120439	Best loss: 0.053348	Accuracy: 98.28%
59	Validation loss: 0.070893	Best loss: 0.053348	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  13.4s
[CV] n_neurons=30, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.080869	Best loss: 0.080869	Accuracy: 97.65%
1	Validation loss: 0.084524	Best loss: 0.080869	Accuracy: 97.54%
2	Validation loss: 0.074145	Best loss: 0.074145	Accuracy: 97.73%
3	Validation loss: 0.081274	Best loss: 0.074145	Accuracy: 98.01%
4	Validation loss: 0.065801	Best loss: 0.065801	Accuracy: 97.93%
5	Validation loss: 0.066376	Best loss: 0.065801	Accuracy: 98.20%
6	Validation loss: 0.073685	Best loss: 0.065801	Accuracy: 98.28%
7	Validation loss: 0.078323	Best loss: 0.065801	Accuracy: 98.32%
8	Validation loss: 0.071870	Best loss: 0.065801	Accuracy: 98.40%
9	Validation loss: 0.066500	Best loss: 0.065801	Accuracy: 98.28%
10	Validation loss: 0.069576	Best loss: 0.065801	Accuracy: 98.44%
11	Validation loss: 0.081919	Best loss: 0.065801	Accuracy: 98.44%
12	Validation loss: 0.075154	Best loss: 0.065801	Accuracy: 98.36%
13	Validation loss: 0.066943	Best loss: 0.065801	Accuracy: 98.51%
14	Validation loss: 0.072168	Best loss: 0.065801	Accuracy: 98.63%
15	Validation loss: 0.087233	Best loss: 0.065801	Accuracy: 98.20%
16	Validation loss: 0.082973	Best loss: 0.065801	Accuracy: 98.28%
17	Validation loss: 0.071021	Best loss: 0.065801	Accuracy: 98.24%
18	Validation loss: 0.094328	Best loss: 0.065801	Accuracy: 97.97%
19	Validation loss: 0.055629	Best loss: 0.055629	Accuracy: 98.63%
20	Validation loss: 0.129327	Best loss: 0.055629	Accuracy: 98.44%
21	Validation loss: 0.057160	Best loss: 0.055629	Accuracy: 98.40%
22	Validation loss: 0.092910	Best loss: 0.055629	Accuracy: 98.32%
23	Validation loss: 0.085445	Best loss: 0.055629	Accuracy: 98.55%
24	Validation loss: 0.113057	Best loss: 0.055629	Accuracy: 98.05%
25	Validation loss: 0.073361	Best loss: 0.055629	Accuracy: 98.63%
26	Validation loss: 0.106151	Best loss: 0.055629	Accuracy: 98.32%
27	Validation loss: 0.107551	Best loss: 0.055629	Accuracy: 98.51%
28	Validation loss: 0.092952	Best loss: 0.055629	Accuracy: 98.28%
29	Validation loss: 0.116296	Best loss: 0.055629	Accuracy: 98.44%
30	Validation loss: 0.098278	Best loss: 0.055629	Accuracy: 98.40%
31	Validation loss: 0.102469	Best loss: 0.055629	Accuracy: 98.36%
32	Validation loss: 0.075887	Best loss: 0.055629	Accuracy: 98.44%
33	Validation loss: 0.084920	Best loss: 0.055629	Accuracy: 98.44%
34	Validation loss: 0.080884	Best loss: 0.055629	Accuracy: 98.59%
35	Validation loss: 0.106208	Best loss: 0.055629	Accuracy: 98.40%
36	Validation loss: 0.067980	Best loss: 0.055629	Accuracy: 98.79%
37	Validation loss: 0.076105	Best loss: 0.055629	Accuracy: 98.36%
38	Validation loss: 0.068911	Best loss: 0.055629	Accuracy: 98.32%
39	Validation loss: 0.119129	Best loss: 0.055629	Accuracy: 98.08%
40	Validation loss: 0.093066	Best loss: 0.055629	Accuracy: 98.48%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   9.2s
[CV] n_neurons=50, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.220462	Best loss: 0.220462	Accuracy: 95.35%
1	Validation loss: 0.102329	Best loss: 0.102329	Accuracy: 97.07%
2	Validation loss: 0.093551	Best loss: 0.093551	Accuracy: 97.46%
3	Validation loss: 0.086895	Best loss: 0.086895	Accuracy: 97.34%
4	Validation loss: 0.078364	Best loss: 0.078364	Accuracy: 97.65%
5	Validation loss: 0.073043	Best loss: 0.073043	Accuracy: 97.89%
6	Validation loss: 0.086542	Best loss: 0.073043	Accuracy: 97.46%
7	Validation loss: 0.069208	Best loss: 0.069208	Accuracy: 98.05%
8	Validation loss: 0.076330	Best loss: 0.069208	Accuracy: 97.65%
9	Validation loss: 0.064043	Best loss: 0.064043	Accuracy: 98.16%
10	Validation loss: 0.078338	Best loss: 0.064043	Accuracy: 97.81%
11	Validation loss: 0.080043	Best loss: 0.064043	Accuracy: 97.97%
12	Validation loss: 0.077815	Best loss: 0.064043	Accuracy: 97.93%
13	Validation loss: 0.061990	Best loss: 0.061990	Accuracy: 98.12%
14	Validation loss: 0.072119	Best loss: 0.061990	Accuracy: 98.20%
15	Validation loss: 0.078268	Best loss: 0.061990	Accuracy: 97.85%
16	Validation loss: 0.071846	Best loss: 0.061990	Accuracy: 98.24%
17	Validation loss: 0.100700	Best loss: 0.061990	Accuracy: 97.46%
18	Validation loss: 0.079790	Best loss: 0.061990	Accuracy: 98.16%
19	Validation loss: 0.079886	Best loss: 0.061990	Accuracy: 98.28%
20	Validation loss: 0.089560	Best loss: 0.061990	Accuracy: 98.32%
21	Validation loss: 0.094174	Best loss: 0.061990	Accuracy: 98.01%
22	Validation loss: 0.095188	Best loss: 0.061990	Accuracy: 98.05%
23	Validation loss: 0.105342	Best loss: 0.061990	Accuracy: 98.36%
24	Validation loss: 0.078120	Best loss: 0.061990	Accuracy: 98.44%
25	Validation loss: 0.085624	Best loss: 0.061990	Accuracy: 98.44%
26	Validation loss: 0.062913	Best loss: 0.061990	Accuracy: 98.75%
27	Validation loss: 0.082886	Best loss: 0.061990	Accuracy: 98.32%
28	Validation loss: 0.083259	Best loss: 0.061990	Accuracy: 98.44%
29	Validation loss: 0.094778	Best loss: 0.061990	Accuracy: 98.16%
30	Validation loss: 0.075841	Best loss: 0.061990	Accuracy: 98.71%
31	Validation loss: 0.103757	Best loss: 0.061990	Accuracy: 98.44%
32	Validation loss: 0.128671	Best loss: 0.061990	Accuracy: 98.28%
33	Validation loss: 0.145155	Best loss: 0.061990	Accuracy: 97.46%
34	Validation loss: 0.180444	Best loss: 0.061990	Accuracy: 97.30%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   5.4s
[CV] n_neurons=50, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.238308	Best loss: 0.238308	Accuracy: 92.57%
1	Validation loss: 0.123416	Best loss: 0.123416	Accuracy: 96.33%
2	Validation loss: 0.103236	Best loss: 0.103236	Accuracy: 96.79%
3	Validation loss: 0.091092	Best loss: 0.091092	Accuracy: 97.42%
4	Validation loss: 0.082097	Best loss: 0.082097	Accuracy: 97.65%
5	Validation loss: 0.085721	Best loss: 0.082097	Accuracy: 97.58%
6	Validation loss: 0.084400	Best loss: 0.082097	Accuracy: 97.54%
7	Validation loss: 0.082108	Best loss: 0.082097	Accuracy: 97.69%
8	Validation loss: 0.090850	Best loss: 0.082097	Accuracy: 97.50%
9	Validation loss: 0.109445	Best loss: 0.082097	Accuracy: 97.46%
10	Validation loss: 0.105353	Best loss: 0.082097	Accuracy: 97.69%
11	Validation loss: 0.096280	Best loss: 0.082097	Accuracy: 97.85%
12	Validation loss: 0.072483	Best loss: 0.072483	Accuracy: 98.16%
13	Validation loss: 0.086499	Best loss: 0.072483	Accuracy: 97.65%
14	Validation loss: 0.090273	Best loss: 0.072483	Accuracy: 97.77%
15	Validation loss: 0.092534	Best loss: 0.072483	Accuracy: 97.85%
16	Validation loss: 0.103466	Best loss: 0.072483	Accuracy: 97.93%
17	Validation loss: 0.078845	Best loss: 0.072483	Accuracy: 98.05%
18	Validation loss: 0.112846	Best loss: 0.072483	Accuracy: 97.93%
19	Validation loss: 0.135559	Best loss: 0.072483	Accuracy: 97.69%
20	Validation loss: 0.104622	Best loss: 0.072483	Accuracy: 97.77%
21	Validation loss: 0.126370	Best loss: 0.072483	Accuracy: 97.73%
22	Validation loss: 0.157195	Best loss: 0.072483	Accuracy: 97.65%
23	Validation loss: 0.134374	Best loss: 0.072483	Accuracy: 97.97%
24	Validation loss: 0.122086	Best loss: 0.072483	Accuracy: 98.32%
25	Validation loss: 0.133244	Best loss: 0.072483	Accuracy: 98.01%
26	Validation loss: 0.099445	Best loss: 0.072483	Accuracy: 97.89%
27	Validation loss: 0.123669	Best loss: 0.072483	Accuracy: 97.77%
28	Validation loss: 0.107637	Best loss: 0.072483	Accuracy: 98.01%
29	Validation loss: 0.094235	Best loss: 0.072483	Accuracy: 97.89%
30	Validation loss: 0.097016	Best loss: 0.072483	Accuracy: 98.01%
31	Validation loss: 0.128443	Best loss: 0.072483	Accuracy: 98.08%
32	Validation loss: 0.112200	Best loss: 0.072483	Accuracy: 98.05%
33	Validation loss: 0.118449	Best loss: 0.072483	Accuracy: 97.85%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   5.2s
[CV] n_neurons=50, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.173667	Best loss: 0.173667	Accuracy: 95.00%
1	Validation loss: 0.164588	Best loss: 0.164588	Accuracy: 96.48%
2	Validation loss: 0.108659	Best loss: 0.108659	Accuracy: 97.22%
3	Validation loss: 0.111217	Best loss: 0.108659	Accuracy: 97.50%
4	Validation loss: 0.100970	Best loss: 0.100970	Accuracy: 97.62%
5	Validation loss: 0.103399	Best loss: 0.100970	Accuracy: 97.93%
6	Validation loss: 0.095836	Best loss: 0.095836	Accuracy: 98.01%
7	Validation loss: 0.085848	Best loss: 0.085848	Accuracy: 98.05%
8	Validation loss: 0.080528	Best loss: 0.080528	Accuracy: 98.20%
9	Validation loss: 0.074648	Best loss: 0.074648	Accuracy: 98.28%
10	Validation loss: 0.099949	Best loss: 0.074648	Accuracy: 97.81%
11	Validation loss: 0.093673	Best loss: 0.074648	Accuracy: 97.77%
12	Validation loss: 0.089282	Best loss: 0.074648	Accuracy: 98.12%
13	Validation loss: 0.083124	Best loss: 0.074648	Accuracy: 98.20%
14	Validation loss: 0.115571	Best loss: 0.074648	Accuracy: 97.89%
15	Validation loss: 0.095761	Best loss: 0.074648	Accuracy: 97.85%
16	Validation loss: 0.080661	Best loss: 0.074648	Accuracy: 98.24%
17	Validation loss: 0.090167	Best loss: 0.074648	Accuracy: 98.01%
18	Validation loss: 0.093826	Best loss: 0.074648	Accuracy: 97.93%
19	Validation loss: 0.086385	Best loss: 0.074648	Accuracy: 97.85%
20	Validation loss: 0.089027	Best loss: 0.074648	Accuracy: 98.16%
21	Validation loss: 0.099162	Best loss: 0.074648	Accuracy: 98.01%
22	Validation loss: 0.114786	Best loss: 0.074648	Accuracy: 98.16%
23	Validation loss: 0.098045	Best loss: 0.074648	Accuracy: 98.12%
24	Validation loss: 0.113740	Best loss: 0.074648	Accuracy: 97.62%
25	Validation loss: 0.105465	Best loss: 0.074648	Accuracy: 98.55%
26	Validation loss: 0.083286	Best loss: 0.074648	Accuracy: 98.36%
27	Validation loss: 0.083036	Best loss: 0.074648	Accuracy: 98.32%
28	Validation loss: 0.120292	Best loss: 0.074648	Accuracy: 98.24%
29	Validation loss: 0.119189	Best loss: 0.074648	Accuracy: 98.20%
30	Validation loss: 0.111200	Best loss: 0.074648	Accuracy: 98.08%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   5.0s
[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.095463	Best loss: 0.095463	Accuracy: 96.95%
1	Validation loss: 0.081282	Best loss: 0.081282	Accuracy: 97.85%
2	Validation loss: 0.065540	Best loss: 0.065540	Accuracy: 98.16%
3	Validation loss: 35.319805	Best loss: 0.065540	Accuracy: 62.98%
4	Validation loss: 0.253817	Best loss: 0.065540	Accuracy: 95.15%
5	Validation loss: 0.237956	Best loss: 0.065540	Accuracy: 93.71%
6	Validation loss: 0.200224	Best loss: 0.065540	Accuracy: 94.72%
7	Validation loss: 0.174394	Best loss: 0.065540	Accuracy: 96.13%
8	Validation loss: 0.136553	Best loss: 0.065540	Accuracy: 96.99%
9	Validation loss: 0.146469	Best loss: 0.065540	Accuracy: 97.15%
10	Validation loss: 0.151866	Best loss: 0.065540	Accuracy: 97.58%
11	Validation loss: 0.127755	Best loss: 0.065540	Accuracy: 96.79%
12	Validation loss: 0.104326	Best loss: 0.065540	Accuracy: 97.58%
13	Validation loss: 0.123440	Best loss: 0.065540	Accuracy: 96.76%
14	Validation loss: 0.178256	Best loss: 0.065540	Accuracy: 96.17%
15	Validation loss: 0.117186	Best loss: 0.065540	Accuracy: 97.62%
16	Validation loss: 0.161868	Best loss: 0.065540	Accuracy: 97.54%
17	Validation loss: 0.126675	Best loss: 0.065540	Accuracy: 97.73%
18	Validation loss: 0.157282	Best loss: 0.065540	Accuracy: 97.22%
19	Validation loss: 0.160086	Best loss: 0.065540	Accuracy: 97.19%
20	Validation loss: 0.327441	Best loss: 0.065540	Accuracy: 96.99%
21	Validation loss: 0.183425	Best loss: 0.065540	Accuracy: 97.07%
22	Validation loss: 0.158852	Best loss: 0.065540	Accuracy: 97.65%
23	Validation loss: 0.159516	Best loss: 0.065540	Accuracy: 97.77%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   8.7s
[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.117530	Best loss: 0.117530	Accuracy: 96.72%
1	Validation loss: 0.106544	Best loss: 0.106544	Accuracy: 97.07%
2	Validation loss: 0.083596	Best loss: 0.083596	Accuracy: 97.73%
3	Validation loss: 0.070700	Best loss: 0.070700	Accuracy: 98.51%
4	Validation loss: 4.549089	Best loss: 0.070700	Accuracy: 92.81%
5	Validation loss: 2.427062	Best loss: 0.070700	Accuracy: 96.40%
6	Validation loss: 1.255408	Best loss: 0.070700	Accuracy: 96.44%
7	Validation loss: 0.394563	Best loss: 0.070700	Accuracy: 97.42%
8	Validation loss: 0.327600	Best loss: 0.070700	Accuracy: 97.58%
9	Validation loss: 0.516453	Best loss: 0.070700	Accuracy: 96.48%
10	Validation loss: 0.354633	Best loss: 0.070700	Accuracy: 97.34%
11	Validation loss: 0.263344	Best loss: 0.070700	Accuracy: 97.58%
12	Validation loss: 0.229538	Best loss: 0.070700	Accuracy: 97.38%
13	Validation loss: 0.234300	Best loss: 0.070700	Accuracy: 97.34%
14	Validation loss: 0.266898	Best loss: 0.070700	Accuracy: 97.89%
15	Validation loss: 218.257843	Best loss: 0.070700	Accuracy: 88.04%
16	Validation loss: 9.543195	Best loss: 0.070700	Accuracy: 96.21%
17	Validation loss: 5.387187	Best loss: 0.070700	Accuracy: 96.36%
18	Validation loss: 3.649756	Best loss: 0.070700	Accuracy: 96.60%
19	Validation loss: 2.456712	Best loss: 0.070700	Accuracy: 97.03%
20	Validation loss: 3.312870	Best loss: 0.070700	Accuracy: 97.26%
21	Validation loss: 2.691339	Best loss: 0.070700	Accuracy: 97.73%
22	Validation loss: 2.759547	Best loss: 0.070700	Accuracy: 95.86%
23	Validation loss: 2.548956	Best loss: 0.070700	Accuracy: 96.13%
24	Validation loss: 2.201921	Best loss: 0.070700	Accuracy: 97.03%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   9.3s
[CV] n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.099218	Best loss: 0.099218	Accuracy: 97.42%
1	Validation loss: 0.701663	Best loss: 0.099218	Accuracy: 92.34%
2	Validation loss: 0.106376	Best loss: 0.099218	Accuracy: 96.79%
3	Validation loss: 0.088275	Best loss: 0.088275	Accuracy: 97.46%
4	Validation loss: 0.084077	Best loss: 0.084077	Accuracy: 98.05%
5	Validation loss: 0.081540	Best loss: 0.081540	Accuracy: 97.93%
6	Validation loss: 0.110807	Best loss: 0.081540	Accuracy: 97.42%
7	Validation loss: 0.142401	Best loss: 0.081540	Accuracy: 97.89%
8	Validation loss: 0.107554	Best loss: 0.081540	Accuracy: 98.32%
9	Validation loss: 0.119748	Best loss: 0.081540	Accuracy: 97.46%
10	Validation loss: 0.132494	Best loss: 0.081540	Accuracy: 97.89%
11	Validation loss: 0.151759	Best loss: 0.081540	Accuracy: 97.54%
12	Validation loss: 0.098972	Best loss: 0.081540	Accuracy: 98.12%
13	Validation loss: 0.107866	Best loss: 0.081540	Accuracy: 98.44%
14	Validation loss: 0.126809	Best loss: 0.081540	Accuracy: 98.28%
15	Validation loss: 0.098034	Best loss: 0.081540	Accuracy: 98.36%
16	Validation loss: 0.124463	Best loss: 0.081540	Accuracy: 98.12%
17	Validation loss: 0.094485	Best loss: 0.081540	Accuracy: 98.36%
18	Validation loss: 11178.884766	Best loss: 0.081540	Accuracy: 22.83%
19	Validation loss: 36.245380	Best loss: 0.081540	Accuracy: 84.60%
20	Validation loss: 15.277415	Best loss: 0.081540	Accuracy: 94.29%
21	Validation loss: 10.562207	Best loss: 0.081540	Accuracy: 95.19%
22	Validation loss: 11.579782	Best loss: 0.081540	Accuracy: 95.74%
23	Validation loss: 8.743070	Best loss: 0.081540	Accuracy: 95.74%
24	Validation loss: 17.420231	Best loss: 0.081540	Accuracy: 95.97%
25	Validation loss: 13.008014	Best loss: 0.081540	Accuracy: 96.01%
26	Validation loss: 9.107890	Best loss: 0.081540	Accuracy: 94.21%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   9.7s
[CV] n_neurons=90, learning_rate=0.01, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.232393	Best loss: 0.232393	Accuracy: 93.90%
1	Validation loss: 0.132383	Best loss: 0.132383	Accuracy: 96.56%
2	Validation loss: 3.794890	Best loss: 0.132383	Accuracy: 90.85%
3	Validation loss: 0.819215	Best loss: 0.132383	Accuracy: 94.06%
4	Validation loss: 2.097519	Best loss: 0.132383	Accuracy: 95.97%
5	Validation loss: 27.169447	Best loss: 0.132383	Accuracy: 96.17%
6	Validation loss: 6.941096	Best loss: 0.132383	Accuracy: 96.09%
7	Validation loss: 2.748666	Best loss: 0.132383	Accuracy: 96.17%
8	Validation loss: 10.193844	Best loss: 0.132383	Accuracy: 96.79%
9	Validation loss: 10.657201	Best loss: 0.132383	Accuracy: 97.38%
10	Validation loss: 3.001720	Best loss: 0.132383	Accuracy: 97.62%
11	Validation loss: 17.685385	Best loss: 0.132383	Accuracy: 96.64%
12	Validation loss: 12.974003	Best loss: 0.132383	Accuracy: 95.11%
13	Validation loss: 11.388881	Best loss: 0.132383	Accuracy: 92.85%
14	Validation loss: 11.801395	Best loss: 0.132383	Accuracy: 96.95%
15	Validation loss: 7.694859	Best loss: 0.132383	Accuracy: 97.22%
16	Validation loss: 9.824970	Best loss: 0.132383	Accuracy: 96.17%
17	Validation loss: 85.812332	Best loss: 0.132383	Accuracy: 96.52%
18	Validation loss: 27.775465	Best loss: 0.132383	Accuracy: 97.15%
19	Validation loss: 12.000754	Best loss: 0.132383	Accuracy: 97.34%
20	Validation loss: 13.507078	Best loss: 0.132383	Accuracy: 97.26%
21	Validation loss: 16.388885	Best loss: 0.132383	Accuracy: 97.11%
22	Validation loss: 24.335510	Best loss: 0.132383	Accuracy: 97.69%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.01, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  48.1s
[CV] n_neurons=90, learning_rate=0.01, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.323620	Best loss: 0.323620	Accuracy: 93.94%
1	Validation loss: 0.176954	Best loss: 0.176954	Accuracy: 95.66%
2	Validation loss: 3.314410	Best loss: 0.176954	Accuracy: 90.58%
3	Validation loss: 0.742840	Best loss: 0.176954	Accuracy: 96.87%
4	Validation loss: 1.440363	Best loss: 0.176954	Accuracy: 91.48%
5	Validation loss: 6.551682	Best loss: 0.176954	Accuracy: 94.68%
6	Validation loss: 2.762953	Best loss: 0.176954	Accuracy: 96.52%
7	Validation loss: 5.702206	Best loss: 0.176954	Accuracy: 93.67%
8	Validation loss: 58.675472	Best loss: 0.176954	Accuracy: 93.78%
9	Validation loss: 6.158517	Best loss: 0.176954	Accuracy: 95.50%
10	Validation loss: 44.793148	Best loss: 0.176954	Accuracy: 94.80%
11	Validation loss: 4.485351	Best loss: 0.176954	Accuracy: 97.62%
12	Validation loss: 24.461506	Best loss: 0.176954	Accuracy: 95.43%
13	Validation loss: 42.499947	Best loss: 0.176954	Accuracy: 96.25%
14	Validation loss: 59.395153	Best loss: 0.176954	Accuracy: 96.05%
15	Validation loss: 20.997759	Best loss: 0.176954	Accuracy: 97.11%
16	Validation loss: 43.870007	Best loss: 0.176954	Accuracy: 86.40%
17	Validation loss: 55.371067	Best loss: 0.176954	Accuracy: 96.48%
18	Validation loss: 8.608695	Best loss: 0.176954	Accuracy: 97.62%
19	Validation loss: 91.739975	Best loss: 0.176954	Accuracy: 95.97%
20	Validation loss: 55.415508	Best loss: 0.176954	Accuracy: 98.12%
21	Validation loss: 53.672192	Best loss: 0.176954	Accuracy: 97.19%
22	Validation loss: 288.322357	Best loss: 0.176954	Accuracy: 97.85%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.01, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  47.5s
[CV] n_neurons=90, learning_rate=0.01, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.308358	Best loss: 0.308358	Accuracy: 93.75%
1	Validation loss: 0.388606	Best loss: 0.308358	Accuracy: 94.80%
2	Validation loss: 1.620993	Best loss: 0.308358	Accuracy: 96.29%
3	Validation loss: 1.928575	Best loss: 0.308358	Accuracy: 96.17%
4	Validation loss: 1.709890	Best loss: 0.308358	Accuracy: 96.48%
5	Validation loss: 0.631592	Best loss: 0.308358	Accuracy: 96.40%
6	Validation loss: 1.144660	Best loss: 0.308358	Accuracy: 96.87%
7	Validation loss: 1.261193	Best loss: 0.308358	Accuracy: 95.15%
8	Validation loss: 1.024612	Best loss: 0.308358	Accuracy: 96.13%
9	Validation loss: 3.054350	Best loss: 0.308358	Accuracy: 96.60%
10	Validation loss: 1.447935	Best loss: 0.308358	Accuracy: 96.21%
11	Validation loss: 204.187469	Best loss: 0.308358	Accuracy: 91.09%
12	Validation loss: 16.607450	Best loss: 0.308358	Accuracy: 95.39%
13	Validation loss: 10.291462	Best loss: 0.308358	Accuracy: 96.76%
14	Validation loss: 40.474827	Best loss: 0.308358	Accuracy: 93.35%
15	Validation loss: 17.293716	Best loss: 0.308358	Accuracy: 96.87%
16	Validation loss: 7.306488	Best loss: 0.308358	Accuracy: 96.52%
17	Validation loss: 11.101712	Best loss: 0.308358	Accuracy: 97.26%
18	Validation loss: 18.080551	Best loss: 0.308358	Accuracy: 98.05%
19	Validation loss: 14.964914	Best loss: 0.308358	Accuracy: 96.87%
20	Validation loss: 26.626848	Best loss: 0.308358	Accuracy: 93.51%
21	Validation loss: 37.867924	Best loss: 0.308358	Accuracy: 93.67%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.01, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  45.7s
[CV] n_neurons=160, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 22.064384	Best loss: 22.064384	Accuracy: 84.09%
1	Validation loss: 3.564418	Best loss: 3.564418	Accuracy: 96.36%
2	Validation loss: 6.893640	Best loss: 3.564418	Accuracy: 94.33%
3	Validation loss: 6.515145	Best loss: 3.564418	Accuracy: 94.14%
4	Validation loss: 169649488.000000	Best loss: 3.564418	Accuracy: 18.73%
5	Validation loss: 407340.843750	Best loss: 3.564418	Accuracy: 93.12%
6	Validation loss: 129037.460938	Best loss: 3.564418	Accuracy: 95.39%
7	Validation loss: 105578.070312	Best loss: 3.564418	Accuracy: 95.35%
8	Validation loss: 102727.796875	Best loss: 3.564418	Accuracy: 94.68%
9	Validation loss: 103209.570312	Best loss: 3.564418	Accuracy: 94.33%
10	Validation loss: 45913.265625	Best loss: 3.564418	Accuracy: 96.64%
11	Validation loss: 61700.554688	Best loss: 3.564418	Accuracy: 96.36%
12	Validation loss: 56949.578125	Best loss: 3.564418	Accuracy: 96.36%
13	Validation loss: 78093.101562	Best loss: 3.564418	Accuracy: 93.98%
14	Validation loss: 76069.429688	Best loss: 3.564418	Accuracy: 94.18%
15	Validation loss: 32081.226562	Best loss: 3.564418	Accuracy: 97.30%
16	Validation loss: 41115.421875	Best loss: 3.564418	Accuracy: 96.36%
17	Validation loss: 39791.554688	Best loss: 3.564418	Accuracy: 97.19%
18	Validation loss: 37583.597656	Best loss: 3.564418	Accuracy: 96.17%
19	Validation loss: 39284.574219	Best loss: 3.564418	Accuracy: 96.76%
20	Validation loss: 64641.082031	Best loss: 3.564418	Accuracy: 94.14%
21	Validation loss: 34373.593750	Best loss: 3.564418	Accuracy: 96.56%
22	Validation loss: 39599.617188	Best loss: 3.564418	Accuracy: 97.07%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  12.8s
[CV] n_neurons=160, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 91.463173	Best loss: 91.463173	Accuracy: 91.87%
1	Validation loss: 24.745493	Best loss: 24.745493	Accuracy: 93.39%
2	Validation loss: 39.728271	Best loss: 24.745493	Accuracy: 91.63%
3	Validation loss: 11.958393	Best loss: 11.958393	Accuracy: 93.82%
4	Validation loss: 20.260851	Best loss: 11.958393	Accuracy: 93.08%
5	Validation loss: 7.782712	Best loss: 7.782712	Accuracy: 94.80%
6	Validation loss: 8.363883	Best loss: 7.782712	Accuracy: 95.19%
7	Validation loss: 2365645.250000	Best loss: 7.782712	Accuracy: 79.95%
8	Validation loss: 257570.328125	Best loss: 7.782712	Accuracy: 92.85%
9	Validation loss: 228960.031250	Best loss: 7.782712	Accuracy: 94.68%
10	Validation loss: 108794.773438	Best loss: 7.782712	Accuracy: 94.53%
11	Validation loss: 121662.570312	Best loss: 7.782712	Accuracy: 94.61%
12	Validation loss: 135827.921875	Best loss: 7.782712	Accuracy: 94.72%
13	Validation loss: 141236.734375	Best loss: 7.782712	Accuracy: 95.58%
14	Validation loss: 157922.515625	Best loss: 7.782712	Accuracy: 94.02%
15	Validation loss: 162041.171875	Best loss: 7.782712	Accuracy: 90.07%
16	Validation loss: 54353.914062	Best loss: 7.782712	Accuracy: 96.44%
17	Validation loss: 76941.890625	Best loss: 7.782712	Accuracy: 95.74%
18	Validation loss: 52108.039062	Best loss: 7.782712	Accuracy: 96.17%
19	Validation loss: 75267.804688	Best loss: 7.782712	Accuracy: 93.08%
20	Validation loss: 50309.398438	Best loss: 7.782712	Accuracy: 97.15%
21	Validation loss: 45048.910156	Best loss: 7.782712	Accuracy: 96.13%
22	Validation loss: 33853.339844	Best loss: 7.782712	Accuracy: 96.79%
23	Validation loss: 37021.097656	Best loss: 7.782712	Accuracy: 95.74%
24	Validation loss: 28302.142578	Best loss: 7.782712	Accuracy: 97.07%
25	Validation loss: 29685.164062	Best loss: 7.782712	Accuracy: 97.19%
26	Validation loss: 29588.853516	Best loss: 7.782712	Accuracy: 97.19%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  14.7s
[CV] n_neurons=160, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 32.515369	Best loss: 32.515369	Accuracy: 93.98%
1	Validation loss: 16.289715	Best loss: 16.289715	Accuracy: 94.53%
2	Validation loss: 8.129853	Best loss: 8.129853	Accuracy: 94.06%
3	Validation loss: 7.175116	Best loss: 7.175116	Accuracy: 94.14%
4	Validation loss: 7.231346	Best loss: 7.175116	Accuracy: 96.05%
5	Validation loss: 2.579101	Best loss: 2.579101	Accuracy: 96.60%
6	Validation loss: 2.906160	Best loss: 2.579101	Accuracy: 95.97%
7	Validation loss: 13.786381	Best loss: 2.579101	Accuracy: 95.97%
8	Validation loss: 3.207865	Best loss: 2.579101	Accuracy: 97.11%
9	Validation loss: 2.542268	Best loss: 2.542268	Accuracy: 97.58%
10	Validation loss: 11.163155	Best loss: 2.542268	Accuracy: 95.23%
11	Validation loss: 3.392864	Best loss: 2.542268	Accuracy: 97.19%
12	Validation loss: 7.271116	Best loss: 2.542268	Accuracy: 96.76%
13	Validation loss: 5.287818	Best loss: 2.542268	Accuracy: 95.31%
14	Validation loss: 2.231087	Best loss: 2.231087	Accuracy: 97.34%
15	Validation loss: 2.203002	Best loss: 2.203002	Accuracy: 96.60%
16	Validation loss: 1.644449	Best loss: 1.644449	Accuracy: 97.30%
17	Validation loss: 5.987047	Best loss: 1.644449	Accuracy: 90.77%
18	Validation loss: 3.255816	Best loss: 1.644449	Accuracy: 97.03%
19	Validation loss: 4.721100	Best loss: 1.644449	Accuracy: 95.82%
20	Validation loss: 4.220914	Best loss: 1.644449	Accuracy: 97.22%
21	Validation loss: 15581531.000000	Best loss: 1.644449	Accuracy: 53.87%
22	Validation loss: 785354.625000	Best loss: 1.644449	Accuracy: 80.06%
23	Validation loss: 460766.312500	Best loss: 1.644449	Accuracy: 80.73%
24	Validation loss: 462602.093750	Best loss: 1.644449	Accuracy: 83.15%
25	Validation loss: 503352.156250	Best loss: 1.644449	Accuracy: 81.47%
26	Validation loss: 285683.000000	Best loss: 1.644449	Accuracy: 87.76%
27	Validation loss: 265309.437500	Best loss: 1.644449	Accuracy: 89.44%
28	Validation loss: 166228.156250	Best loss: 1.644449	Accuracy: 91.36%
29	Validation loss: 356829.687500	Best loss: 1.644449	Accuracy: 88.12%
30	Validation loss: 228582.953125	Best loss: 1.644449	Accuracy: 88.27%
31	Validation loss: 158458.640625	Best loss: 1.644449	Accuracy: 93.78%
32	Validation loss: 159676.328125	Best loss: 1.644449	Accuracy: 92.69%
33	Validation loss: 120866.148438	Best loss: 1.644449	Accuracy: 94.53%
34	Validation loss: 373850.875000	Best loss: 1.644449	Accuracy: 89.09%
35	Validation loss: 163039.265625	Best loss: 1.644449	Accuracy: 93.55%
36	Validation loss: 124161.000000	Best loss: 1.644449	Accuracy: 94.21%
37	Validation loss: 97590.742188	Best loss: 1.644449	Accuracy: 95.23%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  20.4s
[CV] n_neurons=50, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.128084	Best loss: 0.128084	Accuracy: 96.21%
1	Validation loss: 0.121882	Best loss: 0.121882	Accuracy: 97.42%
2	Validation loss: 0.077325	Best loss: 0.077325	Accuracy: 97.62%
3	Validation loss: 0.088737	Best loss: 0.077325	Accuracy: 97.69%
4	Validation loss: 0.244980	Best loss: 0.077325	Accuracy: 92.73%
5	Validation loss: 0.217784	Best loss: 0.077325	Accuracy: 95.82%
6	Validation loss: 0.106914	Best loss: 0.077325	Accuracy: 97.46%
7	Validation loss: 0.090666	Best loss: 0.077325	Accuracy: 98.05%
8	Validation loss: 0.117954	Best loss: 0.077325	Accuracy: 97.62%
9	Validation loss: 0.081684	Best loss: 0.077325	Accuracy: 98.05%
10	Validation loss: 0.082885	Best loss: 0.077325	Accuracy: 98.16%
11	Validation loss: 0.080812	Best loss: 0.077325	Accuracy: 98.08%
12	Validation loss: 0.101237	Best loss: 0.077325	Accuracy: 98.08%
13	Validation loss: 0.113208	Best loss: 0.077325	Accuracy: 97.73%
14	Validation loss: 0.093481	Best loss: 0.077325	Accuracy: 97.93%
15	Validation loss: 1.416567	Best loss: 0.077325	Accuracy: 33.07%
16	Validation loss: 1.632955	Best loss: 0.077325	Accuracy: 22.01%
17	Validation loss: 1.646952	Best loss: 0.077325	Accuracy: 20.91%
18	Validation loss: 1.634119	Best loss: 0.077325	Accuracy: 22.01%
19	Validation loss: 1.638276	Best loss: 0.077325	Accuracy: 19.08%
20	Validation loss: 1.632526	Best loss: 0.077325	Accuracy: 19.27%
21	Validation loss: 1.626489	Best loss: 0.077325	Accuracy: 18.73%
22	Validation loss: 1.617957	Best loss: 0.077325	Accuracy: 19.27%
23	Validation loss: 1.613388	Best loss: 0.077325	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   6.7s
[CV] n_neurons=50, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.097509	Best loss: 0.097509	Accuracy: 97.38%
1	Validation loss: 0.073418	Best loss: 0.073418	Accuracy: 97.89%
2	Validation loss: 0.068885	Best loss: 0.068885	Accuracy: 98.05%
3	Validation loss: 0.088722	Best loss: 0.068885	Accuracy: 98.20%
4	Validation loss: 0.578344	Best loss: 0.068885	Accuracy: 89.91%
5	Validation loss: 0.272800	Best loss: 0.068885	Accuracy: 91.71%
6	Validation loss: 0.148294	Best loss: 0.068885	Accuracy: 96.60%
7	Validation loss: 0.117646	Best loss: 0.068885	Accuracy: 96.95%
8	Validation loss: 0.115828	Best loss: 0.068885	Accuracy: 97.73%
9	Validation loss: 0.093386	Best loss: 0.068885	Accuracy: 97.81%
10	Validation loss: 0.105454	Best loss: 0.068885	Accuracy: 97.69%
11	Validation loss: 0.076066	Best loss: 0.068885	Accuracy: 98.16%
12	Validation loss: 0.094465	Best loss: 0.068885	Accuracy: 97.73%
13	Validation loss: 0.076660	Best loss: 0.068885	Accuracy: 98.16%
14	Validation loss: 0.066589	Best loss: 0.066589	Accuracy: 98.59%
15	Validation loss: 0.120040	Best loss: 0.066589	Accuracy: 98.59%
16	Validation loss: 0.114365	Best loss: 0.066589	Accuracy: 98.28%
17	Validation loss: 0.105127	Best loss: 0.066589	Accuracy: 97.77%
18	Validation loss: 1.266648	Best loss: 0.066589	Accuracy: 78.69%
19	Validation loss: 1.639745	Best loss: 0.066589	Accuracy: 19.27%
20	Validation loss: 1.650227	Best loss: 0.066589	Accuracy: 18.73%
21	Validation loss: 1.716461	Best loss: 0.066589	Accuracy: 18.73%
22	Validation loss: 1.652310	Best loss: 0.066589	Accuracy: 19.27%
23	Validation loss: 1.656049	Best loss: 0.066589	Accuracy: 22.01%
24	Validation loss: 1.653264	Best loss: 0.066589	Accuracy: 22.01%
25	Validation loss: 1.612929	Best loss: 0.066589	Accuracy: 19.08%
26	Validation loss: 1.643953	Best loss: 0.066589	Accuracy: 19.08%
27	Validation loss: 1.657784	Best loss: 0.066589	Accuracy: 19.08%
28	Validation loss: 1.630839	Best loss: 0.066589	Accuracy: 22.01%
29	Validation loss: 1.630064	Best loss: 0.066589	Accuracy: 20.91%
30	Validation loss: 1.616685	Best loss: 0.066589	Accuracy: 22.01%
31	Validation loss: 1.656565	Best loss: 0.066589	Accuracy: 18.73%
32	Validation loss: 1.610963	Best loss: 0.066589	Accuracy: 22.01%
33	Validation loss: 1.616565	Best loss: 0.066589	Accuracy: 19.27%
34	Validation loss: 1.610133	Best loss: 0.066589	Accuracy: 22.01%
35	Validation loss: 1.632564	Best loss: 0.066589	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.8s
[CV] n_neurons=50, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.093458	Best loss: 0.093458	Accuracy: 97.62%
1	Validation loss: 0.070399	Best loss: 0.070399	Accuracy: 98.20%
2	Validation loss: 0.104814	Best loss: 0.070399	Accuracy: 97.85%
3	Validation loss: 0.102726	Best loss: 0.070399	Accuracy: 97.22%
4	Validation loss: 0.779591	Best loss: 0.070399	Accuracy: 59.03%
5	Validation loss: 0.509724	Best loss: 0.070399	Accuracy: 76.82%
6	Validation loss: 0.766806	Best loss: 0.070399	Accuracy: 60.71%
7	Validation loss: 0.445529	Best loss: 0.070399	Accuracy: 78.03%
8	Validation loss: 0.272775	Best loss: 0.070399	Accuracy: 93.86%
9	Validation loss: 0.247231	Best loss: 0.070399	Accuracy: 95.07%
10	Validation loss: 0.196814	Best loss: 0.070399	Accuracy: 96.76%
11	Validation loss: 0.190161	Best loss: 0.070399	Accuracy: 96.95%
12	Validation loss: 0.176320	Best loss: 0.070399	Accuracy: 97.19%
13	Validation loss: 0.455916	Best loss: 0.070399	Accuracy: 86.20%
14	Validation loss: 0.211113	Best loss: 0.070399	Accuracy: 96.79%
15	Validation loss: 0.235758	Best loss: 0.070399	Accuracy: 96.95%
16	Validation loss: 0.224668	Best loss: 0.070399	Accuracy: 97.30%
17	Validation loss: 0.210018	Best loss: 0.070399	Accuracy: 97.03%
18	Validation loss: 0.296305	Best loss: 0.070399	Accuracy: 95.11%
19	Validation loss: 0.297873	Best loss: 0.070399	Accuracy: 96.56%
20	Validation loss: 0.244018	Best loss: 0.070399	Accuracy: 97.22%
21	Validation loss: 0.294960	Best loss: 0.070399	Accuracy: 96.91%
22	Validation loss: 0.252831	Best loss: 0.070399	Accuracy: 97.50%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=   6.4s
[CV] n_neurons=10, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.105742	Best loss: 0.105742	Accuracy: 96.99%
1	Validation loss: 0.116264	Best loss: 0.105742	Accuracy: 96.68%
2	Validation loss: 0.110830	Best loss: 0.105742	Accuracy: 97.30%
3	Validation loss: 0.103399	Best loss: 0.103399	Accuracy: 97.26%
4	Validation loss: 0.123369	Best loss: 0.103399	Accuracy: 96.76%
5	Validation loss: 0.106893	Best loss: 0.103399	Accuracy: 96.99%
6	Validation loss: 0.114585	Best loss: 0.103399	Accuracy: 96.91%
7	Validation loss: 0.111959	Best loss: 0.103399	Accuracy: 97.42%
8	Validation loss: 0.111728	Best loss: 0.103399	Accuracy: 97.19%
9	Validation loss: 0.110477	Best loss: 0.103399	Accuracy: 97.26%
10	Validation loss: 0.119861	Best loss: 0.103399	Accuracy: 97.22%
11	Validation loss: 0.126853	Best loss: 0.103399	Accuracy: 97.22%
12	Validation loss: 0.118163	Best loss: 0.103399	Accuracy: 97.19%
13	Validation loss: 0.129422	Best loss: 0.103399	Accuracy: 97.07%
14	Validation loss: 0.109360	Best loss: 0.103399	Accuracy: 96.99%
15	Validation loss: 0.118025	Best loss: 0.103399	Accuracy: 97.22%
16	Validation loss: 0.101232	Best loss: 0.101232	Accuracy: 97.38%
17	Validation loss: 0.151086	Best loss: 0.101232	Accuracy: 97.07%
18	Validation loss: 0.156729	Best loss: 0.101232	Accuracy: 96.48%
19	Validation loss: 0.157383	Best loss: 0.101232	Accuracy: 96.44%
20	Validation loss: 0.136215	Best loss: 0.101232	Accuracy: 97.07%
21	Validation loss: 0.133051	Best loss: 0.101232	Accuracy: 97.11%
22	Validation loss: 0.122598	Best loss: 0.101232	Accuracy: 97.42%
23	Validation loss: 0.153968	Best loss: 0.101232	Accuracy: 97.26%
24	Validation loss: 0.126776	Best loss: 0.101232	Accuracy: 97.26%
25	Validation loss: 0.220475	Best loss: 0.101232	Accuracy: 95.15%
26	Validation loss: 0.132162	Best loss: 0.101232	Accuracy: 97.19%
27	Validation loss: 0.115831	Best loss: 0.101232	Accuracy: 97.22%
28	Validation loss: 0.157583	Best loss: 0.101232	Accuracy: 96.99%
29	Validation loss: 0.153928	Best loss: 0.101232	Accuracy: 96.83%
30	Validation loss: 0.158906	Best loss: 0.101232	Accuracy: 96.76%
31	Validation loss: 0.126074	Best loss: 0.101232	Accuracy: 96.87%
32	Validation loss: 0.114233	Best loss: 0.101232	Accuracy: 97.26%
33	Validation loss: 0.153629	Best loss: 0.101232	Accuracy: 96.29%
34	Validation loss: 0.153165	Best loss: 0.101232	Accuracy: 97.03%
35	Validation loss: 0.140010	Best loss: 0.101232	Accuracy: 96.99%
36	Validation loss: 0.113041	Best loss: 0.101232	Accuracy: 97.15%
37	Validation loss: 0.109880	Best loss: 0.101232	Accuracy: 97.54%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   6.7s
[CV] n_neurons=10, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.205076	Best loss: 0.205076	Accuracy: 94.41%
1	Validation loss: 0.174270	Best loss: 0.174270	Accuracy: 95.47%
2	Validation loss: 0.145838	Best loss: 0.145838	Accuracy: 95.54%
3	Validation loss: 0.136267	Best loss: 0.136267	Accuracy: 96.21%
4	Validation loss: 0.137646	Best loss: 0.136267	Accuracy: 95.70%
5	Validation loss: 0.131628	Best loss: 0.131628	Accuracy: 96.48%
6	Validation loss: 0.112321	Best loss: 0.112321	Accuracy: 96.91%
7	Validation loss: 0.120691	Best loss: 0.112321	Accuracy: 96.56%
8	Validation loss: 0.130301	Best loss: 0.112321	Accuracy: 96.48%
9	Validation loss: 0.122084	Best loss: 0.112321	Accuracy: 96.83%
10	Validation loss: 0.149156	Best loss: 0.112321	Accuracy: 96.21%
11	Validation loss: 0.113842	Best loss: 0.112321	Accuracy: 96.76%
12	Validation loss: 0.128375	Best loss: 0.112321	Accuracy: 96.13%
13	Validation loss: 0.121957	Best loss: 0.112321	Accuracy: 96.56%
14	Validation loss: 0.127771	Best loss: 0.112321	Accuracy: 96.68%
15	Validation loss: 0.119942	Best loss: 0.112321	Accuracy: 96.52%
16	Validation loss: 0.165527	Best loss: 0.112321	Accuracy: 95.35%
17	Validation loss: 0.150534	Best loss: 0.112321	Accuracy: 95.90%
18	Validation loss: 0.144012	Best loss: 0.112321	Accuracy: 96.01%
19	Validation loss: 0.145979	Best loss: 0.112321	Accuracy: 96.33%
20	Validation loss: 0.135823	Best loss: 0.112321	Accuracy: 96.33%
21	Validation loss: 0.129461	Best loss: 0.112321	Accuracy: 96.36%
22	Validation loss: 0.132725	Best loss: 0.112321	Accuracy: 96.64%
23	Validation loss: 0.123434	Best loss: 0.112321	Accuracy: 96.76%
24	Validation loss: 0.154676	Best loss: 0.112321	Accuracy: 96.17%
25	Validation loss: 0.166454	Best loss: 0.112321	Accuracy: 96.05%
26	Validation loss: 0.150264	Best loss: 0.112321	Accuracy: 96.29%
27	Validation loss: 0.146350	Best loss: 0.112321	Accuracy: 96.48%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   5.1s
[CV] n_neurons=10, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.126185	Best loss: 0.126185	Accuracy: 96.40%
1	Validation loss: 0.117986	Best loss: 0.117986	Accuracy: 96.83%
2	Validation loss: 0.130621	Best loss: 0.117986	Accuracy: 96.95%
3	Validation loss: 0.116715	Best loss: 0.116715	Accuracy: 96.91%
4	Validation loss: 0.117696	Best loss: 0.116715	Accuracy: 97.07%
5	Validation loss: 0.128077	Best loss: 0.116715	Accuracy: 96.52%
6	Validation loss: 0.143549	Best loss: 0.116715	Accuracy: 96.48%
7	Validation loss: 0.131631	Best loss: 0.116715	Accuracy: 96.36%
8	Validation loss: 0.132102	Best loss: 0.116715	Accuracy: 96.79%
9	Validation loss: 0.116015	Best loss: 0.116015	Accuracy: 97.30%
10	Validation loss: 0.096474	Best loss: 0.096474	Accuracy: 97.42%
11	Validation loss: 0.118935	Best loss: 0.096474	Accuracy: 96.87%
12	Validation loss: 0.125182	Best loss: 0.096474	Accuracy: 96.48%
13	Validation loss: 0.109730	Best loss: 0.096474	Accuracy: 97.15%
14	Validation loss: 0.106193	Best loss: 0.096474	Accuracy: 97.46%
15	Validation loss: 0.111280	Best loss: 0.096474	Accuracy: 97.42%
16	Validation loss: 0.124466	Best loss: 0.096474	Accuracy: 96.91%
17	Validation loss: 0.108865	Best loss: 0.096474	Accuracy: 97.30%
18	Validation loss: 0.113521	Best loss: 0.096474	Accuracy: 96.99%
19	Validation loss: 0.119364	Best loss: 0.096474	Accuracy: 96.72%
20	Validation loss: 0.108892	Best loss: 0.096474	Accuracy: 97.34%
21	Validation loss: 0.145961	Best loss: 0.096474	Accuracy: 95.58%
22	Validation loss: 0.131299	Best loss: 0.096474	Accuracy: 97.11%
23	Validation loss: 0.111454	Best loss: 0.096474	Accuracy: 97.38%
24	Validation loss: 0.097983	Best loss: 0.096474	Accuracy: 97.30%
25	Validation loss: 0.104193	Best loss: 0.096474	Accuracy: 97.11%
26	Validation loss: 0.114877	Best loss: 0.096474	Accuracy: 97.26%
27	Validation loss: 0.139946	Best loss: 0.096474	Accuracy: 96.17%
28	Validation loss: 0.118245	Best loss: 0.096474	Accuracy: 97.30%
29	Validation loss: 0.113720	Best loss: 0.096474	Accuracy: 97.38%
30	Validation loss: 0.137463	Best loss: 0.096474	Accuracy: 97.11%
31	Validation loss: 0.112100	Best loss: 0.096474	Accuracy: 97.22%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=   5.7s
[CV] n_neurons=30, learning_rate=0.02, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.261187	Best loss: 0.261187	Accuracy: 94.25%
1	Validation loss: 0.625411	Best loss: 0.261187	Accuracy: 81.08%
2	Validation loss: 0.882587	Best loss: 0.261187	Accuracy: 56.88%
3	Validation loss: 1.263756	Best loss: 0.261187	Accuracy: 36.63%
4	Validation loss: 1.596097	Best loss: 0.261187	Accuracy: 23.53%
5	Validation loss: 1.456450	Best loss: 0.261187	Accuracy: 31.08%
6	Validation loss: 1.588990	Best loss: 0.261187	Accuracy: 21.81%
7	Validation loss: 1.580399	Best loss: 0.261187	Accuracy: 21.62%
8	Validation loss: 1.593953	Best loss: 0.261187	Accuracy: 21.81%
9	Validation loss: 1.588406	Best loss: 0.261187	Accuracy: 21.62%
10	Validation loss: 1.576711	Best loss: 0.261187	Accuracy: 23.46%
11	Validation loss: 1.577388	Best loss: 0.261187	Accuracy: 21.62%
12	Validation loss: 1.581589	Best loss: 0.261187	Accuracy: 21.62%
13	Validation loss: 1.573264	Best loss: 0.261187	Accuracy: 21.27%
14	Validation loss: 1.576695	Best loss: 0.261187	Accuracy: 21.62%
15	Validation loss: 1.577210	Best loss: 0.261187	Accuracy: 22.01%
16	Validation loss: 1.574502	Best loss: 0.261187	Accuracy: 21.27%
17	Validation loss: 1.570035	Best loss: 0.261187	Accuracy: 21.81%
18	Validation loss: 1.580881	Best loss: 0.261187	Accuracy: 21.27%
19	Validation loss: 1.585420	Best loss: 0.261187	Accuracy: 23.46%
20	Validation loss: 1.577258	Best loss: 0.261187	Accuracy: 23.46%
21	Validation loss: 1.586756	Best loss: 0.261187	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total=  28.7s
[CV] n_neurons=30, learning_rate=0.02, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.179073	Best loss: 0.179073	Accuracy: 94.72%
1	Validation loss: 0.231466	Best loss: 0.179073	Accuracy: 93.00%
2	Validation loss: 0.152511	Best loss: 0.152511	Accuracy: 96.36%
3	Validation loss: 0.508296	Best loss: 0.152511	Accuracy: 80.73%
4	Validation loss: 0.577914	Best loss: 0.152511	Accuracy: 84.44%
5	Validation loss: 0.474993	Best loss: 0.152511	Accuracy: 75.88%
6	Validation loss: 0.727341	Best loss: 0.152511	Accuracy: 59.93%
7	Validation loss: 0.861227	Best loss: 0.152511	Accuracy: 59.66%
8	Validation loss: 0.916090	Best loss: 0.152511	Accuracy: 54.89%
9	Validation loss: 1.192407	Best loss: 0.152511	Accuracy: 41.56%
10	Validation loss: 1.513961	Best loss: 0.152511	Accuracy: 27.95%
11	Validation loss: 1.515107	Best loss: 0.152511	Accuracy: 27.95%
12	Validation loss: 1.517213	Best loss: 0.152511	Accuracy: 27.95%
13	Validation loss: 1.524968	Best loss: 0.152511	Accuracy: 24.51%
14	Validation loss: 1.149237	Best loss: 0.152511	Accuracy: 42.85%
15	Validation loss: 1.152367	Best loss: 0.152511	Accuracy: 42.81%
16	Validation loss: 1.234012	Best loss: 0.152511	Accuracy: 38.35%
17	Validation loss: 0.946171	Best loss: 0.152511	Accuracy: 52.11%
18	Validation loss: 1.086892	Best loss: 0.152511	Accuracy: 39.17%
19	Validation loss: 1.122566	Best loss: 0.152511	Accuracy: 39.84%
20	Validation loss: 1.176286	Best loss: 0.152511	Accuracy: 37.10%
21	Validation loss: 1.195623	Best loss: 0.152511	Accuracy: 39.56%
22	Validation loss: 1.186669	Best loss: 0.152511	Accuracy: 37.10%
23	Validation loss: 1.188370	Best loss: 0.152511	Accuracy: 37.10%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total=  31.1s
[CV] n_neurons=30, learning_rate=0.02, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.731865	Best loss: 0.731865	Accuracy: 93.32%
1	Validation loss: 0.157246	Best loss: 0.157246	Accuracy: 95.78%
2	Validation loss: 0.213597	Best loss: 0.157246	Accuracy: 95.43%
3	Validation loss: 0.148358	Best loss: 0.148358	Accuracy: 96.60%
4	Validation loss: 0.236871	Best loss: 0.148358	Accuracy: 93.08%
5	Validation loss: 0.217133	Best loss: 0.148358	Accuracy: 95.35%
6	Validation loss: 0.285649	Best loss: 0.148358	Accuracy: 93.32%
7	Validation loss: 0.260371	Best loss: 0.148358	Accuracy: 94.25%
8	Validation loss: 0.218526	Best loss: 0.148358	Accuracy: 94.88%
9	Validation loss: 0.411371	Best loss: 0.148358	Accuracy: 88.90%
10	Validation loss: 0.643125	Best loss: 0.148358	Accuracy: 70.88%
11	Validation loss: 0.521395	Best loss: 0.148358	Accuracy: 74.20%
12	Validation loss: 0.605391	Best loss: 0.148358	Accuracy: 76.19%
13	Validation loss: 0.495829	Best loss: 0.148358	Accuracy: 82.10%
14	Validation loss: 0.567087	Best loss: 0.148358	Accuracy: 77.56%
15	Validation loss: 0.526334	Best loss: 0.148358	Accuracy: 83.31%
16	Validation loss: 0.645155	Best loss: 0.148358	Accuracy: 73.92%
17	Validation loss: 0.557730	Best loss: 0.148358	Accuracy: 85.61%
18	Validation loss: 0.418144	Best loss: 0.148358	Accuracy: 88.39%
19	Validation loss: 0.432810	Best loss: 0.148358	Accuracy: 88.82%
20	Validation loss: 0.911479	Best loss: 0.148358	Accuracy: 60.40%
21	Validation loss: 0.923057	Best loss: 0.148358	Accuracy: 57.04%
22	Validation loss: 0.849493	Best loss: 0.148358	Accuracy: 60.32%
23	Validation loss: 0.865971	Best loss: 0.148358	Accuracy: 60.32%
24	Validation loss: 0.902506	Best loss: 0.148358	Accuracy: 60.32%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total=  32.4s
[CV] n_neurons=120, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.072833	Best loss: 0.072833	Accuracy: 98.08%
1	Validation loss: 0.076418	Best loss: 0.072833	Accuracy: 97.97%
2	Validation loss: 0.077608	Best loss: 0.072833	Accuracy: 98.08%
3	Validation loss: 0.095008	Best loss: 0.072833	Accuracy: 97.73%
4	Validation loss: 0.104744	Best loss: 0.072833	Accuracy: 97.77%
5	Validation loss: 0.105473	Best loss: 0.072833	Accuracy: 98.16%
6	Validation loss: 0.062860	Best loss: 0.062860	Accuracy: 98.32%
7	Validation loss: 0.085528	Best loss: 0.062860	Accuracy: 98.36%
8	Validation loss: 0.076974	Best loss: 0.062860	Accuracy: 98.28%
9	Validation loss: 0.085123	Best loss: 0.062860	Accuracy: 98.24%
10	Validation loss: 0.554614	Best loss: 0.062860	Accuracy: 87.76%
11	Validation loss: 0.120995	Best loss: 0.062860	Accuracy: 98.01%
12	Validation loss: 0.083828	Best loss: 0.062860	Accuracy: 98.36%
13	Validation loss: 0.081963	Best loss: 0.062860	Accuracy: 98.48%
14	Validation loss: 0.085729	Best loss: 0.062860	Accuracy: 98.59%
15	Validation loss: 0.085307	Best loss: 0.062860	Accuracy: 98.75%
16	Validation loss: 0.069851	Best loss: 0.062860	Accuracy: 98.87%
17	Validation loss: 0.089594	Best loss: 0.062860	Accuracy: 98.83%
18	Validation loss: 0.101560	Best loss: 0.062860	Accuracy: 98.98%
19	Validation loss: 0.117430	Best loss: 0.062860	Accuracy: 98.28%
20	Validation loss: 0.073279	Best loss: 0.062860	Accuracy: 98.83%
21	Validation loss: 0.103664	Best loss: 0.062860	Accuracy: 98.75%
22	Validation loss: 0.100924	Best loss: 0.062860	Accuracy: 98.87%
23	Validation loss: 0.091810	Best loss: 0.062860	Accuracy: 98.48%
24	Validation loss: 0.998135	Best loss: 0.062860	Accuracy: 64.78%
25	Validation loss: 0.565008	Best loss: 0.062860	Accuracy: 91.01%
26	Validation loss: 0.521373	Best loss: 0.062860	Accuracy: 75.06%
27	Validation loss: 0.470369	Best loss: 0.062860	Accuracy: 77.44%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  11.6s
[CV] n_neurons=120, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.096605	Best loss: 0.096605	Accuracy: 97.69%
1	Validation loss: 0.076199	Best loss: 0.076199	Accuracy: 97.93%
2	Validation loss: 0.078768	Best loss: 0.076199	Accuracy: 97.69%
3	Validation loss: 0.056607	Best loss: 0.056607	Accuracy: 98.40%
4	Validation loss: 0.113674	Best loss: 0.056607	Accuracy: 98.36%
5	Validation loss: 0.068552	Best loss: 0.056607	Accuracy: 98.32%
6	Validation loss: 0.065105	Best loss: 0.056607	Accuracy: 97.97%
7	Validation loss: 0.096096	Best loss: 0.056607	Accuracy: 98.59%
8	Validation loss: 0.037631	Best loss: 0.037631	Accuracy: 98.87%
9	Validation loss: 0.057820	Best loss: 0.037631	Accuracy: 98.71%
10	Validation loss: 0.053147	Best loss: 0.037631	Accuracy: 98.48%
11	Validation loss: 0.044881	Best loss: 0.037631	Accuracy: 98.79%
12	Validation loss: 0.291436	Best loss: 0.037631	Accuracy: 96.44%
13	Validation loss: 0.143386	Best loss: 0.037631	Accuracy: 98.05%
14	Validation loss: 0.093490	Best loss: 0.037631	Accuracy: 98.48%
15	Validation loss: 0.058400	Best loss: 0.037631	Accuracy: 98.63%
16	Validation loss: 0.065269	Best loss: 0.037631	Accuracy: 98.71%
17	Validation loss: 0.095075	Best loss: 0.037631	Accuracy: 97.62%
18	Validation loss: 0.070456	Best loss: 0.037631	Accuracy: 98.67%
19	Validation loss: 0.158652	Best loss: 0.037631	Accuracy: 97.97%
20	Validation loss: 0.122303	Best loss: 0.037631	Accuracy: 97.19%
21	Validation loss: 0.095261	Best loss: 0.037631	Accuracy: 98.08%
22	Validation loss: 0.075873	Best loss: 0.037631	Accuracy: 98.28%
23	Validation loss: 0.101343	Best loss: 0.037631	Accuracy: 98.63%
24	Validation loss: 0.206738	Best loss: 0.037631	Accuracy: 97.11%
25	Validation loss: 0.188438	Best loss: 0.037631	Accuracy: 97.58%
26	Validation loss: 0.117095	Best loss: 0.037631	Accuracy: 98.20%
27	Validation loss: 0.099175	Best loss: 0.037631	Accuracy: 98.83%
28	Validation loss: 0.159694	Best loss: 0.037631	Accuracy: 97.93%
29	Validation loss: 0.070927	Best loss: 0.037631	Accuracy: 98.36%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  12.3s
[CV] n_neurons=120, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.081258	Best loss: 0.081258	Accuracy: 97.89%
1	Validation loss: 0.067638	Best loss: 0.067638	Accuracy: 98.36%
2	Validation loss: 0.113702	Best loss: 0.067638	Accuracy: 97.46%
3	Validation loss: 0.070572	Best loss: 0.067638	Accuracy: 98.48%
4	Validation loss: 0.091244	Best loss: 0.067638	Accuracy: 98.24%
5	Validation loss: 0.084833	Best loss: 0.067638	Accuracy: 98.40%
6	Validation loss: 0.078447	Best loss: 0.067638	Accuracy: 98.48%
7	Validation loss: 0.143845	Best loss: 0.067638	Accuracy: 97.46%
8	Validation loss: 0.112540	Best loss: 0.067638	Accuracy: 97.81%
9	Validation loss: 0.054460	Best loss: 0.054460	Accuracy: 98.63%
10	Validation loss: 0.135176	Best loss: 0.054460	Accuracy: 98.40%
11	Validation loss: 0.135728	Best loss: 0.054460	Accuracy: 97.54%
12	Validation loss: 0.151691	Best loss: 0.054460	Accuracy: 96.83%
13	Validation loss: 0.168533	Best loss: 0.054460	Accuracy: 98.05%
14	Validation loss: 0.069865	Best loss: 0.054460	Accuracy: 98.20%
15	Validation loss: 0.100028	Best loss: 0.054460	Accuracy: 98.40%
16	Validation loss: 0.182748	Best loss: 0.054460	Accuracy: 98.20%
17	Validation loss: 0.114434	Best loss: 0.054460	Accuracy: 98.01%
18	Validation loss: 0.083578	Best loss: 0.054460	Accuracy: 98.48%
19	Validation loss: 0.144671	Best loss: 0.054460	Accuracy: 98.55%
20	Validation loss: 0.089629	Best loss: 0.054460	Accuracy: 98.83%
21	Validation loss: 0.155098	Best loss: 0.054460	Accuracy: 98.28%
22	Validation loss: 0.109119	Best loss: 0.054460	Accuracy: 97.30%
23	Validation loss: 0.108499	Best loss: 0.054460	Accuracy: 98.40%
24	Validation loss: 0.087259	Best loss: 0.054460	Accuracy: 98.75%
25	Validation loss: 0.282182	Best loss: 0.054460	Accuracy: 98.55%
26	Validation loss: 0.072217	Best loss: 0.054460	Accuracy: 98.63%
27	Validation loss: 0.121661	Best loss: 0.054460	Accuracy: 98.71%
28	Validation loss: 0.088180	Best loss: 0.054460	Accuracy: 98.71%
29	Validation loss: 0.108987	Best loss: 0.054460	Accuracy: 98.83%
30	Validation loss: 0.110945	Best loss: 0.054460	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  12.6s
[CV] n_neurons=120, learning_rate=0.01, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.158743	Best loss: 0.158743	Accuracy: 95.97%
1	Validation loss: 0.175356	Best loss: 0.158743	Accuracy: 96.25%
2	Validation loss: 2.742762	Best loss: 0.158743	Accuracy: 75.18%
3	Validation loss: 0.182539	Best loss: 0.158743	Accuracy: 96.17%
4	Validation loss: 0.160232	Best loss: 0.158743	Accuracy: 96.56%
5	Validation loss: 0.192232	Best loss: 0.158743	Accuracy: 96.29%
6	Validation loss: 0.441217	Best loss: 0.158743	Accuracy: 94.25%
7	Validation loss: 0.209317	Best loss: 0.158743	Accuracy: 94.80%
8	Validation loss: 0.154749	Best loss: 0.154749	Accuracy: 96.21%
9	Validation loss: 0.351243	Best loss: 0.154749	Accuracy: 92.06%
10	Validation loss: 0.441359	Best loss: 0.154749	Accuracy: 90.70%
11	Validation loss: 0.380843	Best loss: 0.154749	Accuracy: 94.45%
12	Validation loss: 0.469308	Best loss: 0.154749	Accuracy: 93.32%
13	Validation loss: 0.416868	Best loss: 0.154749	Accuracy: 90.23%
14	Validation loss: 0.617168	Best loss: 0.154749	Accuracy: 94.02%
15	Validation loss: 0.329243	Best loss: 0.154749	Accuracy: 95.23%
16	Validation loss: 0.275941	Best loss: 0.154749	Accuracy: 95.35%
17	Validation loss: 0.577408	Best loss: 0.154749	Accuracy: 91.56%
18	Validation loss: 0.433793	Best loss: 0.154749	Accuracy: 92.73%
19	Validation loss: 0.578490	Best loss: 0.154749	Accuracy: 73.85%
20	Validation loss: 0.613846	Best loss: 0.154749	Accuracy: 78.77%
21	Validation loss: 0.565662	Best loss: 0.154749	Accuracy: 78.97%
22	Validation loss: 0.723192	Best loss: 0.154749	Accuracy: 78.26%
23	Validation loss: 0.744353	Best loss: 0.154749	Accuracy: 76.11%
24	Validation loss: 1.097645	Best loss: 0.154749	Accuracy: 77.87%
25	Validation loss: 0.482328	Best loss: 0.154749	Accuracy: 86.43%
26	Validation loss: 0.288647	Best loss: 0.154749	Accuracy: 96.01%
27	Validation loss: 0.254202	Best loss: 0.154749	Accuracy: 96.09%
28	Validation loss: 1.283489	Best loss: 0.154749	Accuracy: 77.17%
29	Validation loss: 0.930232	Best loss: 0.154749	Accuracy: 89.95%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total=  58.9s
[CV] n_neurons=120, learning_rate=0.01, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.434419	Best loss: 0.434419	Accuracy: 94.84%
1	Validation loss: 0.165357	Best loss: 0.165357	Accuracy: 96.95%
2	Validation loss: 0.193230	Best loss: 0.165357	Accuracy: 93.98%
3	Validation loss: 0.133972	Best loss: 0.133972	Accuracy: 95.47%
4	Validation loss: 0.494334	Best loss: 0.133972	Accuracy: 96.36%
5	Validation loss: 0.236575	Best loss: 0.133972	Accuracy: 93.55%
6	Validation loss: 0.443476	Best loss: 0.133972	Accuracy: 88.86%
7	Validation loss: 0.394323	Best loss: 0.133972	Accuracy: 92.85%
8	Validation loss: 0.431708	Best loss: 0.133972	Accuracy: 96.36%
9	Validation loss: 0.267810	Best loss: 0.133972	Accuracy: 92.10%
10	Validation loss: 0.471572	Best loss: 0.133972	Accuracy: 77.05%
11	Validation loss: 0.380449	Best loss: 0.133972	Accuracy: 79.71%
12	Validation loss: 0.612960	Best loss: 0.133972	Accuracy: 71.46%
13	Validation loss: 0.520732	Best loss: 0.133972	Accuracy: 74.90%
14	Validation loss: 0.480766	Best loss: 0.133972	Accuracy: 77.37%
15	Validation loss: 1.148001	Best loss: 0.133972	Accuracy: 60.20%
16	Validation loss: 0.821323	Best loss: 0.133972	Accuracy: 58.80%
17	Validation loss: 0.872459	Best loss: 0.133972	Accuracy: 57.15%
18	Validation loss: 0.953050	Best loss: 0.133972	Accuracy: 55.63%
19	Validation loss: 0.844159	Best loss: 0.133972	Accuracy: 58.41%
20	Validation loss: 1.114879	Best loss: 0.133972	Accuracy: 56.10%
21	Validation loss: 0.915553	Best loss: 0.133972	Accuracy: 58.87%
22	Validation loss: 1.146612	Best loss: 0.133972	Accuracy: 40.42%
23	Validation loss: 1.031542	Best loss: 0.133972	Accuracy: 40.42%
24	Validation loss: 1.033368	Best loss: 0.133972	Accuracy: 39.87%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total=  49.7s
[CV] n_neurons=120, learning_rate=0.01, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.241853	Best loss: 1.241853	Accuracy: 94.80%
1	Validation loss: 0.164476	Best loss: 0.164476	Accuracy: 96.99%
2	Validation loss: 0.244041	Best loss: 0.164476	Accuracy: 94.37%
3	Validation loss: 0.649923	Best loss: 0.164476	Accuracy: 94.72%
4	Validation loss: 0.192394	Best loss: 0.164476	Accuracy: 96.99%
5	Validation loss: 0.309441	Best loss: 0.164476	Accuracy: 95.62%
6	Validation loss: 0.190648	Best loss: 0.164476	Accuracy: 95.00%
7	Validation loss: 0.262070	Best loss: 0.164476	Accuracy: 94.80%
8	Validation loss: 0.454981	Best loss: 0.164476	Accuracy: 94.68%
9	Validation loss: 0.184467	Best loss: 0.164476	Accuracy: 95.97%
10	Validation loss: 0.183467	Best loss: 0.164476	Accuracy: 94.37%
11	Validation loss: 0.265678	Best loss: 0.164476	Accuracy: 95.50%
12	Validation loss: 1.036189	Best loss: 0.164476	Accuracy: 94.76%
13	Validation loss: 13.363233	Best loss: 0.164476	Accuracy: 91.67%
14	Validation loss: 0.433272	Best loss: 0.164476	Accuracy: 88.39%
15	Validation loss: 0.212787	Best loss: 0.164476	Accuracy: 95.19%
16	Validation loss: 0.352658	Best loss: 0.164476	Accuracy: 94.61%
17	Validation loss: 0.711965	Best loss: 0.164476	Accuracy: 94.80%
18	Validation loss: 0.430510	Best loss: 0.164476	Accuracy: 78.46%
19	Validation loss: 0.364117	Best loss: 0.164476	Accuracy: 93.32%
20	Validation loss: 0.272212	Best loss: 0.164476	Accuracy: 95.07%
21	Validation loss: 0.943589	Best loss: 0.164476	Accuracy: 73.85%
22	Validation loss: 0.801749	Best loss: 0.164476	Accuracy: 66.18%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total=  46.0s
[CV] n_neurons=100, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 31.555161	Best loss: 31.555161	Accuracy: 79.98%
1	Validation loss: 3.928186	Best loss: 3.928186	Accuracy: 91.05%
2	Validation loss: 1.316685	Best loss: 1.316685	Accuracy: 94.02%
3	Validation loss: 1.056424	Best loss: 1.056424	Accuracy: 93.59%
4	Validation loss: 1.124339	Best loss: 1.056424	Accuracy: 93.59%
5	Validation loss: 0.793123	Best loss: 0.793123	Accuracy: 94.80%
6	Validation loss: 1.723117	Best loss: 0.793123	Accuracy: 93.82%
7	Validation loss: 0.758391	Best loss: 0.758391	Accuracy: 95.04%
8	Validation loss: 0.729438	Best loss: 0.729438	Accuracy: 94.57%
9	Validation loss: 0.490089	Best loss: 0.490089	Accuracy: 96.36%
10	Validation loss: 0.425908	Best loss: 0.425908	Accuracy: 97.03%
11	Validation loss: 0.399491	Best loss: 0.399491	Accuracy: 97.03%
12	Validation loss: 0.428112	Best loss: 0.399491	Accuracy: 96.40%
13	Validation loss: 0.378064	Best loss: 0.378064	Accuracy: 96.60%
14	Validation loss: 0.371523	Best loss: 0.371523	Accuracy: 95.47%
15	Validation loss: 0.528916	Best loss: 0.371523	Accuracy: 94.88%
16	Validation loss: 0.332896	Best loss: 0.332896	Accuracy: 97.19%
17	Validation loss: 0.337620	Best loss: 0.332896	Accuracy: 97.07%
18	Validation loss: 0.319067	Best loss: 0.319067	Accuracy: 96.72%
19	Validation loss: 0.304633	Best loss: 0.304633	Accuracy: 97.42%
20	Validation loss: 0.264977	Best loss: 0.264977	Accuracy: 97.54%
21	Validation loss: 0.295209	Best loss: 0.264977	Accuracy: 96.60%
22	Validation loss: 0.345669	Best loss: 0.264977	Accuracy: 97.07%
23	Validation loss: 0.371207	Best loss: 0.264977	Accuracy: 94.18%
24	Validation loss: 0.355928	Best loss: 0.264977	Accuracy: 97.54%
25	Validation loss: 0.466448	Best loss: 0.264977	Accuracy: 94.18%
26	Validation loss: 0.266277	Best loss: 0.264977	Accuracy: 97.34%
27	Validation loss: 0.317190	Best loss: 0.264977	Accuracy: 96.72%
28	Validation loss: 0.450156	Best loss: 0.264977	Accuracy: 97.54%
29	Validation loss: 0.380622	Best loss: 0.264977	Accuracy: 96.29%
30	Validation loss: 0.419951	Best loss: 0.264977	Accuracy: 97.54%
31	Validation loss: 0.337237	Best loss: 0.264977	Accuracy: 97.50%
32	Validation loss: 0.299938	Best loss: 0.264977	Accuracy: 97.85%
33	Validation loss: 0.379523	Best loss: 0.264977	Accuracy: 96.56%
34	Validation loss: 0.306031	Best loss: 0.264977	Accuracy: 97.15%
35	Validation loss: 0.321359	Best loss: 0.264977	Accuracy: 97.54%
36	Validation loss: 0.261933	Best loss: 0.261933	Accuracy: 97.26%
37	Validation loss: 0.390640	Best loss: 0.261933	Accuracy: 95.86%
38	Validation loss: 0.324195	Best loss: 0.261933	Accuracy: 97.93%
39	Validation loss: 0.277407	Best loss: 0.261933	Accuracy: 97.89%
40	Validation loss: 0.422299	Best loss: 0.261933	Accuracy: 97.81%
41	Validation loss: 0.351587	Best loss: 0.261933	Accuracy: 97.69%
42	Validation loss: 0.524073	Best loss: 0.261933	Accuracy: 96.44%
43	Validation loss: 0.375860	Best loss: 0.261933	Accuracy: 97.62%
44	Validation loss: 0.324828	Best loss: 0.261933	Accuracy: 97.58%
45	Validation loss: 0.335857	Best loss: 0.261933	Accuracy: 97.65%
46	Validation loss: 0.238994	Best loss: 0.238994	Accuracy: 97.73%
47	Validation loss: 0.326151	Best loss: 0.238994	Accuracy: 97.19%
48	Validation loss: 0.227151	Best loss: 0.227151	Accuracy: 97.73%
49	Validation loss: 0.311577	Best loss: 0.227151	Accuracy: 97.77%
50	Validation loss: 0.309218	Best loss: 0.227151	Accuracy: 97.50%
51	Validation loss: 0.258453	Best loss: 0.227151	Accuracy: 97.81%
52	Validation loss: 0.314777	Best loss: 0.227151	Accuracy: 97.85%
53	Validation loss: 0.304067	Best loss: 0.227151	Accuracy: 97.62%
54	Validation loss: 0.266175	Best loss: 0.227151	Accuracy: 97.73%
55	Validation loss: 161124352.000000	Best loss: 0.227151	Accuracy: 18.73%
56	Validation loss: 462797.875000	Best loss: 0.227151	Accuracy: 62.35%
57	Validation loss: 63120.460938	Best loss: 0.227151	Accuracy: 87.69%
58	Validation loss: 35409.601562	Best loss: 0.227151	Accuracy: 89.09%
59	Validation loss: 24257.121094	Best loss: 0.227151	Accuracy: 92.06%
60	Validation loss: 16146.052734	Best loss: 0.227151	Accuracy: 93.28%
61	Validation loss: 15166.475586	Best loss: 0.227151	Accuracy: 92.42%
62	Validation loss: 11487.037109	Best loss: 0.227151	Accuracy: 94.80%
63	Validation loss: 11829.608398	Best loss: 0.227151	Accuracy: 93.78%
64	Validation loss: 8591.123047	Best loss: 0.227151	Accuracy: 95.04%
65	Validation loss: 9168.173828	Best loss: 0.227151	Accuracy: 94.57%
66	Validation loss: 6858.642578	Best loss: 0.227151	Accuracy: 95.23%
67	Validation loss: 12176.087891	Best loss: 0.227151	Accuracy: 92.77%
68	Validation loss: 7227.361328	Best loss: 0.227151	Accuracy: 95.27%
69	Validation loss: 8690.676758	Best loss: 0.227151	Accuracy: 93.67%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  15.2s
[CV] n_neurons=100, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 1238.200195	Best loss: 1238.200195	Accuracy: 42.81%
1	Validation loss: 108.376930	Best loss: 108.376930	Accuracy: 82.92%
2	Validation loss: 13.069304	Best loss: 13.069304	Accuracy: 93.35%
3	Validation loss: 3.060661	Best loss: 3.060661	Accuracy: 95.04%
4	Validation loss: 3.864383	Best loss: 3.060661	Accuracy: 94.02%
5	Validation loss: 2.406933	Best loss: 2.406933	Accuracy: 95.11%
6	Validation loss: 1.628515	Best loss: 1.628515	Accuracy: 95.50%
7	Validation loss: 2.150198	Best loss: 1.628515	Accuracy: 94.61%
8	Validation loss: 2.062745	Best loss: 1.628515	Accuracy: 94.10%
9	Validation loss: 1.028423	Best loss: 1.028423	Accuracy: 95.93%
10	Validation loss: 0.863015	Best loss: 0.863015	Accuracy: 96.25%
11	Validation loss: 2.793723	Best loss: 0.863015	Accuracy: 93.12%
12	Validation loss: 1.900060	Best loss: 0.863015	Accuracy: 95.70%
13	Validation loss: 2.305579	Best loss: 0.863015	Accuracy: 92.73%
14	Validation loss: 1.293545	Best loss: 0.863015	Accuracy: 95.62%
15	Validation loss: 1.040533	Best loss: 0.863015	Accuracy: 95.78%
16	Validation loss: 0.677321	Best loss: 0.677321	Accuracy: 96.83%
17	Validation loss: 1.381776	Best loss: 0.677321	Accuracy: 93.00%
18	Validation loss: 1.893996	Best loss: 0.677321	Accuracy: 95.97%
19	Validation loss: 1.034049	Best loss: 0.677321	Accuracy: 95.78%
20	Validation loss: 0.653358	Best loss: 0.653358	Accuracy: 96.72%
21	Validation loss: 0.486237	Best loss: 0.486237	Accuracy: 97.15%
22	Validation loss: 0.611698	Best loss: 0.486237	Accuracy: 96.68%
23	Validation loss: 0.508406	Best loss: 0.486237	Accuracy: 96.52%
24	Validation loss: 0.398525	Best loss: 0.398525	Accuracy: 97.30%
25	Validation loss: 0.431143	Best loss: 0.398525	Accuracy: 96.91%
26	Validation loss: 0.372289	Best loss: 0.372289	Accuracy: 96.99%
27	Validation loss: 0.359618	Best loss: 0.359618	Accuracy: 97.46%
28	Validation loss: 0.357724	Best loss: 0.357724	Accuracy: 97.38%
29	Validation loss: 0.393064	Best loss: 0.357724	Accuracy: 96.17%
30	Validation loss: 0.443318	Best loss: 0.357724	Accuracy: 96.79%
31	Validation loss: 0.516616	Best loss: 0.357724	Accuracy: 97.07%
32	Validation loss: 0.433500	Best loss: 0.357724	Accuracy: 97.22%
33	Validation loss: 0.462869	Best loss: 0.357724	Accuracy: 96.95%
34	Validation loss: 0.431953	Best loss: 0.357724	Accuracy: 97.15%
35	Validation loss: 0.474026	Best loss: 0.357724	Accuracy: 96.40%
36	Validation loss: 0.398510	Best loss: 0.357724	Accuracy: 97.03%
37	Validation loss: 0.401428	Best loss: 0.357724	Accuracy: 97.07%
38	Validation loss: 0.626622	Best loss: 0.357724	Accuracy: 95.11%
39	Validation loss: 0.538155	Best loss: 0.357724	Accuracy: 97.15%
40	Validation loss: 0.372156	Best loss: 0.357724	Accuracy: 97.26%
41	Validation loss: 0.384950	Best loss: 0.357724	Accuracy: 97.15%
42	Validation loss: 0.368423	Best loss: 0.357724	Accuracy: 97.77%
43	Validation loss: 0.385502	Best loss: 0.357724	Accuracy: 97.34%
44	Validation loss: 0.346143	Best loss: 0.346143	Accuracy: 97.50%
45	Validation loss: 0.399749	Best loss: 0.346143	Accuracy: 97.03%
46	Validation loss: 0.424005	Best loss: 0.346143	Accuracy: 97.11%
47	Validation loss: 0.507458	Best loss: 0.346143	Accuracy: 96.40%
48	Validation loss: 1.333148	Best loss: 0.346143	Accuracy: 94.21%
49	Validation loss: 0.688419	Best loss: 0.346143	Accuracy: 97.34%
50	Validation loss: 0.535647	Best loss: 0.346143	Accuracy: 97.03%
51	Validation loss: 0.450994	Best loss: 0.346143	Accuracy: 96.99%
52	Validation loss: 0.613125	Best loss: 0.346143	Accuracy: 97.19%
53	Validation loss: 0.505352	Best loss: 0.346143	Accuracy: 97.42%
54	Validation loss: 0.544522	Best loss: 0.346143	Accuracy: 96.48%
55	Validation loss: 0.558400	Best loss: 0.346143	Accuracy: 97.46%
56	Validation loss: 0.684337	Best loss: 0.346143	Accuracy: 97.26%
57	Validation loss: 1.173833	Best loss: 0.346143	Accuracy: 97.50%
58	Validation loss: 0.808644	Best loss: 0.346143	Accuracy: 95.11%
59	Validation loss: 0.579853	Best loss: 0.346143	Accuracy: 97.15%
60	Validation loss: 0.451219	Best loss: 0.346143	Accuracy: 96.95%
61	Validation loss: 0.504243	Best loss: 0.346143	Accuracy: 97.73%
62	Validation loss: 0.854212	Best loss: 0.346143	Accuracy: 97.85%
63	Validation loss: 0.444477	Best loss: 0.346143	Accuracy: 97.81%
64	Validation loss: 0.509757	Best loss: 0.346143	Accuracy: 97.65%
65	Validation loss: 0.385785	Best loss: 0.346143	Accuracy: 97.38%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  15.6s
[CV] n_neurons=100, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 162.902328	Best loss: 162.902328	Accuracy: 61.65%
1	Validation loss: 7.931857	Best loss: 7.931857	Accuracy: 86.40%
2	Validation loss: 1.588187	Best loss: 1.588187	Accuracy: 91.20%
3	Validation loss: 1.823591	Best loss: 1.588187	Accuracy: 90.85%
4	Validation loss: 1.059242	Best loss: 1.059242	Accuracy: 92.65%
5	Validation loss: 0.970608	Best loss: 0.970608	Accuracy: 91.83%
6	Validation loss: 0.965453	Best loss: 0.965453	Accuracy: 95.15%
7	Validation loss: 0.561417	Best loss: 0.561417	Accuracy: 95.27%
8	Validation loss: 0.466333	Best loss: 0.466333	Accuracy: 94.76%
9	Validation loss: 0.652940	Best loss: 0.466333	Accuracy: 93.63%
10	Validation loss: 0.484361	Best loss: 0.466333	Accuracy: 94.57%
11	Validation loss: 0.460164	Best loss: 0.460164	Accuracy: 96.05%
12	Validation loss: 0.377755	Best loss: 0.377755	Accuracy: 95.74%
13	Validation loss: 0.631626	Best loss: 0.377755	Accuracy: 90.30%
14	Validation loss: 0.454498	Best loss: 0.377755	Accuracy: 95.54%
15	Validation loss: 0.338255	Best loss: 0.338255	Accuracy: 94.96%
16	Validation loss: 0.735941	Best loss: 0.338255	Accuracy: 94.57%
17	Validation loss: 0.395013	Best loss: 0.338255	Accuracy: 95.90%
18	Validation loss: 0.311957	Best loss: 0.311957	Accuracy: 96.56%
19	Validation loss: 0.274621	Best loss: 0.274621	Accuracy: 96.09%
20	Validation loss: 0.254359	Best loss: 0.254359	Accuracy: 96.40%
21	Validation loss: 0.372483	Best loss: 0.254359	Accuracy: 95.74%
22	Validation loss: 0.261715	Best loss: 0.254359	Accuracy: 97.26%
23	Validation loss: 0.350603	Best loss: 0.254359	Accuracy: 95.66%
24	Validation loss: 0.334500	Best loss: 0.254359	Accuracy: 96.52%
25	Validation loss: 0.300465	Best loss: 0.254359	Accuracy: 95.90%
26	Validation loss: 0.351924	Best loss: 0.254359	Accuracy: 96.09%
27	Validation loss: 0.248664	Best loss: 0.248664	Accuracy: 96.25%
28	Validation loss: 0.219868	Best loss: 0.219868	Accuracy: 96.79%
29	Validation loss: 0.178581	Best loss: 0.178581	Accuracy: 97.11%
30	Validation loss: 0.207146	Best loss: 0.178581	Accuracy: 96.91%
31	Validation loss: 0.233154	Best loss: 0.178581	Accuracy: 96.21%
32	Validation loss: 0.208904	Best loss: 0.178581	Accuracy: 97.03%
33	Validation loss: 0.194841	Best loss: 0.178581	Accuracy: 97.19%
34	Validation loss: 0.224382	Best loss: 0.178581	Accuracy: 97.38%
35	Validation loss: 0.203317	Best loss: 0.178581	Accuracy: 97.15%
36	Validation loss: 0.312459	Best loss: 0.178581	Accuracy: 94.88%
37	Validation loss: 0.249969	Best loss: 0.178581	Accuracy: 96.01%
38	Validation loss: 0.332804	Best loss: 0.178581	Accuracy: 95.70%
39	Validation loss: 0.261208	Best loss: 0.178581	Accuracy: 97.11%
40	Validation loss: 0.256882	Best loss: 0.178581	Accuracy: 96.79%
41	Validation loss: 0.177040	Best loss: 0.177040	Accuracy: 97.26%
42	Validation loss: 0.174325	Best loss: 0.174325	Accuracy: 97.15%
43	Validation loss: 0.172332	Best loss: 0.172332	Accuracy: 97.11%
44	Validation loss: 0.191950	Best loss: 0.172332	Accuracy: 96.60%
45	Validation loss: 0.174555	Best loss: 0.172332	Accuracy: 97.54%
46	Validation loss: 0.202407	Best loss: 0.172332	Accuracy: 96.99%
47	Validation loss: 0.168811	Best loss: 0.168811	Accuracy: 97.50%
48	Validation loss: 0.195559	Best loss: 0.168811	Accuracy: 97.11%
49	Validation loss: 0.226280	Best loss: 0.168811	Accuracy: 96.72%
50	Validation loss: 0.187029	Best loss: 0.168811	Accuracy: 97.19%
51	Validation loss: 0.160212	Best loss: 0.160212	Accuracy: 97.19%
52	Validation loss: 0.182766	Best loss: 0.160212	Accuracy: 96.68%
53	Validation loss: 0.274176	Best loss: 0.160212	Accuracy: 96.36%
54	Validation loss: 0.248144	Best loss: 0.160212	Accuracy: 97.03%
55	Validation loss: 0.251340	Best loss: 0.160212	Accuracy: 96.56%
56	Validation loss: 1459112.375000	Best loss: 0.160212	Accuracy: 18.73%
57	Validation loss: 3401717.500000	Best loss: 0.160212	Accuracy: 61.18%
58	Validation loss: 264056.593750	Best loss: 0.160212	Accuracy: 85.81%
59	Validation loss: 114351.664062	Best loss: 0.160212	Accuracy: 89.33%
60	Validation loss: 166142.796875	Best loss: 0.160212	Accuracy: 89.05%
61	Validation loss: 103309.398438	Best loss: 0.160212	Accuracy: 92.03%
62	Validation loss: 67763.210938	Best loss: 0.160212	Accuracy: 92.57%
63	Validation loss: 81996.531250	Best loss: 0.160212	Accuracy: 90.85%
64	Validation loss: 106419.242188	Best loss: 0.160212	Accuracy: 91.32%
65	Validation loss: 53604.949219	Best loss: 0.160212	Accuracy: 93.63%
66	Validation loss: 49006.929688	Best loss: 0.160212	Accuracy: 92.03%
67	Validation loss: 36375.054688	Best loss: 0.160212	Accuracy: 94.18%
68	Validation loss: 36424.828125	Best loss: 0.160212	Accuracy: 93.67%
69	Validation loss: 23629.306641	Best loss: 0.160212	Accuracy: 95.15%
70	Validation loss: 38855.390625	Best loss: 0.160212	Accuracy: 91.32%
71	Validation loss: 32658.382812	Best loss: 0.160212	Accuracy: 92.61%
72	Validation loss: 27937.175781	Best loss: 0.160212	Accuracy: 91.32%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  15.9s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.106335	Best loss: 0.106335	Accuracy: 96.76%
1	Validation loss: 0.062812	Best loss: 0.062812	Accuracy: 98.28%
2	Validation loss: 0.063751	Best loss: 0.062812	Accuracy: 98.01%
3	Validation loss: 0.051145	Best loss: 0.051145	Accuracy: 98.28%
4	Validation loss: 0.055745	Best loss: 0.051145	Accuracy: 98.32%
5	Validation loss: 0.057626	Best loss: 0.051145	Accuracy: 98.67%
6	Validation loss: 0.047655	Best loss: 0.047655	Accuracy: 98.83%
7	Validation loss: 0.070040	Best loss: 0.047655	Accuracy: 98.20%
8	Validation loss: 0.053620	Best loss: 0.047655	Accuracy: 98.40%
9	Validation loss: 0.067154	Best loss: 0.047655	Accuracy: 98.71%
10	Validation loss: 0.075971	Best loss: 0.047655	Accuracy: 98.59%
11	Validation loss: 0.058257	Best loss: 0.047655	Accuracy: 98.63%
12	Validation loss: 0.041674	Best loss: 0.041674	Accuracy: 98.87%
13	Validation loss: 0.055147	Best loss: 0.041674	Accuracy: 98.94%
14	Validation loss: 0.070983	Best loss: 0.041674	Accuracy: 98.67%
15	Validation loss: 0.063671	Best loss: 0.041674	Accuracy: 98.94%
16	Validation loss: 0.053731	Best loss: 0.041674	Accuracy: 98.87%
17	Validation loss: 0.098870	Best loss: 0.041674	Accuracy: 98.28%
18	Validation loss: 0.066281	Best loss: 0.041674	Accuracy: 98.71%
19	Validation loss: 0.049702	Best loss: 0.041674	Accuracy: 98.94%
20	Validation loss: 0.125261	Best loss: 0.041674	Accuracy: 98.63%
21	Validation loss: 0.094367	Best loss: 0.041674	Accuracy: 98.79%
22	Validation loss: 0.037052	Best loss: 0.037052	Accuracy: 99.02%
23	Validation loss: 0.060770	Best loss: 0.037052	Accuracy: 98.91%
24	Validation loss: 0.098285	Best loss: 0.037052	Accuracy: 97.97%
25	Validation loss: 0.088461	Best loss: 0.037052	Accuracy: 98.87%
26	Validation loss: 0.076213	Best loss: 0.037052	Accuracy: 98.87%
27	Validation loss: 0.179080	Best loss: 0.037052	Accuracy: 95.78%
28	Validation loss: 6.170707	Best loss: 0.037052	Accuracy: 94.64%
29	Validation loss: 1.052260	Best loss: 0.037052	Accuracy: 97.19%
30	Validation loss: 0.416062	Best loss: 0.037052	Accuracy: 97.62%
31	Validation loss: 0.382061	Best loss: 0.037052	Accuracy: 96.40%
32	Validation loss: 0.279429	Best loss: 0.037052	Accuracy: 97.30%
33	Validation loss: 0.256048	Best loss: 0.037052	Accuracy: 97.69%
34	Validation loss: 0.236565	Best loss: 0.037052	Accuracy: 97.50%
35	Validation loss: 0.204096	Best loss: 0.037052	Accuracy: 97.69%
36	Validation loss: 0.228857	Best loss: 0.037052	Accuracy: 97.81%
37	Validation loss: 0.216513	Best loss: 0.037052	Accuracy: 97.62%
38	Validation loss: 0.193183	Best loss: 0.037052	Accuracy: 97.93%
39	Validation loss: 0.194565	Best loss: 0.037052	Accuracy: 97.89%
40	Validation loss: 0.176766	Best loss: 0.037052	Accuracy: 98.05%
41	Validation loss: 0.174136	Best loss: 0.037052	Accuracy: 98.08%
42	Validation loss: 0.176465	Best loss: 0.037052	Accuracy: 98.05%
43	Validation loss: 0.208952	Best loss: 0.037052	Accuracy: 97.69%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  13.5s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.085892	Best loss: 0.085892	Accuracy: 97.50%
1	Validation loss: 0.056002	Best loss: 0.056002	Accuracy: 98.24%
2	Validation loss: 0.048823	Best loss: 0.048823	Accuracy: 98.55%
3	Validation loss: 0.057721	Best loss: 0.048823	Accuracy: 98.36%
4	Validation loss: 0.056111	Best loss: 0.048823	Accuracy: 98.36%
5	Validation loss: 0.065189	Best loss: 0.048823	Accuracy: 98.48%
6	Validation loss: 0.067335	Best loss: 0.048823	Accuracy: 98.28%
7	Validation loss: 0.057885	Best loss: 0.048823	Accuracy: 98.55%
8	Validation loss: 0.053104	Best loss: 0.048823	Accuracy: 98.32%
9	Validation loss: 0.066991	Best loss: 0.048823	Accuracy: 98.55%
10	Validation loss: 0.064325	Best loss: 0.048823	Accuracy: 98.48%
11	Validation loss: 0.073082	Best loss: 0.048823	Accuracy: 98.94%
12	Validation loss: 0.051473	Best loss: 0.048823	Accuracy: 98.71%
13	Validation loss: 0.064067	Best loss: 0.048823	Accuracy: 98.63%
14	Validation loss: 0.081029	Best loss: 0.048823	Accuracy: 98.79%
15	Validation loss: 0.070137	Best loss: 0.048823	Accuracy: 98.71%
16	Validation loss: 0.052165	Best loss: 0.048823	Accuracy: 98.83%
17	Validation loss: 0.053578	Best loss: 0.048823	Accuracy: 98.94%
18	Validation loss: 0.068871	Best loss: 0.048823	Accuracy: 99.14%
19	Validation loss: 0.080274	Best loss: 0.048823	Accuracy: 99.10%
20	Validation loss: 0.109767	Best loss: 0.048823	Accuracy: 98.55%
21	Validation loss: 0.084188	Best loss: 0.048823	Accuracy: 98.48%
22	Validation loss: 0.056215	Best loss: 0.048823	Accuracy: 98.59%
23	Validation loss: 0.081617	Best loss: 0.048823	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=   8.5s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 0.082524	Best loss: 0.082524	Accuracy: 97.50%
1	Validation loss: 0.058590	Best loss: 0.058590	Accuracy: 98.32%
2	Validation loss: 0.058883	Best loss: 0.058590	Accuracy: 98.12%
3	Validation loss: 0.046575	Best loss: 0.046575	Accuracy: 98.63%
4	Validation loss: 0.045565	Best loss: 0.045565	Accuracy: 98.63%
5	Validation loss: 0.065401	Best loss: 0.045565	Accuracy: 98.63%
6	Validation loss: 0.064592	Best loss: 0.045565	Accuracy: 98.20%
7	Validation loss: 0.048024	Best loss: 0.045565	Accuracy: 98.51%
8	Validation loss: 0.068900	Best loss: 0.045565	Accuracy: 98.71%
9	Validation loss: 0.066030	Best loss: 0.045565	Accuracy: 98.51%
10	Validation loss: 0.065232	Best loss: 0.045565	Accuracy: 98.55%
11	Validation loss: 0.065415	Best loss: 0.045565	Accuracy: 98.55%
12	Validation loss: 0.068447	Best loss: 0.045565	Accuracy: 98.32%
13	Validation loss: 0.054511	Best loss: 0.045565	Accuracy: 98.83%
14	Validation loss: 0.075187	Best loss: 0.045565	Accuracy: 98.44%
15	Validation loss: 0.088081	Best loss: 0.045565	Accuracy: 98.28%
16	Validation loss: 0.072729	Best loss: 0.045565	Accuracy: 98.83%
17	Validation loss: 0.070681	Best loss: 0.045565	Accuracy: 98.75%
18	Validation loss: 0.051958	Best loss: 0.045565	Accuracy: 98.98%
19	Validation loss: 0.085098	Best loss: 0.045565	Accuracy: 98.71%
20	Validation loss: 0.070974	Best loss: 0.045565	Accuracy: 98.91%
21	Validation loss: 0.104365	Best loss: 0.045565	Accuracy: 98.44%
22	Validation loss: 0.075328	Best loss: 0.045565	Accuracy: 98.98%
23	Validation loss: 0.042029	Best loss: 0.042029	Accuracy: 99.06%
24	Validation loss: 0.052558	Best loss: 0.042029	Accuracy: 99.02%
25	Validation loss: 0.057282	Best loss: 0.042029	Accuracy: 99.06%
26	Validation loss: 0.070242	Best loss: 0.042029	Accuracy: 98.91%
27	Validation loss: 0.043481	Best loss: 0.042029	Accuracy: 99.06%
28	Validation loss: 0.045982	Best loss: 0.042029	Accuracy: 99.10%
29	Validation loss: 0.053918	Best loss: 0.042029	Accuracy: 99.10%
30	Validation loss: 0.069793	Best loss: 0.042029	Accuracy: 99.02%
31	Validation loss: 0.063908	Best loss: 0.042029	Accuracy: 98.87%
32	Validation loss: 0.129079	Best loss: 0.042029	Accuracy: 97.81%
33	Validation loss: 0.072238	Best loss: 0.042029	Accuracy: 98.91%
34	Validation loss: 0.140533	Best loss: 0.042029	Accuracy: 97.77%
35	Validation loss: 0.083059	Best loss: 0.042029	Accuracy: 98.20%
36	Validation loss: 116.607712	Best loss: 0.042029	Accuracy: 85.77%
37	Validation loss: 4.685955	Best loss: 0.042029	Accuracy: 95.04%
38	Validation loss: 1.474712	Best loss: 0.042029	Accuracy: 96.76%
39	Validation loss: 0.660338	Best loss: 0.042029	Accuracy: 97.50%
40	Validation loss: 0.569672	Best loss: 0.042029	Accuracy: 97.46%
41	Validation loss: 0.450574	Best loss: 0.042029	Accuracy: 97.58%
42	Validation loss: 0.444840	Best loss: 0.042029	Accuracy: 97.62%
43	Validation loss: 0.372406	Best loss: 0.042029	Accuracy: 97.42%
44	Validation loss: 0.342103	Best loss: 0.042029	Accuracy: 97.85%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  13.3s
[CV] n_neurons=30, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.832502	Best loss: 1.832502	Accuracy: 18.73%
1	Validation loss: 1.636965	Best loss: 1.636965	Accuracy: 20.91%
2	Validation loss: 1.742296	Best loss: 1.636965	Accuracy: 18.73%
3	Validation loss: 1.667329	Best loss: 1.636965	Accuracy: 19.27%
4	Validation loss: 1.640375	Best loss: 1.636965	Accuracy: 22.01%
5	Validation loss: 1.907959	Best loss: 1.636965	Accuracy: 19.08%
6	Validation loss: 1.861143	Best loss: 1.636965	Accuracy: 22.01%
7	Validation loss: 1.646458	Best loss: 1.636965	Accuracy: 19.27%
8	Validation loss: 1.682899	Best loss: 1.636965	Accuracy: 22.01%
9	Validation loss: 1.800023	Best loss: 1.636965	Accuracy: 18.73%
10	Validation loss: 1.787576	Best loss: 1.636965	Accuracy: 22.01%
11	Validation loss: 1.982575	Best loss: 1.636965	Accuracy: 22.01%
12	Validation loss: 1.943093	Best loss: 1.636965	Accuracy: 20.91%
13	Validation loss: 1.777600	Best loss: 1.636965	Accuracy: 19.27%
14	Validation loss: 1.791734	Best loss: 1.636965	Accuracy: 19.08%
15	Validation loss: 1.913678	Best loss: 1.636965	Accuracy: 22.01%
16	Validation loss: 1.850441	Best loss: 1.636965	Accuracy: 19.08%
17	Validation loss: 1.676425	Best loss: 1.636965	Accuracy: 20.91%
18	Validation loss: 1.803297	Best loss: 1.636965	Accuracy: 22.01%
19	Validation loss: 1.727336	Best loss: 1.636965	Accuracy: 22.01%
20	Validation loss: 1.709795	Best loss: 1.636965	Accuracy: 18.73%
21	Validation loss: 1.744466	Best loss: 1.636965	Accuracy: 18.73%
22	Validation loss: 1.651262	Best loss: 1.636965	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  30.1s
[CV] n_neurons=30, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.827315	Best loss: 1.827315	Accuracy: 18.73%
1	Validation loss: 1.652240	Best loss: 1.652240	Accuracy: 19.08%
2	Validation loss: 1.791734	Best loss: 1.652240	Accuracy: 19.08%
3	Validation loss: 1.838193	Best loss: 1.652240	Accuracy: 19.27%
4	Validation loss: 1.800224	Best loss: 1.652240	Accuracy: 19.08%
5	Validation loss: 1.669686	Best loss: 1.652240	Accuracy: 18.73%
6	Validation loss: 1.851125	Best loss: 1.652240	Accuracy: 19.08%
7	Validation loss: 1.649280	Best loss: 1.649280	Accuracy: 19.27%
8	Validation loss: 1.899554	Best loss: 1.649280	Accuracy: 20.91%
9	Validation loss: 1.914810	Best loss: 1.649280	Accuracy: 19.27%
10	Validation loss: 1.686125	Best loss: 1.649280	Accuracy: 22.01%
11	Validation loss: 1.647475	Best loss: 1.647475	Accuracy: 20.91%
12	Validation loss: 1.853961	Best loss: 1.647475	Accuracy: 20.91%
13	Validation loss: 1.669569	Best loss: 1.647475	Accuracy: 22.01%
14	Validation loss: 1.703600	Best loss: 1.647475	Accuracy: 19.27%
15	Validation loss: 1.998328	Best loss: 1.647475	Accuracy: 19.08%
16	Validation loss: 1.656301	Best loss: 1.647475	Accuracy: 20.91%
17	Validation loss: 1.947685	Best loss: 1.647475	Accuracy: 20.91%
18	Validation loss: 2.143587	Best loss: 1.647475	Accuracy: 20.91%
19	Validation loss: 1.900221	Best loss: 1.647475	Accuracy: 20.91%
20	Validation loss: 1.930351	Best loss: 1.647475	Accuracy: 18.73%
21	Validation loss: 1.900987	Best loss: 1.647475	Accuracy: 22.01%
22	Validation loss: 1.750339	Best loss: 1.647475	Accuracy: 19.27%
23	Validation loss: 2.205178	Best loss: 1.647475	Accuracy: 18.73%
24	Validation loss: 1.718392	Best loss: 1.647475	Accuracy: 22.01%
25	Validation loss: 1.836214	Best loss: 1.647475	Accuracy: 22.01%
26	Validation loss: 1.777826	Best loss: 1.647475	Accuracy: 22.01%
27	Validation loss: 1.763802	Best loss: 1.647475	Accuracy: 19.27%
28	Validation loss: 1.860310	Best loss: 1.647475	Accuracy: 19.27%
29	Validation loss: 1.954945	Best loss: 1.647475	Accuracy: 18.73%
30	Validation loss: 1.838278	Best loss: 1.647475	Accuracy: 18.73%
31	Validation loss: 1.988403	Best loss: 1.647475	Accuracy: 22.01%
32	Validation loss: 2.001386	Best loss: 1.647475	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  42.9s
[CV] n_neurons=30, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.653493	Best loss: 1.653493	Accuracy: 19.31%
1	Validation loss: 1.857467	Best loss: 1.653493	Accuracy: 19.12%
2	Validation loss: 1.731448	Best loss: 1.653493	Accuracy: 18.73%
3	Validation loss: 1.989829	Best loss: 1.653493	Accuracy: 19.12%
4	Validation loss: 2.139915	Best loss: 1.653493	Accuracy: 19.12%
5	Validation loss: 1.856261	Best loss: 1.653493	Accuracy: 19.27%
6	Validation loss: 1.663324	Best loss: 1.653493	Accuracy: 19.08%
7	Validation loss: 1.695256	Best loss: 1.653493	Accuracy: 22.01%
8	Validation loss: 1.799713	Best loss: 1.653493	Accuracy: 22.01%
9	Validation loss: 2.142704	Best loss: 1.653493	Accuracy: 22.01%
10	Validation loss: 1.702230	Best loss: 1.653493	Accuracy: 22.01%
11	Validation loss: 1.685415	Best loss: 1.653493	Accuracy: 18.73%
12	Validation loss: 1.739547	Best loss: 1.653493	Accuracy: 20.91%
13	Validation loss: 1.695205	Best loss: 1.653493	Accuracy: 19.08%
14	Validation loss: 1.795816	Best loss: 1.653493	Accuracy: 22.01%
15	Validation loss: 1.878645	Best loss: 1.653493	Accuracy: 20.91%
16	Validation loss: 1.704602	Best loss: 1.653493	Accuracy: 18.73%
17	Validation loss: 1.803620	Best loss: 1.653493	Accuracy: 19.08%
18	Validation loss: 2.113106	Best loss: 1.653493	Accuracy: 19.08%
19	Validation loss: 1.628994	Best loss: 1.628994	Accuracy: 19.27%
20	Validation loss: 1.801367	Best loss: 1.628994	Accuracy: 19.27%
21	Validation loss: 1.647704	Best loss: 1.628994	Accuracy: 22.01%
22	Validation loss: 1.653075	Best loss: 1.628994	Accuracy: 20.91%
23	Validation loss: 1.739613	Best loss: 1.628994	Accuracy: 22.01%
24	Validation loss: 1.861974	Best loss: 1.628994	Accuracy: 22.01%
25	Validation loss: 1.744549	Best loss: 1.628994	Accuracy: 19.27%
26	Validation loss: 1.719069	Best loss: 1.628994	Accuracy: 19.08%
27	Validation loss: 1.685106	Best loss: 1.628994	Accuracy: 22.01%
28	Validation loss: 2.323920	Best loss: 1.628994	Accuracy: 19.08%
29	Validation loss: 2.050604	Best loss: 1.628994	Accuracy: 18.73%
30	Validation loss: 1.854669	Best loss: 1.628994	Accuracy: 19.27%
31	Validation loss: 1.956270	Best loss: 1.628994	Accuracy: 18.73%
32	Validation loss: 1.666341	Best loss: 1.628994	Accuracy: 19.08%
33	Validation loss: 1.684591	Best loss: 1.628994	Accuracy: 20.91%
34	Validation loss: 1.747720	Best loss: 1.628994	Accuracy: 19.08%
35	Validation loss: 1.761387	Best loss: 1.628994	Accuracy: 22.01%
36	Validation loss: 1.971676	Best loss: 1.628994	Accuracy: 22.01%
37	Validation loss: 1.921873	Best loss: 1.628994	Accuracy: 22.01%
38	Validation loss: 1.989988	Best loss: 1.628994	Accuracy: 18.73%
39	Validation loss: 1.904658	Best loss: 1.628994	Accuracy: 19.08%
40	Validation loss: 1.730791	Best loss: 1.628994	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  52.9s
[CV] n_neurons=100, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.376774	Best loss: 0.376774	Accuracy: 88.86%
1	Validation loss: 61.936985	Best loss: 0.376774	Accuracy: 32.17%
2	Validation loss: 4.024376	Best loss: 0.376774	Accuracy: 54.50%
3	Validation loss: 2.327265	Best loss: 0.376774	Accuracy: 59.70%
4	Validation loss: 1.744864	Best loss: 0.376774	Accuracy: 65.83%
5	Validation loss: 1.427517	Best loss: 0.376774	Accuracy: 67.08%
6	Validation loss: 1.812712	Best loss: 0.376774	Accuracy: 65.95%
7	Validation loss: 1.459436	Best loss: 0.376774	Accuracy: 64.97%
8	Validation loss: 1.092585	Best loss: 0.376774	Accuracy: 74.98%
9	Validation loss: 1.248168	Best loss: 0.376774	Accuracy: 71.38%
10	Validation loss: 6531.232422	Best loss: 0.376774	Accuracy: 25.02%
11	Validation loss: 98.203644	Best loss: 0.376774	Accuracy: 64.07%
12	Validation loss: 50.884308	Best loss: 0.376774	Accuracy: 43.28%
13	Validation loss: 41.159859	Best loss: 0.376774	Accuracy: 70.48%
14	Validation loss: 13.319558	Best loss: 0.376774	Accuracy: 68.49%
15	Validation loss: 11.672280	Best loss: 0.376774	Accuracy: 63.64%
16	Validation loss: 44.753693	Best loss: 0.376774	Accuracy: 75.53%
17	Validation loss: 26.309696	Best loss: 0.376774	Accuracy: 80.30%
18	Validation loss: 15.637346	Best loss: 0.376774	Accuracy: 81.08%
19	Validation loss: 18.212481	Best loss: 0.376774	Accuracy: 81.90%
20	Validation loss: 9.285827	Best loss: 0.376774	Accuracy: 86.59%
21	Validation loss: 23.328739	Best loss: 0.376774	Accuracy: 76.70%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   9.5s
[CV] n_neurons=100, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.172807	Best loss: 0.172807	Accuracy: 95.86%
1	Validation loss: 0.118179	Best loss: 0.118179	Accuracy: 96.52%
2	Validation loss: 69164.750000	Best loss: 0.118179	Accuracy: 18.96%
3	Validation loss: 13.360556	Best loss: 0.118179	Accuracy: 32.33%
4	Validation loss: 7.407450	Best loss: 0.118179	Accuracy: 42.49%
5	Validation loss: 3.696779	Best loss: 0.118179	Accuracy: 62.71%
6	Validation loss: 16.301590	Best loss: 0.118179	Accuracy: 48.24%
7	Validation loss: 2.489043	Best loss: 0.118179	Accuracy: 60.91%
8	Validation loss: 1.303954	Best loss: 0.118179	Accuracy: 75.22%
9	Validation loss: 1.968811	Best loss: 0.118179	Accuracy: 72.44%
10	Validation loss: 1.256041	Best loss: 0.118179	Accuracy: 70.60%
11	Validation loss: 93.286316	Best loss: 0.118179	Accuracy: 30.06%
12	Validation loss: 31.971172	Best loss: 0.118179	Accuracy: 22.44%
13	Validation loss: 2.151331	Best loss: 0.118179	Accuracy: 54.53%
14	Validation loss: 1.840086	Best loss: 0.118179	Accuracy: 58.99%
15	Validation loss: 1.403434	Best loss: 0.118179	Accuracy: 62.08%
16	Validation loss: 1.631724	Best loss: 0.118179	Accuracy: 60.83%
17	Validation loss: 1.202060	Best loss: 0.118179	Accuracy: 65.01%
18	Validation loss: 0.841601	Best loss: 0.118179	Accuracy: 73.89%
19	Validation loss: 0.885089	Best loss: 0.118179	Accuracy: 72.83%
20	Validation loss: 0.797698	Best loss: 0.118179	Accuracy: 75.53%
21	Validation loss: 9.823364	Best loss: 0.118179	Accuracy: 44.68%
22	Validation loss: 0.768243	Best loss: 0.118179	Accuracy: 80.92%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=   9.9s
[CV] n_neurons=100, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.712843	Best loss: 0.712843	Accuracy: 74.51%
1	Validation loss: 1.706727	Best loss: 0.712843	Accuracy: 95.35%
2	Validation loss: 4.992863	Best loss: 0.712843	Accuracy: 71.42%
3	Validation loss: 1.186182	Best loss: 0.712843	Accuracy: 86.43%
4	Validation loss: 0.606169	Best loss: 0.606169	Accuracy: 88.66%
5	Validation loss: 0.661237	Best loss: 0.606169	Accuracy: 83.23%
6	Validation loss: 0.389243	Best loss: 0.389243	Accuracy: 90.19%
7	Validation loss: 0.311324	Best loss: 0.311324	Accuracy: 92.96%
8	Validation loss: 0.339031	Best loss: 0.311324	Accuracy: 90.73%
9	Validation loss: 61.389824	Best loss: 0.311324	Accuracy: 53.13%
10	Validation loss: 407.379730	Best loss: 0.311324	Accuracy: 62.55%
11	Validation loss: 14.514718	Best loss: 0.311324	Accuracy: 62.04%
12	Validation loss: 1.904386	Best loss: 0.311324	Accuracy: 87.61%
13	Validation loss: 1.434482	Best loss: 0.311324	Accuracy: 88.90%
14	Validation loss: 4.542013	Best loss: 0.311324	Accuracy: 84.36%
15	Validation loss: 1.064976	Best loss: 0.311324	Accuracy: 90.27%
16	Validation loss: 1.122151	Best loss: 0.311324	Accuracy: 88.08%
17	Validation loss: 1.275843	Best loss: 0.311324	Accuracy: 89.52%
18	Validation loss: 0.712639	Best loss: 0.311324	Accuracy: 92.57%
19	Validation loss: 1.547141	Best loss: 0.311324	Accuracy: 91.99%
20	Validation loss: 1.341160	Best loss: 0.311324	Accuracy: 87.84%
21	Validation loss: 0.762376	Best loss: 0.311324	Accuracy: 92.61%
22	Validation loss: 0.853752	Best loss: 0.311324	Accuracy: 90.38%
23	Validation loss: 0.614156	Best loss: 0.311324	Accuracy: 91.32%
24	Validation loss: 0.518054	Best loss: 0.311324	Accuracy: 91.13%
25	Validation loss: 946.730957	Best loss: 0.311324	Accuracy: 27.91%
26	Validation loss: 298.105225	Best loss: 0.311324	Accuracy: 71.34%
27	Validation loss: 343.398956	Best loss: 0.311324	Accuracy: 72.79%
28	Validation loss: 438.144257	Best loss: 0.311324	Accuracy: 74.28%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  12.4s
[CV] n_neurons=100, learning_rate=0.02, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.226237	Best loss: 0.226237	Accuracy: 93.63%
1	Validation loss: 0.134569	Best loss: 0.134569	Accuracy: 96.17%
2	Validation loss: 0.090633	Best loss: 0.090633	Accuracy: 97.73%
3	Validation loss: 0.085628	Best loss: 0.085628	Accuracy: 97.58%
4	Validation loss: 9.045177	Best loss: 0.085628	Accuracy: 93.51%
5	Validation loss: 1.840681	Best loss: 0.085628	Accuracy: 92.77%
6	Validation loss: 2.328829	Best loss: 0.085628	Accuracy: 95.54%
7	Validation loss: 1.667652	Best loss: 0.085628	Accuracy: 94.72%
8	Validation loss: 1.007808	Best loss: 0.085628	Accuracy: 96.21%
9	Validation loss: 0.596551	Best loss: 0.085628	Accuracy: 95.54%
10	Validation loss: 0.554296	Best loss: 0.085628	Accuracy: 95.93%
11	Validation loss: 0.788677	Best loss: 0.085628	Accuracy: 95.43%
12	Validation loss: 0.363923	Best loss: 0.085628	Accuracy: 95.93%
13	Validation loss: 0.660710	Best loss: 0.085628	Accuracy: 96.52%
14	Validation loss: 0.501548	Best loss: 0.085628	Accuracy: 96.29%
15	Validation loss: 0.329811	Best loss: 0.085628	Accuracy: 95.82%
16	Validation loss: 10524.064453	Best loss: 0.085628	Accuracy: 94.29%
17	Validation loss: 9.777562	Best loss: 0.085628	Accuracy: 88.70%
18	Validation loss: 2.926532	Best loss: 0.085628	Accuracy: 96.95%
19	Validation loss: 2.195154	Best loss: 0.085628	Accuracy: 97.30%
20	Validation loss: 2.427302	Best loss: 0.085628	Accuracy: 96.36%
21	Validation loss: 0.952680	Best loss: 0.085628	Accuracy: 96.95%
22	Validation loss: 1.482661	Best loss: 0.085628	Accuracy: 96.79%
23	Validation loss: 1.052619	Best loss: 0.085628	Accuracy: 97.62%
24	Validation loss: 3.795689	Best loss: 0.085628	Accuracy: 92.92%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  16.0s
[CV] n_neurons=100, learning_rate=0.02, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.259318	Best loss: 0.259318	Accuracy: 95.35%
1	Validation loss: 0.203060	Best loss: 0.203060	Accuracy: 94.49%
2	Validation loss: 0.222531	Best loss: 0.203060	Accuracy: 94.25%
3	Validation loss: 0.117139	Best loss: 0.117139	Accuracy: 96.01%
4	Validation loss: 0.117167	Best loss: 0.117139	Accuracy: 96.87%
5	Validation loss: 70.143761	Best loss: 0.117139	Accuracy: 89.29%
6	Validation loss: 4.575537	Best loss: 0.117139	Accuracy: 93.98%
7	Validation loss: 2.271456	Best loss: 0.117139	Accuracy: 95.39%
8	Validation loss: 1.812420	Best loss: 0.117139	Accuracy: 95.90%
9	Validation loss: 1.320370	Best loss: 0.117139	Accuracy: 96.01%
10	Validation loss: 2.122722	Best loss: 0.117139	Accuracy: 94.10%
11	Validation loss: 0.886517	Best loss: 0.117139	Accuracy: 96.01%
12	Validation loss: 0.585718	Best loss: 0.117139	Accuracy: 96.44%
13	Validation loss: 0.703169	Best loss: 0.117139	Accuracy: 95.93%
14	Validation loss: 0.384647	Best loss: 0.117139	Accuracy: 96.33%
15	Validation loss: 0.426316	Best loss: 0.117139	Accuracy: 96.44%
16	Validation loss: 0.653274	Best loss: 0.117139	Accuracy: 89.99%
17	Validation loss: 0.348240	Best loss: 0.117139	Accuracy: 96.29%
18	Validation loss: 0.300463	Best loss: 0.117139	Accuracy: 96.72%
19	Validation loss: 0.280553	Best loss: 0.117139	Accuracy: 96.87%
20	Validation loss: 1.511964	Best loss: 0.117139	Accuracy: 96.99%
21	Validation loss: 20.630276	Best loss: 0.117139	Accuracy: 93.24%
22	Validation loss: 9.298032	Best loss: 0.117139	Accuracy: 94.49%
23	Validation loss: 8.369840	Best loss: 0.117139	Accuracy: 93.82%
24	Validation loss: 8.705938	Best loss: 0.117139	Accuracy: 92.65%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  15.8s
[CV] n_neurons=100, learning_rate=0.02, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt; 
0	Validation loss: 0.249837	Best loss: 0.249837	Accuracy: 97.11%
1	Validation loss: 0.172328	Best loss: 0.172328	Accuracy: 95.66%
2	Validation loss: 0.110387	Best loss: 0.110387	Accuracy: 96.68%
3	Validation loss: 792.409180	Best loss: 0.110387	Accuracy: 41.59%
4	Validation loss: 1.316684	Best loss: 0.110387	Accuracy: 71.15%
5	Validation loss: 0.787006	Best loss: 0.110387	Accuracy: 78.81%
6	Validation loss: 0.474117	Best loss: 0.110387	Accuracy: 87.41%
7	Validation loss: 0.475539	Best loss: 0.110387	Accuracy: 86.98%
8	Validation loss: 0.509602	Best loss: 0.110387	Accuracy: 86.40%
9	Validation loss: 0.390536	Best loss: 0.110387	Accuracy: 89.87%
10	Validation loss: 0.336500	Best loss: 0.110387	Accuracy: 91.24%
11	Validation loss: 0.347023	Best loss: 0.110387	Accuracy: 91.01%
12	Validation loss: 0.272483	Best loss: 0.110387	Accuracy: 93.24%
13	Validation loss: 0.232012	Best loss: 0.110387	Accuracy: 94.37%
14	Validation loss: 95.997498	Best loss: 0.110387	Accuracy: 27.48%
15	Validation loss: 1.825218	Best loss: 0.110387	Accuracy: 77.09%
16	Validation loss: 2.074027	Best loss: 0.110387	Accuracy: 73.85%
17	Validation loss: 0.675689	Best loss: 0.110387	Accuracy: 89.05%
18	Validation loss: 0.558121	Best loss: 0.110387	Accuracy: 93.51%
19	Validation loss: 0.354771	Best loss: 0.110387	Accuracy: 94.41%
20	Validation loss: 3.391469	Best loss: 0.110387	Accuracy: 88.19%
21	Validation loss: 14.892896	Best loss: 0.110387	Accuracy: 72.17%
22	Validation loss: 4.847514	Best loss: 0.110387	Accuracy: 78.15%
23	Validation loss: 1.687809	Best loss: 0.110387	Accuracy: 84.87%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, total=  15.3s
[CV] n_neurons=70, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.140938	Best loss: 2.140938	Accuracy: 19.27%
1	Validation loss: 1.818714	Best loss: 1.818714	Accuracy: 22.01%
2	Validation loss: 1.793246	Best loss: 1.793246	Accuracy: 18.73%
3	Validation loss: 1.871856	Best loss: 1.793246	Accuracy: 19.08%
4	Validation loss: 1.731620	Best loss: 1.731620	Accuracy: 19.27%
5	Validation loss: 1.666489	Best loss: 1.666489	Accuracy: 19.08%
6	Validation loss: 2.220949	Best loss: 1.666489	Accuracy: 19.27%
7	Validation loss: 2.015726	Best loss: 1.666489	Accuracy: 19.08%
8	Validation loss: 2.090927	Best loss: 1.666489	Accuracy: 19.08%
9	Validation loss: 1.947277	Best loss: 1.666489	Accuracy: 22.01%
10	Validation loss: 1.792379	Best loss: 1.666489	Accuracy: 19.08%
11	Validation loss: 1.672830	Best loss: 1.666489	Accuracy: 19.08%
12	Validation loss: 1.912346	Best loss: 1.666489	Accuracy: 18.73%
13	Validation loss: 2.118673	Best loss: 1.666489	Accuracy: 19.08%
14	Validation loss: 2.516825	Best loss: 1.666489	Accuracy: 20.91%
15	Validation loss: 1.914143	Best loss: 1.666489	Accuracy: 22.01%
16	Validation loss: 2.225626	Best loss: 1.666489	Accuracy: 18.73%
17	Validation loss: 1.658232	Best loss: 1.658232	Accuracy: 20.91%
18	Validation loss: 2.221774	Best loss: 1.658232	Accuracy: 19.08%
19	Validation loss: 2.665048	Best loss: 1.658232	Accuracy: 22.01%
20	Validation loss: 2.008373	Best loss: 1.658232	Accuracy: 22.01%
21	Validation loss: 2.444499	Best loss: 1.658232	Accuracy: 20.91%
22	Validation loss: 2.119730	Best loss: 1.658232	Accuracy: 22.01%
23	Validation loss: 2.245321	Best loss: 1.658232	Accuracy: 19.27%
24	Validation loss: 1.920225	Best loss: 1.658232	Accuracy: 20.91%
25	Validation loss: 1.993324	Best loss: 1.658232	Accuracy: 20.91%
26	Validation loss: 1.718372	Best loss: 1.658232	Accuracy: 19.08%
27	Validation loss: 2.015469	Best loss: 1.658232	Accuracy: 18.73%
28	Validation loss: 2.296116	Best loss: 1.658232	Accuracy: 19.08%
29	Validation loss: 1.739967	Best loss: 1.658232	Accuracy: 20.91%
30	Validation loss: 1.709734	Best loss: 1.658232	Accuracy: 18.73%
31	Validation loss: 1.664722	Best loss: 1.658232	Accuracy: 20.91%
32	Validation loss: 1.779046	Best loss: 1.658232	Accuracy: 22.01%
33	Validation loss: 1.714502	Best loss: 1.658232	Accuracy: 19.08%
34	Validation loss: 2.160546	Best loss: 1.658232	Accuracy: 19.08%
35	Validation loss: 2.389711	Best loss: 1.658232	Accuracy: 19.27%
36	Validation loss: 1.843753	Best loss: 1.658232	Accuracy: 18.73%
37	Validation loss: 1.860697	Best loss: 1.658232	Accuracy: 19.08%
38	Validation loss: 2.205712	Best loss: 1.658232	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.1min
[CV] n_neurons=70, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.833752	Best loss: 1.833752	Accuracy: 19.27%
1	Validation loss: 1.862164	Best loss: 1.833752	Accuracy: 22.01%
2	Validation loss: 2.011885	Best loss: 1.833752	Accuracy: 18.73%
3	Validation loss: 2.016301	Best loss: 1.833752	Accuracy: 22.01%
4	Validation loss: 1.970923	Best loss: 1.833752	Accuracy: 19.08%
5	Validation loss: 1.647394	Best loss: 1.647394	Accuracy: 20.91%
6	Validation loss: 1.701503	Best loss: 1.647394	Accuracy: 19.27%
7	Validation loss: 2.395791	Best loss: 1.647394	Accuracy: 19.08%
8	Validation loss: 1.941445	Best loss: 1.647394	Accuracy: 18.73%
9	Validation loss: 1.963344	Best loss: 1.647394	Accuracy: 20.91%
10	Validation loss: 1.835824	Best loss: 1.647394	Accuracy: 19.08%
11	Validation loss: 1.930027	Best loss: 1.647394	Accuracy: 22.01%
12	Validation loss: 1.885898	Best loss: 1.647394	Accuracy: 20.91%
13	Validation loss: 2.390908	Best loss: 1.647394	Accuracy: 19.08%
14	Validation loss: 1.961100	Best loss: 1.647394	Accuracy: 19.27%
15	Validation loss: 1.962756	Best loss: 1.647394	Accuracy: 20.91%
16	Validation loss: 1.886142	Best loss: 1.647394	Accuracy: 18.73%
17	Validation loss: 1.892859	Best loss: 1.647394	Accuracy: 19.08%
18	Validation loss: 1.981896	Best loss: 1.647394	Accuracy: 19.08%
19	Validation loss: 1.750126	Best loss: 1.647394	Accuracy: 22.01%
20	Validation loss: 2.134342	Best loss: 1.647394	Accuracy: 19.27%
21	Validation loss: 1.879302	Best loss: 1.647394	Accuracy: 22.01%
22	Validation loss: 2.148498	Best loss: 1.647394	Accuracy: 19.08%
23	Validation loss: 2.088909	Best loss: 1.647394	Accuracy: 22.01%
24	Validation loss: 1.838559	Best loss: 1.647394	Accuracy: 20.91%
25	Validation loss: 1.905607	Best loss: 1.647394	Accuracy: 19.08%
26	Validation loss: 2.406297	Best loss: 1.647394	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  43.7s
[CV] n_neurons=70, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.795825	Best loss: 1.795825	Accuracy: 22.01%
1	Validation loss: 1.992888	Best loss: 1.795825	Accuracy: 19.27%
2	Validation loss: 1.932379	Best loss: 1.795825	Accuracy: 19.08%
3	Validation loss: 1.853602	Best loss: 1.795825	Accuracy: 19.27%
4	Validation loss: 2.686534	Best loss: 1.795825	Accuracy: 19.08%
5	Validation loss: 1.852323	Best loss: 1.795825	Accuracy: 20.91%
6	Validation loss: 1.812496	Best loss: 1.795825	Accuracy: 22.01%
7	Validation loss: 1.757772	Best loss: 1.757772	Accuracy: 19.08%
8	Validation loss: 1.902279	Best loss: 1.757772	Accuracy: 19.27%
9	Validation loss: 2.087055	Best loss: 1.757772	Accuracy: 19.27%
10	Validation loss: 1.779783	Best loss: 1.757772	Accuracy: 18.73%
11	Validation loss: 2.227393	Best loss: 1.757772	Accuracy: 19.27%
12	Validation loss: 1.950018	Best loss: 1.757772	Accuracy: 20.91%
13	Validation loss: 3.099459	Best loss: 1.757772	Accuracy: 19.08%
14	Validation loss: 1.830344	Best loss: 1.757772	Accuracy: 22.01%
15	Validation loss: 1.733784	Best loss: 1.733784	Accuracy: 20.91%
16	Validation loss: 2.189532	Best loss: 1.733784	Accuracy: 18.73%
17	Validation loss: 1.943922	Best loss: 1.733784	Accuracy: 18.73%
18	Validation loss: 1.848899	Best loss: 1.733784	Accuracy: 19.08%
19	Validation loss: 1.984283	Best loss: 1.733784	Accuracy: 18.73%
20	Validation loss: 1.919438	Best loss: 1.733784	Accuracy: 19.27%
21	Validation loss: 2.156085	Best loss: 1.733784	Accuracy: 22.01%
22	Validation loss: 1.834820	Best loss: 1.733784	Accuracy: 19.27%
23	Validation loss: 2.444076	Best loss: 1.733784	Accuracy: 19.08%
24	Validation loss: 1.866027	Best loss: 1.733784	Accuracy: 19.27%
25	Validation loss: 1.884747	Best loss: 1.733784	Accuracy: 22.01%
26	Validation loss: 1.738111	Best loss: 1.733784	Accuracy: 18.73%
27	Validation loss: 1.679296	Best loss: 1.679296	Accuracy: 20.91%
28	Validation loss: 2.567140	Best loss: 1.679296	Accuracy: 19.08%
29	Validation loss: 2.661166	Best loss: 1.679296	Accuracy: 20.91%
30	Validation loss: 2.013063	Best loss: 1.679296	Accuracy: 19.27%
31	Validation loss: 1.886540	Best loss: 1.679296	Accuracy: 22.01%
32	Validation loss: 1.891881	Best loss: 1.679296	Accuracy: 19.08%
33	Validation loss: 2.216814	Best loss: 1.679296	Accuracy: 22.01%
34	Validation loss: 2.191182	Best loss: 1.679296	Accuracy: 19.27%
35	Validation loss: 1.704168	Best loss: 1.679296	Accuracy: 19.08%
36	Validation loss: 1.731654	Best loss: 1.679296	Accuracy: 19.27%
37	Validation loss: 1.813350	Best loss: 1.679296	Accuracy: 22.01%
38	Validation loss: 3.032772	Best loss: 1.679296	Accuracy: 18.73%
39	Validation loss: 2.401287	Best loss: 1.679296	Accuracy: 19.27%
40	Validation loss: 1.645940	Best loss: 1.645940	Accuracy: 19.08%
41	Validation loss: 2.166194	Best loss: 1.645940	Accuracy: 22.01%
42	Validation loss: 2.384698	Best loss: 1.645940	Accuracy: 22.01%
43	Validation loss: 2.204780	Best loss: 1.645940	Accuracy: 20.91%
44	Validation loss: 1.765178	Best loss: 1.645940	Accuracy: 18.73%
45	Validation loss: 1.696825	Best loss: 1.645940	Accuracy: 18.73%
46	Validation loss: 2.521089	Best loss: 1.645940	Accuracy: 20.91%
47	Validation loss: 2.133251	Best loss: 1.645940	Accuracy: 19.08%
48	Validation loss: 1.695981	Best loss: 1.645940	Accuracy: 22.01%
49	Validation loss: 1.936505	Best loss: 1.645940	Accuracy: 19.08%
50	Validation loss: 2.176908	Best loss: 1.645940	Accuracy: 18.73%
51	Validation loss: 2.179086	Best loss: 1.645940	Accuracy: 19.27%
52	Validation loss: 2.013385	Best loss: 1.645940	Accuracy: 20.91%
53	Validation loss: 2.239873	Best loss: 1.645940	Accuracy: 19.27%
54	Validation loss: 1.942738	Best loss: 1.645940	Accuracy: 20.91%
55	Validation loss: 1.686155	Best loss: 1.645940	Accuracy: 20.91%
56	Validation loss: 2.281741	Best loss: 1.645940	Accuracy: 19.08%
57	Validation loss: 2.154370	Best loss: 1.645940	Accuracy: 22.01%
58	Validation loss: 1.815147	Best loss: 1.645940	Accuracy: 19.27%
59	Validation loss: 1.998513	Best loss: 1.645940	Accuracy: 19.27%
60	Validation loss: 1.969455	Best loss: 1.645940	Accuracy: 19.08%
61	Validation loss: 2.026056	Best loss: 1.645940	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.05, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.7min
[CV] n_neurons=50, learning_rate=0.01, batch_size=500, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.102346	Best loss: 0.102346	Accuracy: 96.91%
1	Validation loss: 0.062035	Best loss: 0.062035	Accuracy: 98.08%
2	Validation loss: 0.063369	Best loss: 0.062035	Accuracy: 97.97%
3	Validation loss: 0.060676	Best loss: 0.060676	Accuracy: 98.08%
4	Validation loss: 0.057493	Best loss: 0.057493	Accuracy: 98.51%
5	Validation loss: 0.060426	Best loss: 0.057493	Accuracy: 98.44%
6	Validation loss: 0.044000	Best loss: 0.044000	Accuracy: 98.63%
7	Validation loss: 0.044906	Best loss: 0.044000	Accuracy: 98.67%
8	Validation loss: 0.088793	Best loss: 0.044000	Accuracy: 97.93%
9	Validation loss: 0.065593	Best loss: 0.044000	Accuracy: 98.12%
10	Validation loss: 0.049003	Best loss: 0.044000	Accuracy: 98.91%
11	Validation loss: 0.069280	Best loss: 0.044000	Accuracy: 98.05%
12	Validation loss: 0.044232	Best loss: 0.044000	Accuracy: 98.59%
13	Validation loss: 0.046922	Best loss: 0.044000	Accuracy: 98.75%
14	Validation loss: 0.042123	Best loss: 0.042123	Accuracy: 99.10%
15	Validation loss: 0.052767	Best loss: 0.042123	Accuracy: 98.83%
16	Validation loss: 0.084379	Best loss: 0.042123	Accuracy: 98.71%
17	Validation loss: 0.058778	Best loss: 0.042123	Accuracy: 98.51%
18	Validation loss: 0.059727	Best loss: 0.042123	Accuracy: 98.48%
19	Validation loss: 0.080102	Best loss: 0.042123	Accuracy: 98.08%
20	Validation loss: 0.042801	Best loss: 0.042123	Accuracy: 99.02%
21	Validation loss: 0.054011	Best loss: 0.042123	Accuracy: 98.75%
22	Validation loss: 0.046625	Best loss: 0.042123	Accuracy: 99.14%
23	Validation loss: 0.054125	Best loss: 0.042123	Accuracy: 99.02%
24	Validation loss: 0.057220	Best loss: 0.042123	Accuracy: 98.91%
25	Validation loss: 0.054646	Best loss: 0.042123	Accuracy: 99.06%
26	Validation loss: 0.058932	Best loss: 0.042123	Accuracy: 98.98%
27	Validation loss: 0.058628	Best loss: 0.042123	Accuracy: 99.02%
28	Validation loss: 0.060502	Best loss: 0.042123	Accuracy: 99.02%
29	Validation loss: 0.061223	Best loss: 0.042123	Accuracy: 99.02%
30	Validation loss: 0.061893	Best loss: 0.042123	Accuracy: 99.06%
31	Validation loss: 0.062474	Best loss: 0.042123	Accuracy: 99.06%
32	Validation loss: 0.062997	Best loss: 0.042123	Accuracy: 99.06%
33	Validation loss: 0.063677	Best loss: 0.042123	Accuracy: 99.10%
34	Validation loss: 0.064204	Best loss: 0.042123	Accuracy: 99.10%
35	Validation loss: 0.064648	Best loss: 0.042123	Accuracy: 99.06%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=500, activation=&lt;function relu at 0x124366d08&gt;, total=   4.6s
[CV] n_neurons=50, learning_rate=0.01, batch_size=500, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.102200	Best loss: 0.102200	Accuracy: 96.60%
1	Validation loss: 0.073656	Best loss: 0.073656	Accuracy: 97.85%
2	Validation loss: 0.055486	Best loss: 0.055486	Accuracy: 98.08%
3	Validation loss: 0.061788	Best loss: 0.055486	Accuracy: 98.24%
4	Validation loss: 0.067512	Best loss: 0.055486	Accuracy: 98.32%
5	Validation loss: 0.045745	Best loss: 0.045745	Accuracy: 98.36%
6	Validation loss: 0.073165	Best loss: 0.045745	Accuracy: 98.12%
7	Validation loss: 0.056184	Best loss: 0.045745	Accuracy: 98.36%
8	Validation loss: 0.064453	Best loss: 0.045745	Accuracy: 98.32%
9	Validation loss: 0.060947	Best loss: 0.045745	Accuracy: 98.20%
10	Validation loss: 0.059344	Best loss: 0.045745	Accuracy: 98.55%
11	Validation loss: 0.056133	Best loss: 0.045745	Accuracy: 98.59%
12	Validation loss: 0.047753	Best loss: 0.045745	Accuracy: 98.94%
13	Validation loss: 0.047840	Best loss: 0.045745	Accuracy: 98.91%
14	Validation loss: 0.061775	Best loss: 0.045745	Accuracy: 98.63%
15	Validation loss: 0.061214	Best loss: 0.045745	Accuracy: 98.63%
16	Validation loss: 0.052678	Best loss: 0.045745	Accuracy: 98.67%
17	Validation loss: 0.049249	Best loss: 0.045745	Accuracy: 98.79%
18	Validation loss: 0.056686	Best loss: 0.045745	Accuracy: 98.83%
19	Validation loss: 0.060863	Best loss: 0.045745	Accuracy: 98.79%
20	Validation loss: 0.069112	Best loss: 0.045745	Accuracy: 98.79%
21	Validation loss: 0.047597	Best loss: 0.045745	Accuracy: 99.10%
22	Validation loss: 0.059034	Best loss: 0.045745	Accuracy: 98.94%
23	Validation loss: 0.062350	Best loss: 0.045745	Accuracy: 99.02%
24	Validation loss: 0.069134	Best loss: 0.045745	Accuracy: 99.02%
25	Validation loss: 0.064960	Best loss: 0.045745	Accuracy: 99.10%
26	Validation loss: 0.066950	Best loss: 0.045745	Accuracy: 99.14%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=500, activation=&lt;function relu at 0x124366d08&gt;, total=   3.8s
[CV] n_neurons=50, learning_rate=0.01, batch_size=500, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.089085	Best loss: 0.089085	Accuracy: 97.30%
1	Validation loss: 0.060839	Best loss: 0.060839	Accuracy: 97.97%
2	Validation loss: 0.070174	Best loss: 0.060839	Accuracy: 97.77%
3	Validation loss: 0.055754	Best loss: 0.055754	Accuracy: 98.24%
4	Validation loss: 0.037676	Best loss: 0.037676	Accuracy: 98.75%
5	Validation loss: 0.064215	Best loss: 0.037676	Accuracy: 97.77%
6	Validation loss: 0.053909	Best loss: 0.037676	Accuracy: 98.40%
7	Validation loss: 0.069545	Best loss: 0.037676	Accuracy: 97.73%
8	Validation loss: 0.069219	Best loss: 0.037676	Accuracy: 98.01%
9	Validation loss: 0.047954	Best loss: 0.037676	Accuracy: 98.59%
10	Validation loss: 0.046717	Best loss: 0.037676	Accuracy: 98.83%
11	Validation loss: 0.046548	Best loss: 0.037676	Accuracy: 98.91%
12	Validation loss: 0.059162	Best loss: 0.037676	Accuracy: 98.59%
13	Validation loss: 0.044970	Best loss: 0.037676	Accuracy: 98.63%
14	Validation loss: 0.039521	Best loss: 0.037676	Accuracy: 98.91%
15	Validation loss: 0.056316	Best loss: 0.037676	Accuracy: 98.94%
16	Validation loss: 0.062432	Best loss: 0.037676	Accuracy: 98.71%
17	Validation loss: 0.057204	Best loss: 0.037676	Accuracy: 98.83%
18	Validation loss: 0.066537	Best loss: 0.037676	Accuracy: 98.79%
19	Validation loss: 0.055908	Best loss: 0.037676	Accuracy: 98.67%
20	Validation loss: 0.065259	Best loss: 0.037676	Accuracy: 98.44%
21	Validation loss: 0.046319	Best loss: 0.037676	Accuracy: 98.83%
22	Validation loss: 0.062513	Best loss: 0.037676	Accuracy: 98.51%
23	Validation loss: 0.044041	Best loss: 0.037676	Accuracy: 98.91%
24	Validation loss: 0.044881	Best loss: 0.037676	Accuracy: 98.67%
25	Validation loss: 0.046743	Best loss: 0.037676	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=500, activation=&lt;function relu at 0x124366d08&gt;, total=   3.7s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.388062	Best loss: 0.388062	Accuracy: 89.13%
1	Validation loss: 0.248579	Best loss: 0.248579	Accuracy: 95.15%
2	Validation loss: 1.894321	Best loss: 0.248579	Accuracy: 19.27%
3	Validation loss: 1.870569	Best loss: 0.248579	Accuracy: 19.27%
4	Validation loss: 1.672370	Best loss: 0.248579	Accuracy: 22.01%
5	Validation loss: 1.641805	Best loss: 0.248579	Accuracy: 22.01%
6	Validation loss: 1.893874	Best loss: 0.248579	Accuracy: 18.73%
7	Validation loss: 1.787318	Best loss: 0.248579	Accuracy: 20.91%
8	Validation loss: 1.677856	Best loss: 0.248579	Accuracy: 19.27%
9	Validation loss: 1.907556	Best loss: 0.248579	Accuracy: 19.27%
10	Validation loss: 1.845188	Best loss: 0.248579	Accuracy: 19.08%
11	Validation loss: 2.019194	Best loss: 0.248579	Accuracy: 18.73%
12	Validation loss: 1.863194	Best loss: 0.248579	Accuracy: 20.91%
13	Validation loss: 1.684019	Best loss: 0.248579	Accuracy: 22.01%
14	Validation loss: 1.835624	Best loss: 0.248579	Accuracy: 22.01%
15	Validation loss: 1.901896	Best loss: 0.248579	Accuracy: 18.73%
16	Validation loss: 1.712274	Best loss: 0.248579	Accuracy: 20.91%
17	Validation loss: 2.277107	Best loss: 0.248579	Accuracy: 19.08%
18	Validation loss: 1.817263	Best loss: 0.248579	Accuracy: 19.08%
19	Validation loss: 1.688507	Best loss: 0.248579	Accuracy: 19.08%
20	Validation loss: 1.686490	Best loss: 0.248579	Accuracy: 19.08%
21	Validation loss: 2.545422	Best loss: 0.248579	Accuracy: 20.91%
22	Validation loss: 1.645230	Best loss: 0.248579	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  11.3s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.524475	Best loss: 0.524475	Accuracy: 86.16%
1	Validation loss: 0.446498	Best loss: 0.446498	Accuracy: 88.66%
2	Validation loss: 1.625575	Best loss: 0.446498	Accuracy: 19.27%
3	Validation loss: 1.671241	Best loss: 0.446498	Accuracy: 22.01%
4	Validation loss: 1.722618	Best loss: 0.446498	Accuracy: 20.91%
5	Validation loss: 1.832005	Best loss: 0.446498	Accuracy: 20.91%
6	Validation loss: 1.738909	Best loss: 0.446498	Accuracy: 22.01%
7	Validation loss: 1.800923	Best loss: 0.446498	Accuracy: 20.91%
8	Validation loss: 1.632135	Best loss: 0.446498	Accuracy: 18.73%
9	Validation loss: 1.805179	Best loss: 0.446498	Accuracy: 20.91%
10	Validation loss: 1.733045	Best loss: 0.446498	Accuracy: 22.01%
11	Validation loss: 1.786799	Best loss: 0.446498	Accuracy: 22.01%
12	Validation loss: 1.716953	Best loss: 0.446498	Accuracy: 19.27%
13	Validation loss: 1.893210	Best loss: 0.446498	Accuracy: 18.73%
14	Validation loss: 1.741268	Best loss: 0.446498	Accuracy: 18.73%
15	Validation loss: 1.752678	Best loss: 0.446498	Accuracy: 18.73%
16	Validation loss: 1.752239	Best loss: 0.446498	Accuracy: 22.01%
17	Validation loss: 2.482581	Best loss: 0.446498	Accuracy: 19.27%
18	Validation loss: 1.812770	Best loss: 0.446498	Accuracy: 22.01%
19	Validation loss: 1.778781	Best loss: 0.446498	Accuracy: 19.27%
20	Validation loss: 1.767402	Best loss: 0.446498	Accuracy: 20.91%
21	Validation loss: 1.825952	Best loss: 0.446498	Accuracy: 22.01%
22	Validation loss: 2.200377	Best loss: 0.446498	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  11.4s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.854376	Best loss: 0.854376	Accuracy: 59.81%
1	Validation loss: 1.261967	Best loss: 0.854376	Accuracy: 38.90%
2	Validation loss: 1.803996	Best loss: 0.854376	Accuracy: 20.91%
3	Validation loss: 1.623479	Best loss: 0.854376	Accuracy: 22.01%
4	Validation loss: 1.675800	Best loss: 0.854376	Accuracy: 20.91%
5	Validation loss: 1.912202	Best loss: 0.854376	Accuracy: 20.91%
6	Validation loss: 1.720005	Best loss: 0.854376	Accuracy: 22.01%
7	Validation loss: 1.770505	Best loss: 0.854376	Accuracy: 18.73%
8	Validation loss: 1.739757	Best loss: 0.854376	Accuracy: 18.73%
9	Validation loss: 1.854592	Best loss: 0.854376	Accuracy: 22.01%
10	Validation loss: 1.640569	Best loss: 0.854376	Accuracy: 18.73%
11	Validation loss: 1.993243	Best loss: 0.854376	Accuracy: 20.91%
12	Validation loss: 2.012863	Best loss: 0.854376	Accuracy: 20.91%
13	Validation loss: 2.137873	Best loss: 0.854376	Accuracy: 19.08%
14	Validation loss: 1.743365	Best loss: 0.854376	Accuracy: 18.73%
15	Validation loss: 1.666610	Best loss: 0.854376	Accuracy: 22.01%
16	Validation loss: 1.781339	Best loss: 0.854376	Accuracy: 19.08%
17	Validation loss: 2.529819	Best loss: 0.854376	Accuracy: 20.91%
18	Validation loss: 1.739411	Best loss: 0.854376	Accuracy: 18.73%
19	Validation loss: 1.682002	Best loss: 0.854376	Accuracy: 20.91%
20	Validation loss: 1.954094	Best loss: 0.854376	Accuracy: 20.91%
21	Validation loss: 1.866161	Best loss: 0.854376	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  10.8s
[CV] n_neurons=120, learning_rate=0.02, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.109160	Best loss: 0.109160	Accuracy: 97.19%
1	Validation loss: 0.099378	Best loss: 0.099378	Accuracy: 97.89%
2	Validation loss: 0.087900	Best loss: 0.087900	Accuracy: 97.50%
3	Validation loss: 3.079095	Best loss: 0.087900	Accuracy: 18.73%
4	Validation loss: 1.729481	Best loss: 0.087900	Accuracy: 18.73%
5	Validation loss: 1.684716	Best loss: 0.087900	Accuracy: 22.01%
6	Validation loss: 1.632858	Best loss: 0.087900	Accuracy: 19.27%
7	Validation loss: 1.688405	Best loss: 0.087900	Accuracy: 19.08%
8	Validation loss: 1.727018	Best loss: 0.087900	Accuracy: 19.27%
9	Validation loss: 1.860067	Best loss: 0.087900	Accuracy: 19.27%
10	Validation loss: 1.729409	Best loss: 0.087900	Accuracy: 22.01%
11	Validation loss: 1.712538	Best loss: 0.087900	Accuracy: 22.01%
12	Validation loss: 1.707598	Best loss: 0.087900	Accuracy: 19.08%
13	Validation loss: 1.742676	Best loss: 0.087900	Accuracy: 20.91%
14	Validation loss: 1.718686	Best loss: 0.087900	Accuracy: 22.01%
15	Validation loss: 1.653131	Best loss: 0.087900	Accuracy: 19.08%
16	Validation loss: 1.763324	Best loss: 0.087900	Accuracy: 18.73%
17	Validation loss: 1.634577	Best loss: 0.087900	Accuracy: 19.27%
18	Validation loss: 1.677371	Best loss: 0.087900	Accuracy: 20.91%
19	Validation loss: 1.633883	Best loss: 0.087900	Accuracy: 22.01%
20	Validation loss: 1.657923	Best loss: 0.087900	Accuracy: 19.27%
21	Validation loss: 1.703251	Best loss: 0.087900	Accuracy: 22.01%
22	Validation loss: 1.677012	Best loss: 0.087900	Accuracy: 19.08%
23	Validation loss: 1.660194	Best loss: 0.087900	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.02, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  15.5s
[CV] n_neurons=120, learning_rate=0.02, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.111298	Best loss: 0.111298	Accuracy: 96.91%
1	Validation loss: 1.768053	Best loss: 0.111298	Accuracy: 19.08%
2	Validation loss: 1.639465	Best loss: 0.111298	Accuracy: 20.91%
3	Validation loss: 1.649286	Best loss: 0.111298	Accuracy: 18.73%
4	Validation loss: 1.680798	Best loss: 0.111298	Accuracy: 19.27%
5	Validation loss: 1.631761	Best loss: 0.111298	Accuracy: 20.91%
6	Validation loss: 1.702934	Best loss: 0.111298	Accuracy: 19.27%
7	Validation loss: 1.791644	Best loss: 0.111298	Accuracy: 18.73%
8	Validation loss: 1.800763	Best loss: 0.111298	Accuracy: 18.73%
9	Validation loss: 1.735287	Best loss: 0.111298	Accuracy: 20.91%
10	Validation loss: 1.739027	Best loss: 0.111298	Accuracy: 19.08%
11	Validation loss: 1.661241	Best loss: 0.111298	Accuracy: 22.01%
12	Validation loss: 1.808053	Best loss: 0.111298	Accuracy: 20.91%
13	Validation loss: 1.665240	Best loss: 0.111298	Accuracy: 19.08%
14	Validation loss: 1.665547	Best loss: 0.111298	Accuracy: 19.08%
15	Validation loss: 1.730525	Best loss: 0.111298	Accuracy: 19.08%
16	Validation loss: 1.662671	Best loss: 0.111298	Accuracy: 18.73%
17	Validation loss: 1.658264	Best loss: 0.111298	Accuracy: 19.27%
18	Validation loss: 1.635462	Best loss: 0.111298	Accuracy: 20.91%
19	Validation loss: 1.648224	Best loss: 0.111298	Accuracy: 19.27%
20	Validation loss: 1.683906	Best loss: 0.111298	Accuracy: 18.73%
21	Validation loss: 1.769955	Best loss: 0.111298	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.02, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  13.6s
[CV] n_neurons=120, learning_rate=0.02, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.520980	Best loss: 1.520980	Accuracy: 70.02%
1	Validation loss: 0.209530	Best loss: 0.209530	Accuracy: 95.00%
2	Validation loss: 0.563498	Best loss: 0.209530	Accuracy: 94.49%
3	Validation loss: 0.180418	Best loss: 0.180418	Accuracy: 95.70%
4	Validation loss: 0.137191	Best loss: 0.137191	Accuracy: 96.56%
5	Validation loss: 1.802174	Best loss: 0.137191	Accuracy: 20.91%
6	Validation loss: 1.658591	Best loss: 0.137191	Accuracy: 19.27%
7	Validation loss: 1.777010	Best loss: 0.137191	Accuracy: 18.73%
8	Validation loss: 1.733897	Best loss: 0.137191	Accuracy: 22.01%
9	Validation loss: 1.698805	Best loss: 0.137191	Accuracy: 20.91%
10	Validation loss: 1.636430	Best loss: 0.137191	Accuracy: 18.73%
11	Validation loss: 1.820185	Best loss: 0.137191	Accuracy: 18.73%
12	Validation loss: 1.916772	Best loss: 0.137191	Accuracy: 19.08%
13	Validation loss: 1.631280	Best loss: 0.137191	Accuracy: 22.01%
14	Validation loss: 1.615935	Best loss: 0.137191	Accuracy: 19.08%
15	Validation loss: 1.621318	Best loss: 0.137191	Accuracy: 22.01%
16	Validation loss: 1.704109	Best loss: 0.137191	Accuracy: 22.01%
17	Validation loss: 1.621600	Best loss: 0.137191	Accuracy: 22.01%
18	Validation loss: 1.641431	Best loss: 0.137191	Accuracy: 22.01%
19	Validation loss: 1.778364	Best loss: 0.137191	Accuracy: 22.01%
20	Validation loss: 1.671140	Best loss: 0.137191	Accuracy: 18.73%
21	Validation loss: 1.831135	Best loss: 0.137191	Accuracy: 22.01%
22	Validation loss: 1.658309	Best loss: 0.137191	Accuracy: 22.01%
23	Validation loss: 1.697491	Best loss: 0.137191	Accuracy: 22.01%
24	Validation loss: 1.738196	Best loss: 0.137191	Accuracy: 20.91%
25	Validation loss: 1.674322	Best loss: 0.137191	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.02, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  16.3s
[CV] n_neurons=140, learning_rate=0.1, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.059981	Best loss: 1.059981	Accuracy: 54.22%
1	Validation loss: 0.768662	Best loss: 0.768662	Accuracy: 68.22%
2	Validation loss: 1.610561	Best loss: 0.768662	Accuracy: 22.01%
3	Validation loss: 1.608323	Best loss: 0.768662	Accuracy: 22.01%
4	Validation loss: 1.618574	Best loss: 0.768662	Accuracy: 22.01%
5	Validation loss: 1.615534	Best loss: 0.768662	Accuracy: 22.01%
6	Validation loss: 1.616574	Best loss: 0.768662	Accuracy: 22.01%
7	Validation loss: 1.617466	Best loss: 0.768662	Accuracy: 22.01%
8	Validation loss: 1.616992	Best loss: 0.768662	Accuracy: 19.27%
9	Validation loss: 1.608659	Best loss: 0.768662	Accuracy: 22.01%
10	Validation loss: 1.609215	Best loss: 0.768662	Accuracy: 22.01%
11	Validation loss: 1.608816	Best loss: 0.768662	Accuracy: 22.01%
12	Validation loss: 1.616525	Best loss: 0.768662	Accuracy: 22.01%
13	Validation loss: 1.625397	Best loss: 0.768662	Accuracy: 18.73%
14	Validation loss: 1.614404	Best loss: 0.768662	Accuracy: 18.73%
15	Validation loss: 1.612100	Best loss: 0.768662	Accuracy: 22.01%
16	Validation loss: 1.608088	Best loss: 0.768662	Accuracy: 22.01%
17	Validation loss: 1.615094	Best loss: 0.768662	Accuracy: 19.27%
18	Validation loss: 1.616774	Best loss: 0.768662	Accuracy: 18.73%
19	Validation loss: 1.612285	Best loss: 0.768662	Accuracy: 22.01%
20	Validation loss: 1.615542	Best loss: 0.768662	Accuracy: 19.08%
21	Validation loss: 1.619599	Best loss: 0.768662	Accuracy: 19.27%
22	Validation loss: 1.616394	Best loss: 0.768662	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.1, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  16.2s
[CV] n_neurons=140, learning_rate=0.1, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.012065	Best loss: 1.012065	Accuracy: 53.24%
1	Validation loss: 0.911166	Best loss: 0.911166	Accuracy: 62.59%
2	Validation loss: 0.883303	Best loss: 0.883303	Accuracy: 60.56%
3	Validation loss: 0.894116	Best loss: 0.883303	Accuracy: 60.28%
4	Validation loss: 0.916296	Best loss: 0.883303	Accuracy: 52.19%
5	Validation loss: 0.831533	Best loss: 0.831533	Accuracy: 62.31%
6	Validation loss: 1.630538	Best loss: 0.831533	Accuracy: 19.08%
7	Validation loss: 1.622566	Best loss: 0.831533	Accuracy: 18.73%
8	Validation loss: 1.613049	Best loss: 0.831533	Accuracy: 22.01%
9	Validation loss: 1.611259	Best loss: 0.831533	Accuracy: 22.01%
10	Validation loss: 1.610392	Best loss: 0.831533	Accuracy: 22.01%
11	Validation loss: 1.611945	Best loss: 0.831533	Accuracy: 22.01%
12	Validation loss: 1.615504	Best loss: 0.831533	Accuracy: 19.27%
13	Validation loss: 1.617266	Best loss: 0.831533	Accuracy: 19.08%
14	Validation loss: 1.610518	Best loss: 0.831533	Accuracy: 22.01%
15	Validation loss: 1.612049	Best loss: 0.831533	Accuracy: 22.01%
16	Validation loss: 1.619837	Best loss: 0.831533	Accuracy: 22.01%
17	Validation loss: 1.614759	Best loss: 0.831533	Accuracy: 19.08%
18	Validation loss: 1.611525	Best loss: 0.831533	Accuracy: 22.01%
19	Validation loss: 1.609967	Best loss: 0.831533	Accuracy: 22.01%
20	Validation loss: 1.608911	Best loss: 0.831533	Accuracy: 22.01%
21	Validation loss: 1.629889	Best loss: 0.831533	Accuracy: 22.01%
22	Validation loss: 1.620676	Best loss: 0.831533	Accuracy: 22.01%
23	Validation loss: 1.622631	Best loss: 0.831533	Accuracy: 22.01%
24	Validation loss: 1.607976	Best loss: 0.831533	Accuracy: 22.01%
25	Validation loss: 1.610930	Best loss: 0.831533	Accuracy: 20.91%
26	Validation loss: 1.611142	Best loss: 0.831533	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.1, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  18.2s
[CV] n_neurons=140, learning_rate=0.1, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.611978	Best loss: 1.611978	Accuracy: 22.01%
1	Validation loss: 1.616483	Best loss: 1.611978	Accuracy: 19.27%
2	Validation loss: 1.613425	Best loss: 1.611978	Accuracy: 22.01%
3	Validation loss: 1.629834	Best loss: 1.611978	Accuracy: 18.73%
4	Validation loss: 1.608644	Best loss: 1.608644	Accuracy: 22.01%
5	Validation loss: 1.615862	Best loss: 1.608644	Accuracy: 22.01%
6	Validation loss: 1.615275	Best loss: 1.608644	Accuracy: 22.01%
7	Validation loss: 1.615586	Best loss: 1.608644	Accuracy: 18.73%
8	Validation loss: 1.612312	Best loss: 1.608644	Accuracy: 22.01%
9	Validation loss: 1.610812	Best loss: 1.608644	Accuracy: 22.01%
10	Validation loss: 1.611904	Best loss: 1.608644	Accuracy: 22.01%
11	Validation loss: 1.612864	Best loss: 1.608644	Accuracy: 22.01%
12	Validation loss: 1.610793	Best loss: 1.608644	Accuracy: 20.91%
13	Validation loss: 1.620048	Best loss: 1.608644	Accuracy: 19.27%
14	Validation loss: 1.612825	Best loss: 1.608644	Accuracy: 22.01%
15	Validation loss: 1.615500	Best loss: 1.608644	Accuracy: 22.01%
16	Validation loss: 1.628751	Best loss: 1.608644	Accuracy: 18.73%
17	Validation loss: 1.625349	Best loss: 1.608644	Accuracy: 19.08%
18	Validation loss: 1.609346	Best loss: 1.608644	Accuracy: 20.91%
19	Validation loss: 1.613124	Best loss: 1.608644	Accuracy: 19.08%
20	Validation loss: 1.613293	Best loss: 1.608644	Accuracy: 19.27%
21	Validation loss: 1.623134	Best loss: 1.608644	Accuracy: 22.01%
22	Validation loss: 1.617782	Best loss: 1.608644	Accuracy: 22.01%
23	Validation loss: 1.617095	Best loss: 1.608644	Accuracy: 22.01%
24	Validation loss: 1.608748	Best loss: 1.608644	Accuracy: 20.91%
25	Validation loss: 1.615597	Best loss: 1.608644	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.1, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  18.1s
[CV] n_neurons=90, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 486.467316	Best loss: 486.467316	Accuracy: 46.09%
1	Validation loss: 53.265144	Best loss: 53.265144	Accuracy: 72.79%
2	Validation loss: 3.521480	Best loss: 3.521480	Accuracy: 93.28%
3	Validation loss: 1.420691	Best loss: 1.420691	Accuracy: 94.61%
4	Validation loss: 3.264486	Best loss: 1.420691	Accuracy: 91.67%
5	Validation loss: 1.391423	Best loss: 1.391423	Accuracy: 94.88%
6	Validation loss: 0.795927	Best loss: 0.795927	Accuracy: 94.72%
7	Validation loss: 0.685705	Best loss: 0.685705	Accuracy: 94.61%
8	Validation loss: 0.579306	Best loss: 0.579306	Accuracy: 95.23%
9	Validation loss: 0.442433	Best loss: 0.442433	Accuracy: 95.66%
10	Validation loss: 0.815964	Best loss: 0.442433	Accuracy: 94.02%
11	Validation loss: 0.647299	Best loss: 0.442433	Accuracy: 94.37%
12	Validation loss: 0.382562	Best loss: 0.382562	Accuracy: 94.33%
13	Validation loss: 0.458787	Best loss: 0.382562	Accuracy: 95.47%
14	Validation loss: 0.372016	Best loss: 0.372016	Accuracy: 95.86%
15	Validation loss: 0.354583	Best loss: 0.354583	Accuracy: 95.00%
16	Validation loss: 0.334237	Best loss: 0.334237	Accuracy: 95.47%
17	Validation loss: 0.249459	Best loss: 0.249459	Accuracy: 95.86%
18	Validation loss: 0.242793	Best loss: 0.242793	Accuracy: 95.58%
19	Validation loss: 0.293112	Best loss: 0.242793	Accuracy: 94.57%
20	Validation loss: 0.227606	Best loss: 0.227606	Accuracy: 95.90%
21	Validation loss: 0.242893	Best loss: 0.227606	Accuracy: 96.29%
22	Validation loss: 0.296214	Best loss: 0.227606	Accuracy: 94.53%
23	Validation loss: 0.185170	Best loss: 0.185170	Accuracy: 96.60%
24	Validation loss: 0.210694	Best loss: 0.185170	Accuracy: 96.21%
25	Validation loss: 0.214509	Best loss: 0.185170	Accuracy: 96.36%
26	Validation loss: 0.177129	Best loss: 0.177129	Accuracy: 96.79%
27	Validation loss: 0.260816	Best loss: 0.177129	Accuracy: 96.56%
28	Validation loss: 0.196197	Best loss: 0.177129	Accuracy: 97.03%
29	Validation loss: 0.163721	Best loss: 0.163721	Accuracy: 97.03%
30	Validation loss: 0.534718	Best loss: 0.163721	Accuracy: 95.15%
31	Validation loss: 0.648791	Best loss: 0.163721	Accuracy: 93.35%
32	Validation loss: 0.365446	Best loss: 0.163721	Accuracy: 95.62%
33	Validation loss: 0.495120	Best loss: 0.163721	Accuracy: 95.39%
34	Validation loss: 1.207311	Best loss: 0.163721	Accuracy: 87.96%
35	Validation loss: 0.499205	Best loss: 0.163721	Accuracy: 95.27%
36	Validation loss: 0.288098	Best loss: 0.163721	Accuracy: 96.76%
37	Validation loss: 0.201627	Best loss: 0.163721	Accuracy: 96.95%
38	Validation loss: 0.174820	Best loss: 0.163721	Accuracy: 96.72%
39	Validation loss: 0.160416	Best loss: 0.160416	Accuracy: 97.15%
40	Validation loss: 0.169714	Best loss: 0.160416	Accuracy: 97.38%
41	Validation loss: 0.189093	Best loss: 0.160416	Accuracy: 96.01%
42	Validation loss: 0.153054	Best loss: 0.153054	Accuracy: 97.34%
43	Validation loss: 0.136861	Best loss: 0.136861	Accuracy: 97.26%
44	Validation loss: 0.143145	Best loss: 0.136861	Accuracy: 97.38%
45	Validation loss: 0.143957	Best loss: 0.136861	Accuracy: 97.46%
46	Validation loss: 0.161872	Best loss: 0.136861	Accuracy: 97.30%
47	Validation loss: 0.147601	Best loss: 0.136861	Accuracy: 97.15%
48	Validation loss: 0.180302	Best loss: 0.136861	Accuracy: 96.95%
49	Validation loss: 0.159225	Best loss: 0.136861	Accuracy: 97.22%
50	Validation loss: 0.334898	Best loss: 0.136861	Accuracy: 95.70%
51	Validation loss: 0.200626	Best loss: 0.136861	Accuracy: 97.07%
52	Validation loss: 0.167544	Best loss: 0.136861	Accuracy: 97.07%
53	Validation loss: 0.167012	Best loss: 0.136861	Accuracy: 96.76%
54	Validation loss: 0.160786	Best loss: 0.136861	Accuracy: 97.22%
55	Validation loss: 0.150468	Best loss: 0.136861	Accuracy: 97.46%
56	Validation loss: 0.168388	Best loss: 0.136861	Accuracy: 97.22%
57	Validation loss: 0.147109	Best loss: 0.136861	Accuracy: 97.38%
58	Validation loss: 0.167807	Best loss: 0.136861	Accuracy: 97.19%
59	Validation loss: 0.151255	Best loss: 0.136861	Accuracy: 97.62%
60	Validation loss: 0.723379	Best loss: 0.136861	Accuracy: 90.85%
61	Validation loss: 79261728.000000	Best loss: 0.136861	Accuracy: 19.08%
62	Validation loss: 5634722.000000	Best loss: 0.136861	Accuracy: 29.98%
63	Validation loss: 193153.562500	Best loss: 0.136861	Accuracy: 78.69%
64	Validation loss: 89779.281250	Best loss: 0.136861	Accuracy: 86.98%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  14.2s
[CV] n_neurons=90, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 69.041710	Best loss: 69.041710	Accuracy: 49.65%
1	Validation loss: 37.849579	Best loss: 37.849579	Accuracy: 78.77%
2	Validation loss: 6.170990	Best loss: 6.170990	Accuracy: 89.87%
3	Validation loss: 2.502172	Best loss: 2.502172	Accuracy: 91.01%
4	Validation loss: 1.443635	Best loss: 1.443635	Accuracy: 92.30%
5	Validation loss: 1.600182	Best loss: 1.443635	Accuracy: 92.46%
6	Validation loss: 1.172397	Best loss: 1.172397	Accuracy: 93.00%
7	Validation loss: 0.990443	Best loss: 0.990443	Accuracy: 93.24%
8	Validation loss: 5.407336	Best loss: 0.990443	Accuracy: 91.16%
9	Validation loss: 1.909123	Best loss: 0.990443	Accuracy: 93.90%
10	Validation loss: 1.616194	Best loss: 0.990443	Accuracy: 93.12%
11	Validation loss: 0.977706	Best loss: 0.977706	Accuracy: 93.78%
12	Validation loss: 0.866807	Best loss: 0.866807	Accuracy: 95.27%
13	Validation loss: 0.961313	Best loss: 0.866807	Accuracy: 93.75%
14	Validation loss: 1.002676	Best loss: 0.866807	Accuracy: 95.39%
15	Validation loss: 0.698735	Best loss: 0.698735	Accuracy: 95.62%
16	Validation loss: 0.551926	Best loss: 0.551926	Accuracy: 95.39%
17	Validation loss: 0.566543	Best loss: 0.551926	Accuracy: 96.05%
18	Validation loss: 0.483141	Best loss: 0.483141	Accuracy: 96.25%
19	Validation loss: 0.726811	Best loss: 0.483141	Accuracy: 95.43%
20	Validation loss: 3.572718	Best loss: 0.483141	Accuracy: 87.88%
21	Validation loss: 1.406057	Best loss: 0.483141	Accuracy: 96.56%
22	Validation loss: 0.728345	Best loss: 0.483141	Accuracy: 96.25%
23	Validation loss: 0.660423	Best loss: 0.483141	Accuracy: 95.04%
24	Validation loss: 0.565331	Best loss: 0.483141	Accuracy: 96.60%
25	Validation loss: 0.501277	Best loss: 0.483141	Accuracy: 96.83%
26	Validation loss: 0.461284	Best loss: 0.461284	Accuracy: 96.87%
27	Validation loss: 0.405895	Best loss: 0.405895	Accuracy: 96.68%
28	Validation loss: 0.393800	Best loss: 0.393800	Accuracy: 96.72%
29	Validation loss: 0.415642	Best loss: 0.393800	Accuracy: 95.82%
30	Validation loss: 0.426454	Best loss: 0.393800	Accuracy: 97.15%
31	Validation loss: 0.409773	Best loss: 0.393800	Accuracy: 97.50%
32	Validation loss: 0.391382	Best loss: 0.391382	Accuracy: 97.38%
33	Validation loss: 0.709292	Best loss: 0.391382	Accuracy: 97.30%
34	Validation loss: 0.493149	Best loss: 0.391382	Accuracy: 97.03%
35	Validation loss: 0.490843	Best loss: 0.391382	Accuracy: 97.07%
36	Validation loss: 0.455118	Best loss: 0.391382	Accuracy: 97.50%
37	Validation loss: 0.498086	Best loss: 0.391382	Accuracy: 97.07%
38	Validation loss: 0.562876	Best loss: 0.391382	Accuracy: 97.19%
39	Validation loss: 0.512812	Best loss: 0.391382	Accuracy: 97.58%
40	Validation loss: 0.601767	Best loss: 0.391382	Accuracy: 97.65%
41	Validation loss: 0.652422	Best loss: 0.391382	Accuracy: 97.26%
42	Validation loss: 0.692773	Best loss: 0.391382	Accuracy: 97.69%
43	Validation loss: 0.692215	Best loss: 0.391382	Accuracy: 97.69%
44	Validation loss: 1.469405	Best loss: 0.391382	Accuracy: 97.11%
45	Validation loss: 2.409849	Best loss: 0.391382	Accuracy: 94.76%
46	Validation loss: 27.457972	Best loss: 0.391382	Accuracy: 92.03%
47	Validation loss: 33690032.000000	Best loss: 0.391382	Accuracy: 22.09%
48	Validation loss: 1187088.875000	Best loss: 0.391382	Accuracy: 51.06%
49	Validation loss: 103634.062500	Best loss: 0.391382	Accuracy: 69.59%
50	Validation loss: 51305.164062	Best loss: 0.391382	Accuracy: 72.05%
51	Validation loss: 18724.990234	Best loss: 0.391382	Accuracy: 84.68%
52	Validation loss: 14274.974609	Best loss: 0.391382	Accuracy: 84.99%
53	Validation loss: 13225.734375	Best loss: 0.391382	Accuracy: 81.70%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  11.0s
[CV] n_neurons=90, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt; 
0	Validation loss: 10191.711914	Best loss: 10191.711914	Accuracy: 18.73%
1	Validation loss: 243.165070	Best loss: 243.165070	Accuracy: 43.20%
2	Validation loss: 5.946767	Best loss: 5.946767	Accuracy: 82.56%
3	Validation loss: 2.152546	Best loss: 2.152546	Accuracy: 91.63%
4	Validation loss: 1.314367	Best loss: 1.314367	Accuracy: 92.49%
5	Validation loss: 1.597167	Best loss: 1.314367	Accuracy: 93.08%
6	Validation loss: 0.988802	Best loss: 0.988802	Accuracy: 93.08%
7	Validation loss: 0.857060	Best loss: 0.857060	Accuracy: 94.92%
8	Validation loss: 1.005038	Best loss: 0.857060	Accuracy: 93.59%
9	Validation loss: 0.896942	Best loss: 0.857060	Accuracy: 94.02%
10	Validation loss: 0.692364	Best loss: 0.692364	Accuracy: 95.47%
11	Validation loss: 0.792748	Best loss: 0.692364	Accuracy: 95.39%
12	Validation loss: 0.798815	Best loss: 0.692364	Accuracy: 94.25%
13	Validation loss: 2.168675	Best loss: 0.692364	Accuracy: 90.30%
14	Validation loss: 1.084360	Best loss: 0.692364	Accuracy: 96.25%
15	Validation loss: 0.857488	Best loss: 0.692364	Accuracy: 95.82%
16	Validation loss: 0.772651	Best loss: 0.692364	Accuracy: 96.17%
17	Validation loss: 0.677337	Best loss: 0.677337	Accuracy: 95.66%
18	Validation loss: 0.620409	Best loss: 0.620409	Accuracy: 96.52%
19	Validation loss: 0.556151	Best loss: 0.556151	Accuracy: 96.56%
20	Validation loss: 0.538992	Best loss: 0.538992	Accuracy: 96.52%
21	Validation loss: 0.558556	Best loss: 0.538992	Accuracy: 96.56%
22	Validation loss: 0.585911	Best loss: 0.538992	Accuracy: 96.33%
23	Validation loss: 0.523775	Best loss: 0.523775	Accuracy: 96.21%
24	Validation loss: 0.529410	Best loss: 0.523775	Accuracy: 96.33%
25	Validation loss: 0.562923	Best loss: 0.523775	Accuracy: 96.36%
26	Validation loss: 0.491024	Best loss: 0.491024	Accuracy: 96.25%
27	Validation loss: 0.587556	Best loss: 0.491024	Accuracy: 96.87%
28	Validation loss: 0.601852	Best loss: 0.491024	Accuracy: 96.79%
29	Validation loss: 0.551982	Best loss: 0.491024	Accuracy: 96.56%
30	Validation loss: 0.340183	Best loss: 0.340183	Accuracy: 95.35%
31	Validation loss: 0.265362	Best loss: 0.265362	Accuracy: 96.91%
32	Validation loss: 0.363118	Best loss: 0.265362	Accuracy: 96.64%
33	Validation loss: 0.428719	Best loss: 0.265362	Accuracy: 96.44%
34	Validation loss: 0.500404	Best loss: 0.265362	Accuracy: 96.05%
35	Validation loss: 0.553789	Best loss: 0.265362	Accuracy: 96.60%
36	Validation loss: 0.475037	Best loss: 0.265362	Accuracy: 96.48%
37	Validation loss: 0.557321	Best loss: 0.265362	Accuracy: 96.13%
38	Validation loss: 0.465553	Best loss: 0.265362	Accuracy: 95.78%
39	Validation loss: 0.516833	Best loss: 0.265362	Accuracy: 96.21%
40	Validation loss: 0.763628	Best loss: 0.265362	Accuracy: 92.92%
41	Validation loss: 0.349764	Best loss: 0.265362	Accuracy: 95.97%
42	Validation loss: 0.339455	Best loss: 0.265362	Accuracy: 97.42%
43	Validation loss: 0.525006	Best loss: 0.265362	Accuracy: 95.04%
44	Validation loss: 0.393755	Best loss: 0.265362	Accuracy: 97.15%
45	Validation loss: 0.345408	Best loss: 0.265362	Accuracy: 96.29%
46	Validation loss: 0.340202	Best loss: 0.265362	Accuracy: 97.34%
47	Validation loss: 0.407685	Best loss: 0.265362	Accuracy: 97.03%
48	Validation loss: 0.372787	Best loss: 0.265362	Accuracy: 96.87%
49	Validation loss: 0.316430	Best loss: 0.265362	Accuracy: 97.03%
50	Validation loss: 0.366160	Best loss: 0.265362	Accuracy: 97.42%
51	Validation loss: 0.400818	Best loss: 0.265362	Accuracy: 97.11%
52	Validation loss: 0.509595	Best loss: 0.265362	Accuracy: 96.99%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;, total=  11.3s
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 46.9min finished
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.069587	Best loss: 0.069587	Accuracy: 98.12%
1	Validation loss: 0.045462	Best loss: 0.045462	Accuracy: 98.48%
2	Validation loss: 0.046439	Best loss: 0.045462	Accuracy: 98.40%
3	Validation loss: 0.037278	Best loss: 0.037278	Accuracy: 98.59%
4	Validation loss: 0.039989	Best loss: 0.037278	Accuracy: 98.51%
5	Validation loss: 0.039621	Best loss: 0.037278	Accuracy: 98.79%
6	Validation loss: 0.035959	Best loss: 0.035959	Accuracy: 99.06%
7	Validation loss: 0.033321	Best loss: 0.033321	Accuracy: 99.06%
8	Validation loss: 0.044559	Best loss: 0.033321	Accuracy: 98.87%
9	Validation loss: 0.035999	Best loss: 0.033321	Accuracy: 99.10%
10	Validation loss: 0.042629	Best loss: 0.033321	Accuracy: 98.98%
11	Validation loss: 0.059839	Best loss: 0.033321	Accuracy: 98.71%
12	Validation loss: 0.044683	Best loss: 0.033321	Accuracy: 98.87%
13	Validation loss: 0.051294	Best loss: 0.033321	Accuracy: 98.75%
14	Validation loss: 0.050140	Best loss: 0.033321	Accuracy: 98.98%
15	Validation loss: 0.051109	Best loss: 0.033321	Accuracy: 98.79%
16	Validation loss: 0.072444	Best loss: 0.033321	Accuracy: 97.97%
17	Validation loss: 0.063308	Best loss: 0.033321	Accuracy: 98.71%
18	Validation loss: 0.051853	Best loss: 0.033321	Accuracy: 98.87%
19	Validation loss: 0.058982	Best loss: 0.033321	Accuracy: 98.91%
20	Validation loss: 0.046894	Best loss: 0.033321	Accuracy: 99.06%
21	Validation loss: 0.039036	Best loss: 0.033321	Accuracy: 99.02%
22	Validation loss: 0.057221	Best loss: 0.033321	Accuracy: 98.32%
23	Validation loss: 0.054618	Best loss: 0.033321	Accuracy: 98.75%
24	Validation loss: 0.039252	Best loss: 0.033321	Accuracy: 99.14%
25	Validation loss: 0.111809	Best loss: 0.033321	Accuracy: 98.05%
26	Validation loss: 0.060662	Best loss: 0.033321	Accuracy: 98.98%
27	Validation loss: 0.073774	Best loss: 0.033321	Accuracy: 99.02%
28	Validation loss: 0.048667	Best loss: 0.033321	Accuracy: 99.18%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[122]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>RandomizedSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;,
          estimator=DNNClassifier(activation=&lt;function elu at 0x1243639d8&gt;,
       batch_norm_momentum=None, batch_size=20, dropout_rate=None,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42),
          fit_params=None, iid=&#39;warn&#39;, n_iter=50, n_jobs=None,
          param_distributions={&#39;n_neurons&#39;: [10, 30, 50, 70, 90, 100, 120, 140, 160], &#39;batch_size&#39;: [10, 50, 100, 500], &#39;learning_rate&#39;: [0.01, 0.02, 0.05, 0.1], &#39;activation&#39;: [&lt;function relu at 0x124366d08&gt;, &lt;function elu at 0x1243639d8&gt;, &lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x133807c80&gt;, &lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13eb49f28&gt;]},
          pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True,
          return_train_score=&#39;warn&#39;, scoring=None, verbose=2)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[123]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnd_search</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[123]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;n_neurons&#39;: 90,
 &#39;learning_rate&#39;: 0.01,
 &#39;batch_size&#39;: 500,
 &#39;activation&#39;: &lt;function __main__.leaky_relu.&lt;locals&gt;.parametrized_leaky_relu(z, name=None)&gt;}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[124]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">rnd_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[124]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9891029383148473</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wonderful! Tuning the hyperparameters got us up to 98.91% accuracy! It may not sound like a great improvement to go from 97.26% to 98.91% accuracy, but consider the error rate: it went from roughly 2.6% to 1.1%. That's almost 60% reduction of the number of errors this model will produce!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's a good idea to save this model:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[125]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnd_search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;./my_best_mnist_model_0_to_4&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="8.4.">8.4.<a class="anchor-link" href="#8.4.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's train the best model found, once again, to see how fast it converges (alternatively, you could tweak the code above to make it write summaries for TensorBoard, so you can visualize the learning curve):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[126]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn_clf</span> <span class="o">=</span> <span class="n">DNNClassifier</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                        <span class="n">n_neurons</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dnn_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid1</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.083541	Best loss: 0.083541	Accuracy: 97.54%
1	Validation loss: 0.052198	Best loss: 0.052198	Accuracy: 98.40%
2	Validation loss: 0.044553	Best loss: 0.044553	Accuracy: 98.71%
3	Validation loss: 0.051113	Best loss: 0.044553	Accuracy: 98.48%
4	Validation loss: 0.046304	Best loss: 0.044553	Accuracy: 98.75%
5	Validation loss: 0.037796	Best loss: 0.037796	Accuracy: 98.91%
6	Validation loss: 0.048525	Best loss: 0.037796	Accuracy: 98.67%
7	Validation loss: 0.039877	Best loss: 0.037796	Accuracy: 98.75%
8	Validation loss: 0.038729	Best loss: 0.037796	Accuracy: 98.98%
9	Validation loss: 0.064167	Best loss: 0.037796	Accuracy: 98.24%
10	Validation loss: 0.057274	Best loss: 0.037796	Accuracy: 98.79%
11	Validation loss: 0.064388	Best loss: 0.037796	Accuracy: 98.55%
12	Validation loss: 0.056382	Best loss: 0.037796	Accuracy: 98.63%
13	Validation loss: 0.049408	Best loss: 0.037796	Accuracy: 98.91%
14	Validation loss: 0.038494	Best loss: 0.037796	Accuracy: 99.10%
15	Validation loss: 0.064619	Best loss: 0.037796	Accuracy: 98.67%
16	Validation loss: 0.055027	Best loss: 0.037796	Accuracy: 98.91%
17	Validation loss: 0.054773	Best loss: 0.037796	Accuracy: 98.91%
18	Validation loss: 0.076131	Best loss: 0.037796	Accuracy: 98.71%
19	Validation loss: 0.063031	Best loss: 0.037796	Accuracy: 98.59%
20	Validation loss: 0.120501	Best loss: 0.037796	Accuracy: 98.55%
21	Validation loss: 3.922006	Best loss: 0.037796	Accuracy: 94.14%
22	Validation loss: 0.395737	Best loss: 0.037796	Accuracy: 96.83%
23	Validation loss: 0.237014	Best loss: 0.037796	Accuracy: 96.56%
24	Validation loss: 0.159249	Best loss: 0.037796	Accuracy: 97.07%
25	Validation loss: 0.228444	Best loss: 0.037796	Accuracy: 95.74%
26	Validation loss: 0.134490	Best loss: 0.037796	Accuracy: 96.99%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[126]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>DNNClassifier(activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x13e25ea60&gt;,
       batch_norm_momentum=None, batch_size=500, dropout_rate=None,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=5, n_neurons=140,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The best loss is reached at epoch 5.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's check that we do indeed get 98.9% accuracy on the test set:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[127]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">dnn_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[127]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9898812998637867</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Good, now let's use the exact same model, but this time with batch normalization:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[128]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn_clf_bn</span> <span class="o">=</span> <span class="n">DNNClassifier</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                           <span class="n">n_neurons</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                           <span class="n">batch_norm_momentum</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">dnn_clf_bn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid1</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.046685	Best loss: 0.046685	Accuracy: 98.63%
1	Validation loss: 0.040820	Best loss: 0.040820	Accuracy: 98.79%
2	Validation loss: 0.046557	Best loss: 0.040820	Accuracy: 98.67%
3	Validation loss: 0.032236	Best loss: 0.032236	Accuracy: 98.94%
4	Validation loss: 0.056148	Best loss: 0.032236	Accuracy: 98.44%
5	Validation loss: 0.035988	Best loss: 0.032236	Accuracy: 98.98%
6	Validation loss: 0.037958	Best loss: 0.032236	Accuracy: 98.94%
7	Validation loss: 0.034588	Best loss: 0.032236	Accuracy: 99.02%
8	Validation loss: 0.031261	Best loss: 0.031261	Accuracy: 99.34%
9	Validation loss: 0.050791	Best loss: 0.031261	Accuracy: 98.79%
10	Validation loss: 0.035324	Best loss: 0.031261	Accuracy: 99.02%
11	Validation loss: 0.039875	Best loss: 0.031261	Accuracy: 98.98%
12	Validation loss: 0.048575	Best loss: 0.031261	Accuracy: 98.94%
13	Validation loss: 0.028059	Best loss: 0.028059	Accuracy: 99.18%
14	Validation loss: 0.044112	Best loss: 0.028059	Accuracy: 99.14%
15	Validation loss: 0.039050	Best loss: 0.028059	Accuracy: 99.22%
16	Validation loss: 0.033278	Best loss: 0.028059	Accuracy: 99.14%
17	Validation loss: 0.031734	Best loss: 0.028059	Accuracy: 99.18%
18	Validation loss: 0.034500	Best loss: 0.028059	Accuracy: 99.14%
19	Validation loss: 0.032757	Best loss: 0.028059	Accuracy: 99.26%
20	Validation loss: 0.023842	Best loss: 0.023842	Accuracy: 99.53%
21	Validation loss: 0.026727	Best loss: 0.023842	Accuracy: 99.41%
22	Validation loss: 0.027016	Best loss: 0.023842	Accuracy: 99.41%
23	Validation loss: 0.033038	Best loss: 0.023842	Accuracy: 99.34%
24	Validation loss: 0.035490	Best loss: 0.023842	Accuracy: 99.18%
25	Validation loss: 0.060346	Best loss: 0.023842	Accuracy: 98.75%
26	Validation loss: 0.051341	Best loss: 0.023842	Accuracy: 99.26%
27	Validation loss: 0.033108	Best loss: 0.023842	Accuracy: 99.26%
28	Validation loss: 0.042162	Best loss: 0.023842	Accuracy: 99.18%
29	Validation loss: 0.036313	Best loss: 0.023842	Accuracy: 99.26%
30	Validation loss: 0.033812	Best loss: 0.023842	Accuracy: 99.26%
31	Validation loss: 0.038173	Best loss: 0.023842	Accuracy: 99.26%
32	Validation loss: 0.029853	Best loss: 0.023842	Accuracy: 99.37%
33	Validation loss: 0.026557	Best loss: 0.023842	Accuracy: 99.37%
34	Validation loss: 0.035003	Best loss: 0.023842	Accuracy: 99.37%
35	Validation loss: 0.027140	Best loss: 0.023842	Accuracy: 99.34%
36	Validation loss: 0.038988	Best loss: 0.023842	Accuracy: 99.34%
37	Validation loss: 0.048149	Best loss: 0.023842	Accuracy: 98.98%
38	Validation loss: 0.049070	Best loss: 0.023842	Accuracy: 99.02%
39	Validation loss: 0.041233	Best loss: 0.023842	Accuracy: 99.26%
40	Validation loss: 0.038571	Best loss: 0.023842	Accuracy: 99.26%
41	Validation loss: 0.036886	Best loss: 0.023842	Accuracy: 99.34%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[128]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>DNNClassifier(activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14030a378&gt;,
       batch_norm_momentum=0.95, batch_size=500, dropout_rate=None,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The best params are reached during epoch 20, that's actually a slower convergence than earlier. Let's check the accuracy:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[129]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">dnn_clf_bn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[129]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9941622883829538</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great, batch normalization improved accuracy! Let's see if we can find a good set of hyperparameters that will work even better with batch normalization:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[130]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">param_distribs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_neurons&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="mi">160</span><span class="p">],</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)],</span>
    <span class="c1"># you could also try exploring different numbers of hidden layers, different optimizers, etc.</span>
    <span class="c1">#&quot;n_hidden_layers&quot;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],</span>
    <span class="c1">#&quot;optimizer_class&quot;: [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],</span>
    <span class="s2">&quot;batch_norm_momentum&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">rnd_search_bn</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">DNNClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">param_distribs</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                   <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rnd_search_bn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid1</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:</span>
<span class="c1"># fit_params = dict(X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)</span>
<span class="c1"># rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,</span>
<span class="c1">#                                    fit_params=fit_params, random_state=42, verbose=2)</span>
<span class="c1"># rnd_search_bn.fit(X_train1, y_train1)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 3 folds for each of 50 candidates, totalling 150 fits
[CV] n_neurons=70, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt; 
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.098522	Best loss: 0.098522	Accuracy: 97.81%
1	Validation loss: 0.080233	Best loss: 0.080233	Accuracy: 98.08%
2	Validation loss: 0.068767	Best loss: 0.068767	Accuracy: 98.01%
3	Validation loss: 0.057095	Best loss: 0.057095	Accuracy: 98.28%
4	Validation loss: 0.067008	Best loss: 0.057095	Accuracy: 98.12%
5	Validation loss: 0.058910	Best loss: 0.057095	Accuracy: 98.55%
6	Validation loss: 0.038421	Best loss: 0.038421	Accuracy: 98.91%
7	Validation loss: 0.071075	Best loss: 0.038421	Accuracy: 98.36%
8	Validation loss: 0.063073	Best loss: 0.038421	Accuracy: 98.28%
9	Validation loss: 0.057488	Best loss: 0.038421	Accuracy: 98.75%
10	Validation loss: 0.049557	Best loss: 0.038421	Accuracy: 98.75%
11	Validation loss: 0.039810	Best loss: 0.038421	Accuracy: 99.06%
12	Validation loss: 0.061837	Best loss: 0.038421	Accuracy: 98.55%
13	Validation loss: 0.062008	Best loss: 0.038421	Accuracy: 98.51%
14	Validation loss: 0.075937	Best loss: 0.038421	Accuracy: 98.44%
15	Validation loss: 0.053910	Best loss: 0.038421	Accuracy: 98.71%
16	Validation loss: 0.051419	Best loss: 0.038421	Accuracy: 98.94%
17	Validation loss: 0.049013	Best loss: 0.038421	Accuracy: 98.98%
18	Validation loss: 0.048979	Best loss: 0.038421	Accuracy: 99.10%
19	Validation loss: 0.058969	Best loss: 0.038421	Accuracy: 98.59%
20	Validation loss: 0.060048	Best loss: 0.038421	Accuracy: 98.79%
21	Validation loss: 0.088256	Best loss: 0.038421	Accuracy: 98.32%
22	Validation loss: 0.055535	Best loss: 0.038421	Accuracy: 98.59%
23	Validation loss: 0.054632	Best loss: 0.038421	Accuracy: 98.94%
24	Validation loss: 0.092021	Best loss: 0.038421	Accuracy: 98.20%
25	Validation loss: 0.042263	Best loss: 0.038421	Accuracy: 99.02%
26	Validation loss: 0.041139	Best loss: 0.038421	Accuracy: 99.30%
27	Validation loss: 0.054255	Best loss: 0.038421	Accuracy: 99.06%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt;, total=  39.0s
[CV] n_neurons=70, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt; 
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.1s remaining:    0.0s
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.123638	Best loss: 0.123638	Accuracy: 96.68%
1	Validation loss: 0.048079	Best loss: 0.048079	Accuracy: 98.55%
2	Validation loss: 0.082311	Best loss: 0.048079	Accuracy: 97.58%
3	Validation loss: 0.059053	Best loss: 0.048079	Accuracy: 98.28%
4	Validation loss: 0.056156	Best loss: 0.048079	Accuracy: 98.59%
5	Validation loss: 0.071631	Best loss: 0.048079	Accuracy: 98.36%
6	Validation loss: 0.044482	Best loss: 0.044482	Accuracy: 98.79%
7	Validation loss: 0.049969	Best loss: 0.044482	Accuracy: 98.67%
8	Validation loss: 0.053734	Best loss: 0.044482	Accuracy: 98.87%
9	Validation loss: 0.057046	Best loss: 0.044482	Accuracy: 98.40%
10	Validation loss: 0.059210	Best loss: 0.044482	Accuracy: 98.44%
11	Validation loss: 0.044404	Best loss: 0.044404	Accuracy: 98.83%
12	Validation loss: 0.048075	Best loss: 0.044404	Accuracy: 98.83%
13	Validation loss: 0.041242	Best loss: 0.041242	Accuracy: 98.87%
14	Validation loss: 0.037449	Best loss: 0.037449	Accuracy: 98.71%
15	Validation loss: 0.059479	Best loss: 0.037449	Accuracy: 98.71%
16	Validation loss: 0.054563	Best loss: 0.037449	Accuracy: 98.87%
17	Validation loss: 0.040756	Best loss: 0.037449	Accuracy: 98.91%
18	Validation loss: 0.055704	Best loss: 0.037449	Accuracy: 98.59%
19	Validation loss: 0.057695	Best loss: 0.037449	Accuracy: 98.71%
20	Validation loss: 0.039648	Best loss: 0.037449	Accuracy: 98.98%
21	Validation loss: 0.056484	Best loss: 0.037449	Accuracy: 98.91%
22	Validation loss: 0.064908	Best loss: 0.037449	Accuracy: 98.63%
23	Validation loss: 0.071069	Best loss: 0.037449	Accuracy: 98.40%
24	Validation loss: 0.053500	Best loss: 0.037449	Accuracy: 98.83%
25	Validation loss: 0.048135	Best loss: 0.037449	Accuracy: 98.87%
26	Validation loss: 0.039053	Best loss: 0.037449	Accuracy: 98.98%
27	Validation loss: 0.046941	Best loss: 0.037449	Accuracy: 99.22%
28	Validation loss: 0.048853	Best loss: 0.037449	Accuracy: 99.02%
29	Validation loss: 0.038146	Best loss: 0.037449	Accuracy: 99.02%
30	Validation loss: 0.040098	Best loss: 0.037449	Accuracy: 99.02%
31	Validation loss: 0.044647	Best loss: 0.037449	Accuracy: 98.98%
32	Validation loss: 0.049060	Best loss: 0.037449	Accuracy: 98.83%
33	Validation loss: 0.047708	Best loss: 0.037449	Accuracy: 98.91%
34	Validation loss: 0.041760	Best loss: 0.037449	Accuracy: 99.34%
35	Validation loss: 0.051382	Best loss: 0.037449	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt;, total=  49.8s
[CV] n_neurons=70, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.069197	Best loss: 0.069197	Accuracy: 98.44%
1	Validation loss: 0.058152	Best loss: 0.058152	Accuracy: 98.48%
2	Validation loss: 0.061270	Best loss: 0.058152	Accuracy: 98.28%
3	Validation loss: 0.036191	Best loss: 0.036191	Accuracy: 98.71%
4	Validation loss: 0.031603	Best loss: 0.031603	Accuracy: 98.98%
5	Validation loss: 0.077961	Best loss: 0.031603	Accuracy: 97.77%
6	Validation loss: 0.052401	Best loss: 0.031603	Accuracy: 98.44%
7	Validation loss: 0.058968	Best loss: 0.031603	Accuracy: 98.55%
8	Validation loss: 0.043009	Best loss: 0.031603	Accuracy: 98.79%
9	Validation loss: 0.039525	Best loss: 0.031603	Accuracy: 98.98%
10	Validation loss: 0.044521	Best loss: 0.031603	Accuracy: 98.79%
11	Validation loss: 0.036137	Best loss: 0.031603	Accuracy: 99.26%
12	Validation loss: 0.043276	Best loss: 0.031603	Accuracy: 98.83%
13	Validation loss: 0.035167	Best loss: 0.031603	Accuracy: 99.02%
14	Validation loss: 0.037344	Best loss: 0.031603	Accuracy: 99.06%
15	Validation loss: 0.044321	Best loss: 0.031603	Accuracy: 98.75%
16	Validation loss: 0.037237	Best loss: 0.031603	Accuracy: 99.18%
17	Validation loss: 0.045287	Best loss: 0.031603	Accuracy: 98.83%
18	Validation loss: 0.045312	Best loss: 0.031603	Accuracy: 98.94%
19	Validation loss: 0.064129	Best loss: 0.031603	Accuracy: 98.94%
20	Validation loss: 0.048898	Best loss: 0.031603	Accuracy: 98.55%
21	Validation loss: 0.041875	Best loss: 0.031603	Accuracy: 98.98%
22	Validation loss: 0.035282	Best loss: 0.031603	Accuracy: 99.18%
23	Validation loss: 0.038116	Best loss: 0.031603	Accuracy: 99.02%
24	Validation loss: 0.044614	Best loss: 0.031603	Accuracy: 98.94%
25	Validation loss: 0.032476	Best loss: 0.031603	Accuracy: 98.98%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt;, total=  36.1s
[CV] n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.112992	Best loss: 0.112992	Accuracy: 96.76%
1	Validation loss: 0.095698	Best loss: 0.095698	Accuracy: 96.83%
2	Validation loss: 0.060505	Best loss: 0.060505	Accuracy: 98.12%
3	Validation loss: 0.065914	Best loss: 0.060505	Accuracy: 98.08%
4	Validation loss: 0.057185	Best loss: 0.057185	Accuracy: 98.16%
5	Validation loss: 0.050672	Best loss: 0.050672	Accuracy: 98.59%
6	Validation loss: 0.052624	Best loss: 0.050672	Accuracy: 98.28%
7	Validation loss: 0.061267	Best loss: 0.050672	Accuracy: 98.40%
8	Validation loss: 0.043540	Best loss: 0.043540	Accuracy: 98.71%
9	Validation loss: 0.049360	Best loss: 0.043540	Accuracy: 98.59%
10	Validation loss: 0.051714	Best loss: 0.043540	Accuracy: 98.55%
11	Validation loss: 0.050444	Best loss: 0.043540	Accuracy: 98.32%
12	Validation loss: 0.040451	Best loss: 0.040451	Accuracy: 98.87%
13	Validation loss: 0.048953	Best loss: 0.040451	Accuracy: 98.75%
14	Validation loss: 0.058870	Best loss: 0.040451	Accuracy: 98.16%
15	Validation loss: 0.039579	Best loss: 0.039579	Accuracy: 98.79%
16	Validation loss: 0.040951	Best loss: 0.039579	Accuracy: 98.91%
17	Validation loss: 0.035358	Best loss: 0.035358	Accuracy: 99.02%
18	Validation loss: 0.042577	Best loss: 0.035358	Accuracy: 98.87%
19	Validation loss: 0.032532	Best loss: 0.032532	Accuracy: 98.87%
20	Validation loss: 0.046544	Best loss: 0.032532	Accuracy: 98.71%
21	Validation loss: 0.043771	Best loss: 0.032532	Accuracy: 98.91%
22	Validation loss: 0.043396	Best loss: 0.032532	Accuracy: 98.71%
23	Validation loss: 0.048832	Best loss: 0.032532	Accuracy: 98.71%
24	Validation loss: 0.064032	Best loss: 0.032532	Accuracy: 98.44%
25	Validation loss: 0.040126	Best loss: 0.032532	Accuracy: 98.83%
26	Validation loss: 0.048920	Best loss: 0.032532	Accuracy: 98.91%
27	Validation loss: 0.053214	Best loss: 0.032532	Accuracy: 98.71%
28	Validation loss: 0.050188	Best loss: 0.032532	Accuracy: 98.83%
29	Validation loss: 0.051491	Best loss: 0.032532	Accuracy: 98.63%
30	Validation loss: 0.035190	Best loss: 0.032532	Accuracy: 99.14%
31	Validation loss: 0.046421	Best loss: 0.032532	Accuracy: 98.63%
32	Validation loss: 0.042582	Best loss: 0.032532	Accuracy: 98.79%
33	Validation loss: 0.047291	Best loss: 0.032532	Accuracy: 98.87%
34	Validation loss: 0.040986	Best loss: 0.032532	Accuracy: 99.06%
35	Validation loss: 0.052942	Best loss: 0.032532	Accuracy: 98.44%
36	Validation loss: 0.044477	Best loss: 0.032532	Accuracy: 98.94%
37	Validation loss: 0.056184	Best loss: 0.032532	Accuracy: 98.48%
38	Validation loss: 0.044237	Best loss: 0.032532	Accuracy: 98.83%
39	Validation loss: 0.045182	Best loss: 0.032532	Accuracy: 98.91%
40	Validation loss: 0.054388	Best loss: 0.032532	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 3.7min
[CV] n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.095060	Best loss: 0.095060	Accuracy: 97.11%
1	Validation loss: 0.067804	Best loss: 0.067804	Accuracy: 98.01%
2	Validation loss: 0.060333	Best loss: 0.060333	Accuracy: 98.12%
3	Validation loss: 0.047293	Best loss: 0.047293	Accuracy: 98.67%
4	Validation loss: 0.048011	Best loss: 0.047293	Accuracy: 98.44%
5	Validation loss: 0.044941	Best loss: 0.044941	Accuracy: 98.51%
6	Validation loss: 0.048827	Best loss: 0.044941	Accuracy: 98.71%
7	Validation loss: 0.057416	Best loss: 0.044941	Accuracy: 98.48%
8	Validation loss: 0.044532	Best loss: 0.044532	Accuracy: 98.67%
9	Validation loss: 0.048548	Best loss: 0.044532	Accuracy: 98.63%
10	Validation loss: 0.042116	Best loss: 0.042116	Accuracy: 98.83%
11	Validation loss: 0.037085	Best loss: 0.037085	Accuracy: 98.79%
12	Validation loss: 0.036663	Best loss: 0.036663	Accuracy: 98.59%
13	Validation loss: 0.053090	Best loss: 0.036663	Accuracy: 98.71%
14	Validation loss: 0.037770	Best loss: 0.036663	Accuracy: 98.71%
15	Validation loss: 0.042717	Best loss: 0.036663	Accuracy: 98.63%
16	Validation loss: 0.082460	Best loss: 0.036663	Accuracy: 97.54%
17	Validation loss: 0.045501	Best loss: 0.036663	Accuracy: 98.94%
18	Validation loss: 0.034169	Best loss: 0.034169	Accuracy: 98.91%
19	Validation loss: 0.048191	Best loss: 0.034169	Accuracy: 98.75%
20	Validation loss: 0.035344	Best loss: 0.034169	Accuracy: 98.83%
21	Validation loss: 0.045509	Best loss: 0.034169	Accuracy: 99.06%
22	Validation loss: 0.041123	Best loss: 0.034169	Accuracy: 98.91%
23	Validation loss: 0.045400	Best loss: 0.034169	Accuracy: 98.71%
24	Validation loss: 0.046546	Best loss: 0.034169	Accuracy: 98.75%
25	Validation loss: 0.036283	Best loss: 0.034169	Accuracy: 99.10%
26	Validation loss: 0.038149	Best loss: 0.034169	Accuracy: 99.10%
27	Validation loss: 0.030796	Best loss: 0.030796	Accuracy: 99.06%
28	Validation loss: 0.036487	Best loss: 0.030796	Accuracy: 99.10%
29	Validation loss: 0.047860	Best loss: 0.030796	Accuracy: 98.91%
30	Validation loss: 0.048839	Best loss: 0.030796	Accuracy: 98.63%
31	Validation loss: 0.044899	Best loss: 0.030796	Accuracy: 98.87%
32	Validation loss: 0.039380	Best loss: 0.030796	Accuracy: 99.06%
33	Validation loss: 0.032126	Best loss: 0.030796	Accuracy: 99.14%
34	Validation loss: 0.037027	Best loss: 0.030796	Accuracy: 98.94%
35	Validation loss: 0.038286	Best loss: 0.030796	Accuracy: 98.83%
36	Validation loss: 0.042560	Best loss: 0.030796	Accuracy: 99.02%
37	Validation loss: 0.041549	Best loss: 0.030796	Accuracy: 99.02%
38	Validation loss: 0.041030	Best loss: 0.030796	Accuracy: 98.91%
39	Validation loss: 0.028827	Best loss: 0.028827	Accuracy: 99.34%
40	Validation loss: 0.043447	Best loss: 0.028827	Accuracy: 98.91%
41	Validation loss: 0.040127	Best loss: 0.028827	Accuracy: 98.98%
42	Validation loss: 0.051053	Best loss: 0.028827	Accuracy: 98.87%
43	Validation loss: 0.049610	Best loss: 0.028827	Accuracy: 98.91%
44	Validation loss: 0.039013	Best loss: 0.028827	Accuracy: 99.26%
45	Validation loss: 0.043108	Best loss: 0.028827	Accuracy: 98.75%
46	Validation loss: 0.043585	Best loss: 0.028827	Accuracy: 98.91%
47	Validation loss: 0.048468	Best loss: 0.028827	Accuracy: 98.79%
48	Validation loss: 0.037927	Best loss: 0.028827	Accuracy: 98.91%
49	Validation loss: 0.043057	Best loss: 0.028827	Accuracy: 99.02%
50	Validation loss: 0.031451	Best loss: 0.028827	Accuracy: 99.02%
51	Validation loss: 0.033046	Best loss: 0.028827	Accuracy: 99.14%
52	Validation loss: 0.037844	Best loss: 0.028827	Accuracy: 99.14%
53	Validation loss: 0.036293	Best loss: 0.028827	Accuracy: 98.98%
54	Validation loss: 0.044120	Best loss: 0.028827	Accuracy: 98.94%
55	Validation loss: 0.034515	Best loss: 0.028827	Accuracy: 99.18%
56	Validation loss: 0.031506	Best loss: 0.028827	Accuracy: 98.94%
57	Validation loss: 0.044020	Best loss: 0.028827	Accuracy: 98.98%
58	Validation loss: 0.046666	Best loss: 0.028827	Accuracy: 99.02%
59	Validation loss: 0.046428	Best loss: 0.028827	Accuracy: 98.79%
60	Validation loss: 0.036624	Best loss: 0.028827	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 5.5min
[CV] n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.094407	Best loss: 0.094407	Accuracy: 97.62%
1	Validation loss: 0.060873	Best loss: 0.060873	Accuracy: 98.28%
2	Validation loss: 0.090122	Best loss: 0.060873	Accuracy: 97.19%
3	Validation loss: 0.062084	Best loss: 0.060873	Accuracy: 97.81%
4	Validation loss: 0.069160	Best loss: 0.060873	Accuracy: 97.65%
5	Validation loss: 0.068229	Best loss: 0.060873	Accuracy: 98.01%
6	Validation loss: 0.061443	Best loss: 0.060873	Accuracy: 97.81%
7	Validation loss: 0.055139	Best loss: 0.055139	Accuracy: 98.16%
8	Validation loss: 0.039724	Best loss: 0.039724	Accuracy: 98.59%
9	Validation loss: 0.041235	Best loss: 0.039724	Accuracy: 98.71%
10	Validation loss: 0.056987	Best loss: 0.039724	Accuracy: 98.40%
11	Validation loss: 0.043604	Best loss: 0.039724	Accuracy: 98.55%
12	Validation loss: 0.041433	Best loss: 0.039724	Accuracy: 98.63%
13	Validation loss: 0.045753	Best loss: 0.039724	Accuracy: 98.59%
14	Validation loss: 0.048135	Best loss: 0.039724	Accuracy: 98.71%
15	Validation loss: 0.052062	Best loss: 0.039724	Accuracy: 98.51%
16	Validation loss: 0.033063	Best loss: 0.033063	Accuracy: 98.83%
17	Validation loss: 0.040737	Best loss: 0.033063	Accuracy: 98.79%
18	Validation loss: 0.045784	Best loss: 0.033063	Accuracy: 98.63%
19	Validation loss: 0.039696	Best loss: 0.033063	Accuracy: 98.55%
20	Validation loss: 0.035677	Best loss: 0.033063	Accuracy: 98.94%
21	Validation loss: 0.039575	Best loss: 0.033063	Accuracy: 98.79%
22	Validation loss: 0.047383	Best loss: 0.033063	Accuracy: 98.59%
23	Validation loss: 0.049944	Best loss: 0.033063	Accuracy: 98.67%
24	Validation loss: 0.028832	Best loss: 0.028832	Accuracy: 99.18%
25	Validation loss: 0.029948	Best loss: 0.028832	Accuracy: 99.10%
26	Validation loss: 0.037231	Best loss: 0.028832	Accuracy: 98.94%
27	Validation loss: 0.044429	Best loss: 0.028832	Accuracy: 98.94%
28	Validation loss: 0.034070	Best loss: 0.028832	Accuracy: 98.94%
29	Validation loss: 0.042515	Best loss: 0.028832	Accuracy: 98.59%
30	Validation loss: 0.063137	Best loss: 0.028832	Accuracy: 98.44%
31	Validation loss: 0.039266	Best loss: 0.028832	Accuracy: 98.87%
32	Validation loss: 0.038507	Best loss: 0.028832	Accuracy: 98.98%
33	Validation loss: 0.038770	Best loss: 0.028832	Accuracy: 98.87%
34	Validation loss: 0.045927	Best loss: 0.028832	Accuracy: 98.63%
35	Validation loss: 0.039206	Best loss: 0.028832	Accuracy: 98.94%
36	Validation loss: 0.032092	Best loss: 0.028832	Accuracy: 98.94%
37	Validation loss: 0.044294	Best loss: 0.028832	Accuracy: 98.87%
38	Validation loss: 0.037682	Best loss: 0.028832	Accuracy: 98.98%
39	Validation loss: 0.047856	Best loss: 0.028832	Accuracy: 98.94%
40	Validation loss: 0.045665	Best loss: 0.028832	Accuracy: 98.63%
41	Validation loss: 0.048828	Best loss: 0.028832	Accuracy: 98.91%
42	Validation loss: 0.036557	Best loss: 0.028832	Accuracy: 98.87%
43	Validation loss: 0.035819	Best loss: 0.028832	Accuracy: 99.18%
44	Validation loss: 0.050968	Best loss: 0.028832	Accuracy: 98.55%
45	Validation loss: 0.036922	Best loss: 0.028832	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 4.1min
[CV] n_neurons=30, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.075577	Best loss: 0.075577	Accuracy: 97.38%
1	Validation loss: 0.062396	Best loss: 0.062396	Accuracy: 97.97%
2	Validation loss: 0.051663	Best loss: 0.051663	Accuracy: 98.48%
3	Validation loss: 0.043757	Best loss: 0.043757	Accuracy: 98.59%
4	Validation loss: 0.041803	Best loss: 0.041803	Accuracy: 98.75%
5	Validation loss: 0.055648	Best loss: 0.041803	Accuracy: 98.51%
6	Validation loss: 0.045646	Best loss: 0.041803	Accuracy: 98.75%
7	Validation loss: 0.044091	Best loss: 0.041803	Accuracy: 98.83%
8	Validation loss: 0.057110	Best loss: 0.041803	Accuracy: 98.59%
9	Validation loss: 0.065077	Best loss: 0.041803	Accuracy: 98.55%
10	Validation loss: 0.061184	Best loss: 0.041803	Accuracy: 98.55%
11	Validation loss: 0.055219	Best loss: 0.041803	Accuracy: 98.59%
12	Validation loss: 0.040919	Best loss: 0.040919	Accuracy: 98.98%
13	Validation loss: 0.042747	Best loss: 0.040919	Accuracy: 98.87%
14	Validation loss: 0.056631	Best loss: 0.040919	Accuracy: 98.55%
15	Validation loss: 0.057617	Best loss: 0.040919	Accuracy: 98.55%
16	Validation loss: 0.057728	Best loss: 0.040919	Accuracy: 98.55%
17	Validation loss: 0.051721	Best loss: 0.040919	Accuracy: 98.63%
18	Validation loss: 0.074713	Best loss: 0.040919	Accuracy: 98.36%
19	Validation loss: 0.043760	Best loss: 0.040919	Accuracy: 98.67%
20	Validation loss: 0.051259	Best loss: 0.040919	Accuracy: 98.83%
21	Validation loss: 0.060851	Best loss: 0.040919	Accuracy: 98.87%
22	Validation loss: 0.048177	Best loss: 0.040919	Accuracy: 99.06%
23	Validation loss: 0.045889	Best loss: 0.040919	Accuracy: 98.98%
24	Validation loss: 0.044008	Best loss: 0.040919	Accuracy: 98.98%
25	Validation loss: 0.063886	Best loss: 0.040919	Accuracy: 98.67%
26	Validation loss: 0.044332	Best loss: 0.040919	Accuracy: 98.91%
27	Validation loss: 0.044943	Best loss: 0.040919	Accuracy: 98.83%
28	Validation loss: 0.047196	Best loss: 0.040919	Accuracy: 98.98%
29	Validation loss: 0.064629	Best loss: 0.040919	Accuracy: 98.48%
30	Validation loss: 0.062239	Best loss: 0.040919	Accuracy: 98.48%
31	Validation loss: 0.058487	Best loss: 0.040919	Accuracy: 98.75%
32	Validation loss: 0.065908	Best loss: 0.040919	Accuracy: 98.75%
33	Validation loss: 0.048411	Best loss: 0.040919	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  24.7s
[CV] n_neurons=30, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.121854	Best loss: 0.121854	Accuracy: 96.52%
1	Validation loss: 0.058933	Best loss: 0.058933	Accuracy: 98.36%
2	Validation loss: 0.050318	Best loss: 0.050318	Accuracy: 98.40%
3	Validation loss: 0.057457	Best loss: 0.050318	Accuracy: 98.28%
4	Validation loss: 0.064028	Best loss: 0.050318	Accuracy: 98.08%
5	Validation loss: 0.052065	Best loss: 0.050318	Accuracy: 98.44%
6	Validation loss: 0.051847	Best loss: 0.050318	Accuracy: 98.28%
7	Validation loss: 0.054232	Best loss: 0.050318	Accuracy: 98.32%
8	Validation loss: 0.051929	Best loss: 0.050318	Accuracy: 98.55%
9	Validation loss: 0.057670	Best loss: 0.050318	Accuracy: 98.48%
10	Validation loss: 0.055719	Best loss: 0.050318	Accuracy: 98.71%
11	Validation loss: 0.047884	Best loss: 0.047884	Accuracy: 98.71%
12	Validation loss: 0.048575	Best loss: 0.047884	Accuracy: 98.83%
13	Validation loss: 0.052868	Best loss: 0.047884	Accuracy: 98.36%
14	Validation loss: 0.055078	Best loss: 0.047884	Accuracy: 98.44%
15	Validation loss: 0.051848	Best loss: 0.047884	Accuracy: 98.94%
16	Validation loss: 0.046497	Best loss: 0.046497	Accuracy: 98.59%
17	Validation loss: 0.066573	Best loss: 0.046497	Accuracy: 98.48%
18	Validation loss: 0.045055	Best loss: 0.045055	Accuracy: 98.91%
19	Validation loss: 0.050389	Best loss: 0.045055	Accuracy: 98.75%
20	Validation loss: 0.046288	Best loss: 0.045055	Accuracy: 98.98%
21	Validation loss: 0.068536	Best loss: 0.045055	Accuracy: 98.79%
22	Validation loss: 0.048695	Best loss: 0.045055	Accuracy: 98.87%
23	Validation loss: 0.047164	Best loss: 0.045055	Accuracy: 99.10%
24	Validation loss: 0.053349	Best loss: 0.045055	Accuracy: 98.79%
25	Validation loss: 0.057947	Best loss: 0.045055	Accuracy: 98.79%
26	Validation loss: 0.057024	Best loss: 0.045055	Accuracy: 98.79%
27	Validation loss: 0.048875	Best loss: 0.045055	Accuracy: 98.94%
28	Validation loss: 0.069255	Best loss: 0.045055	Accuracy: 98.40%
29	Validation loss: 0.064697	Best loss: 0.045055	Accuracy: 98.75%
30	Validation loss: 0.054623	Best loss: 0.045055	Accuracy: 98.83%
31	Validation loss: 0.061774	Best loss: 0.045055	Accuracy: 98.87%
32	Validation loss: 0.041178	Best loss: 0.041178	Accuracy: 99.06%
33	Validation loss: 0.060911	Best loss: 0.041178	Accuracy: 98.67%
34	Validation loss: 0.067028	Best loss: 0.041178	Accuracy: 98.83%
35	Validation loss: 0.070188	Best loss: 0.041178	Accuracy: 98.79%
36	Validation loss: 0.057755	Best loss: 0.041178	Accuracy: 98.71%
37	Validation loss: 0.056854	Best loss: 0.041178	Accuracy: 98.83%
38	Validation loss: 0.071256	Best loss: 0.041178	Accuracy: 98.79%
39	Validation loss: 0.068662	Best loss: 0.041178	Accuracy: 98.91%
40	Validation loss: 0.059352	Best loss: 0.041178	Accuracy: 98.98%
41	Validation loss: 0.056795	Best loss: 0.041178	Accuracy: 98.98%
42	Validation loss: 0.056563	Best loss: 0.041178	Accuracy: 98.98%
43	Validation loss: 0.063149	Best loss: 0.041178	Accuracy: 98.67%
44	Validation loss: 0.061991	Best loss: 0.041178	Accuracy: 99.02%
45	Validation loss: 0.055710	Best loss: 0.041178	Accuracy: 99.02%
46	Validation loss: 0.053873	Best loss: 0.041178	Accuracy: 98.91%
47	Validation loss: 0.051659	Best loss: 0.041178	Accuracy: 99.10%
48	Validation loss: 0.060397	Best loss: 0.041178	Accuracy: 98.87%
49	Validation loss: 0.061378	Best loss: 0.041178	Accuracy: 98.94%
50	Validation loss: 0.061041	Best loss: 0.041178	Accuracy: 98.44%
51	Validation loss: 0.053692	Best loss: 0.041178	Accuracy: 98.87%
52	Validation loss: 0.067956	Best loss: 0.041178	Accuracy: 98.87%
53	Validation loss: 0.063130	Best loss: 0.041178	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  38.4s
[CV] n_neurons=30, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.077536	Best loss: 0.077536	Accuracy: 97.89%
1	Validation loss: 0.066242	Best loss: 0.066242	Accuracy: 97.85%
2	Validation loss: 0.051722	Best loss: 0.051722	Accuracy: 98.20%
3	Validation loss: 0.051092	Best loss: 0.051092	Accuracy: 98.28%
4	Validation loss: 0.039609	Best loss: 0.039609	Accuracy: 98.91%
5	Validation loss: 0.047897	Best loss: 0.039609	Accuracy: 98.63%
6	Validation loss: 0.042889	Best loss: 0.039609	Accuracy: 98.67%
7	Validation loss: 0.041629	Best loss: 0.039609	Accuracy: 98.79%
8	Validation loss: 0.034590	Best loss: 0.034590	Accuracy: 98.94%
9	Validation loss: 0.037380	Best loss: 0.034590	Accuracy: 98.87%
10	Validation loss: 0.038298	Best loss: 0.034590	Accuracy: 98.91%
11	Validation loss: 0.044107	Best loss: 0.034590	Accuracy: 98.83%
12	Validation loss: 0.045985	Best loss: 0.034590	Accuracy: 98.67%
13	Validation loss: 0.063342	Best loss: 0.034590	Accuracy: 98.24%
14	Validation loss: 0.067029	Best loss: 0.034590	Accuracy: 98.51%
15	Validation loss: 0.057151	Best loss: 0.034590	Accuracy: 98.59%
16	Validation loss: 0.045966	Best loss: 0.034590	Accuracy: 98.87%
17	Validation loss: 0.049625	Best loss: 0.034590	Accuracy: 98.75%
18	Validation loss: 0.040478	Best loss: 0.034590	Accuracy: 98.98%
19	Validation loss: 0.042837	Best loss: 0.034590	Accuracy: 98.79%
20	Validation loss: 0.045011	Best loss: 0.034590	Accuracy: 98.87%
21	Validation loss: 0.040026	Best loss: 0.034590	Accuracy: 98.98%
22	Validation loss: 0.038993	Best loss: 0.034590	Accuracy: 99.14%
23	Validation loss: 0.048820	Best loss: 0.034590	Accuracy: 98.67%
24	Validation loss: 0.047246	Best loss: 0.034590	Accuracy: 98.67%
25	Validation loss: 0.037580	Best loss: 0.034590	Accuracy: 99.22%
26	Validation loss: 0.036335	Best loss: 0.034590	Accuracy: 99.02%
27	Validation loss: 0.039998	Best loss: 0.034590	Accuracy: 98.91%
28	Validation loss: 0.043946	Best loss: 0.034590	Accuracy: 98.98%
29	Validation loss: 0.042086	Best loss: 0.034590	Accuracy: 99.14%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  22.3s
[CV] n_neurons=160, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.106572	Best loss: 0.106572	Accuracy: 96.99%
1	Validation loss: 0.108748	Best loss: 0.106572	Accuracy: 96.76%
2	Validation loss: 0.057375	Best loss: 0.057375	Accuracy: 98.20%
3	Validation loss: 0.063955	Best loss: 0.057375	Accuracy: 98.01%
4	Validation loss: 0.051913	Best loss: 0.051913	Accuracy: 98.51%
5	Validation loss: 0.068872	Best loss: 0.051913	Accuracy: 97.97%
6	Validation loss: 0.059696	Best loss: 0.051913	Accuracy: 98.05%
7	Validation loss: 0.037091	Best loss: 0.037091	Accuracy: 98.98%
8	Validation loss: 0.042413	Best loss: 0.037091	Accuracy: 98.51%
9	Validation loss: 0.044188	Best loss: 0.037091	Accuracy: 98.40%
10	Validation loss: 0.043015	Best loss: 0.037091	Accuracy: 98.79%
11	Validation loss: 0.038704	Best loss: 0.037091	Accuracy: 98.75%
12	Validation loss: 0.041080	Best loss: 0.037091	Accuracy: 98.79%
13	Validation loss: 0.040616	Best loss: 0.037091	Accuracy: 98.91%
14	Validation loss: 0.042647	Best loss: 0.037091	Accuracy: 98.67%
15	Validation loss: 0.038943	Best loss: 0.037091	Accuracy: 98.79%
16	Validation loss: 0.041917	Best loss: 0.037091	Accuracy: 98.87%
17	Validation loss: 0.031449	Best loss: 0.031449	Accuracy: 98.98%
18	Validation loss: 0.051964	Best loss: 0.031449	Accuracy: 98.67%
19	Validation loss: 0.045436	Best loss: 0.031449	Accuracy: 98.67%
20	Validation loss: 0.049426	Best loss: 0.031449	Accuracy: 98.71%
21	Validation loss: 0.053903	Best loss: 0.031449	Accuracy: 98.75%
22	Validation loss: 0.048486	Best loss: 0.031449	Accuracy: 98.67%
23	Validation loss: 0.049591	Best loss: 0.031449	Accuracy: 98.87%
24	Validation loss: 0.047388	Best loss: 0.031449	Accuracy: 98.83%
25	Validation loss: 0.032209	Best loss: 0.031449	Accuracy: 99.10%
26	Validation loss: 0.039848	Best loss: 0.031449	Accuracy: 98.91%
27	Validation loss: 0.045818	Best loss: 0.031449	Accuracy: 98.91%
28	Validation loss: 0.056123	Best loss: 0.031449	Accuracy: 98.59%
29	Validation loss: 0.056363	Best loss: 0.031449	Accuracy: 98.67%
30	Validation loss: 0.042706	Best loss: 0.031449	Accuracy: 99.02%
31	Validation loss: 0.063143	Best loss: 0.031449	Accuracy: 98.67%
32	Validation loss: 0.042904	Best loss: 0.031449	Accuracy: 99.06%
33	Validation loss: 0.056717	Best loss: 0.031449	Accuracy: 98.79%
34	Validation loss: 0.048683	Best loss: 0.031449	Accuracy: 98.87%
35	Validation loss: 0.058730	Best loss: 0.031449	Accuracy: 98.63%
36	Validation loss: 0.044600	Best loss: 0.031449	Accuracy: 99.18%
37	Validation loss: 0.045664	Best loss: 0.031449	Accuracy: 98.83%
38	Validation loss: 0.060420	Best loss: 0.031449	Accuracy: 98.59%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 4.3min
[CV] n_neurons=160, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.113165	Best loss: 0.113165	Accuracy: 96.87%
1	Validation loss: 0.084785	Best loss: 0.084785	Accuracy: 98.01%
2	Validation loss: 0.060899	Best loss: 0.060899	Accuracy: 98.16%
3	Validation loss: 0.053998	Best loss: 0.053998	Accuracy: 98.32%
4	Validation loss: 0.056868	Best loss: 0.053998	Accuracy: 98.44%
5	Validation loss: 0.049034	Best loss: 0.049034	Accuracy: 98.44%
6	Validation loss: 0.041205	Best loss: 0.041205	Accuracy: 98.79%
7	Validation loss: 0.048824	Best loss: 0.041205	Accuracy: 98.63%
8	Validation loss: 0.048828	Best loss: 0.041205	Accuracy: 98.44%
9	Validation loss: 0.050719	Best loss: 0.041205	Accuracy: 98.55%
10	Validation loss: 0.056160	Best loss: 0.041205	Accuracy: 98.36%
11	Validation loss: 0.032115	Best loss: 0.032115	Accuracy: 98.91%
12	Validation loss: 0.052177	Best loss: 0.032115	Accuracy: 98.51%
13	Validation loss: 0.036450	Best loss: 0.032115	Accuracy: 98.67%
14	Validation loss: 0.036760	Best loss: 0.032115	Accuracy: 98.87%
15	Validation loss: 0.032806	Best loss: 0.032115	Accuracy: 98.98%
16	Validation loss: 0.054748	Best loss: 0.032115	Accuracy: 98.67%
17	Validation loss: 0.053535	Best loss: 0.032115	Accuracy: 98.40%
18	Validation loss: 0.048218	Best loss: 0.032115	Accuracy: 98.67%
19	Validation loss: 0.057373	Best loss: 0.032115	Accuracy: 98.51%
20	Validation loss: 0.036762	Best loss: 0.032115	Accuracy: 98.94%
21	Validation loss: 0.031698	Best loss: 0.031698	Accuracy: 99.06%
22	Validation loss: 0.042275	Best loss: 0.031698	Accuracy: 98.87%
23	Validation loss: 0.031132	Best loss: 0.031132	Accuracy: 98.98%
24	Validation loss: 0.038265	Best loss: 0.031132	Accuracy: 98.79%
25	Validation loss: 0.038641	Best loss: 0.031132	Accuracy: 99.06%
26	Validation loss: 0.034911	Best loss: 0.031132	Accuracy: 98.94%
27	Validation loss: 0.034628	Best loss: 0.031132	Accuracy: 98.91%
28	Validation loss: 0.063901	Best loss: 0.031132	Accuracy: 98.44%
29	Validation loss: 0.041095	Best loss: 0.031132	Accuracy: 98.87%
30	Validation loss: 0.043590	Best loss: 0.031132	Accuracy: 98.59%
31	Validation loss: 0.040541	Best loss: 0.031132	Accuracy: 99.06%
32	Validation loss: 0.034601	Best loss: 0.031132	Accuracy: 99.02%
33	Validation loss: 0.055578	Best loss: 0.031132	Accuracy: 98.83%
34	Validation loss: 0.037845	Best loss: 0.031132	Accuracy: 98.94%
35	Validation loss: 0.026072	Best loss: 0.026072	Accuracy: 99.18%
36	Validation loss: 0.055402	Best loss: 0.026072	Accuracy: 98.98%
37	Validation loss: 0.050775	Best loss: 0.026072	Accuracy: 98.87%
38	Validation loss: 0.035603	Best loss: 0.026072	Accuracy: 99.18%
39	Validation loss: 0.040884	Best loss: 0.026072	Accuracy: 99.10%
40	Validation loss: 0.040820	Best loss: 0.026072	Accuracy: 99.02%
41	Validation loss: 0.036662	Best loss: 0.026072	Accuracy: 99.18%
42	Validation loss: 0.053542	Best loss: 0.026072	Accuracy: 98.71%
43	Validation loss: 0.050629	Best loss: 0.026072	Accuracy: 98.55%
44	Validation loss: 0.041647	Best loss: 0.026072	Accuracy: 99.02%
45	Validation loss: 0.068495	Best loss: 0.026072	Accuracy: 98.40%
46	Validation loss: 0.039172	Best loss: 0.026072	Accuracy: 99.14%
47	Validation loss: 0.051182	Best loss: 0.026072	Accuracy: 98.91%
48	Validation loss: 0.033265	Best loss: 0.026072	Accuracy: 99.10%
49	Validation loss: 0.047341	Best loss: 0.026072	Accuracy: 98.79%
50	Validation loss: 0.041034	Best loss: 0.026072	Accuracy: 98.91%
51	Validation loss: 0.049770	Best loss: 0.026072	Accuracy: 98.59%
52	Validation loss: 0.033596	Best loss: 0.026072	Accuracy: 99.37%
53	Validation loss: 0.048062	Best loss: 0.026072	Accuracy: 98.94%
54	Validation loss: 0.056947	Best loss: 0.026072	Accuracy: 98.59%
55	Validation loss: 0.050098	Best loss: 0.026072	Accuracy: 98.83%
56	Validation loss: 0.034752	Best loss: 0.026072	Accuracy: 99.14%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 6.5min
[CV] n_neurons=160, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.077172	Best loss: 0.077172	Accuracy: 98.01%
1	Validation loss: 0.057648	Best loss: 0.057648	Accuracy: 98.40%
2	Validation loss: 0.077678	Best loss: 0.057648	Accuracy: 97.50%
3	Validation loss: 0.056048	Best loss: 0.056048	Accuracy: 98.40%
4	Validation loss: 0.067637	Best loss: 0.056048	Accuracy: 97.89%
5	Validation loss: 0.068067	Best loss: 0.056048	Accuracy: 98.20%
6	Validation loss: 0.044124	Best loss: 0.044124	Accuracy: 98.75%
7	Validation loss: 0.046174	Best loss: 0.044124	Accuracy: 98.44%
8	Validation loss: 0.050163	Best loss: 0.044124	Accuracy: 98.67%
9	Validation loss: 0.039510	Best loss: 0.039510	Accuracy: 98.71%
10	Validation loss: 0.040621	Best loss: 0.039510	Accuracy: 98.59%
11	Validation loss: 0.047274	Best loss: 0.039510	Accuracy: 98.48%
12	Validation loss: 0.057160	Best loss: 0.039510	Accuracy: 98.32%
13	Validation loss: 0.035345	Best loss: 0.035345	Accuracy: 99.02%
14	Validation loss: 0.051816	Best loss: 0.035345	Accuracy: 98.55%
15	Validation loss: 0.028952	Best loss: 0.028952	Accuracy: 98.98%
16	Validation loss: 0.052908	Best loss: 0.028952	Accuracy: 98.48%
17	Validation loss: 0.034330	Best loss: 0.028952	Accuracy: 99.10%
18	Validation loss: 0.031824	Best loss: 0.028952	Accuracy: 98.94%
19	Validation loss: 0.031221	Best loss: 0.028952	Accuracy: 99.10%
20	Validation loss: 0.030150	Best loss: 0.028952	Accuracy: 99.18%
21	Validation loss: 0.026877	Best loss: 0.026877	Accuracy: 99.06%
22	Validation loss: 0.032223	Best loss: 0.026877	Accuracy: 98.79%
23	Validation loss: 0.026484	Best loss: 0.026484	Accuracy: 99.06%
24	Validation loss: 0.034571	Best loss: 0.026484	Accuracy: 99.02%
25	Validation loss: 0.035927	Best loss: 0.026484	Accuracy: 99.14%
26	Validation loss: 0.030666	Best loss: 0.026484	Accuracy: 98.94%
27	Validation loss: 0.033597	Best loss: 0.026484	Accuracy: 98.94%
28	Validation loss: 0.034601	Best loss: 0.026484	Accuracy: 99.10%
29	Validation loss: 0.050166	Best loss: 0.026484	Accuracy: 98.48%
30	Validation loss: 0.045013	Best loss: 0.026484	Accuracy: 98.63%
31	Validation loss: 0.046446	Best loss: 0.026484	Accuracy: 98.87%
32	Validation loss: 0.040409	Best loss: 0.026484	Accuracy: 99.14%
33	Validation loss: 0.038439	Best loss: 0.026484	Accuracy: 98.98%
34	Validation loss: 0.048212	Best loss: 0.026484	Accuracy: 98.71%
35	Validation loss: 0.042898	Best loss: 0.026484	Accuracy: 99.14%
36	Validation loss: 0.048141	Best loss: 0.026484	Accuracy: 99.10%
37	Validation loss: 0.044311	Best loss: 0.026484	Accuracy: 98.91%
38	Validation loss: 0.040242	Best loss: 0.026484	Accuracy: 99.06%
39	Validation loss: 0.037981	Best loss: 0.026484	Accuracy: 99.02%
40	Validation loss: 0.034064	Best loss: 0.026484	Accuracy: 98.91%
41	Validation loss: 0.045988	Best loss: 0.026484	Accuracy: 98.91%
42	Validation loss: 0.048288	Best loss: 0.026484	Accuracy: 98.83%
43	Validation loss: 0.041383	Best loss: 0.026484	Accuracy: 98.75%
44	Validation loss: 0.047202	Best loss: 0.026484	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 5.0min
[CV] n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.077824	Best loss: 0.077824	Accuracy: 97.34%
1	Validation loss: 0.072228	Best loss: 0.072228	Accuracy: 98.01%
2	Validation loss: 0.067589	Best loss: 0.067589	Accuracy: 97.85%
3	Validation loss: 0.052690	Best loss: 0.052690	Accuracy: 98.48%
4	Validation loss: 0.053734	Best loss: 0.052690	Accuracy: 98.16%
5	Validation loss: 0.047630	Best loss: 0.047630	Accuracy: 98.59%
6	Validation loss: 0.042510	Best loss: 0.042510	Accuracy: 98.48%
7	Validation loss: 0.059006	Best loss: 0.042510	Accuracy: 98.59%
8	Validation loss: 0.107766	Best loss: 0.042510	Accuracy: 97.11%
9	Validation loss: 0.057345	Best loss: 0.042510	Accuracy: 98.51%
10	Validation loss: 0.048291	Best loss: 0.042510	Accuracy: 98.71%
11	Validation loss: 0.049560	Best loss: 0.042510	Accuracy: 98.91%
12	Validation loss: 0.051730	Best loss: 0.042510	Accuracy: 98.75%
13	Validation loss: 0.047488	Best loss: 0.042510	Accuracy: 98.91%
14	Validation loss: 0.048212	Best loss: 0.042510	Accuracy: 98.79%
15	Validation loss: 0.059501	Best loss: 0.042510	Accuracy: 98.94%
16	Validation loss: 0.049880	Best loss: 0.042510	Accuracy: 99.18%
17	Validation loss: 0.048663	Best loss: 0.042510	Accuracy: 98.71%
18	Validation loss: 0.053244	Best loss: 0.042510	Accuracy: 98.91%
19	Validation loss: 0.060154	Best loss: 0.042510	Accuracy: 98.71%
20	Validation loss: 0.056857	Best loss: 0.042510	Accuracy: 98.71%
21	Validation loss: 0.073094	Best loss: 0.042510	Accuracy: 98.75%
22	Validation loss: 0.045255	Best loss: 0.042510	Accuracy: 98.94%
23	Validation loss: 0.046726	Best loss: 0.042510	Accuracy: 98.79%
24	Validation loss: 0.041114	Best loss: 0.041114	Accuracy: 99.10%
25	Validation loss: 0.061862	Best loss: 0.041114	Accuracy: 98.63%
26	Validation loss: 0.056469	Best loss: 0.041114	Accuracy: 98.83%
27	Validation loss: 0.038650	Best loss: 0.038650	Accuracy: 99.06%
28	Validation loss: 0.053816	Best loss: 0.038650	Accuracy: 98.87%
29	Validation loss: 0.057017	Best loss: 0.038650	Accuracy: 98.98%
30	Validation loss: 0.052904	Best loss: 0.038650	Accuracy: 98.71%
31	Validation loss: 0.061304	Best loss: 0.038650	Accuracy: 98.71%
32	Validation loss: 0.047407	Best loss: 0.038650	Accuracy: 98.87%
33	Validation loss: 0.044453	Best loss: 0.038650	Accuracy: 98.98%
34	Validation loss: 0.054953	Best loss: 0.038650	Accuracy: 98.94%
35	Validation loss: 0.057420	Best loss: 0.038650	Accuracy: 98.94%
36	Validation loss: 0.041972	Best loss: 0.038650	Accuracy: 99.02%
37	Validation loss: 0.060251	Best loss: 0.038650	Accuracy: 98.98%
38	Validation loss: 0.064241	Best loss: 0.038650	Accuracy: 98.63%
39	Validation loss: 0.058946	Best loss: 0.038650	Accuracy: 98.75%
40	Validation loss: 0.081541	Best loss: 0.038650	Accuracy: 98.36%
41	Validation loss: 0.046383	Best loss: 0.038650	Accuracy: 98.98%
42	Validation loss: 0.048331	Best loss: 0.038650	Accuracy: 99.02%
43	Validation loss: 0.053397	Best loss: 0.038650	Accuracy: 98.94%
44	Validation loss: 0.053434	Best loss: 0.038650	Accuracy: 99.02%
45	Validation loss: 0.067121	Best loss: 0.038650	Accuracy: 98.63%
46	Validation loss: 0.037538	Best loss: 0.037538	Accuracy: 99.22%
47	Validation loss: 0.049910	Best loss: 0.037538	Accuracy: 98.91%
48	Validation loss: 0.045957	Best loss: 0.037538	Accuracy: 99.02%
49	Validation loss: 0.048290	Best loss: 0.037538	Accuracy: 98.98%
50	Validation loss: 0.051248	Best loss: 0.037538	Accuracy: 99.06%
51	Validation loss: 0.066392	Best loss: 0.037538	Accuracy: 98.98%
52	Validation loss: 0.077434	Best loss: 0.037538	Accuracy: 98.55%
53	Validation loss: 0.089464	Best loss: 0.037538	Accuracy: 98.79%
54	Validation loss: 0.054245	Best loss: 0.037538	Accuracy: 98.98%
55	Validation loss: 0.059969	Best loss: 0.037538	Accuracy: 99.10%
56	Validation loss: 0.046418	Best loss: 0.037538	Accuracy: 99.14%
57	Validation loss: 0.048189	Best loss: 0.037538	Accuracy: 99.18%
58	Validation loss: 0.054218	Best loss: 0.037538	Accuracy: 99.02%
59	Validation loss: 0.049095	Best loss: 0.037538	Accuracy: 98.67%
60	Validation loss: 0.041496	Best loss: 0.037538	Accuracy: 99.10%
61	Validation loss: 0.063009	Best loss: 0.037538	Accuracy: 98.75%
62	Validation loss: 0.050382	Best loss: 0.037538	Accuracy: 99.10%
63	Validation loss: 0.057541	Best loss: 0.037538	Accuracy: 98.91%
64	Validation loss: 0.071746	Best loss: 0.037538	Accuracy: 98.79%
65	Validation loss: 0.057800	Best loss: 0.037538	Accuracy: 98.71%
66	Validation loss: 0.048784	Best loss: 0.037538	Accuracy: 98.98%
67	Validation loss: 0.084061	Best loss: 0.037538	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.3min
[CV] n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.150275	Best loss: 0.150275	Accuracy: 95.82%
1	Validation loss: 0.062501	Best loss: 0.062501	Accuracy: 97.97%
2	Validation loss: 0.054681	Best loss: 0.054681	Accuracy: 98.12%
3	Validation loss: 0.084233	Best loss: 0.054681	Accuracy: 97.69%
4	Validation loss: 0.095717	Best loss: 0.054681	Accuracy: 97.50%
5	Validation loss: 0.050646	Best loss: 0.050646	Accuracy: 98.67%
6	Validation loss: 0.078570	Best loss: 0.050646	Accuracy: 97.89%
7	Validation loss: 0.068917	Best loss: 0.050646	Accuracy: 98.05%
8	Validation loss: 0.051079	Best loss: 0.050646	Accuracy: 98.51%
9	Validation loss: 0.067083	Best loss: 0.050646	Accuracy: 98.51%
10	Validation loss: 0.043534	Best loss: 0.043534	Accuracy: 98.94%
11	Validation loss: 0.066275	Best loss: 0.043534	Accuracy: 98.55%
12	Validation loss: 0.051157	Best loss: 0.043534	Accuracy: 98.87%
13	Validation loss: 0.057410	Best loss: 0.043534	Accuracy: 98.36%
14	Validation loss: 0.053934	Best loss: 0.043534	Accuracy: 98.63%
15	Validation loss: 0.070600	Best loss: 0.043534	Accuracy: 98.40%
16	Validation loss: 0.040299	Best loss: 0.040299	Accuracy: 98.94%
17	Validation loss: 0.085184	Best loss: 0.040299	Accuracy: 98.36%
18	Validation loss: 0.091375	Best loss: 0.040299	Accuracy: 98.16%
19	Validation loss: 0.055462	Best loss: 0.040299	Accuracy: 98.51%
20	Validation loss: 0.079933	Best loss: 0.040299	Accuracy: 98.28%
21	Validation loss: 0.048745	Best loss: 0.040299	Accuracy: 98.87%
22	Validation loss: 0.047963	Best loss: 0.040299	Accuracy: 98.71%
23	Validation loss: 0.064831	Best loss: 0.040299	Accuracy: 98.67%
24	Validation loss: 0.043316	Best loss: 0.040299	Accuracy: 99.02%
25	Validation loss: 0.072270	Best loss: 0.040299	Accuracy: 98.67%
26	Validation loss: 0.043135	Best loss: 0.040299	Accuracy: 99.02%
27	Validation loss: 0.070326	Best loss: 0.040299	Accuracy: 98.67%
28	Validation loss: 0.070225	Best loss: 0.040299	Accuracy: 98.67%
29	Validation loss: 0.064975	Best loss: 0.040299	Accuracy: 98.71%
30	Validation loss: 0.066325	Best loss: 0.040299	Accuracy: 98.67%
31	Validation loss: 0.067367	Best loss: 0.040299	Accuracy: 98.67%
32	Validation loss: 0.058765	Best loss: 0.040299	Accuracy: 98.87%
33	Validation loss: 0.067873	Best loss: 0.040299	Accuracy: 98.67%
34	Validation loss: 0.062787	Best loss: 0.040299	Accuracy: 98.83%
35	Validation loss: 0.055129	Best loss: 0.040299	Accuracy: 98.94%
36	Validation loss: 0.053270	Best loss: 0.040299	Accuracy: 99.06%
37	Validation loss: 0.063972	Best loss: 0.040299	Accuracy: 98.91%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total=  43.5s
[CV] n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.103915	Best loss: 0.103915	Accuracy: 97.22%
1	Validation loss: 0.065965	Best loss: 0.065965	Accuracy: 97.81%
2	Validation loss: 0.058557	Best loss: 0.058557	Accuracy: 97.97%
3	Validation loss: 0.050876	Best loss: 0.050876	Accuracy: 98.55%
4	Validation loss: 0.051217	Best loss: 0.050876	Accuracy: 98.59%
5	Validation loss: 0.054022	Best loss: 0.050876	Accuracy: 98.36%
6	Validation loss: 0.109456	Best loss: 0.050876	Accuracy: 96.79%
7	Validation loss: 0.073753	Best loss: 0.050876	Accuracy: 98.36%
8	Validation loss: 0.050144	Best loss: 0.050144	Accuracy: 98.63%
9	Validation loss: 0.052898	Best loss: 0.050144	Accuracy: 98.83%
10	Validation loss: 0.048530	Best loss: 0.048530	Accuracy: 99.10%
11	Validation loss: 0.064473	Best loss: 0.048530	Accuracy: 98.32%
12	Validation loss: 0.058249	Best loss: 0.048530	Accuracy: 98.75%
13	Validation loss: 0.053811	Best loss: 0.048530	Accuracy: 98.71%
14	Validation loss: 0.058902	Best loss: 0.048530	Accuracy: 98.87%
15	Validation loss: 0.048441	Best loss: 0.048441	Accuracy: 98.87%
16	Validation loss: 0.092394	Best loss: 0.048441	Accuracy: 97.97%
17	Validation loss: 0.060363	Best loss: 0.048441	Accuracy: 98.79%
18	Validation loss: 0.060178	Best loss: 0.048441	Accuracy: 98.55%
19	Validation loss: 0.076509	Best loss: 0.048441	Accuracy: 98.55%
20	Validation loss: 0.056321	Best loss: 0.048441	Accuracy: 98.75%
21	Validation loss: 0.040425	Best loss: 0.040425	Accuracy: 98.98%
22	Validation loss: 0.046766	Best loss: 0.040425	Accuracy: 99.06%
23	Validation loss: 0.044593	Best loss: 0.040425	Accuracy: 99.02%
24	Validation loss: 0.044226	Best loss: 0.040425	Accuracy: 99.26%
25	Validation loss: 0.078098	Best loss: 0.040425	Accuracy: 98.44%
26	Validation loss: 0.047826	Best loss: 0.040425	Accuracy: 98.59%
27	Validation loss: 0.060311	Best loss: 0.040425	Accuracy: 98.87%
28	Validation loss: 0.044405	Best loss: 0.040425	Accuracy: 98.94%
29	Validation loss: 0.058114	Best loss: 0.040425	Accuracy: 98.75%
30	Validation loss: 0.050662	Best loss: 0.040425	Accuracy: 98.71%
31	Validation loss: 0.073770	Best loss: 0.040425	Accuracy: 98.87%
32	Validation loss: 0.050821	Best loss: 0.040425	Accuracy: 99.10%
33	Validation loss: 0.061198	Best loss: 0.040425	Accuracy: 98.83%
34	Validation loss: 0.037858	Best loss: 0.037858	Accuracy: 99.34%
35	Validation loss: 0.048806	Best loss: 0.037858	Accuracy: 98.98%
36	Validation loss: 0.043592	Best loss: 0.037858	Accuracy: 99.14%
37	Validation loss: 0.059209	Best loss: 0.037858	Accuracy: 98.94%
38	Validation loss: 0.043581	Best loss: 0.037858	Accuracy: 98.91%
39	Validation loss: 0.069171	Best loss: 0.037858	Accuracy: 98.98%
40	Validation loss: 0.051105	Best loss: 0.037858	Accuracy: 98.98%
41	Validation loss: 0.068636	Best loss: 0.037858	Accuracy: 98.67%
42	Validation loss: 0.052840	Best loss: 0.037858	Accuracy: 99.06%
43	Validation loss: 0.083158	Best loss: 0.037858	Accuracy: 98.59%
44	Validation loss: 0.046869	Best loss: 0.037858	Accuracy: 98.98%
45	Validation loss: 0.036860	Best loss: 0.036860	Accuracy: 99.26%
46	Validation loss: 0.037522	Best loss: 0.036860	Accuracy: 99.41%
47	Validation loss: 0.043423	Best loss: 0.036860	Accuracy: 99.30%
48	Validation loss: 0.057505	Best loss: 0.036860	Accuracy: 98.79%
49	Validation loss: 0.070981	Best loss: 0.036860	Accuracy: 98.98%
50	Validation loss: 0.052747	Best loss: 0.036860	Accuracy: 99.22%
51	Validation loss: 0.043535	Best loss: 0.036860	Accuracy: 99.18%
52	Validation loss: 0.050189	Best loss: 0.036860	Accuracy: 99.14%
53	Validation loss: 0.050849	Best loss: 0.036860	Accuracy: 98.98%
54	Validation loss: 0.061884	Best loss: 0.036860	Accuracy: 98.94%
55	Validation loss: 0.053439	Best loss: 0.036860	Accuracy: 99.14%
56	Validation loss: 0.049180	Best loss: 0.036860	Accuracy: 99.06%
57	Validation loss: 0.046684	Best loss: 0.036860	Accuracy: 99.10%
58	Validation loss: 0.059681	Best loss: 0.036860	Accuracy: 98.91%
59	Validation loss: 0.057147	Best loss: 0.036860	Accuracy: 98.91%
60	Validation loss: 0.056490	Best loss: 0.036860	Accuracy: 99.10%
61	Validation loss: 0.044615	Best loss: 0.036860	Accuracy: 99.26%
62	Validation loss: 0.060836	Best loss: 0.036860	Accuracy: 98.83%
63	Validation loss: 0.053449	Best loss: 0.036860	Accuracy: 98.94%
64	Validation loss: 0.045364	Best loss: 0.036860	Accuracy: 99.06%
65	Validation loss: 0.050647	Best loss: 0.036860	Accuracy: 99.22%
66	Validation loss: 0.050914	Best loss: 0.036860	Accuracy: 99.18%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.2min
[CV] n_neurons=90, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.120903	Best loss: 0.120903	Accuracy: 96.79%
1	Validation loss: 0.095551	Best loss: 0.095551	Accuracy: 97.34%
2	Validation loss: 0.177181	Best loss: 0.095551	Accuracy: 95.70%
3	Validation loss: 0.204835	Best loss: 0.095551	Accuracy: 94.80%
4	Validation loss: 0.097418	Best loss: 0.095551	Accuracy: 97.22%
5	Validation loss: 0.114373	Best loss: 0.095551	Accuracy: 97.93%
6	Validation loss: 0.089211	Best loss: 0.089211	Accuracy: 97.81%
7	Validation loss: 0.084521	Best loss: 0.084521	Accuracy: 98.20%
8	Validation loss: 0.159197	Best loss: 0.084521	Accuracy: 97.38%
9	Validation loss: 0.087443	Best loss: 0.084521	Accuracy: 98.36%
10	Validation loss: 0.095496	Best loss: 0.084521	Accuracy: 98.32%
11	Validation loss: 0.074034	Best loss: 0.074034	Accuracy: 98.16%
12	Validation loss: 0.150718	Best loss: 0.074034	Accuracy: 97.89%
13	Validation loss: 0.233397	Best loss: 0.074034	Accuracy: 97.65%
14	Validation loss: 0.095693	Best loss: 0.074034	Accuracy: 98.28%
15	Validation loss: 0.086163	Best loss: 0.074034	Accuracy: 98.55%
16	Validation loss: 0.089879	Best loss: 0.074034	Accuracy: 98.16%
17	Validation loss: 0.086874	Best loss: 0.074034	Accuracy: 98.40%
18	Validation loss: 0.080832	Best loss: 0.074034	Accuracy: 98.59%
19	Validation loss: 0.061769	Best loss: 0.061769	Accuracy: 98.59%
20	Validation loss: 0.128393	Best loss: 0.061769	Accuracy: 98.44%
21	Validation loss: 1.519262	Best loss: 0.061769	Accuracy: 95.27%
22	Validation loss: 0.245676	Best loss: 0.061769	Accuracy: 98.67%
23	Validation loss: 0.162667	Best loss: 0.061769	Accuracy: 98.01%
24	Validation loss: 0.087560	Best loss: 0.061769	Accuracy: 99.02%
25	Validation loss: 0.065874	Best loss: 0.061769	Accuracy: 98.91%
26	Validation loss: 0.058260	Best loss: 0.058260	Accuracy: 98.87%
27	Validation loss: 0.048640	Best loss: 0.048640	Accuracy: 99.26%
28	Validation loss: 0.054578	Best loss: 0.048640	Accuracy: 98.91%
29	Validation loss: 0.061976	Best loss: 0.048640	Accuracy: 98.91%
30	Validation loss: 0.065781	Best loss: 0.048640	Accuracy: 99.06%
31	Validation loss: 0.068345	Best loss: 0.048640	Accuracy: 98.94%
32	Validation loss: 0.082759	Best loss: 0.048640	Accuracy: 98.83%
33	Validation loss: 0.063968	Best loss: 0.048640	Accuracy: 98.94%
34	Validation loss: 1.786901	Best loss: 0.048640	Accuracy: 96.33%
35	Validation loss: 0.445849	Best loss: 0.048640	Accuracy: 98.59%
36	Validation loss: 0.239433	Best loss: 0.048640	Accuracy: 98.05%
37	Validation loss: 0.208738	Best loss: 0.048640	Accuracy: 97.97%
38	Validation loss: 0.170424	Best loss: 0.048640	Accuracy: 98.67%
39	Validation loss: 0.182203	Best loss: 0.048640	Accuracy: 98.91%
40	Validation loss: 0.171915	Best loss: 0.048640	Accuracy: 98.67%
41	Validation loss: 0.221570	Best loss: 0.048640	Accuracy: 98.75%
42	Validation loss: 0.174823	Best loss: 0.048640	Accuracy: 98.83%
43	Validation loss: 0.151894	Best loss: 0.048640	Accuracy: 98.98%
44	Validation loss: 0.152338	Best loss: 0.048640	Accuracy: 98.87%
45	Validation loss: 0.178107	Best loss: 0.048640	Accuracy: 98.67%
46	Validation loss: 0.197136	Best loss: 0.048640	Accuracy: 98.48%
47	Validation loss: 0.117359	Best loss: 0.048640	Accuracy: 99.06%
48	Validation loss: 0.154053	Best loss: 0.048640	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total=  53.0s
[CV] n_neurons=90, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.166847	Best loss: 0.166847	Accuracy: 95.97%
1	Validation loss: 0.080371	Best loss: 0.080371	Accuracy: 97.65%
2	Validation loss: 0.073759	Best loss: 0.073759	Accuracy: 98.20%
3	Validation loss: 0.081911	Best loss: 0.073759	Accuracy: 97.93%
4	Validation loss: 0.111686	Best loss: 0.073759	Accuracy: 97.54%
5	Validation loss: 0.076930	Best loss: 0.073759	Accuracy: 98.12%
6	Validation loss: 0.074352	Best loss: 0.073759	Accuracy: 98.51%
7	Validation loss: 0.053917	Best loss: 0.053917	Accuracy: 98.67%
8	Validation loss: 0.068889	Best loss: 0.053917	Accuracy: 98.48%
9	Validation loss: 0.083947	Best loss: 0.053917	Accuracy: 98.24%
10	Validation loss: 0.080806	Best loss: 0.053917	Accuracy: 98.24%
11	Validation loss: 0.086971	Best loss: 0.053917	Accuracy: 97.93%
12	Validation loss: 0.293862	Best loss: 0.053917	Accuracy: 96.48%
13	Validation loss: 0.802381	Best loss: 0.053917	Accuracy: 95.93%
14	Validation loss: 0.237223	Best loss: 0.053917	Accuracy: 97.85%
15	Validation loss: 0.129762	Best loss: 0.053917	Accuracy: 98.40%
16	Validation loss: 0.059824	Best loss: 0.053917	Accuracy: 98.79%
17	Validation loss: 0.060116	Best loss: 0.053917	Accuracy: 98.91%
18	Validation loss: 0.048677	Best loss: 0.048677	Accuracy: 98.98%
19	Validation loss: 0.049316	Best loss: 0.048677	Accuracy: 98.71%
20	Validation loss: 0.068250	Best loss: 0.048677	Accuracy: 98.87%
21	Validation loss: 0.063018	Best loss: 0.048677	Accuracy: 98.91%
22	Validation loss: 0.069999	Best loss: 0.048677	Accuracy: 98.67%
23	Validation loss: 0.061007	Best loss: 0.048677	Accuracy: 98.87%
24	Validation loss: 0.076241	Best loss: 0.048677	Accuracy: 98.63%
25	Validation loss: 0.065299	Best loss: 0.048677	Accuracy: 98.71%
26	Validation loss: 0.210502	Best loss: 0.048677	Accuracy: 98.08%
27	Validation loss: 3.057349	Best loss: 0.048677	Accuracy: 94.14%
28	Validation loss: 0.240090	Best loss: 0.048677	Accuracy: 98.51%
29	Validation loss: 0.186732	Best loss: 0.048677	Accuracy: 98.63%
30	Validation loss: 0.219370	Best loss: 0.048677	Accuracy: 98.67%
31	Validation loss: 0.222112	Best loss: 0.048677	Accuracy: 98.71%
32	Validation loss: 0.161363	Best loss: 0.048677	Accuracy: 98.87%
33	Validation loss: 0.178275	Best loss: 0.048677	Accuracy: 98.83%
34	Validation loss: 0.140004	Best loss: 0.048677	Accuracy: 98.91%
35	Validation loss: 0.140737	Best loss: 0.048677	Accuracy: 98.98%
36	Validation loss: 0.212462	Best loss: 0.048677	Accuracy: 98.87%
37	Validation loss: 0.145483	Best loss: 0.048677	Accuracy: 98.87%
38	Validation loss: 0.183366	Best loss: 0.048677	Accuracy: 98.87%
39	Validation loss: 0.166401	Best loss: 0.048677	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total=  43.2s
[CV] n_neurons=90, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.128063	Best loss: 0.128063	Accuracy: 97.11%
1	Validation loss: 0.097631	Best loss: 0.097631	Accuracy: 97.58%
2	Validation loss: 0.071242	Best loss: 0.071242	Accuracy: 97.93%
3	Validation loss: 0.089797	Best loss: 0.071242	Accuracy: 97.93%
4	Validation loss: 0.109477	Best loss: 0.071242	Accuracy: 98.01%
5	Validation loss: 0.086396	Best loss: 0.071242	Accuracy: 97.97%
6	Validation loss: 0.054952	Best loss: 0.054952	Accuracy: 98.16%
7	Validation loss: 0.097821	Best loss: 0.054952	Accuracy: 98.12%
8	Validation loss: 0.066134	Best loss: 0.054952	Accuracy: 98.40%
9	Validation loss: 0.090900	Best loss: 0.054952	Accuracy: 98.36%
10	Validation loss: 0.089004	Best loss: 0.054952	Accuracy: 98.36%
11	Validation loss: 0.076219	Best loss: 0.054952	Accuracy: 98.44%
12	Validation loss: 0.099525	Best loss: 0.054952	Accuracy: 98.12%
13	Validation loss: 0.046247	Best loss: 0.046247	Accuracy: 98.71%
14	Validation loss: 0.061809	Best loss: 0.046247	Accuracy: 98.67%
15	Validation loss: 7.180145	Best loss: 0.046247	Accuracy: 94.06%
16	Validation loss: 0.458124	Best loss: 0.046247	Accuracy: 97.22%
17	Validation loss: 0.096659	Best loss: 0.046247	Accuracy: 98.55%
18	Validation loss: 0.087457	Best loss: 0.046247	Accuracy: 98.59%
19	Validation loss: 0.083693	Best loss: 0.046247	Accuracy: 98.71%
20	Validation loss: 0.067831	Best loss: 0.046247	Accuracy: 98.79%
21	Validation loss: 0.056544	Best loss: 0.046247	Accuracy: 98.94%
22	Validation loss: 0.069754	Best loss: 0.046247	Accuracy: 98.75%
23	Validation loss: 0.084413	Best loss: 0.046247	Accuracy: 98.63%
24	Validation loss: 0.069513	Best loss: 0.046247	Accuracy: 98.67%
25	Validation loss: 0.052243	Best loss: 0.046247	Accuracy: 99.10%
26	Validation loss: 0.053864	Best loss: 0.046247	Accuracy: 98.87%
27	Validation loss: 0.046520	Best loss: 0.046247	Accuracy: 98.63%
28	Validation loss: 0.108775	Best loss: 0.046247	Accuracy: 98.36%
29	Validation loss: 0.445014	Best loss: 0.046247	Accuracy: 94.76%
30	Validation loss: 0.291123	Best loss: 0.046247	Accuracy: 98.08%
31	Validation loss: 0.081508	Best loss: 0.046247	Accuracy: 99.10%
32	Validation loss: 0.074445	Best loss: 0.046247	Accuracy: 99.10%
33	Validation loss: 0.092032	Best loss: 0.046247	Accuracy: 98.59%
34	Validation loss: 0.082611	Best loss: 0.046247	Accuracy: 99.06%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total=  37.8s
[CV] n_neurons=70, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.216942	Best loss: 0.216942	Accuracy: 96.40%
1	Validation loss: 0.088625	Best loss: 0.088625	Accuracy: 97.54%
2	Validation loss: 0.086330	Best loss: 0.086330	Accuracy: 97.62%
3	Validation loss: 0.067267	Best loss: 0.067267	Accuracy: 98.08%
4	Validation loss: 0.051317	Best loss: 0.051317	Accuracy: 98.28%
5	Validation loss: 0.071554	Best loss: 0.051317	Accuracy: 98.08%
6	Validation loss: 0.082762	Best loss: 0.051317	Accuracy: 97.62%
7	Validation loss: 0.065998	Best loss: 0.051317	Accuracy: 98.51%
8	Validation loss: 0.058924	Best loss: 0.051317	Accuracy: 98.48%
9	Validation loss: 0.067968	Best loss: 0.051317	Accuracy: 98.67%
10	Validation loss: 0.069615	Best loss: 0.051317	Accuracy: 98.32%
11	Validation loss: 0.101888	Best loss: 0.051317	Accuracy: 97.93%
12	Validation loss: 0.083588	Best loss: 0.051317	Accuracy: 98.24%
13	Validation loss: 0.107273	Best loss: 0.051317	Accuracy: 97.89%
14	Validation loss: 0.058656	Best loss: 0.051317	Accuracy: 98.75%
15	Validation loss: 0.059488	Best loss: 0.051317	Accuracy: 98.75%
16	Validation loss: 0.069815	Best loss: 0.051317	Accuracy: 98.71%
17	Validation loss: 0.059786	Best loss: 0.051317	Accuracy: 98.71%
18	Validation loss: 0.058007	Best loss: 0.051317	Accuracy: 98.75%
19	Validation loss: 0.067636	Best loss: 0.051317	Accuracy: 98.79%
20	Validation loss: 0.073145	Best loss: 0.051317	Accuracy: 98.75%
21	Validation loss: 0.079183	Best loss: 0.051317	Accuracy: 98.59%
22	Validation loss: 0.097058	Best loss: 0.051317	Accuracy: 98.08%
23	Validation loss: 0.089006	Best loss: 0.051317	Accuracy: 98.12%
24	Validation loss: 0.069624	Best loss: 0.051317	Accuracy: 98.51%
25	Validation loss: 0.071819	Best loss: 0.051317	Accuracy: 98.36%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total=  12.6s
[CV] n_neurons=70, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.252658	Best loss: 0.252658	Accuracy: 95.23%
1	Validation loss: 0.085001	Best loss: 0.085001	Accuracy: 97.65%
2	Validation loss: 0.077455	Best loss: 0.077455	Accuracy: 97.97%
3	Validation loss: 0.074700	Best loss: 0.074700	Accuracy: 98.01%
4	Validation loss: 0.077376	Best loss: 0.074700	Accuracy: 98.12%
5	Validation loss: 0.064827	Best loss: 0.064827	Accuracy: 97.97%
6	Validation loss: 0.060251	Best loss: 0.060251	Accuracy: 98.24%
7	Validation loss: 0.066553	Best loss: 0.060251	Accuracy: 98.40%
8	Validation loss: 0.063268	Best loss: 0.060251	Accuracy: 98.44%
9	Validation loss: 0.073526	Best loss: 0.060251	Accuracy: 98.01%
10	Validation loss: 0.072687	Best loss: 0.060251	Accuracy: 98.40%
11	Validation loss: 0.075418	Best loss: 0.060251	Accuracy: 98.20%
12	Validation loss: 0.065352	Best loss: 0.060251	Accuracy: 98.48%
13	Validation loss: 0.066429	Best loss: 0.060251	Accuracy: 98.63%
14	Validation loss: 0.069456	Best loss: 0.060251	Accuracy: 98.51%
15	Validation loss: 0.084977	Best loss: 0.060251	Accuracy: 98.28%
16	Validation loss: 0.081313	Best loss: 0.060251	Accuracy: 98.44%
17	Validation loss: 0.081581	Best loss: 0.060251	Accuracy: 98.48%
18	Validation loss: 0.070442	Best loss: 0.060251	Accuracy: 98.20%
19	Validation loss: 0.073481	Best loss: 0.060251	Accuracy: 98.44%
20	Validation loss: 0.068100	Best loss: 0.060251	Accuracy: 98.44%
21	Validation loss: 0.104938	Best loss: 0.060251	Accuracy: 98.24%
22	Validation loss: 0.077506	Best loss: 0.060251	Accuracy: 98.71%
23	Validation loss: 0.086031	Best loss: 0.060251	Accuracy: 98.48%
24	Validation loss: 0.102089	Best loss: 0.060251	Accuracy: 98.32%
25	Validation loss: 0.072248	Best loss: 0.060251	Accuracy: 98.75%
26	Validation loss: 0.063927	Best loss: 0.060251	Accuracy: 98.83%
27	Validation loss: 0.070415	Best loss: 0.060251	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total=  13.0s
[CV] n_neurons=70, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.237057	Best loss: 0.237057	Accuracy: 95.82%
1	Validation loss: 0.085786	Best loss: 0.085786	Accuracy: 97.97%
2	Validation loss: 0.071232	Best loss: 0.071232	Accuracy: 97.89%
3	Validation loss: 0.069646	Best loss: 0.069646	Accuracy: 98.01%
4	Validation loss: 0.055963	Best loss: 0.055963	Accuracy: 98.59%
5	Validation loss: 0.094369	Best loss: 0.055963	Accuracy: 97.69%
6	Validation loss: 0.060553	Best loss: 0.055963	Accuracy: 98.71%
7	Validation loss: 0.057002	Best loss: 0.055963	Accuracy: 98.40%
8	Validation loss: 0.055149	Best loss: 0.055149	Accuracy: 98.44%
9	Validation loss: 0.051342	Best loss: 0.051342	Accuracy: 98.55%
10	Validation loss: 0.057167	Best loss: 0.051342	Accuracy: 98.63%
11	Validation loss: 0.141069	Best loss: 0.051342	Accuracy: 97.26%
12	Validation loss: 0.059375	Best loss: 0.051342	Accuracy: 98.51%
13	Validation loss: 0.088185	Best loss: 0.051342	Accuracy: 98.28%
14	Validation loss: 0.062093	Best loss: 0.051342	Accuracy: 98.55%
15	Validation loss: 0.122138	Best loss: 0.051342	Accuracy: 97.58%
16	Validation loss: 0.050878	Best loss: 0.050878	Accuracy: 98.75%
17	Validation loss: 0.052386	Best loss: 0.050878	Accuracy: 98.59%
18	Validation loss: 0.070827	Best loss: 0.050878	Accuracy: 98.59%
19	Validation loss: 0.061702	Best loss: 0.050878	Accuracy: 98.59%
20	Validation loss: 0.073774	Best loss: 0.050878	Accuracy: 98.55%
21	Validation loss: 0.083395	Best loss: 0.050878	Accuracy: 98.63%
22	Validation loss: 0.081604	Best loss: 0.050878	Accuracy: 98.67%
23	Validation loss: 0.099461	Best loss: 0.050878	Accuracy: 98.28%
24	Validation loss: 0.097973	Best loss: 0.050878	Accuracy: 98.44%
25	Validation loss: 0.065473	Best loss: 0.050878	Accuracy: 98.55%
26	Validation loss: 0.078119	Best loss: 0.050878	Accuracy: 98.48%
27	Validation loss: 0.040299	Best loss: 0.040299	Accuracy: 99.06%
28	Validation loss: 0.064482	Best loss: 0.040299	Accuracy: 98.63%
29	Validation loss: 0.052653	Best loss: 0.040299	Accuracy: 98.83%
30	Validation loss: 0.071790	Best loss: 0.040299	Accuracy: 98.28%
31	Validation loss: 0.042014	Best loss: 0.040299	Accuracy: 98.94%
32	Validation loss: 0.059738	Best loss: 0.040299	Accuracy: 98.63%
33	Validation loss: 0.045233	Best loss: 0.040299	Accuracy: 98.94%
34	Validation loss: 0.051329	Best loss: 0.040299	Accuracy: 98.79%
35	Validation loss: 0.059773	Best loss: 0.040299	Accuracy: 98.67%
36	Validation loss: 0.052525	Best loss: 0.040299	Accuracy: 98.67%
37	Validation loss: 0.069035	Best loss: 0.040299	Accuracy: 98.67%
38	Validation loss: 0.133558	Best loss: 0.040299	Accuracy: 97.58%
39	Validation loss: 0.090709	Best loss: 0.040299	Accuracy: 98.24%
40	Validation loss: 0.088792	Best loss: 0.040299	Accuracy: 98.67%
41	Validation loss: 0.087125	Best loss: 0.040299	Accuracy: 98.36%
42	Validation loss: 0.083515	Best loss: 0.040299	Accuracy: 98.79%
43	Validation loss: 0.077299	Best loss: 0.040299	Accuracy: 98.83%
44	Validation loss: 0.071973	Best loss: 0.040299	Accuracy: 98.63%
45	Validation loss: 0.060503	Best loss: 0.040299	Accuracy: 98.71%
46	Validation loss: 0.079527	Best loss: 0.040299	Accuracy: 98.55%
47	Validation loss: 0.060237	Best loss: 0.040299	Accuracy: 98.87%
48	Validation loss: 0.069516	Best loss: 0.040299	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total=  21.6s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.088971	Best loss: 0.088971	Accuracy: 97.50%
1	Validation loss: 0.066251	Best loss: 0.066251	Accuracy: 97.77%
2	Validation loss: 0.082775	Best loss: 0.066251	Accuracy: 97.58%
3	Validation loss: 0.060286	Best loss: 0.060286	Accuracy: 98.08%
4	Validation loss: 0.064628	Best loss: 0.060286	Accuracy: 98.01%
5	Validation loss: 0.064738	Best loss: 0.060286	Accuracy: 98.16%
6	Validation loss: 0.049356	Best loss: 0.049356	Accuracy: 98.55%
7	Validation loss: 0.063463	Best loss: 0.049356	Accuracy: 98.36%
8	Validation loss: 0.080480	Best loss: 0.049356	Accuracy: 98.16%
9	Validation loss: 0.054847	Best loss: 0.049356	Accuracy: 98.59%
10	Validation loss: 0.054405	Best loss: 0.049356	Accuracy: 98.55%
11	Validation loss: 0.059983	Best loss: 0.049356	Accuracy: 98.63%
12	Validation loss: 0.056650	Best loss: 0.049356	Accuracy: 98.51%
13	Validation loss: 0.039208	Best loss: 0.039208	Accuracy: 99.02%
14	Validation loss: 0.045384	Best loss: 0.039208	Accuracy: 98.55%
15	Validation loss: 0.069531	Best loss: 0.039208	Accuracy: 98.63%
16	Validation loss: 0.047288	Best loss: 0.039208	Accuracy: 98.67%
17	Validation loss: 0.062942	Best loss: 0.039208	Accuracy: 98.55%
18	Validation loss: 0.052353	Best loss: 0.039208	Accuracy: 98.44%
19	Validation loss: 0.076916	Best loss: 0.039208	Accuracy: 98.48%
20	Validation loss: 0.061367	Best loss: 0.039208	Accuracy: 98.75%
21	Validation loss: 0.050475	Best loss: 0.039208	Accuracy: 98.67%
22	Validation loss: 0.058514	Best loss: 0.039208	Accuracy: 98.83%
23	Validation loss: 0.061046	Best loss: 0.039208	Accuracy: 98.79%
24	Validation loss: 0.051116	Best loss: 0.039208	Accuracy: 98.44%
25	Validation loss: 0.057472	Best loss: 0.039208	Accuracy: 98.75%
26	Validation loss: 0.066657	Best loss: 0.039208	Accuracy: 98.51%
27	Validation loss: 0.056806	Best loss: 0.039208	Accuracy: 98.83%
28	Validation loss: 0.061548	Best loss: 0.039208	Accuracy: 98.63%
29	Validation loss: 0.073203	Best loss: 0.039208	Accuracy: 98.55%
30	Validation loss: 0.062439	Best loss: 0.039208	Accuracy: 98.71%
31	Validation loss: 0.085530	Best loss: 0.039208	Accuracy: 98.71%
32	Validation loss: 0.066686	Best loss: 0.039208	Accuracy: 98.79%
33	Validation loss: 0.077081	Best loss: 0.039208	Accuracy: 98.44%
34	Validation loss: 0.060439	Best loss: 0.039208	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  29.1s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.092572	Best loss: 0.092572	Accuracy: 97.34%
1	Validation loss: 0.055537	Best loss: 0.055537	Accuracy: 98.36%
2	Validation loss: 0.053820	Best loss: 0.053820	Accuracy: 98.44%
3	Validation loss: 0.077648	Best loss: 0.053820	Accuracy: 98.05%
4	Validation loss: 0.064882	Best loss: 0.053820	Accuracy: 98.24%
5	Validation loss: 0.051348	Best loss: 0.051348	Accuracy: 98.51%
6	Validation loss: 0.061747	Best loss: 0.051348	Accuracy: 98.40%
7	Validation loss: 0.054961	Best loss: 0.051348	Accuracy: 98.48%
8	Validation loss: 0.046225	Best loss: 0.046225	Accuracy: 98.75%
9	Validation loss: 0.060306	Best loss: 0.046225	Accuracy: 98.40%
10	Validation loss: 0.043458	Best loss: 0.043458	Accuracy: 98.87%
11	Validation loss: 0.045325	Best loss: 0.043458	Accuracy: 98.71%
12	Validation loss: 0.044797	Best loss: 0.043458	Accuracy: 98.87%
13	Validation loss: 0.047740	Best loss: 0.043458	Accuracy: 98.67%
14	Validation loss: 0.047665	Best loss: 0.043458	Accuracy: 98.59%
15	Validation loss: 0.062295	Best loss: 0.043458	Accuracy: 98.44%
16	Validation loss: 0.043536	Best loss: 0.043458	Accuracy: 98.94%
17	Validation loss: 0.054845	Best loss: 0.043458	Accuracy: 98.79%
18	Validation loss: 0.053421	Best loss: 0.043458	Accuracy: 98.67%
19	Validation loss: 0.057253	Best loss: 0.043458	Accuracy: 98.83%
20	Validation loss: 0.060291	Best loss: 0.043458	Accuracy: 98.63%
21	Validation loss: 0.060441	Best loss: 0.043458	Accuracy: 98.71%
22	Validation loss: 0.048310	Best loss: 0.043458	Accuracy: 98.75%
23	Validation loss: 0.050514	Best loss: 0.043458	Accuracy: 98.71%
24	Validation loss: 0.063292	Best loss: 0.043458	Accuracy: 98.55%
25	Validation loss: 0.066467	Best loss: 0.043458	Accuracy: 98.59%
26	Validation loss: 0.064237	Best loss: 0.043458	Accuracy: 98.48%
27	Validation loss: 0.065007	Best loss: 0.043458	Accuracy: 98.63%
28	Validation loss: 0.087879	Best loss: 0.043458	Accuracy: 97.89%
29	Validation loss: 0.085511	Best loss: 0.043458	Accuracy: 98.08%
30	Validation loss: 0.057759	Best loss: 0.043458	Accuracy: 98.67%
31	Validation loss: 0.146290	Best loss: 0.043458	Accuracy: 96.95%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  27.5s
[CV] n_neurons=50, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.119255	Best loss: 0.119255	Accuracy: 96.91%
1	Validation loss: 0.084663	Best loss: 0.084663	Accuracy: 97.62%
2	Validation loss: 0.071806	Best loss: 0.071806	Accuracy: 97.46%
3	Validation loss: 0.046557	Best loss: 0.046557	Accuracy: 98.36%
4	Validation loss: 0.065159	Best loss: 0.046557	Accuracy: 98.28%
5	Validation loss: 0.047991	Best loss: 0.046557	Accuracy: 98.48%
6	Validation loss: 0.062450	Best loss: 0.046557	Accuracy: 98.20%
7	Validation loss: 0.065767	Best loss: 0.046557	Accuracy: 98.28%
8	Validation loss: 0.042923	Best loss: 0.042923	Accuracy: 98.67%
9	Validation loss: 0.040542	Best loss: 0.040542	Accuracy: 98.75%
10	Validation loss: 0.052834	Best loss: 0.040542	Accuracy: 98.63%
11	Validation loss: 0.035894	Best loss: 0.035894	Accuracy: 98.94%
12	Validation loss: 0.055617	Best loss: 0.035894	Accuracy: 98.71%
13	Validation loss: 0.053648	Best loss: 0.035894	Accuracy: 98.71%
14	Validation loss: 0.046781	Best loss: 0.035894	Accuracy: 98.67%
15	Validation loss: 0.041618	Best loss: 0.035894	Accuracy: 99.18%
16	Validation loss: 0.056922	Best loss: 0.035894	Accuracy: 98.59%
17	Validation loss: 0.056582	Best loss: 0.035894	Accuracy: 98.71%
18	Validation loss: 0.041217	Best loss: 0.035894	Accuracy: 98.94%
19	Validation loss: 0.036503	Best loss: 0.035894	Accuracy: 99.10%
20	Validation loss: 0.034634	Best loss: 0.034634	Accuracy: 99.02%
21	Validation loss: 0.045094	Best loss: 0.034634	Accuracy: 98.94%
22	Validation loss: 0.039070	Best loss: 0.034634	Accuracy: 99.26%
23	Validation loss: 0.040580	Best loss: 0.034634	Accuracy: 98.83%
24	Validation loss: 0.052854	Best loss: 0.034634	Accuracy: 98.75%
25	Validation loss: 0.043198	Best loss: 0.034634	Accuracy: 98.87%
26	Validation loss: 0.064175	Best loss: 0.034634	Accuracy: 98.59%
27	Validation loss: 0.058730	Best loss: 0.034634	Accuracy: 98.44%
28	Validation loss: 0.052143	Best loss: 0.034634	Accuracy: 98.79%
29	Validation loss: 0.043499	Best loss: 0.034634	Accuracy: 99.10%
30	Validation loss: 0.039754	Best loss: 0.034634	Accuracy: 99.22%
31	Validation loss: 0.052103	Best loss: 0.034634	Accuracy: 98.91%
32	Validation loss: 0.053179	Best loss: 0.034634	Accuracy: 98.98%
33	Validation loss: 0.047360	Best loss: 0.034634	Accuracy: 98.98%
34	Validation loss: 0.054588	Best loss: 0.034634	Accuracy: 98.75%
35	Validation loss: 0.048349	Best loss: 0.034634	Accuracy: 99.18%
36	Validation loss: 0.046526	Best loss: 0.034634	Accuracy: 99.06%
37	Validation loss: 0.059938	Best loss: 0.034634	Accuracy: 98.83%
38	Validation loss: 0.065149	Best loss: 0.034634	Accuracy: 98.71%
39	Validation loss: 0.048868	Best loss: 0.034634	Accuracy: 99.02%
40	Validation loss: 0.057131	Best loss: 0.034634	Accuracy: 98.87%
41	Validation loss: 0.062151	Best loss: 0.034634	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  34.1s
[CV] n_neurons=10, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.117939	Best loss: 0.117939	Accuracy: 96.33%
1	Validation loss: 0.085174	Best loss: 0.085174	Accuracy: 97.50%
2	Validation loss: 0.107691	Best loss: 0.085174	Accuracy: 96.68%
3	Validation loss: 0.075586	Best loss: 0.075586	Accuracy: 97.73%
4	Validation loss: 0.093538	Best loss: 0.075586	Accuracy: 96.99%
5	Validation loss: 0.092764	Best loss: 0.075586	Accuracy: 97.19%
6	Validation loss: 0.073015	Best loss: 0.073015	Accuracy: 97.73%
7	Validation loss: 0.066209	Best loss: 0.066209	Accuracy: 97.93%
8	Validation loss: 0.119754	Best loss: 0.066209	Accuracy: 96.09%
9	Validation loss: 0.091763	Best loss: 0.066209	Accuracy: 97.54%
10	Validation loss: 0.071911	Best loss: 0.066209	Accuracy: 97.73%
11	Validation loss: 0.052073	Best loss: 0.052073	Accuracy: 98.51%
12	Validation loss: 0.063026	Best loss: 0.052073	Accuracy: 98.08%
13	Validation loss: 0.051657	Best loss: 0.051657	Accuracy: 98.28%
14	Validation loss: 0.073007	Best loss: 0.051657	Accuracy: 97.73%
15	Validation loss: 0.052672	Best loss: 0.051657	Accuracy: 98.59%
16	Validation loss: 0.051221	Best loss: 0.051221	Accuracy: 98.51%
17	Validation loss: 0.057577	Best loss: 0.051221	Accuracy: 98.36%
18	Validation loss: 0.070546	Best loss: 0.051221	Accuracy: 98.05%
19	Validation loss: 0.058151	Best loss: 0.051221	Accuracy: 98.16%
20	Validation loss: 0.059713	Best loss: 0.051221	Accuracy: 98.44%
21	Validation loss: 0.075696	Best loss: 0.051221	Accuracy: 97.73%
22	Validation loss: 0.049604	Best loss: 0.049604	Accuracy: 98.24%
23	Validation loss: 0.064980	Best loss: 0.049604	Accuracy: 98.20%
24	Validation loss: 0.068772	Best loss: 0.049604	Accuracy: 98.08%
25	Validation loss: 0.068206	Best loss: 0.049604	Accuracy: 98.16%
26	Validation loss: 0.069841	Best loss: 0.049604	Accuracy: 98.16%
27	Validation loss: 0.064899	Best loss: 0.049604	Accuracy: 98.12%
28	Validation loss: 0.052618	Best loss: 0.049604	Accuracy: 98.48%
29	Validation loss: 0.094497	Best loss: 0.049604	Accuracy: 97.62%
30	Validation loss: 0.077622	Best loss: 0.049604	Accuracy: 97.97%
31	Validation loss: 0.080781	Best loss: 0.049604	Accuracy: 98.05%
32	Validation loss: 0.085012	Best loss: 0.049604	Accuracy: 98.01%
33	Validation loss: 0.056814	Best loss: 0.049604	Accuracy: 98.48%
34	Validation loss: 0.073718	Best loss: 0.049604	Accuracy: 98.16%
35	Validation loss: 0.073221	Best loss: 0.049604	Accuracy: 98.20%
36	Validation loss: 0.063206	Best loss: 0.049604	Accuracy: 98.51%
37	Validation loss: 0.069456	Best loss: 0.049604	Accuracy: 98.08%
38	Validation loss: 0.069936	Best loss: 0.049604	Accuracy: 98.32%
39	Validation loss: 0.078494	Best loss: 0.049604	Accuracy: 98.01%
40	Validation loss: 0.086566	Best loss: 0.049604	Accuracy: 98.01%
41	Validation loss: 0.083187	Best loss: 0.049604	Accuracy: 98.05%
42	Validation loss: 0.065673	Best loss: 0.049604	Accuracy: 98.51%
43	Validation loss: 0.073479	Best loss: 0.049604	Accuracy: 98.28%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt;, total=  24.1s
[CV] n_neurons=10, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.130837	Best loss: 0.130837	Accuracy: 95.74%
1	Validation loss: 0.080298	Best loss: 0.080298	Accuracy: 97.62%
2	Validation loss: 0.064846	Best loss: 0.064846	Accuracy: 98.24%
3	Validation loss: 0.099189	Best loss: 0.064846	Accuracy: 96.95%
4	Validation loss: 0.072545	Best loss: 0.064846	Accuracy: 98.01%
5	Validation loss: 0.071475	Best loss: 0.064846	Accuracy: 97.77%
6	Validation loss: 0.075532	Best loss: 0.064846	Accuracy: 97.81%
7	Validation loss: 0.089925	Best loss: 0.064846	Accuracy: 97.26%
8	Validation loss: 0.049905	Best loss: 0.049905	Accuracy: 98.51%
9	Validation loss: 0.077805	Best loss: 0.049905	Accuracy: 97.85%
10	Validation loss: 0.069319	Best loss: 0.049905	Accuracy: 97.89%
11	Validation loss: 0.067975	Best loss: 0.049905	Accuracy: 98.01%
12	Validation loss: 0.063380	Best loss: 0.049905	Accuracy: 98.51%
13	Validation loss: 0.069272	Best loss: 0.049905	Accuracy: 98.08%
14	Validation loss: 0.066041	Best loss: 0.049905	Accuracy: 97.81%
15	Validation loss: 0.076431	Best loss: 0.049905	Accuracy: 97.81%
16	Validation loss: 0.066763	Best loss: 0.049905	Accuracy: 98.20%
17	Validation loss: 0.056003	Best loss: 0.049905	Accuracy: 98.48%
18	Validation loss: 0.086828	Best loss: 0.049905	Accuracy: 98.08%
19	Validation loss: 0.065554	Best loss: 0.049905	Accuracy: 98.05%
20	Validation loss: 0.071349	Best loss: 0.049905	Accuracy: 97.89%
21	Validation loss: 0.066989	Best loss: 0.049905	Accuracy: 98.05%
22	Validation loss: 0.098295	Best loss: 0.049905	Accuracy: 97.54%
23	Validation loss: 0.071104	Best loss: 0.049905	Accuracy: 98.48%
24	Validation loss: 0.069030	Best loss: 0.049905	Accuracy: 98.40%
25	Validation loss: 0.075156	Best loss: 0.049905	Accuracy: 98.01%
26	Validation loss: 0.066688	Best loss: 0.049905	Accuracy: 98.32%
27	Validation loss: 0.069566	Best loss: 0.049905	Accuracy: 98.20%
28	Validation loss: 0.075818	Best loss: 0.049905	Accuracy: 97.93%
29	Validation loss: 0.081977	Best loss: 0.049905	Accuracy: 98.40%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt;, total=  17.2s
[CV] n_neurons=10, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.129174	Best loss: 0.129174	Accuracy: 96.13%
1	Validation loss: 0.106426	Best loss: 0.106426	Accuracy: 97.07%
2	Validation loss: 0.082623	Best loss: 0.082623	Accuracy: 97.69%
3	Validation loss: 0.070477	Best loss: 0.070477	Accuracy: 97.69%
4	Validation loss: 0.083300	Best loss: 0.070477	Accuracy: 97.42%
5	Validation loss: 0.068931	Best loss: 0.068931	Accuracy: 97.85%
6	Validation loss: 0.066246	Best loss: 0.066246	Accuracy: 98.12%
7	Validation loss: 0.072650	Best loss: 0.066246	Accuracy: 97.81%
8	Validation loss: 0.068915	Best loss: 0.066246	Accuracy: 98.12%
9	Validation loss: 0.069087	Best loss: 0.066246	Accuracy: 98.01%
10	Validation loss: 0.062854	Best loss: 0.062854	Accuracy: 98.05%
11	Validation loss: 0.059984	Best loss: 0.059984	Accuracy: 98.20%
12	Validation loss: 0.061878	Best loss: 0.059984	Accuracy: 98.24%
13	Validation loss: 0.081660	Best loss: 0.059984	Accuracy: 97.69%
14	Validation loss: 0.076593	Best loss: 0.059984	Accuracy: 97.81%
15	Validation loss: 0.071471	Best loss: 0.059984	Accuracy: 98.16%
16	Validation loss: 0.058004	Best loss: 0.058004	Accuracy: 98.51%
17	Validation loss: 0.056867	Best loss: 0.056867	Accuracy: 98.59%
18	Validation loss: 0.062825	Best loss: 0.056867	Accuracy: 98.44%
19	Validation loss: 0.070769	Best loss: 0.056867	Accuracy: 98.16%
20	Validation loss: 0.062489	Best loss: 0.056867	Accuracy: 98.48%
21	Validation loss: 0.061790	Best loss: 0.056867	Accuracy: 98.16%
22	Validation loss: 0.072054	Best loss: 0.056867	Accuracy: 98.08%
23	Validation loss: 0.067156	Best loss: 0.056867	Accuracy: 98.28%
24	Validation loss: 0.069396	Best loss: 0.056867	Accuracy: 98.32%
25	Validation loss: 0.063460	Best loss: 0.056867	Accuracy: 98.44%
26	Validation loss: 0.063505	Best loss: 0.056867	Accuracy: 98.44%
27	Validation loss: 0.059678	Best loss: 0.056867	Accuracy: 98.24%
28	Validation loss: 0.075865	Best loss: 0.056867	Accuracy: 98.44%
29	Validation loss: 0.065118	Best loss: 0.056867	Accuracy: 98.67%
30	Validation loss: 0.064076	Best loss: 0.056867	Accuracy: 98.36%
31	Validation loss: 0.070387	Best loss: 0.056867	Accuracy: 98.40%
32	Validation loss: 0.083473	Best loss: 0.056867	Accuracy: 97.93%
33	Validation loss: 0.076999	Best loss: 0.056867	Accuracy: 98.59%
34	Validation loss: 0.084685	Best loss: 0.056867	Accuracy: 98.44%
35	Validation loss: 0.059550	Best loss: 0.056867	Accuracy: 98.40%
36	Validation loss: 0.075531	Best loss: 0.056867	Accuracy: 98.40%
37	Validation loss: 0.077365	Best loss: 0.056867	Accuracy: 98.20%
38	Validation loss: 0.074113	Best loss: 0.056867	Accuracy: 98.32%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt;, total=  21.7s
[CV] n_neurons=10, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.110876	Best loss: 0.110876	Accuracy: 97.15%
1	Validation loss: 0.097136	Best loss: 0.097136	Accuracy: 97.30%
2	Validation loss: 0.108762	Best loss: 0.097136	Accuracy: 97.11%
3	Validation loss: 0.077975	Best loss: 0.077975	Accuracy: 97.65%
4	Validation loss: 0.076345	Best loss: 0.076345	Accuracy: 97.81%
5	Validation loss: 0.086008	Best loss: 0.076345	Accuracy: 97.38%
6	Validation loss: 0.070425	Best loss: 0.070425	Accuracy: 97.62%
7	Validation loss: 0.080605	Best loss: 0.070425	Accuracy: 97.65%
8	Validation loss: 0.083085	Best loss: 0.070425	Accuracy: 97.73%
9	Validation loss: 0.075073	Best loss: 0.070425	Accuracy: 97.85%
10	Validation loss: 0.081174	Best loss: 0.070425	Accuracy: 97.73%
11	Validation loss: 0.080930	Best loss: 0.070425	Accuracy: 97.81%
12	Validation loss: 0.067873	Best loss: 0.067873	Accuracy: 98.32%
13	Validation loss: 0.082684	Best loss: 0.067873	Accuracy: 97.58%
14	Validation loss: 0.076687	Best loss: 0.067873	Accuracy: 97.93%
15	Validation loss: 0.086362	Best loss: 0.067873	Accuracy: 97.93%
16	Validation loss: 0.070666	Best loss: 0.067873	Accuracy: 98.20%
17	Validation loss: 0.085122	Best loss: 0.067873	Accuracy: 97.81%
18	Validation loss: 0.070560	Best loss: 0.067873	Accuracy: 98.08%
19	Validation loss: 0.068691	Best loss: 0.067873	Accuracy: 98.24%
20	Validation loss: 0.085489	Best loss: 0.067873	Accuracy: 97.85%
21	Validation loss: 0.083958	Best loss: 0.067873	Accuracy: 97.73%
22	Validation loss: 0.075616	Best loss: 0.067873	Accuracy: 97.85%
23	Validation loss: 0.068407	Best loss: 0.067873	Accuracy: 98.16%
24	Validation loss: 0.062856	Best loss: 0.062856	Accuracy: 98.12%
25	Validation loss: 0.075329	Best loss: 0.062856	Accuracy: 97.85%
26	Validation loss: 0.065291	Best loss: 0.062856	Accuracy: 98.36%
27	Validation loss: 0.071871	Best loss: 0.062856	Accuracy: 98.08%
28	Validation loss: 0.074878	Best loss: 0.062856	Accuracy: 98.24%
29	Validation loss: 0.074841	Best loss: 0.062856	Accuracy: 98.12%
30	Validation loss: 0.067025	Best loss: 0.062856	Accuracy: 98.16%
31	Validation loss: 0.089892	Best loss: 0.062856	Accuracy: 97.46%
32	Validation loss: 0.073965	Best loss: 0.062856	Accuracy: 97.85%
33	Validation loss: 0.088968	Best loss: 0.062856	Accuracy: 97.77%
34	Validation loss: 0.079037	Best loss: 0.062856	Accuracy: 98.20%
35	Validation loss: 0.070606	Best loss: 0.062856	Accuracy: 98.40%
36	Validation loss: 0.069871	Best loss: 0.062856	Accuracy: 98.40%
37	Validation loss: 0.087421	Best loss: 0.062856	Accuracy: 97.73%
38	Validation loss: 0.087077	Best loss: 0.062856	Accuracy: 97.81%
39	Validation loss: 0.080832	Best loss: 0.062856	Accuracy: 98.36%
40	Validation loss: 0.074682	Best loss: 0.062856	Accuracy: 97.73%
41	Validation loss: 0.079750	Best loss: 0.062856	Accuracy: 97.85%
42	Validation loss: 0.077463	Best loss: 0.062856	Accuracy: 97.81%
43	Validation loss: 0.083109	Best loss: 0.062856	Accuracy: 98.12%
44	Validation loss: 0.070288	Best loss: 0.062856	Accuracy: 98.24%
45	Validation loss: 0.063948	Best loss: 0.062856	Accuracy: 98.32%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  26.1s
[CV] n_neurons=10, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.128317	Best loss: 0.128317	Accuracy: 96.48%
1	Validation loss: 0.075092	Best loss: 0.075092	Accuracy: 98.01%
2	Validation loss: 0.075293	Best loss: 0.075092	Accuracy: 97.97%
3	Validation loss: 0.081456	Best loss: 0.075092	Accuracy: 97.69%
4	Validation loss: 0.080812	Best loss: 0.075092	Accuracy: 97.81%
5	Validation loss: 0.063330	Best loss: 0.063330	Accuracy: 98.24%
6	Validation loss: 0.059880	Best loss: 0.059880	Accuracy: 98.20%
7	Validation loss: 0.061692	Best loss: 0.059880	Accuracy: 98.28%
8	Validation loss: 0.066429	Best loss: 0.059880	Accuracy: 98.16%
9	Validation loss: 0.084705	Best loss: 0.059880	Accuracy: 97.77%
10	Validation loss: 0.083511	Best loss: 0.059880	Accuracy: 97.69%
11	Validation loss: 0.073107	Best loss: 0.059880	Accuracy: 98.05%
12	Validation loss: 0.063289	Best loss: 0.059880	Accuracy: 98.20%
13	Validation loss: 0.058274	Best loss: 0.058274	Accuracy: 98.24%
14	Validation loss: 0.071322	Best loss: 0.058274	Accuracy: 98.12%
15	Validation loss: 0.069666	Best loss: 0.058274	Accuracy: 98.16%
16	Validation loss: 0.071233	Best loss: 0.058274	Accuracy: 98.12%
17	Validation loss: 0.074383	Best loss: 0.058274	Accuracy: 98.01%
18	Validation loss: 0.073818	Best loss: 0.058274	Accuracy: 97.81%
19	Validation loss: 0.079111	Best loss: 0.058274	Accuracy: 97.97%
20	Validation loss: 0.068530	Best loss: 0.058274	Accuracy: 98.20%
21	Validation loss: 0.063785	Best loss: 0.058274	Accuracy: 98.40%
22	Validation loss: 0.076214	Best loss: 0.058274	Accuracy: 98.12%
23	Validation loss: 0.072015	Best loss: 0.058274	Accuracy: 98.16%
24	Validation loss: 0.058780	Best loss: 0.058274	Accuracy: 98.59%
25	Validation loss: 0.066425	Best loss: 0.058274	Accuracy: 98.40%
26	Validation loss: 0.072151	Best loss: 0.058274	Accuracy: 98.08%
27	Validation loss: 0.097723	Best loss: 0.058274	Accuracy: 97.77%
28	Validation loss: 0.082524	Best loss: 0.058274	Accuracy: 97.77%
29	Validation loss: 0.074120	Best loss: 0.058274	Accuracy: 98.32%
30	Validation loss: 0.067043	Best loss: 0.058274	Accuracy: 98.51%
31	Validation loss: 0.072830	Best loss: 0.058274	Accuracy: 98.28%
32	Validation loss: 0.070611	Best loss: 0.058274	Accuracy: 98.28%
33	Validation loss: 0.083896	Best loss: 0.058274	Accuracy: 98.08%
34	Validation loss: 0.065639	Best loss: 0.058274	Accuracy: 98.51%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  20.6s
[CV] n_neurons=10, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.103742	Best loss: 0.103742	Accuracy: 97.03%
1	Validation loss: 0.094116	Best loss: 0.094116	Accuracy: 97.46%
2	Validation loss: 0.070600	Best loss: 0.070600	Accuracy: 97.81%
3	Validation loss: 0.064489	Best loss: 0.064489	Accuracy: 98.16%
4	Validation loss: 0.061718	Best loss: 0.061718	Accuracy: 98.12%
5	Validation loss: 0.061196	Best loss: 0.061196	Accuracy: 98.20%
6	Validation loss: 0.064247	Best loss: 0.061196	Accuracy: 98.16%
7	Validation loss: 0.059370	Best loss: 0.059370	Accuracy: 98.16%
8	Validation loss: 0.072341	Best loss: 0.059370	Accuracy: 98.16%
9	Validation loss: 0.064684	Best loss: 0.059370	Accuracy: 98.24%
10	Validation loss: 0.064537	Best loss: 0.059370	Accuracy: 98.32%
11	Validation loss: 0.076439	Best loss: 0.059370	Accuracy: 97.65%
12	Validation loss: 0.088139	Best loss: 0.059370	Accuracy: 97.89%
13	Validation loss: 0.073791	Best loss: 0.059370	Accuracy: 98.16%
14	Validation loss: 0.059833	Best loss: 0.059370	Accuracy: 98.40%
15	Validation loss: 0.064499	Best loss: 0.059370	Accuracy: 98.59%
16	Validation loss: 0.068393	Best loss: 0.059370	Accuracy: 97.89%
17	Validation loss: 0.060191	Best loss: 0.059370	Accuracy: 98.05%
18	Validation loss: 0.093175	Best loss: 0.059370	Accuracy: 97.58%
19	Validation loss: 0.060778	Best loss: 0.059370	Accuracy: 98.36%
20	Validation loss: 0.076974	Best loss: 0.059370	Accuracy: 97.97%
21	Validation loss: 0.070255	Best loss: 0.059370	Accuracy: 98.28%
22	Validation loss: 0.065272	Best loss: 0.059370	Accuracy: 98.48%
23	Validation loss: 0.068849	Best loss: 0.059370	Accuracy: 98.44%
24	Validation loss: 0.065552	Best loss: 0.059370	Accuracy: 98.28%
25	Validation loss: 0.071510	Best loss: 0.059370	Accuracy: 98.28%
26	Validation loss: 0.065192	Best loss: 0.059370	Accuracy: 98.32%
27	Validation loss: 0.082874	Best loss: 0.059370	Accuracy: 97.93%
28	Validation loss: 0.076359	Best loss: 0.059370	Accuracy: 98.44%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  17.4s
[CV] n_neurons=50, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.533008	Best loss: 0.533008	Accuracy: 96.56%
1	Validation loss: 0.409109	Best loss: 0.409109	Accuracy: 97.46%
2	Validation loss: 0.272203	Best loss: 0.272203	Accuracy: 97.85%
3	Validation loss: 0.220530	Best loss: 0.220530	Accuracy: 97.97%
4	Validation loss: 0.180164	Best loss: 0.180164	Accuracy: 97.85%
5	Validation loss: 0.133087	Best loss: 0.133087	Accuracy: 98.55%
6	Validation loss: 0.084767	Best loss: 0.084767	Accuracy: 98.75%
7	Validation loss: 0.083154	Best loss: 0.083154	Accuracy: 98.79%
8	Validation loss: 0.137526	Best loss: 0.083154	Accuracy: 98.32%
9	Validation loss: 0.064483	Best loss: 0.064483	Accuracy: 98.87%
10	Validation loss: 0.085900	Best loss: 0.064483	Accuracy: 98.71%
11	Validation loss: 0.079485	Best loss: 0.064483	Accuracy: 98.55%
12	Validation loss: 0.051605	Best loss: 0.051605	Accuracy: 98.83%
13	Validation loss: 0.063876	Best loss: 0.051605	Accuracy: 98.87%
14	Validation loss: 0.116165	Best loss: 0.051605	Accuracy: 97.97%
15	Validation loss: 0.127160	Best loss: 0.051605	Accuracy: 98.05%
16	Validation loss: 0.108921	Best loss: 0.051605	Accuracy: 98.24%
17	Validation loss: 0.167338	Best loss: 0.051605	Accuracy: 97.85%
18	Validation loss: 0.126481	Best loss: 0.051605	Accuracy: 98.20%
19	Validation loss: 0.088520	Best loss: 0.051605	Accuracy: 98.75%
20	Validation loss: 0.088167	Best loss: 0.051605	Accuracy: 98.55%
21	Validation loss: 0.078480	Best loss: 0.051605	Accuracy: 98.59%
22	Validation loss: 0.063172	Best loss: 0.051605	Accuracy: 98.55%
23	Validation loss: 0.057909	Best loss: 0.051605	Accuracy: 98.94%
24	Validation loss: 0.050520	Best loss: 0.050520	Accuracy: 98.98%
25	Validation loss: 0.052607	Best loss: 0.050520	Accuracy: 98.67%
26	Validation loss: 0.048516	Best loss: 0.048516	Accuracy: 99.06%
27	Validation loss: 0.061341	Best loss: 0.048516	Accuracy: 98.87%
28	Validation loss: 0.106222	Best loss: 0.048516	Accuracy: 98.08%
29	Validation loss: 0.110163	Best loss: 0.048516	Accuracy: 98.59%
30	Validation loss: 0.121928	Best loss: 0.048516	Accuracy: 98.55%
31	Validation loss: 0.081042	Best loss: 0.048516	Accuracy: 98.59%
32	Validation loss: 0.081887	Best loss: 0.048516	Accuracy: 98.83%
33	Validation loss: 0.115547	Best loss: 0.048516	Accuracy: 98.20%
34	Validation loss: 0.059520	Best loss: 0.048516	Accuracy: 98.83%
35	Validation loss: 0.051501	Best loss: 0.048516	Accuracy: 98.94%
36	Validation loss: 0.055887	Best loss: 0.048516	Accuracy: 98.94%
37	Validation loss: 0.054992	Best loss: 0.048516	Accuracy: 99.18%
38	Validation loss: 0.041054	Best loss: 0.041054	Accuracy: 99.06%
39	Validation loss: 0.058739	Best loss: 0.041054	Accuracy: 98.75%
40	Validation loss: 0.064774	Best loss: 0.041054	Accuracy: 98.87%
41	Validation loss: 0.080057	Best loss: 0.041054	Accuracy: 98.67%
42	Validation loss: 0.060740	Best loss: 0.041054	Accuracy: 98.83%
43	Validation loss: 0.053522	Best loss: 0.041054	Accuracy: 98.98%
44	Validation loss: 0.080498	Best loss: 0.041054	Accuracy: 98.67%
45	Validation loss: 0.059170	Best loss: 0.041054	Accuracy: 98.71%
46	Validation loss: 0.060503	Best loss: 0.041054	Accuracy: 99.06%
47	Validation loss: 0.054420	Best loss: 0.041054	Accuracy: 98.91%
48	Validation loss: 0.047151	Best loss: 0.041054	Accuracy: 99.06%
49	Validation loss: 0.057592	Best loss: 0.041054	Accuracy: 99.02%
50	Validation loss: 0.115790	Best loss: 0.041054	Accuracy: 98.75%
51	Validation loss: 0.081332	Best loss: 0.041054	Accuracy: 98.71%
52	Validation loss: 0.074642	Best loss: 0.041054	Accuracy: 98.63%
53	Validation loss: 0.065328	Best loss: 0.041054	Accuracy: 98.75%
54	Validation loss: 0.090204	Best loss: 0.041054	Accuracy: 98.63%
55	Validation loss: 0.065194	Best loss: 0.041054	Accuracy: 98.59%
56	Validation loss: 0.063811	Best loss: 0.041054	Accuracy: 98.63%
57	Validation loss: 0.067018	Best loss: 0.041054	Accuracy: 98.71%
58	Validation loss: 0.086265	Best loss: 0.041054	Accuracy: 98.71%
59	Validation loss: 0.071222	Best loss: 0.041054	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.3min
[CV] n_neurons=50, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.610880	Best loss: 0.610880	Accuracy: 96.56%
1	Validation loss: 0.297678	Best loss: 0.297678	Accuracy: 97.65%
2	Validation loss: 0.212271	Best loss: 0.212271	Accuracy: 98.08%
3	Validation loss: 0.174562	Best loss: 0.174562	Accuracy: 98.08%
4	Validation loss: 0.214716	Best loss: 0.174562	Accuracy: 97.58%
5	Validation loss: 0.136293	Best loss: 0.136293	Accuracy: 98.40%
6	Validation loss: 0.123013	Best loss: 0.123013	Accuracy: 98.36%
7	Validation loss: 0.110199	Best loss: 0.110199	Accuracy: 98.51%
8	Validation loss: 0.070733	Best loss: 0.070733	Accuracy: 98.83%
9	Validation loss: 0.107344	Best loss: 0.070733	Accuracy: 98.44%
10	Validation loss: 0.138104	Best loss: 0.070733	Accuracy: 98.36%
11	Validation loss: 0.120876	Best loss: 0.070733	Accuracy: 98.36%
12	Validation loss: 0.089411	Best loss: 0.070733	Accuracy: 98.36%
13	Validation loss: 0.079825	Best loss: 0.070733	Accuracy: 98.40%
14	Validation loss: 0.078849	Best loss: 0.070733	Accuracy: 98.75%
15	Validation loss: 0.082388	Best loss: 0.070733	Accuracy: 98.67%
16	Validation loss: 0.074513	Best loss: 0.070733	Accuracy: 98.71%
17	Validation loss: 0.094155	Best loss: 0.070733	Accuracy: 98.16%
18	Validation loss: 0.092445	Best loss: 0.070733	Accuracy: 98.67%
19	Validation loss: 0.073322	Best loss: 0.070733	Accuracy: 98.83%
20	Validation loss: 0.075842	Best loss: 0.070733	Accuracy: 98.87%
21	Validation loss: 0.100143	Best loss: 0.070733	Accuracy: 98.83%
22	Validation loss: 0.074486	Best loss: 0.070733	Accuracy: 98.83%
23	Validation loss: 0.096176	Best loss: 0.070733	Accuracy: 98.55%
24	Validation loss: 0.069573	Best loss: 0.069573	Accuracy: 98.75%
25	Validation loss: 0.065429	Best loss: 0.065429	Accuracy: 98.71%
26	Validation loss: 0.114342	Best loss: 0.065429	Accuracy: 98.63%
27	Validation loss: 0.061110	Best loss: 0.061110	Accuracy: 98.94%
28	Validation loss: 0.071274	Best loss: 0.061110	Accuracy: 98.75%
29	Validation loss: 0.072533	Best loss: 0.061110	Accuracy: 98.75%
30	Validation loss: 0.064867	Best loss: 0.061110	Accuracy: 98.75%
31	Validation loss: 0.113679	Best loss: 0.061110	Accuracy: 98.24%
32	Validation loss: 0.076449	Best loss: 0.061110	Accuracy: 98.75%
33	Validation loss: 0.113502	Best loss: 0.061110	Accuracy: 98.20%
34	Validation loss: 0.093599	Best loss: 0.061110	Accuracy: 98.91%
35	Validation loss: 0.058427	Best loss: 0.058427	Accuracy: 98.87%
36	Validation loss: 0.083320	Best loss: 0.058427	Accuracy: 98.71%
37	Validation loss: 0.085583	Best loss: 0.058427	Accuracy: 98.59%
38	Validation loss: 0.080198	Best loss: 0.058427	Accuracy: 98.51%
39	Validation loss: 0.132208	Best loss: 0.058427	Accuracy: 98.71%
40	Validation loss: 0.051974	Best loss: 0.051974	Accuracy: 99.02%
41	Validation loss: 0.061323	Best loss: 0.051974	Accuracy: 98.87%
42	Validation loss: 0.069418	Best loss: 0.051974	Accuracy: 98.87%
43	Validation loss: 0.051901	Best loss: 0.051901	Accuracy: 98.87%
44	Validation loss: 0.068134	Best loss: 0.051901	Accuracy: 98.91%
45	Validation loss: 0.068133	Best loss: 0.051901	Accuracy: 98.87%
46	Validation loss: 0.076351	Best loss: 0.051901	Accuracy: 98.83%
47	Validation loss: 0.080728	Best loss: 0.051901	Accuracy: 98.71%
48	Validation loss: 0.080243	Best loss: 0.051901	Accuracy: 98.67%
49	Validation loss: 0.067614	Best loss: 0.051901	Accuracy: 98.94%
50	Validation loss: 0.106507	Best loss: 0.051901	Accuracy: 98.75%
51	Validation loss: 0.106207	Best loss: 0.051901	Accuracy: 98.71%
52	Validation loss: 0.084424	Best loss: 0.051901	Accuracy: 98.75%
53	Validation loss: 0.073955	Best loss: 0.051901	Accuracy: 98.87%
54	Validation loss: 0.071296	Best loss: 0.051901	Accuracy: 98.79%
55	Validation loss: 0.058455	Best loss: 0.051901	Accuracy: 99.10%
56	Validation loss: 0.068347	Best loss: 0.051901	Accuracy: 98.87%
57	Validation loss: 0.055441	Best loss: 0.051901	Accuracy: 98.98%
58	Validation loss: 0.065925	Best loss: 0.051901	Accuracy: 98.79%
59	Validation loss: 0.058657	Best loss: 0.051901	Accuracy: 98.87%
60	Validation loss: 0.081319	Best loss: 0.051901	Accuracy: 98.83%
61	Validation loss: 0.067706	Best loss: 0.051901	Accuracy: 98.98%
62	Validation loss: 0.072807	Best loss: 0.051901	Accuracy: 98.91%
63	Validation loss: 0.066975	Best loss: 0.051901	Accuracy: 98.91%
64	Validation loss: 0.066679	Best loss: 0.051901	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.4min
[CV] n_neurons=50, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.430595	Best loss: 0.430595	Accuracy: 96.76%
1	Validation loss: 0.271660	Best loss: 0.271660	Accuracy: 97.85%
2	Validation loss: 0.254797	Best loss: 0.254797	Accuracy: 97.89%
3	Validation loss: 0.185860	Best loss: 0.185860	Accuracy: 97.93%
4	Validation loss: 0.175529	Best loss: 0.175529	Accuracy: 97.89%
5	Validation loss: 0.112007	Best loss: 0.112007	Accuracy: 98.44%
6	Validation loss: 0.087601	Best loss: 0.087601	Accuracy: 98.67%
7	Validation loss: 0.165923	Best loss: 0.087601	Accuracy: 98.20%
8	Validation loss: 0.072535	Best loss: 0.072535	Accuracy: 98.79%
9	Validation loss: 0.051831	Best loss: 0.051831	Accuracy: 98.87%
10	Validation loss: 0.113352	Best loss: 0.051831	Accuracy: 98.24%
11	Validation loss: 0.063853	Best loss: 0.051831	Accuracy: 98.71%
12	Validation loss: 0.085068	Best loss: 0.051831	Accuracy: 98.44%
13	Validation loss: 0.083468	Best loss: 0.051831	Accuracy: 98.91%
14	Validation loss: 0.045675	Best loss: 0.045675	Accuracy: 99.18%
15	Validation loss: 0.107811	Best loss: 0.045675	Accuracy: 98.48%
16	Validation loss: 0.064623	Best loss: 0.045675	Accuracy: 98.87%
17	Validation loss: 0.049911	Best loss: 0.045675	Accuracy: 98.98%
18	Validation loss: 0.070545	Best loss: 0.045675	Accuracy: 98.75%
19	Validation loss: 0.113387	Best loss: 0.045675	Accuracy: 98.55%
20	Validation loss: 0.072280	Best loss: 0.045675	Accuracy: 98.75%
21	Validation loss: 0.066280	Best loss: 0.045675	Accuracy: 98.79%
22	Validation loss: 0.056014	Best loss: 0.045675	Accuracy: 98.83%
23	Validation loss: 0.043394	Best loss: 0.043394	Accuracy: 99.37%
24	Validation loss: 0.079474	Best loss: 0.043394	Accuracy: 98.98%
25	Validation loss: 0.078034	Best loss: 0.043394	Accuracy: 98.83%
26	Validation loss: 0.065492	Best loss: 0.043394	Accuracy: 98.98%
27	Validation loss: 0.051854	Best loss: 0.043394	Accuracy: 99.06%
28	Validation loss: 0.051665	Best loss: 0.043394	Accuracy: 98.91%
29	Validation loss: 0.048123	Best loss: 0.043394	Accuracy: 99.06%
30	Validation loss: 0.055075	Best loss: 0.043394	Accuracy: 98.79%
31	Validation loss: 0.047857	Best loss: 0.043394	Accuracy: 98.98%
32	Validation loss: 0.042458	Best loss: 0.042458	Accuracy: 99.06%
33	Validation loss: 0.052368	Best loss: 0.042458	Accuracy: 99.06%
34	Validation loss: 0.054192	Best loss: 0.042458	Accuracy: 99.14%
35	Validation loss: 0.056286	Best loss: 0.042458	Accuracy: 99.06%
36	Validation loss: 0.048538	Best loss: 0.042458	Accuracy: 99.30%
37	Validation loss: 0.055224	Best loss: 0.042458	Accuracy: 99.14%
38	Validation loss: 0.055108	Best loss: 0.042458	Accuracy: 98.94%
39	Validation loss: 0.101272	Best loss: 0.042458	Accuracy: 98.55%
40	Validation loss: 0.069054	Best loss: 0.042458	Accuracy: 98.48%
41	Validation loss: 0.045423	Best loss: 0.042458	Accuracy: 99.10%
42	Validation loss: 0.078641	Best loss: 0.042458	Accuracy: 98.59%
43	Validation loss: 0.074372	Best loss: 0.042458	Accuracy: 98.79%
44	Validation loss: 0.053113	Best loss: 0.042458	Accuracy: 98.87%
45	Validation loss: 0.039164	Best loss: 0.039164	Accuracy: 99.10%
46	Validation loss: 0.048144	Best loss: 0.039164	Accuracy: 99.26%
47	Validation loss: 0.063286	Best loss: 0.039164	Accuracy: 98.98%
48	Validation loss: 0.083949	Best loss: 0.039164	Accuracy: 98.71%
49	Validation loss: 0.047078	Best loss: 0.039164	Accuracy: 99.10%
50	Validation loss: 0.047785	Best loss: 0.039164	Accuracy: 99.02%
51	Validation loss: 0.046169	Best loss: 0.039164	Accuracy: 99.06%
52	Validation loss: 0.039927	Best loss: 0.039164	Accuracy: 98.94%
53	Validation loss: 0.065570	Best loss: 0.039164	Accuracy: 99.10%
54	Validation loss: 0.049364	Best loss: 0.039164	Accuracy: 99.18%
55	Validation loss: 0.044314	Best loss: 0.039164	Accuracy: 99.18%
56	Validation loss: 0.032824	Best loss: 0.032824	Accuracy: 99.26%
57	Validation loss: 0.076231	Best loss: 0.032824	Accuracy: 98.55%
58	Validation loss: 0.056755	Best loss: 0.032824	Accuracy: 99.02%
59	Validation loss: 0.036735	Best loss: 0.032824	Accuracy: 99.18%
60	Validation loss: 0.049929	Best loss: 0.032824	Accuracy: 99.06%
61	Validation loss: 0.064037	Best loss: 0.032824	Accuracy: 98.91%
62	Validation loss: 0.056905	Best loss: 0.032824	Accuracy: 99.02%
63	Validation loss: 0.061239	Best loss: 0.032824	Accuracy: 98.87%
64	Validation loss: 0.071391	Best loss: 0.032824	Accuracy: 98.98%
65	Validation loss: 0.047869	Best loss: 0.032824	Accuracy: 99.26%
66	Validation loss: 0.046549	Best loss: 0.032824	Accuracy: 98.98%
67	Validation loss: 0.061597	Best loss: 0.032824	Accuracy: 99.10%
68	Validation loss: 0.053136	Best loss: 0.032824	Accuracy: 99.14%
69	Validation loss: 0.058187	Best loss: 0.032824	Accuracy: 98.98%
70	Validation loss: 0.063641	Best loss: 0.032824	Accuracy: 98.98%
71	Validation loss: 0.054975	Best loss: 0.032824	Accuracy: 99.06%
72	Validation loss: 0.062007	Best loss: 0.032824	Accuracy: 99.02%
73	Validation loss: 0.053355	Best loss: 0.032824	Accuracy: 99.10%
74	Validation loss: 0.048235	Best loss: 0.032824	Accuracy: 99.22%
75	Validation loss: 0.060264	Best loss: 0.032824	Accuracy: 99.02%
76	Validation loss: 0.034596	Best loss: 0.032824	Accuracy: 99.30%
77	Validation loss: 0.043796	Best loss: 0.032824	Accuracy: 99.26%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.7min
[CV] n_neurons=10, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 10.944671	Best loss: 10.944671	Accuracy: 76.90%
1	Validation loss: 3.965482	Best loss: 3.965482	Accuracy: 76.58%
2	Validation loss: 1.150698	Best loss: 1.150698	Accuracy: 87.57%
3	Validation loss: 0.732714	Best loss: 0.732714	Accuracy: 90.93%
4	Validation loss: 0.685892	Best loss: 0.685892	Accuracy: 90.89%
5	Validation loss: 0.511286	Best loss: 0.511286	Accuracy: 92.57%
6	Validation loss: 0.688839	Best loss: 0.511286	Accuracy: 90.58%
7	Validation loss: 0.409808	Best loss: 0.409808	Accuracy: 93.08%
8	Validation loss: 0.260680	Best loss: 0.260680	Accuracy: 95.23%
9	Validation loss: 0.409109	Best loss: 0.260680	Accuracy: 93.98%
10	Validation loss: 0.503037	Best loss: 0.260680	Accuracy: 92.89%
11	Validation loss: 0.283955	Best loss: 0.260680	Accuracy: 95.11%
12	Validation loss: 0.553312	Best loss: 0.260680	Accuracy: 92.73%
13	Validation loss: 0.283005	Best loss: 0.260680	Accuracy: 95.70%
14	Validation loss: 0.233721	Best loss: 0.233721	Accuracy: 95.62%
15	Validation loss: 0.236920	Best loss: 0.233721	Accuracy: 95.47%
16	Validation loss: 0.266289	Best loss: 0.233721	Accuracy: 95.43%
17	Validation loss: 0.199075	Best loss: 0.199075	Accuracy: 96.56%
18	Validation loss: 0.182125	Best loss: 0.182125	Accuracy: 96.21%
19	Validation loss: 0.127360	Best loss: 0.127360	Accuracy: 97.19%
20	Validation loss: 0.171017	Best loss: 0.127360	Accuracy: 96.72%
21	Validation loss: 0.161836	Best loss: 0.127360	Accuracy: 96.29%
22	Validation loss: 0.133311	Best loss: 0.127360	Accuracy: 96.95%
23	Validation loss: 0.300654	Best loss: 0.127360	Accuracy: 94.92%
24	Validation loss: 0.229052	Best loss: 0.127360	Accuracy: 96.09%
25	Validation loss: 0.169035	Best loss: 0.127360	Accuracy: 96.44%
26	Validation loss: 0.153240	Best loss: 0.127360	Accuracy: 97.07%
27	Validation loss: 0.219183	Best loss: 0.127360	Accuracy: 96.95%
28	Validation loss: 0.191678	Best loss: 0.127360	Accuracy: 96.68%
29	Validation loss: 0.167680	Best loss: 0.127360	Accuracy: 97.07%
30	Validation loss: 0.215338	Best loss: 0.127360	Accuracy: 96.09%
31	Validation loss: 0.262632	Best loss: 0.127360	Accuracy: 95.78%
32	Validation loss: 0.174438	Best loss: 0.127360	Accuracy: 97.07%
33	Validation loss: 0.135872	Best loss: 0.127360	Accuracy: 97.42%
34	Validation loss: 0.204584	Best loss: 0.127360	Accuracy: 96.56%
35	Validation loss: 0.298407	Best loss: 0.127360	Accuracy: 95.15%
36	Validation loss: 0.187864	Best loss: 0.127360	Accuracy: 96.56%
37	Validation loss: 0.185460	Best loss: 0.127360	Accuracy: 96.60%
38	Validation loss: 0.202047	Best loss: 0.127360	Accuracy: 96.40%
39	Validation loss: 0.215712	Best loss: 0.127360	Accuracy: 95.90%
40	Validation loss: 0.151535	Best loss: 0.127360	Accuracy: 96.95%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  10.6s
[CV] n_neurons=10, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 27.927504	Best loss: 27.927504	Accuracy: 54.38%
1	Validation loss: 14.565822	Best loss: 14.565822	Accuracy: 49.41%
2	Validation loss: 5.827413	Best loss: 5.827413	Accuracy: 54.14%
3	Validation loss: 2.983169	Best loss: 2.983169	Accuracy: 59.50%
4	Validation loss: 1.272479	Best loss: 1.272479	Accuracy: 74.75%
5	Validation loss: 1.126921	Best loss: 1.126921	Accuracy: 80.02%
6	Validation loss: 0.697768	Best loss: 0.697768	Accuracy: 85.89%
7	Validation loss: 0.743390	Best loss: 0.697768	Accuracy: 85.38%
8	Validation loss: 0.250208	Best loss: 0.250208	Accuracy: 94.37%
9	Validation loss: 0.214589	Best loss: 0.214589	Accuracy: 95.54%
10	Validation loss: 0.481041	Best loss: 0.214589	Accuracy: 91.63%
11	Validation loss: 0.233213	Best loss: 0.214589	Accuracy: 94.92%
12	Validation loss: 0.215314	Best loss: 0.214589	Accuracy: 96.13%
13	Validation loss: 0.177559	Best loss: 0.177559	Accuracy: 96.09%
14	Validation loss: 0.192299	Best loss: 0.177559	Accuracy: 95.31%
15	Validation loss: 0.272739	Best loss: 0.177559	Accuracy: 94.84%
16	Validation loss: 0.155834	Best loss: 0.155834	Accuracy: 97.03%
17	Validation loss: 0.155864	Best loss: 0.155834	Accuracy: 96.79%
18	Validation loss: 0.170858	Best loss: 0.155834	Accuracy: 96.60%
19	Validation loss: 0.181635	Best loss: 0.155834	Accuracy: 96.76%
20	Validation loss: 0.304427	Best loss: 0.155834	Accuracy: 95.62%
21	Validation loss: 0.270227	Best loss: 0.155834	Accuracy: 95.66%
22	Validation loss: 0.202035	Best loss: 0.155834	Accuracy: 96.09%
23	Validation loss: 0.177135	Best loss: 0.155834	Accuracy: 96.83%
24	Validation loss: 0.149395	Best loss: 0.149395	Accuracy: 96.95%
25	Validation loss: 0.094757	Best loss: 0.094757	Accuracy: 97.81%
26	Validation loss: 0.100703	Best loss: 0.094757	Accuracy: 97.89%
27	Validation loss: 0.176204	Best loss: 0.094757	Accuracy: 96.83%
28	Validation loss: 0.179054	Best loss: 0.094757	Accuracy: 96.91%
29	Validation loss: 0.171116	Best loss: 0.094757	Accuracy: 96.95%
30	Validation loss: 0.124486	Best loss: 0.094757	Accuracy: 97.54%
31	Validation loss: 0.146956	Best loss: 0.094757	Accuracy: 97.22%
32	Validation loss: 0.134342	Best loss: 0.094757	Accuracy: 97.50%
33	Validation loss: 0.118032	Best loss: 0.094757	Accuracy: 97.85%
34	Validation loss: 0.150646	Best loss: 0.094757	Accuracy: 97.65%
35	Validation loss: 0.145565	Best loss: 0.094757	Accuracy: 97.62%
36	Validation loss: 0.179058	Best loss: 0.094757	Accuracy: 97.07%
37	Validation loss: 0.133366	Best loss: 0.094757	Accuracy: 97.73%
38	Validation loss: 0.143212	Best loss: 0.094757	Accuracy: 97.97%
39	Validation loss: 0.179720	Best loss: 0.094757	Accuracy: 97.42%
40	Validation loss: 0.181740	Best loss: 0.094757	Accuracy: 96.76%
41	Validation loss: 0.127605	Best loss: 0.094757	Accuracy: 98.28%
42	Validation loss: 0.146948	Best loss: 0.094757	Accuracy: 97.93%
43	Validation loss: 0.198929	Best loss: 0.094757	Accuracy: 97.22%
44	Validation loss: 0.141142	Best loss: 0.094757	Accuracy: 97.97%
45	Validation loss: 0.164099	Best loss: 0.094757	Accuracy: 97.42%
46	Validation loss: 0.227640	Best loss: 0.094757	Accuracy: 96.40%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  11.7s
[CV] n_neurons=10, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 5.273319	Best loss: 5.273319	Accuracy: 82.76%
1	Validation loss: 0.725046	Best loss: 0.725046	Accuracy: 93.04%
2	Validation loss: 0.557098	Best loss: 0.557098	Accuracy: 92.96%
3	Validation loss: 0.241573	Best loss: 0.241573	Accuracy: 96.25%
4	Validation loss: 0.229374	Best loss: 0.229374	Accuracy: 96.40%
5	Validation loss: 0.375524	Best loss: 0.229374	Accuracy: 94.18%
6	Validation loss: 0.205276	Best loss: 0.205276	Accuracy: 96.29%
7	Validation loss: 0.172000	Best loss: 0.172000	Accuracy: 95.97%
8	Validation loss: 0.141545	Best loss: 0.141545	Accuracy: 97.03%
9	Validation loss: 0.125344	Best loss: 0.125344	Accuracy: 97.54%
10	Validation loss: 0.132201	Best loss: 0.125344	Accuracy: 96.99%
11	Validation loss: 0.135031	Best loss: 0.125344	Accuracy: 97.22%
12	Validation loss: 0.124012	Best loss: 0.124012	Accuracy: 97.19%
13	Validation loss: 0.161179	Best loss: 0.124012	Accuracy: 96.91%
14	Validation loss: 0.131857	Best loss: 0.124012	Accuracy: 97.62%
15	Validation loss: 0.159686	Best loss: 0.124012	Accuracy: 96.91%
16	Validation loss: 0.148231	Best loss: 0.124012	Accuracy: 97.58%
17	Validation loss: 0.137664	Best loss: 0.124012	Accuracy: 97.03%
18	Validation loss: 0.103353	Best loss: 0.103353	Accuracy: 97.58%
19	Validation loss: 0.143720	Best loss: 0.103353	Accuracy: 97.30%
20	Validation loss: 0.123049	Best loss: 0.103353	Accuracy: 98.05%
21	Validation loss: 0.157318	Best loss: 0.103353	Accuracy: 96.99%
22	Validation loss: 0.161461	Best loss: 0.103353	Accuracy: 97.62%
23	Validation loss: 0.153319	Best loss: 0.103353	Accuracy: 97.42%
24	Validation loss: 0.132319	Best loss: 0.103353	Accuracy: 98.01%
25	Validation loss: 0.126268	Best loss: 0.103353	Accuracy: 97.89%
26	Validation loss: 0.137258	Best loss: 0.103353	Accuracy: 97.69%
27	Validation loss: 0.129887	Best loss: 0.103353	Accuracy: 97.58%
28	Validation loss: 0.153580	Best loss: 0.103353	Accuracy: 97.42%
29	Validation loss: 0.155681	Best loss: 0.103353	Accuracy: 97.46%
30	Validation loss: 0.132235	Best loss: 0.103353	Accuracy: 97.77%
31	Validation loss: 0.159612	Best loss: 0.103353	Accuracy: 97.26%
32	Validation loss: 0.119781	Best loss: 0.103353	Accuracy: 98.08%
33	Validation loss: 0.140116	Best loss: 0.103353	Accuracy: 97.85%
34	Validation loss: 0.166450	Best loss: 0.103353	Accuracy: 97.07%
35	Validation loss: 0.154991	Best loss: 0.103353	Accuracy: 97.38%
36	Validation loss: 0.134517	Best loss: 0.103353	Accuracy: 97.54%
37	Validation loss: 0.136250	Best loss: 0.103353	Accuracy: 97.62%
38	Validation loss: 0.129622	Best loss: 0.103353	Accuracy: 97.73%
39	Validation loss: 0.168987	Best loss: 0.103353	Accuracy: 97.50%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  10.0s
[CV] n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.110941	Best loss: 0.110941	Accuracy: 97.65%
1	Validation loss: 0.086602	Best loss: 0.086602	Accuracy: 97.73%
2	Validation loss: 0.065194	Best loss: 0.065194	Accuracy: 97.97%
3	Validation loss: 0.048958	Best loss: 0.048958	Accuracy: 98.51%
4	Validation loss: 0.046328	Best loss: 0.046328	Accuracy: 98.71%
5	Validation loss: 0.051417	Best loss: 0.046328	Accuracy: 98.75%
6	Validation loss: 0.037266	Best loss: 0.037266	Accuracy: 98.98%
7	Validation loss: 0.045070	Best loss: 0.037266	Accuracy: 98.87%
8	Validation loss: 0.044019	Best loss: 0.037266	Accuracy: 99.02%
9	Validation loss: 0.037622	Best loss: 0.037266	Accuracy: 98.83%
10	Validation loss: 0.048633	Best loss: 0.037266	Accuracy: 98.75%
11	Validation loss: 0.042313	Best loss: 0.037266	Accuracy: 99.06%
12	Validation loss: 0.055557	Best loss: 0.037266	Accuracy: 98.59%
13	Validation loss: 0.045406	Best loss: 0.037266	Accuracy: 99.06%
14	Validation loss: 0.048013	Best loss: 0.037266	Accuracy: 98.75%
15	Validation loss: 0.069304	Best loss: 0.037266	Accuracy: 98.79%
16	Validation loss: 0.070519	Best loss: 0.037266	Accuracy: 98.83%
17	Validation loss: 0.050894	Best loss: 0.037266	Accuracy: 99.14%
18	Validation loss: 0.044654	Best loss: 0.037266	Accuracy: 99.02%
19	Validation loss: 0.050332	Best loss: 0.037266	Accuracy: 98.94%
20	Validation loss: 0.058205	Best loss: 0.037266	Accuracy: 98.98%
21	Validation loss: 0.049271	Best loss: 0.037266	Accuracy: 99.02%
22	Validation loss: 0.050131	Best loss: 0.037266	Accuracy: 99.14%
23	Validation loss: 0.047431	Best loss: 0.037266	Accuracy: 99.06%
24	Validation loss: 0.034011	Best loss: 0.034011	Accuracy: 99.30%
25	Validation loss: 0.049222	Best loss: 0.034011	Accuracy: 99.06%
26	Validation loss: 0.051062	Best loss: 0.034011	Accuracy: 98.83%
27	Validation loss: 0.051114	Best loss: 0.034011	Accuracy: 99.14%
28	Validation loss: 0.055173	Best loss: 0.034011	Accuracy: 98.91%
29	Validation loss: 0.060403	Best loss: 0.034011	Accuracy: 98.75%
30	Validation loss: 0.045460	Best loss: 0.034011	Accuracy: 98.94%
31	Validation loss: 0.052041	Best loss: 0.034011	Accuracy: 98.91%
32	Validation loss: 0.054388	Best loss: 0.034011	Accuracy: 99.02%
33	Validation loss: 0.085175	Best loss: 0.034011	Accuracy: 98.59%
34	Validation loss: 0.056819	Best loss: 0.034011	Accuracy: 98.40%
35	Validation loss: 0.048641	Best loss: 0.034011	Accuracy: 98.94%
36	Validation loss: 0.048707	Best loss: 0.034011	Accuracy: 98.83%
37	Validation loss: 0.038416	Best loss: 0.034011	Accuracy: 99.06%
38	Validation loss: 0.045787	Best loss: 0.034011	Accuracy: 98.94%
39	Validation loss: 0.054784	Best loss: 0.034011	Accuracy: 99.02%
40	Validation loss: 0.048843	Best loss: 0.034011	Accuracy: 98.94%
41	Validation loss: 0.066003	Best loss: 0.034011	Accuracy: 98.79%
42	Validation loss: 0.050372	Best loss: 0.034011	Accuracy: 99.10%
43	Validation loss: 0.055607	Best loss: 0.034011	Accuracy: 99.10%
44	Validation loss: 0.069044	Best loss: 0.034011	Accuracy: 98.71%
45	Validation loss: 0.053681	Best loss: 0.034011	Accuracy: 99.14%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total=  59.5s
[CV] n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.227818	Best loss: 0.227818	Accuracy: 93.82%
1	Validation loss: 0.060035	Best loss: 0.060035	Accuracy: 98.12%
2	Validation loss: 0.050126	Best loss: 0.050126	Accuracy: 98.51%
3	Validation loss: 0.050191	Best loss: 0.050126	Accuracy: 98.48%
4	Validation loss: 0.062277	Best loss: 0.050126	Accuracy: 98.16%
5	Validation loss: 0.053424	Best loss: 0.050126	Accuracy: 98.63%
6	Validation loss: 0.038809	Best loss: 0.038809	Accuracy: 98.94%
7	Validation loss: 0.040676	Best loss: 0.038809	Accuracy: 98.91%
8	Validation loss: 0.045545	Best loss: 0.038809	Accuracy: 98.83%
9	Validation loss: 0.041480	Best loss: 0.038809	Accuracy: 99.02%
10	Validation loss: 0.051716	Best loss: 0.038809	Accuracy: 98.75%
11	Validation loss: 0.047319	Best loss: 0.038809	Accuracy: 98.79%
12	Validation loss: 0.039581	Best loss: 0.038809	Accuracy: 99.22%
13	Validation loss: 0.071874	Best loss: 0.038809	Accuracy: 98.24%
14	Validation loss: 0.035438	Best loss: 0.035438	Accuracy: 99.34%
15	Validation loss: 0.048674	Best loss: 0.035438	Accuracy: 98.83%
16	Validation loss: 0.038977	Best loss: 0.035438	Accuracy: 98.98%
17	Validation loss: 0.042991	Best loss: 0.035438	Accuracy: 99.10%
18	Validation loss: 0.046785	Best loss: 0.035438	Accuracy: 98.98%
19	Validation loss: 0.045387	Best loss: 0.035438	Accuracy: 98.94%
20	Validation loss: 0.048074	Best loss: 0.035438	Accuracy: 99.06%
21	Validation loss: 0.036463	Best loss: 0.035438	Accuracy: 99.10%
22	Validation loss: 0.040995	Best loss: 0.035438	Accuracy: 98.98%
23	Validation loss: 0.079048	Best loss: 0.035438	Accuracy: 98.44%
24	Validation loss: 0.036432	Best loss: 0.035438	Accuracy: 99.18%
25	Validation loss: 0.050729	Best loss: 0.035438	Accuracy: 98.94%
26	Validation loss: 0.042850	Best loss: 0.035438	Accuracy: 99.02%
27	Validation loss: 0.044767	Best loss: 0.035438	Accuracy: 98.98%
28	Validation loss: 0.049121	Best loss: 0.035438	Accuracy: 98.91%
29	Validation loss: 0.052378	Best loss: 0.035438	Accuracy: 98.87%
30	Validation loss: 0.058924	Best loss: 0.035438	Accuracy: 98.83%
31	Validation loss: 0.040659	Best loss: 0.035438	Accuracy: 99.06%
32	Validation loss: 0.043289	Best loss: 0.035438	Accuracy: 99.10%
33	Validation loss: 0.079219	Best loss: 0.035438	Accuracy: 98.32%
34	Validation loss: 0.056357	Best loss: 0.035438	Accuracy: 98.91%
35	Validation loss: 0.050193	Best loss: 0.035438	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total=  53.4s
[CV] n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.100431	Best loss: 0.100431	Accuracy: 97.46%
1	Validation loss: 0.052242	Best loss: 0.052242	Accuracy: 98.32%
2	Validation loss: 0.053935	Best loss: 0.052242	Accuracy: 98.12%
3	Validation loss: 0.046668	Best loss: 0.046668	Accuracy: 98.63%
4	Validation loss: 0.039538	Best loss: 0.039538	Accuracy: 98.94%
5	Validation loss: 0.044142	Best loss: 0.039538	Accuracy: 98.67%
6	Validation loss: 0.045630	Best loss: 0.039538	Accuracy: 98.55%
7	Validation loss: 0.090270	Best loss: 0.039538	Accuracy: 98.36%
8	Validation loss: 0.035029	Best loss: 0.035029	Accuracy: 99.10%
9	Validation loss: 0.031966	Best loss: 0.031966	Accuracy: 99.22%
10	Validation loss: 0.048708	Best loss: 0.031966	Accuracy: 98.94%
11	Validation loss: 0.043103	Best loss: 0.031966	Accuracy: 98.91%
12	Validation loss: 0.050176	Best loss: 0.031966	Accuracy: 99.14%
13	Validation loss: 0.027568	Best loss: 0.027568	Accuracy: 99.06%
14	Validation loss: 0.045951	Best loss: 0.027568	Accuracy: 98.94%
15	Validation loss: 0.047975	Best loss: 0.027568	Accuracy: 98.87%
16	Validation loss: 0.039741	Best loss: 0.027568	Accuracy: 99.02%
17	Validation loss: 0.047691	Best loss: 0.027568	Accuracy: 98.83%
18	Validation loss: 0.071514	Best loss: 0.027568	Accuracy: 98.24%
19	Validation loss: 0.057298	Best loss: 0.027568	Accuracy: 98.94%
20	Validation loss: 0.041518	Best loss: 0.027568	Accuracy: 99.02%
21	Validation loss: 0.046094	Best loss: 0.027568	Accuracy: 98.94%
22	Validation loss: 0.034156	Best loss: 0.027568	Accuracy: 99.22%
23	Validation loss: 0.041221	Best loss: 0.027568	Accuracy: 99.18%
24	Validation loss: 0.045981	Best loss: 0.027568	Accuracy: 99.14%
25	Validation loss: 0.051341	Best loss: 0.027568	Accuracy: 98.94%
26	Validation loss: 0.053904	Best loss: 0.027568	Accuracy: 98.87%
27	Validation loss: 0.035324	Best loss: 0.027568	Accuracy: 99.10%
28	Validation loss: 0.054733	Best loss: 0.027568	Accuracy: 98.87%
29	Validation loss: 0.048931	Best loss: 0.027568	Accuracy: 99.02%
30	Validation loss: 0.038470	Best loss: 0.027568	Accuracy: 99.22%
31	Validation loss: 0.055471	Best loss: 0.027568	Accuracy: 98.79%
32	Validation loss: 0.050563	Best loss: 0.027568	Accuracy: 99.02%
33	Validation loss: 0.032691	Best loss: 0.027568	Accuracy: 99.34%
34	Validation loss: 0.042959	Best loss: 0.027568	Accuracy: 99.30%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total=  55.1s
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.248657	Best loss: 0.248657	Accuracy: 92.89%
1	Validation loss: 0.116585	Best loss: 0.116585	Accuracy: 96.44%
2	Validation loss: 0.147705	Best loss: 0.116585	Accuracy: 96.01%
3	Validation loss: 0.130481	Best loss: 0.116585	Accuracy: 96.52%
4	Validation loss: 0.112161	Best loss: 0.112161	Accuracy: 96.68%
5	Validation loss: 0.108584	Best loss: 0.108584	Accuracy: 96.56%
6	Validation loss: 0.110020	Best loss: 0.108584	Accuracy: 97.07%
7	Validation loss: 0.068777	Best loss: 0.068777	Accuracy: 98.12%
8	Validation loss: 0.257849	Best loss: 0.068777	Accuracy: 93.67%
9	Validation loss: 0.075992	Best loss: 0.068777	Accuracy: 97.97%
10	Validation loss: 0.066760	Best loss: 0.066760	Accuracy: 97.97%
11	Validation loss: 0.057257	Best loss: 0.057257	Accuracy: 98.20%
12	Validation loss: 0.138438	Best loss: 0.057257	Accuracy: 96.36%
13	Validation loss: 0.054388	Best loss: 0.054388	Accuracy: 98.36%
14	Validation loss: 0.051742	Best loss: 0.051742	Accuracy: 98.48%
15	Validation loss: 0.065669	Best loss: 0.051742	Accuracy: 98.40%
16	Validation loss: 0.084321	Best loss: 0.051742	Accuracy: 97.85%
17	Validation loss: 0.083975	Best loss: 0.051742	Accuracy: 97.89%
18	Validation loss: 0.060255	Best loss: 0.051742	Accuracy: 98.32%
19	Validation loss: 0.041727	Best loss: 0.041727	Accuracy: 98.91%
20	Validation loss: 0.114738	Best loss: 0.041727	Accuracy: 97.19%
21	Validation loss: 0.070797	Best loss: 0.041727	Accuracy: 98.36%
22	Validation loss: 0.064202	Best loss: 0.041727	Accuracy: 98.51%
23	Validation loss: 0.073392	Best loss: 0.041727	Accuracy: 98.51%
24	Validation loss: 0.081319	Best loss: 0.041727	Accuracy: 97.65%
25	Validation loss: 0.098593	Best loss: 0.041727	Accuracy: 98.24%
26	Validation loss: 0.061258	Best loss: 0.041727	Accuracy: 98.83%
27	Validation loss: 0.102054	Best loss: 0.041727	Accuracy: 97.97%
28	Validation loss: 0.069668	Best loss: 0.041727	Accuracy: 98.59%
29	Validation loss: 0.094252	Best loss: 0.041727	Accuracy: 97.77%
30	Validation loss: 0.062824	Best loss: 0.041727	Accuracy: 98.40%
31	Validation loss: 0.069289	Best loss: 0.041727	Accuracy: 98.24%
32	Validation loss: 0.060514	Best loss: 0.041727	Accuracy: 98.83%
33	Validation loss: 0.045102	Best loss: 0.041727	Accuracy: 98.83%
34	Validation loss: 0.048141	Best loss: 0.041727	Accuracy: 98.94%
35	Validation loss: 0.062613	Best loss: 0.041727	Accuracy: 98.51%
36	Validation loss: 0.072952	Best loss: 0.041727	Accuracy: 98.16%
37	Validation loss: 0.076224	Best loss: 0.041727	Accuracy: 98.20%
38	Validation loss: 0.058170	Best loss: 0.041727	Accuracy: 98.63%
39	Validation loss: 0.065142	Best loss: 0.041727	Accuracy: 98.40%
40	Validation loss: 0.054446	Best loss: 0.041727	Accuracy: 98.59%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 4.6min
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.276655	Best loss: 0.276655	Accuracy: 91.40%
1	Validation loss: 0.173439	Best loss: 0.173439	Accuracy: 95.35%
2	Validation loss: 0.109202	Best loss: 0.109202	Accuracy: 96.68%
3	Validation loss: 0.070781	Best loss: 0.070781	Accuracy: 97.89%
4	Validation loss: 0.070934	Best loss: 0.070781	Accuracy: 98.05%
5	Validation loss: 0.113649	Best loss: 0.070781	Accuracy: 97.30%
6	Validation loss: 0.075658	Best loss: 0.070781	Accuracy: 97.65%
7	Validation loss: 0.081551	Best loss: 0.070781	Accuracy: 97.85%
8	Validation loss: 0.080370	Best loss: 0.070781	Accuracy: 97.77%
9	Validation loss: 0.064127	Best loss: 0.064127	Accuracy: 98.24%
10	Validation loss: 0.077136	Best loss: 0.064127	Accuracy: 98.08%
11	Validation loss: 0.076064	Best loss: 0.064127	Accuracy: 98.36%
12	Validation loss: 0.088660	Best loss: 0.064127	Accuracy: 98.05%
13	Validation loss: 0.055455	Best loss: 0.055455	Accuracy: 98.44%
14	Validation loss: 0.158713	Best loss: 0.055455	Accuracy: 96.01%
15	Validation loss: 0.077984	Best loss: 0.055455	Accuracy: 97.97%
16	Validation loss: 0.083500	Best loss: 0.055455	Accuracy: 97.65%
17	Validation loss: 0.103439	Best loss: 0.055455	Accuracy: 98.36%
18	Validation loss: 0.080688	Best loss: 0.055455	Accuracy: 98.44%
19	Validation loss: 0.056690	Best loss: 0.055455	Accuracy: 98.59%
20	Validation loss: 0.112712	Best loss: 0.055455	Accuracy: 97.73%
21	Validation loss: 0.083653	Best loss: 0.055455	Accuracy: 98.20%
22	Validation loss: 0.084579	Best loss: 0.055455	Accuracy: 97.11%
23	Validation loss: 0.081488	Best loss: 0.055455	Accuracy: 97.81%
24	Validation loss: 0.150813	Best loss: 0.055455	Accuracy: 96.87%
25	Validation loss: 0.067150	Best loss: 0.055455	Accuracy: 98.36%
26	Validation loss: 0.072063	Best loss: 0.055455	Accuracy: 98.16%
27	Validation loss: 0.077476	Best loss: 0.055455	Accuracy: 98.12%
28	Validation loss: 0.106517	Best loss: 0.055455	Accuracy: 97.30%
29	Validation loss: 0.061910	Best loss: 0.055455	Accuracy: 98.75%
30	Validation loss: 0.106019	Best loss: 0.055455	Accuracy: 97.58%
31	Validation loss: 0.193026	Best loss: 0.055455	Accuracy: 97.15%
32	Validation loss: 0.072904	Best loss: 0.055455	Accuracy: 98.75%
33	Validation loss: 0.064917	Best loss: 0.055455	Accuracy: 98.67%
34	Validation loss: 0.067237	Best loss: 0.055455	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 3.5min
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.117338	Best loss: 0.117338	Accuracy: 96.52%
1	Validation loss: 0.092318	Best loss: 0.092318	Accuracy: 97.38%
2	Validation loss: 0.097302	Best loss: 0.092318	Accuracy: 97.34%
3	Validation loss: 0.086920	Best loss: 0.086920	Accuracy: 97.50%
4	Validation loss: 0.231204	Best loss: 0.086920	Accuracy: 92.69%
5	Validation loss: 0.081361	Best loss: 0.081361	Accuracy: 97.81%
6	Validation loss: 0.149570	Best loss: 0.081361	Accuracy: 96.64%
7	Validation loss: 0.060252	Best loss: 0.060252	Accuracy: 98.36%
8	Validation loss: 0.073927	Best loss: 0.060252	Accuracy: 97.93%
9	Validation loss: 0.072361	Best loss: 0.060252	Accuracy: 98.08%
10	Validation loss: 0.127703	Best loss: 0.060252	Accuracy: 96.56%
11	Validation loss: 0.078462	Best loss: 0.060252	Accuracy: 98.12%
12	Validation loss: 0.057873	Best loss: 0.057873	Accuracy: 98.44%
13	Validation loss: 0.063342	Best loss: 0.057873	Accuracy: 98.51%
14	Validation loss: 0.088004	Best loss: 0.057873	Accuracy: 98.16%
15	Validation loss: 0.065105	Best loss: 0.057873	Accuracy: 98.16%
16	Validation loss: 0.066966	Best loss: 0.057873	Accuracy: 98.63%
17	Validation loss: 0.057045	Best loss: 0.057045	Accuracy: 98.28%
18	Validation loss: 0.080464	Best loss: 0.057045	Accuracy: 98.20%
19	Validation loss: 0.156331	Best loss: 0.057045	Accuracy: 96.72%
20	Validation loss: 0.072999	Best loss: 0.057045	Accuracy: 97.89%
21	Validation loss: 0.072467	Best loss: 0.057045	Accuracy: 98.32%
22	Validation loss: 0.054721	Best loss: 0.054721	Accuracy: 98.32%
23	Validation loss: 0.055265	Best loss: 0.054721	Accuracy: 98.79%
24	Validation loss: 0.081718	Best loss: 0.054721	Accuracy: 98.40%
25	Validation loss: 0.076472	Best loss: 0.054721	Accuracy: 98.08%
26	Validation loss: 0.039356	Best loss: 0.039356	Accuracy: 99.06%
27	Validation loss: 0.039989	Best loss: 0.039356	Accuracy: 98.83%
28	Validation loss: 0.057372	Best loss: 0.039356	Accuracy: 98.94%
29	Validation loss: 0.061136	Best loss: 0.039356	Accuracy: 98.20%
30	Validation loss: 0.077420	Best loss: 0.039356	Accuracy: 98.24%
31	Validation loss: 0.056614	Best loss: 0.039356	Accuracy: 98.44%
32	Validation loss: 0.063617	Best loss: 0.039356	Accuracy: 97.97%
33	Validation loss: 0.067815	Best loss: 0.039356	Accuracy: 98.16%
34	Validation loss: 0.103526	Best loss: 0.039356	Accuracy: 97.93%
35	Validation loss: 0.056190	Best loss: 0.039356	Accuracy: 98.48%
36	Validation loss: 0.063889	Best loss: 0.039356	Accuracy: 98.16%
37	Validation loss: 0.043173	Best loss: 0.039356	Accuracy: 98.83%
38	Validation loss: 0.068357	Best loss: 0.039356	Accuracy: 98.59%
39	Validation loss: 0.038851	Best loss: 0.038851	Accuracy: 98.91%
40	Validation loss: 0.051894	Best loss: 0.038851	Accuracy: 98.67%
41	Validation loss: 0.135213	Best loss: 0.038851	Accuracy: 98.24%
42	Validation loss: 0.449328	Best loss: 0.038851	Accuracy: 95.70%
43	Validation loss: 0.037754	Best loss: 0.037754	Accuracy: 98.91%
44	Validation loss: 0.042854	Best loss: 0.037754	Accuracy: 98.91%
45	Validation loss: 0.056069	Best loss: 0.037754	Accuracy: 98.59%
46	Validation loss: 0.067169	Best loss: 0.037754	Accuracy: 98.44%
47	Validation loss: 0.039110	Best loss: 0.037754	Accuracy: 98.98%
48	Validation loss: 0.065623	Best loss: 0.037754	Accuracy: 98.79%
49	Validation loss: 0.069175	Best loss: 0.037754	Accuracy: 98.51%
50	Validation loss: 0.601280	Best loss: 0.037754	Accuracy: 93.00%
51	Validation loss: 0.043424	Best loss: 0.037754	Accuracy: 98.91%
52	Validation loss: 0.061593	Best loss: 0.037754	Accuracy: 98.44%
53	Validation loss: 0.069430	Best loss: 0.037754	Accuracy: 98.40%
54	Validation loss: 0.057394	Best loss: 0.037754	Accuracy: 98.79%
55	Validation loss: 0.060271	Best loss: 0.037754	Accuracy: 98.59%
56	Validation loss: 0.066447	Best loss: 0.037754	Accuracy: 98.71%
57	Validation loss: 0.045430	Best loss: 0.037754	Accuracy: 98.79%
58	Validation loss: 0.064906	Best loss: 0.037754	Accuracy: 98.63%
59	Validation loss: 0.039025	Best loss: 0.037754	Accuracy: 98.98%
60	Validation loss: 0.057318	Best loss: 0.037754	Accuracy: 98.44%
61	Validation loss: 0.048836	Best loss: 0.037754	Accuracy: 99.10%
62	Validation loss: 0.045339	Best loss: 0.037754	Accuracy: 98.98%
63	Validation loss: 0.069071	Best loss: 0.037754	Accuracy: 98.83%
64	Validation loss: 0.049843	Best loss: 0.037754	Accuracy: 99.06%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 6.5min
[CV] n_neurons=140, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.099147	Best loss: 0.099147	Accuracy: 96.79%
1	Validation loss: 0.115294	Best loss: 0.099147	Accuracy: 97.07%
2	Validation loss: 0.063063	Best loss: 0.063063	Accuracy: 98.16%
3	Validation loss: 0.052518	Best loss: 0.052518	Accuracy: 98.24%
4	Validation loss: 0.054502	Best loss: 0.052518	Accuracy: 98.36%
5	Validation loss: 0.046930	Best loss: 0.046930	Accuracy: 98.40%
6	Validation loss: 0.053615	Best loss: 0.046930	Accuracy: 98.63%
7	Validation loss: 0.053984	Best loss: 0.046930	Accuracy: 98.67%
8	Validation loss: 0.065711	Best loss: 0.046930	Accuracy: 98.32%
9	Validation loss: 0.051992	Best loss: 0.046930	Accuracy: 98.59%
10	Validation loss: 0.050824	Best loss: 0.046930	Accuracy: 98.59%
11	Validation loss: 0.048015	Best loss: 0.046930	Accuracy: 98.94%
12	Validation loss: 0.051029	Best loss: 0.046930	Accuracy: 98.83%
13	Validation loss: 0.087032	Best loss: 0.046930	Accuracy: 98.05%
14	Validation loss: 0.076797	Best loss: 0.046930	Accuracy: 98.16%
15	Validation loss: 0.064301	Best loss: 0.046930	Accuracy: 98.51%
16	Validation loss: 0.046504	Best loss: 0.046504	Accuracy: 99.02%
17	Validation loss: 0.057175	Best loss: 0.046504	Accuracy: 98.79%
18	Validation loss: 0.097370	Best loss: 0.046504	Accuracy: 97.77%
19	Validation loss: 0.053528	Best loss: 0.046504	Accuracy: 98.87%
20	Validation loss: 0.064789	Best loss: 0.046504	Accuracy: 98.71%
21	Validation loss: 0.058554	Best loss: 0.046504	Accuracy: 98.67%
22	Validation loss: 0.063439	Best loss: 0.046504	Accuracy: 98.71%
23	Validation loss: 0.049631	Best loss: 0.046504	Accuracy: 98.75%
24	Validation loss: 0.060158	Best loss: 0.046504	Accuracy: 98.83%
25	Validation loss: 0.057713	Best loss: 0.046504	Accuracy: 98.87%
26	Validation loss: 0.349227	Best loss: 0.046504	Accuracy: 97.73%
27	Validation loss: 0.070098	Best loss: 0.046504	Accuracy: 98.55%
28	Validation loss: 0.046709	Best loss: 0.046504	Accuracy: 99.06%
29	Validation loss: 0.046786	Best loss: 0.046504	Accuracy: 99.02%
30	Validation loss: 0.048767	Best loss: 0.046504	Accuracy: 98.91%
31	Validation loss: 0.043163	Best loss: 0.043163	Accuracy: 99.10%
32	Validation loss: 0.051574	Best loss: 0.043163	Accuracy: 99.06%
33	Validation loss: 0.054464	Best loss: 0.043163	Accuracy: 98.91%
34	Validation loss: 0.048372	Best loss: 0.043163	Accuracy: 99.02%
35	Validation loss: 0.043816	Best loss: 0.043163	Accuracy: 99.10%
36	Validation loss: 0.047220	Best loss: 0.043163	Accuracy: 99.02%
37	Validation loss: 0.045256	Best loss: 0.043163	Accuracy: 99.10%
38	Validation loss: 0.049319	Best loss: 0.043163	Accuracy: 99.22%
39	Validation loss: 0.044130	Best loss: 0.043163	Accuracy: 98.79%
40	Validation loss: 0.051349	Best loss: 0.043163	Accuracy: 98.87%
41	Validation loss: 0.059782	Best loss: 0.043163	Accuracy: 98.79%
42	Validation loss: 0.239935	Best loss: 0.043163	Accuracy: 98.01%
43	Validation loss: 0.062053	Best loss: 0.043163	Accuracy: 98.91%
44	Validation loss: 0.042770	Best loss: 0.042770	Accuracy: 99.10%
45	Validation loss: 0.027833	Best loss: 0.027833	Accuracy: 99.45%
46	Validation loss: 0.031901	Best loss: 0.027833	Accuracy: 99.37%
47	Validation loss: 0.041126	Best loss: 0.027833	Accuracy: 99.34%
48	Validation loss: 0.041072	Best loss: 0.027833	Accuracy: 99.30%
49	Validation loss: 0.048453	Best loss: 0.027833	Accuracy: 99.22%
50	Validation loss: 0.041827	Best loss: 0.027833	Accuracy: 99.26%
51	Validation loss: 0.045173	Best loss: 0.027833	Accuracy: 99.22%
52	Validation loss: 0.042325	Best loss: 0.027833	Accuracy: 99.22%
53	Validation loss: 0.063312	Best loss: 0.027833	Accuracy: 99.18%
54	Validation loss: 0.065348	Best loss: 0.027833	Accuracy: 98.94%
55	Validation loss: 0.353587	Best loss: 0.027833	Accuracy: 98.40%
56	Validation loss: 0.098922	Best loss: 0.027833	Accuracy: 98.83%
57	Validation loss: 0.072389	Best loss: 0.027833	Accuracy: 99.10%
58	Validation loss: 0.053099	Best loss: 0.027833	Accuracy: 99.34%
59	Validation loss: 0.054717	Best loss: 0.027833	Accuracy: 99.10%
60	Validation loss: 0.049368	Best loss: 0.027833	Accuracy: 99.30%
61	Validation loss: 0.044071	Best loss: 0.027833	Accuracy: 99.41%
62	Validation loss: 0.044266	Best loss: 0.027833	Accuracy: 99.45%
63	Validation loss: 0.096192	Best loss: 0.027833	Accuracy: 98.87%
64	Validation loss: 0.058277	Best loss: 0.027833	Accuracy: 99.14%
65	Validation loss: 0.083135	Best loss: 0.027833	Accuracy: 98.98%
66	Validation loss: 0.080814	Best loss: 0.027833	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.5min
[CV] n_neurons=140, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.105405	Best loss: 0.105405	Accuracy: 96.56%
1	Validation loss: 0.064897	Best loss: 0.064897	Accuracy: 97.69%
2	Validation loss: 0.054918	Best loss: 0.054918	Accuracy: 98.16%
3	Validation loss: 0.073567	Best loss: 0.054918	Accuracy: 98.12%
4	Validation loss: 0.060245	Best loss: 0.054918	Accuracy: 98.01%
5	Validation loss: 0.054680	Best loss: 0.054680	Accuracy: 98.32%
6	Validation loss: 0.060682	Best loss: 0.054680	Accuracy: 98.40%
7	Validation loss: 0.057467	Best loss: 0.054680	Accuracy: 98.55%
8	Validation loss: 0.062768	Best loss: 0.054680	Accuracy: 98.44%
9	Validation loss: 0.061954	Best loss: 0.054680	Accuracy: 98.48%
10	Validation loss: 0.094901	Best loss: 0.054680	Accuracy: 97.85%
11	Validation loss: 0.072843	Best loss: 0.054680	Accuracy: 98.32%
12	Validation loss: 0.058718	Best loss: 0.054680	Accuracy: 98.83%
13	Validation loss: 0.076611	Best loss: 0.054680	Accuracy: 98.32%
14	Validation loss: 0.047396	Best loss: 0.047396	Accuracy: 98.83%
15	Validation loss: 0.056289	Best loss: 0.047396	Accuracy: 98.55%
16	Validation loss: 0.064071	Best loss: 0.047396	Accuracy: 98.63%
17	Validation loss: 0.081562	Best loss: 0.047396	Accuracy: 98.28%
18	Validation loss: 0.082195	Best loss: 0.047396	Accuracy: 98.28%
19	Validation loss: 0.058829	Best loss: 0.047396	Accuracy: 98.87%
20	Validation loss: 0.062492	Best loss: 0.047396	Accuracy: 98.67%
21	Validation loss: 0.072759	Best loss: 0.047396	Accuracy: 98.48%
22	Validation loss: 0.067814	Best loss: 0.047396	Accuracy: 98.51%
23	Validation loss: 0.059404	Best loss: 0.047396	Accuracy: 99.06%
24	Validation loss: 0.046624	Best loss: 0.046624	Accuracy: 99.06%
25	Validation loss: 0.051287	Best loss: 0.046624	Accuracy: 99.06%
26	Validation loss: 0.056894	Best loss: 0.046624	Accuracy: 98.83%
27	Validation loss: 0.065325	Best loss: 0.046624	Accuracy: 98.71%
28	Validation loss: 0.098663	Best loss: 0.046624	Accuracy: 98.63%
29	Validation loss: 0.242982	Best loss: 0.046624	Accuracy: 98.24%
30	Validation loss: 0.177329	Best loss: 0.046624	Accuracy: 97.85%
31	Validation loss: 0.094567	Best loss: 0.046624	Accuracy: 98.71%
32	Validation loss: 0.115153	Best loss: 0.046624	Accuracy: 98.75%
33	Validation loss: 0.086117	Best loss: 0.046624	Accuracy: 98.87%
34	Validation loss: 0.055954	Best loss: 0.046624	Accuracy: 99.02%
35	Validation loss: 0.074708	Best loss: 0.046624	Accuracy: 99.06%
36	Validation loss: 0.063665	Best loss: 0.046624	Accuracy: 98.83%
37	Validation loss: 0.063986	Best loss: 0.046624	Accuracy: 98.98%
38	Validation loss: 0.069715	Best loss: 0.046624	Accuracy: 99.02%
39	Validation loss: 0.078224	Best loss: 0.046624	Accuracy: 98.98%
40	Validation loss: 0.060874	Best loss: 0.046624	Accuracy: 99.06%
41	Validation loss: 0.067585	Best loss: 0.046624	Accuracy: 98.79%
42	Validation loss: 0.075579	Best loss: 0.046624	Accuracy: 98.75%
43	Validation loss: 0.076914	Best loss: 0.046624	Accuracy: 98.94%
44	Validation loss: 0.081413	Best loss: 0.046624	Accuracy: 98.75%
45	Validation loss: 0.062161	Best loss: 0.046624	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.1min
[CV] n_neurons=140, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.089020	Best loss: 0.089020	Accuracy: 97.19%
1	Validation loss: 0.081038	Best loss: 0.081038	Accuracy: 97.19%
2	Validation loss: 0.066448	Best loss: 0.066448	Accuracy: 97.89%
3	Validation loss: 0.060933	Best loss: 0.060933	Accuracy: 98.24%
4	Validation loss: 0.073257	Best loss: 0.060933	Accuracy: 97.73%
5	Validation loss: 0.073596	Best loss: 0.060933	Accuracy: 97.85%
6	Validation loss: 0.045647	Best loss: 0.045647	Accuracy: 98.59%
7	Validation loss: 0.086369	Best loss: 0.045647	Accuracy: 97.69%
8	Validation loss: 0.063066	Best loss: 0.045647	Accuracy: 98.44%
9	Validation loss: 0.049591	Best loss: 0.045647	Accuracy: 98.55%
10	Validation loss: 0.047466	Best loss: 0.045647	Accuracy: 98.87%
11	Validation loss: 0.040386	Best loss: 0.040386	Accuracy: 98.87%
12	Validation loss: 0.040834	Best loss: 0.040386	Accuracy: 98.91%
13	Validation loss: 0.047792	Best loss: 0.040386	Accuracy: 98.71%
14	Validation loss: 0.082647	Best loss: 0.040386	Accuracy: 98.32%
15	Validation loss: 0.037831	Best loss: 0.037831	Accuracy: 99.10%
16	Validation loss: 0.059999	Best loss: 0.037831	Accuracy: 98.87%
17	Validation loss: 0.039634	Best loss: 0.037831	Accuracy: 98.91%
18	Validation loss: 0.065045	Best loss: 0.037831	Accuracy: 98.40%
19	Validation loss: 0.051401	Best loss: 0.037831	Accuracy: 98.91%
20	Validation loss: 0.160211	Best loss: 0.037831	Accuracy: 96.99%
21	Validation loss: 0.091029	Best loss: 0.037831	Accuracy: 98.48%
22	Validation loss: 0.046387	Best loss: 0.037831	Accuracy: 98.94%
23	Validation loss: 0.072194	Best loss: 0.037831	Accuracy: 98.91%
24	Validation loss: 0.076507	Best loss: 0.037831	Accuracy: 98.83%
25	Validation loss: 0.052159	Best loss: 0.037831	Accuracy: 99.14%
26	Validation loss: 0.049748	Best loss: 0.037831	Accuracy: 99.06%
27	Validation loss: 0.047878	Best loss: 0.037831	Accuracy: 98.94%
28	Validation loss: 0.044026	Best loss: 0.037831	Accuracy: 99.18%
29	Validation loss: 0.045005	Best loss: 0.037831	Accuracy: 99.26%
30	Validation loss: 0.052637	Best loss: 0.037831	Accuracy: 98.94%
31	Validation loss: 0.028227	Best loss: 0.028227	Accuracy: 99.45%
32	Validation loss: 0.072013	Best loss: 0.028227	Accuracy: 98.79%
33	Validation loss: 0.059122	Best loss: 0.028227	Accuracy: 99.14%
34	Validation loss: 0.422039	Best loss: 0.028227	Accuracy: 97.19%
35	Validation loss: 0.084649	Best loss: 0.028227	Accuracy: 98.75%
36	Validation loss: 0.059252	Best loss: 0.028227	Accuracy: 99.06%
37	Validation loss: 0.084116	Best loss: 0.028227	Accuracy: 98.87%
38	Validation loss: 0.063671	Best loss: 0.028227	Accuracy: 98.94%
39	Validation loss: 0.042010	Best loss: 0.028227	Accuracy: 99.26%
40	Validation loss: 0.064104	Best loss: 0.028227	Accuracy: 98.87%
41	Validation loss: 0.066790	Best loss: 0.028227	Accuracy: 98.91%
42	Validation loss: 0.047684	Best loss: 0.028227	Accuracy: 99.18%
43	Validation loss: 0.035445	Best loss: 0.028227	Accuracy: 99.34%
44	Validation loss: 0.050393	Best loss: 0.028227	Accuracy: 99.37%
45	Validation loss: 0.042749	Best loss: 0.028227	Accuracy: 99.14%
46	Validation loss: 0.051545	Best loss: 0.028227	Accuracy: 99.10%
47	Validation loss: 0.065102	Best loss: 0.028227	Accuracy: 98.91%
48	Validation loss: 0.059805	Best loss: 0.028227	Accuracy: 98.91%
49	Validation loss: 0.059581	Best loss: 0.028227	Accuracy: 98.98%
50	Validation loss: 0.055452	Best loss: 0.028227	Accuracy: 99.14%
51	Validation loss: 0.056953	Best loss: 0.028227	Accuracy: 98.98%
52	Validation loss: 0.058929	Best loss: 0.028227	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.3min
[CV] n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.113509	Best loss: 0.113509	Accuracy: 96.68%
1	Validation loss: 0.068946	Best loss: 0.068946	Accuracy: 97.97%
2	Validation loss: 0.064436	Best loss: 0.064436	Accuracy: 98.16%
3	Validation loss: 0.064119	Best loss: 0.064119	Accuracy: 98.24%
4	Validation loss: 0.059988	Best loss: 0.059988	Accuracy: 98.44%
5	Validation loss: 0.087377	Best loss: 0.059988	Accuracy: 98.05%
6	Validation loss: 0.057524	Best loss: 0.057524	Accuracy: 98.51%
7	Validation loss: 0.083269	Best loss: 0.057524	Accuracy: 97.85%
8	Validation loss: 0.049461	Best loss: 0.049461	Accuracy: 98.75%
9	Validation loss: 0.069459	Best loss: 0.049461	Accuracy: 98.40%
10	Validation loss: 0.045145	Best loss: 0.045145	Accuracy: 98.75%
11	Validation loss: 0.066821	Best loss: 0.045145	Accuracy: 98.51%
12	Validation loss: 0.058359	Best loss: 0.045145	Accuracy: 98.71%
13	Validation loss: 0.049564	Best loss: 0.045145	Accuracy: 99.02%
14	Validation loss: 0.046461	Best loss: 0.045145	Accuracy: 98.94%
15	Validation loss: 0.052377	Best loss: 0.045145	Accuracy: 98.87%
16	Validation loss: 0.098062	Best loss: 0.045145	Accuracy: 98.48%
17	Validation loss: 0.095604	Best loss: 0.045145	Accuracy: 98.32%
18	Validation loss: 0.114973	Best loss: 0.045145	Accuracy: 98.40%
19	Validation loss: 0.071444	Best loss: 0.045145	Accuracy: 98.71%
20	Validation loss: 0.079086	Best loss: 0.045145	Accuracy: 98.87%
21	Validation loss: 0.050623	Best loss: 0.045145	Accuracy: 98.98%
22	Validation loss: 0.062900	Best loss: 0.045145	Accuracy: 98.87%
23	Validation loss: 0.131461	Best loss: 0.045145	Accuracy: 98.71%
24	Validation loss: 0.197687	Best loss: 0.045145	Accuracy: 97.93%
25	Validation loss: 0.066267	Best loss: 0.045145	Accuracy: 98.75%
26	Validation loss: 0.050716	Best loss: 0.045145	Accuracy: 98.91%
27	Validation loss: 0.051104	Best loss: 0.045145	Accuracy: 98.98%
28	Validation loss: 0.065278	Best loss: 0.045145	Accuracy: 98.91%
29	Validation loss: 0.054262	Best loss: 0.045145	Accuracy: 98.87%
30	Validation loss: 0.230365	Best loss: 0.045145	Accuracy: 98.63%
31	Validation loss: 0.080041	Best loss: 0.045145	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.2min
[CV] n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.111305	Best loss: 0.111305	Accuracy: 96.60%
1	Validation loss: 0.073859	Best loss: 0.073859	Accuracy: 98.05%
2	Validation loss: 0.065717	Best loss: 0.065717	Accuracy: 98.12%
3	Validation loss: 0.059178	Best loss: 0.059178	Accuracy: 98.12%
4	Validation loss: 0.092094	Best loss: 0.059178	Accuracy: 97.34%
5	Validation loss: 0.056159	Best loss: 0.056159	Accuracy: 98.32%
6	Validation loss: 0.074196	Best loss: 0.056159	Accuracy: 97.97%
7	Validation loss: 0.058281	Best loss: 0.056159	Accuracy: 98.32%
8	Validation loss: 0.038930	Best loss: 0.038930	Accuracy: 98.91%
9	Validation loss: 0.061930	Best loss: 0.038930	Accuracy: 98.48%
10	Validation loss: 0.150320	Best loss: 0.038930	Accuracy: 97.42%
11	Validation loss: 0.050288	Best loss: 0.038930	Accuracy: 98.87%
12	Validation loss: 0.059843	Best loss: 0.038930	Accuracy: 98.67%
13	Validation loss: 0.057067	Best loss: 0.038930	Accuracy: 98.75%
14	Validation loss: 0.098927	Best loss: 0.038930	Accuracy: 97.97%
15	Validation loss: 0.095500	Best loss: 0.038930	Accuracy: 97.73%
16	Validation loss: 0.056817	Best loss: 0.038930	Accuracy: 98.63%
17	Validation loss: 0.083718	Best loss: 0.038930	Accuracy: 98.55%
18	Validation loss: 0.111569	Best loss: 0.038930	Accuracy: 97.22%
19	Validation loss: 0.061712	Best loss: 0.038930	Accuracy: 98.83%
20	Validation loss: 0.112311	Best loss: 0.038930	Accuracy: 98.44%
21	Validation loss: 0.056754	Best loss: 0.038930	Accuracy: 98.98%
22	Validation loss: 0.053968	Best loss: 0.038930	Accuracy: 98.98%
23	Validation loss: 0.085642	Best loss: 0.038930	Accuracy: 98.59%
24	Validation loss: 0.123504	Best loss: 0.038930	Accuracy: 98.44%
25	Validation loss: 0.184139	Best loss: 0.038930	Accuracy: 98.32%
26	Validation loss: 0.063767	Best loss: 0.038930	Accuracy: 98.94%
27	Validation loss: 0.079016	Best loss: 0.038930	Accuracy: 99.06%
28	Validation loss: 0.088997	Best loss: 0.038930	Accuracy: 98.98%
29	Validation loss: 0.065479	Best loss: 0.038930	Accuracy: 99.14%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.1min
[CV] n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.135742	Best loss: 0.135742	Accuracy: 96.48%
1	Validation loss: 0.087115	Best loss: 0.087115	Accuracy: 97.62%
2	Validation loss: 0.075357	Best loss: 0.075357	Accuracy: 97.65%
3	Validation loss: 0.074590	Best loss: 0.074590	Accuracy: 98.12%
4	Validation loss: 0.055400	Best loss: 0.055400	Accuracy: 98.67%
5	Validation loss: 0.073772	Best loss: 0.055400	Accuracy: 98.05%
6	Validation loss: 0.168690	Best loss: 0.055400	Accuracy: 95.82%
7	Validation loss: 0.069853	Best loss: 0.055400	Accuracy: 98.40%
8	Validation loss: 0.040623	Best loss: 0.040623	Accuracy: 99.02%
9	Validation loss: 0.053398	Best loss: 0.040623	Accuracy: 98.51%
10	Validation loss: 0.113728	Best loss: 0.040623	Accuracy: 98.32%
11	Validation loss: 0.051972	Best loss: 0.040623	Accuracy: 98.48%
12	Validation loss: 0.042373	Best loss: 0.040623	Accuracy: 98.67%
13	Validation loss: 0.042700	Best loss: 0.040623	Accuracy: 98.91%
14	Validation loss: 0.091719	Best loss: 0.040623	Accuracy: 98.24%
15	Validation loss: 0.086407	Best loss: 0.040623	Accuracy: 98.44%
16	Validation loss: 0.041015	Best loss: 0.040623	Accuracy: 98.91%
17	Validation loss: 0.235172	Best loss: 0.040623	Accuracy: 96.29%
18	Validation loss: 0.041128	Best loss: 0.040623	Accuracy: 98.75%
19	Validation loss: 0.041372	Best loss: 0.040623	Accuracy: 99.14%
20	Validation loss: 0.036586	Best loss: 0.036586	Accuracy: 99.02%
21	Validation loss: 0.038465	Best loss: 0.036586	Accuracy: 98.98%
22	Validation loss: 0.047592	Best loss: 0.036586	Accuracy: 99.18%
23	Validation loss: 0.076937	Best loss: 0.036586	Accuracy: 98.51%
24	Validation loss: 0.061774	Best loss: 0.036586	Accuracy: 98.87%
25	Validation loss: 0.063146	Best loss: 0.036586	Accuracy: 98.83%
26	Validation loss: 0.061267	Best loss: 0.036586	Accuracy: 99.02%
27	Validation loss: 0.101862	Best loss: 0.036586	Accuracy: 98.28%
28	Validation loss: 0.390649	Best loss: 0.036586	Accuracy: 97.62%
29	Validation loss: 0.058334	Best loss: 0.036586	Accuracy: 98.71%
30	Validation loss: 0.076296	Best loss: 0.036586	Accuracy: 98.75%
31	Validation loss: 0.053988	Best loss: 0.036586	Accuracy: 98.98%
32	Validation loss: 0.055884	Best loss: 0.036586	Accuracy: 99.02%
33	Validation loss: 0.100679	Best loss: 0.036586	Accuracy: 98.83%
34	Validation loss: 0.080053	Best loss: 0.036586	Accuracy: 98.75%
35	Validation loss: 0.096611	Best loss: 0.036586	Accuracy: 98.63%
36	Validation loss: 0.116157	Best loss: 0.036586	Accuracy: 98.87%
37	Validation loss: 0.056937	Best loss: 0.036586	Accuracy: 98.91%
38	Validation loss: 0.059241	Best loss: 0.036586	Accuracy: 99.10%
39	Validation loss: 0.200929	Best loss: 0.036586	Accuracy: 98.55%
40	Validation loss: 0.089525	Best loss: 0.036586	Accuracy: 98.91%
41	Validation loss: 0.085979	Best loss: 0.036586	Accuracy: 98.91%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.5min
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.200016	Best loss: 0.200016	Accuracy: 95.86%
1	Validation loss: 0.105491	Best loss: 0.105491	Accuracy: 97.58%
2	Validation loss: 0.102167	Best loss: 0.102167	Accuracy: 97.58%
3	Validation loss: 0.089401	Best loss: 0.089401	Accuracy: 97.54%
4	Validation loss: 0.064332	Best loss: 0.064332	Accuracy: 98.08%
5	Validation loss: 0.093259	Best loss: 0.064332	Accuracy: 97.73%
6	Validation loss: 0.066473	Best loss: 0.064332	Accuracy: 97.93%
7	Validation loss: 0.054564	Best loss: 0.054564	Accuracy: 98.91%
8	Validation loss: 0.107124	Best loss: 0.054564	Accuracy: 97.15%
9	Validation loss: 0.039538	Best loss: 0.039538	Accuracy: 98.83%
10	Validation loss: 0.058169	Best loss: 0.039538	Accuracy: 98.71%
11	Validation loss: 0.058217	Best loss: 0.039538	Accuracy: 98.83%
12	Validation loss: 0.076818	Best loss: 0.039538	Accuracy: 98.55%
13	Validation loss: 0.124055	Best loss: 0.039538	Accuracy: 97.58%
14	Validation loss: 0.073196	Best loss: 0.039538	Accuracy: 98.55%
15	Validation loss: 0.060090	Best loss: 0.039538	Accuracy: 98.79%
16	Validation loss: 0.059631	Best loss: 0.039538	Accuracy: 98.48%
17	Validation loss: 0.096149	Best loss: 0.039538	Accuracy: 97.54%
18	Validation loss: 0.081353	Best loss: 0.039538	Accuracy: 98.63%
19	Validation loss: 0.070688	Best loss: 0.039538	Accuracy: 98.32%
20	Validation loss: 0.043258	Best loss: 0.039538	Accuracy: 98.83%
21	Validation loss: 0.084163	Best loss: 0.039538	Accuracy: 98.20%
22	Validation loss: 0.046675	Best loss: 0.039538	Accuracy: 98.98%
23	Validation loss: 0.129641	Best loss: 0.039538	Accuracy: 98.08%
24	Validation loss: 0.055376	Best loss: 0.039538	Accuracy: 98.98%
25	Validation loss: 0.060966	Best loss: 0.039538	Accuracy: 98.75%
26	Validation loss: 0.052645	Best loss: 0.039538	Accuracy: 99.10%
27	Validation loss: 0.065755	Best loss: 0.039538	Accuracy: 98.87%
28	Validation loss: 0.062514	Best loss: 0.039538	Accuracy: 98.98%
29	Validation loss: 0.072565	Best loss: 0.039538	Accuracy: 98.51%
30	Validation loss: 0.066857	Best loss: 0.039538	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt;, total=  49.4s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.169846	Best loss: 0.169846	Accuracy: 96.83%
1	Validation loss: 0.131817	Best loss: 0.131817	Accuracy: 97.54%
2	Validation loss: 0.096677	Best loss: 0.096677	Accuracy: 97.97%
3	Validation loss: 0.129117	Best loss: 0.096677	Accuracy: 96.87%
4	Validation loss: 0.103694	Best loss: 0.096677	Accuracy: 97.89%
5	Validation loss: 0.079390	Best loss: 0.079390	Accuracy: 98.12%
6	Validation loss: 0.074453	Best loss: 0.074453	Accuracy: 97.97%
7	Validation loss: 0.132168	Best loss: 0.074453	Accuracy: 97.42%
8	Validation loss: 0.063241	Best loss: 0.063241	Accuracy: 98.44%
9	Validation loss: 0.088743	Best loss: 0.063241	Accuracy: 98.59%
10	Validation loss: 0.071805	Best loss: 0.063241	Accuracy: 98.55%
11	Validation loss: 0.075577	Best loss: 0.063241	Accuracy: 98.24%
12	Validation loss: 0.049079	Best loss: 0.049079	Accuracy: 98.94%
13	Validation loss: 0.101750	Best loss: 0.049079	Accuracy: 97.46%
14	Validation loss: 0.082049	Best loss: 0.049079	Accuracy: 97.85%
15	Validation loss: 0.054294	Best loss: 0.049079	Accuracy: 98.51%
16	Validation loss: 0.097977	Best loss: 0.049079	Accuracy: 98.12%
17	Validation loss: 0.112849	Best loss: 0.049079	Accuracy: 98.28%
18	Validation loss: 0.051307	Best loss: 0.049079	Accuracy: 98.63%
19	Validation loss: 0.055642	Best loss: 0.049079	Accuracy: 98.83%
20	Validation loss: 0.063105	Best loss: 0.049079	Accuracy: 98.59%
21	Validation loss: 0.060870	Best loss: 0.049079	Accuracy: 98.75%
22	Validation loss: 0.073270	Best loss: 0.049079	Accuracy: 98.75%
23	Validation loss: 0.064010	Best loss: 0.049079	Accuracy: 98.83%
24	Validation loss: 0.052764	Best loss: 0.049079	Accuracy: 98.75%
25	Validation loss: 0.047439	Best loss: 0.047439	Accuracy: 98.91%
26	Validation loss: 0.059194	Best loss: 0.047439	Accuracy: 98.63%
27	Validation loss: 0.055914	Best loss: 0.047439	Accuracy: 98.91%
28	Validation loss: 0.089817	Best loss: 0.047439	Accuracy: 98.16%
29	Validation loss: 0.056553	Best loss: 0.047439	Accuracy: 98.79%
30	Validation loss: 0.063655	Best loss: 0.047439	Accuracy: 98.63%
31	Validation loss: 0.109728	Best loss: 0.047439	Accuracy: 98.44%
32	Validation loss: 0.741519	Best loss: 0.047439	Accuracy: 86.63%
33	Validation loss: 0.069819	Best loss: 0.047439	Accuracy: 98.59%
34	Validation loss: 0.088790	Best loss: 0.047439	Accuracy: 98.83%
35	Validation loss: 0.069165	Best loss: 0.047439	Accuracy: 98.67%
36	Validation loss: 0.065787	Best loss: 0.047439	Accuracy: 98.51%
37	Validation loss: 0.106998	Best loss: 0.047439	Accuracy: 98.55%
38	Validation loss: 0.082119	Best loss: 0.047439	Accuracy: 98.48%
39	Validation loss: 0.111966	Best loss: 0.047439	Accuracy: 98.44%
40	Validation loss: 0.067815	Best loss: 0.047439	Accuracy: 98.63%
41	Validation loss: 0.064843	Best loss: 0.047439	Accuracy: 98.83%
42	Validation loss: 0.080797	Best loss: 0.047439	Accuracy: 98.94%
43	Validation loss: 0.053614	Best loss: 0.047439	Accuracy: 98.87%
44	Validation loss: 0.061366	Best loss: 0.047439	Accuracy: 98.91%
45	Validation loss: 0.053874	Best loss: 0.047439	Accuracy: 98.83%
46	Validation loss: 0.049898	Best loss: 0.047439	Accuracy: 99.18%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt;, total= 1.2min
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.170696	Best loss: 0.170696	Accuracy: 96.99%
1	Validation loss: 0.224576	Best loss: 0.170696	Accuracy: 95.19%
2	Validation loss: 0.093323	Best loss: 0.093323	Accuracy: 97.77%
3	Validation loss: 0.062953	Best loss: 0.062953	Accuracy: 98.28%
4	Validation loss: 0.102532	Best loss: 0.062953	Accuracy: 98.05%
5	Validation loss: 0.089136	Best loss: 0.062953	Accuracy: 97.77%
6	Validation loss: 0.064003	Best loss: 0.062953	Accuracy: 98.32%
7	Validation loss: 0.095026	Best loss: 0.062953	Accuracy: 97.89%
8	Validation loss: 0.056263	Best loss: 0.056263	Accuracy: 98.48%
9	Validation loss: 0.055502	Best loss: 0.055502	Accuracy: 98.28%
10	Validation loss: 0.089351	Best loss: 0.055502	Accuracy: 98.20%
11	Validation loss: 0.052819	Best loss: 0.052819	Accuracy: 98.71%
12	Validation loss: 0.050109	Best loss: 0.050109	Accuracy: 98.79%
13	Validation loss: 0.055189	Best loss: 0.050109	Accuracy: 98.48%
14	Validation loss: 0.042433	Best loss: 0.042433	Accuracy: 98.75%
15	Validation loss: 0.052802	Best loss: 0.042433	Accuracy: 98.87%
16	Validation loss: 0.037439	Best loss: 0.037439	Accuracy: 99.10%
17	Validation loss: 0.064928	Best loss: 0.037439	Accuracy: 98.44%
18	Validation loss: 0.061720	Best loss: 0.037439	Accuracy: 98.91%
19	Validation loss: 0.070473	Best loss: 0.037439	Accuracy: 98.48%
20	Validation loss: 0.047061	Best loss: 0.037439	Accuracy: 98.75%
21	Validation loss: 0.056740	Best loss: 0.037439	Accuracy: 98.87%
22	Validation loss: 0.053335	Best loss: 0.037439	Accuracy: 98.63%
23	Validation loss: 0.056846	Best loss: 0.037439	Accuracy: 98.63%
24	Validation loss: 0.053474	Best loss: 0.037439	Accuracy: 99.14%
25	Validation loss: 0.042413	Best loss: 0.037439	Accuracy: 98.94%
26	Validation loss: 0.055651	Best loss: 0.037439	Accuracy: 98.83%
27	Validation loss: 0.044015	Best loss: 0.037439	Accuracy: 98.98%
28	Validation loss: 0.050528	Best loss: 0.037439	Accuracy: 98.98%
29	Validation loss: 0.060737	Best loss: 0.037439	Accuracy: 98.98%
30	Validation loss: 0.059920	Best loss: 0.037439	Accuracy: 98.87%
31	Validation loss: 0.078168	Best loss: 0.037439	Accuracy: 98.75%
32	Validation loss: 0.041867	Best loss: 0.037439	Accuracy: 99.14%
33	Validation loss: 0.043400	Best loss: 0.037439	Accuracy: 99.02%
34	Validation loss: 0.054807	Best loss: 0.037439	Accuracy: 98.94%
35	Validation loss: 0.099948	Best loss: 0.037439	Accuracy: 98.36%
36	Validation loss: 0.064232	Best loss: 0.037439	Accuracy: 98.67%
37	Validation loss: 0.060890	Best loss: 0.037439	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function relu at 0x124366d08&gt;, total=  56.6s
[CV] n_neurons=100, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.071254	Best loss: 0.071254	Accuracy: 97.38%
1	Validation loss: 0.073159	Best loss: 0.071254	Accuracy: 97.85%
2	Validation loss: 0.075095	Best loss: 0.071254	Accuracy: 98.12%
3	Validation loss: 0.046470	Best loss: 0.046470	Accuracy: 98.55%
4	Validation loss: 0.069846	Best loss: 0.046470	Accuracy: 98.01%
5	Validation loss: 0.049434	Best loss: 0.046470	Accuracy: 98.63%
6	Validation loss: 0.042056	Best loss: 0.042056	Accuracy: 98.87%
7	Validation loss: 0.059955	Best loss: 0.042056	Accuracy: 98.48%
8	Validation loss: 0.048834	Best loss: 0.042056	Accuracy: 98.75%
9	Validation loss: 0.062144	Best loss: 0.042056	Accuracy: 98.83%
10	Validation loss: 0.049982	Best loss: 0.042056	Accuracy: 98.87%
11	Validation loss: 0.041952	Best loss: 0.041952	Accuracy: 98.87%
12	Validation loss: 0.036788	Best loss: 0.036788	Accuracy: 98.94%
13	Validation loss: 0.054029	Best loss: 0.036788	Accuracy: 98.79%
14	Validation loss: 0.058340	Best loss: 0.036788	Accuracy: 98.71%
15	Validation loss: 0.106698	Best loss: 0.036788	Accuracy: 98.05%
16	Validation loss: 0.047885	Best loss: 0.036788	Accuracy: 98.94%
17	Validation loss: 0.039881	Best loss: 0.036788	Accuracy: 99.02%
18	Validation loss: 0.042465	Best loss: 0.036788	Accuracy: 99.10%
19	Validation loss: 0.058186	Best loss: 0.036788	Accuracy: 98.87%
20	Validation loss: 0.033147	Best loss: 0.033147	Accuracy: 99.18%
21	Validation loss: 0.047846	Best loss: 0.033147	Accuracy: 99.06%
22	Validation loss: 0.042446	Best loss: 0.033147	Accuracy: 98.98%
23	Validation loss: 0.051950	Best loss: 0.033147	Accuracy: 98.71%
24	Validation loss: 0.044099	Best loss: 0.033147	Accuracy: 98.98%
25	Validation loss: 0.048110	Best loss: 0.033147	Accuracy: 98.94%
26	Validation loss: 0.059174	Best loss: 0.033147	Accuracy: 98.91%
27	Validation loss: 0.049101	Best loss: 0.033147	Accuracy: 98.91%
28	Validation loss: 0.056189	Best loss: 0.033147	Accuracy: 99.10%
29	Validation loss: 0.055131	Best loss: 0.033147	Accuracy: 99.06%
30	Validation loss: 0.056811	Best loss: 0.033147	Accuracy: 99.10%
31	Validation loss: 0.051058	Best loss: 0.033147	Accuracy: 99.10%
32	Validation loss: 0.059565	Best loss: 0.033147	Accuracy: 98.71%
33	Validation loss: 0.062987	Best loss: 0.033147	Accuracy: 98.94%
34	Validation loss: 0.056258	Best loss: 0.033147	Accuracy: 98.87%
35	Validation loss: 0.050597	Best loss: 0.033147	Accuracy: 98.94%
36	Validation loss: 0.044161	Best loss: 0.033147	Accuracy: 99.02%
37	Validation loss: 0.044433	Best loss: 0.033147	Accuracy: 98.94%
38	Validation loss: 0.096584	Best loss: 0.033147	Accuracy: 98.67%
39	Validation loss: 0.055108	Best loss: 0.033147	Accuracy: 99.14%
40	Validation loss: 0.058174	Best loss: 0.033147	Accuracy: 98.91%
41	Validation loss: 0.077171	Best loss: 0.033147	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function relu at 0x124366d08&gt;, total=  47.0s
[CV] n_neurons=100, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.083143	Best loss: 0.083143	Accuracy: 98.05%
1	Validation loss: 0.066133	Best loss: 0.066133	Accuracy: 97.97%
2	Validation loss: 0.060069	Best loss: 0.060069	Accuracy: 97.97%
3	Validation loss: 0.091342	Best loss: 0.060069	Accuracy: 97.62%
4	Validation loss: 0.049658	Best loss: 0.049658	Accuracy: 98.67%
5	Validation loss: 0.054000	Best loss: 0.049658	Accuracy: 98.40%
6	Validation loss: 0.049326	Best loss: 0.049326	Accuracy: 98.48%
7	Validation loss: 0.063552	Best loss: 0.049326	Accuracy: 98.44%
8	Validation loss: 0.042826	Best loss: 0.042826	Accuracy: 98.91%
9	Validation loss: 0.068407	Best loss: 0.042826	Accuracy: 98.59%
10	Validation loss: 0.045786	Best loss: 0.042826	Accuracy: 98.91%
11	Validation loss: 0.062856	Best loss: 0.042826	Accuracy: 98.36%
12	Validation loss: 0.073009	Best loss: 0.042826	Accuracy: 98.51%
13	Validation loss: 0.059914	Best loss: 0.042826	Accuracy: 98.71%
14	Validation loss: 0.055582	Best loss: 0.042826	Accuracy: 98.75%
15	Validation loss: 0.040726	Best loss: 0.040726	Accuracy: 98.83%
16	Validation loss: 0.037034	Best loss: 0.037034	Accuracy: 98.98%
17	Validation loss: 0.051780	Best loss: 0.037034	Accuracy: 98.63%
18	Validation loss: 0.052352	Best loss: 0.037034	Accuracy: 98.55%
19	Validation loss: 0.053533	Best loss: 0.037034	Accuracy: 98.94%
20	Validation loss: 0.041244	Best loss: 0.037034	Accuracy: 98.94%
21	Validation loss: 0.060430	Best loss: 0.037034	Accuracy: 98.59%
22	Validation loss: 0.062077	Best loss: 0.037034	Accuracy: 98.40%
23	Validation loss: 0.059855	Best loss: 0.037034	Accuracy: 98.79%
24	Validation loss: 0.043349	Best loss: 0.037034	Accuracy: 98.83%
25	Validation loss: 0.064491	Best loss: 0.037034	Accuracy: 98.75%
26	Validation loss: 0.067718	Best loss: 0.037034	Accuracy: 98.51%
27	Validation loss: 0.047980	Best loss: 0.037034	Accuracy: 98.67%
28	Validation loss: 0.083930	Best loss: 0.037034	Accuracy: 98.48%
29	Validation loss: 0.046064	Best loss: 0.037034	Accuracy: 98.87%
30	Validation loss: 0.052436	Best loss: 0.037034	Accuracy: 98.79%
31	Validation loss: 0.092791	Best loss: 0.037034	Accuracy: 98.79%
32	Validation loss: 0.072466	Best loss: 0.037034	Accuracy: 98.67%
33	Validation loss: 0.077108	Best loss: 0.037034	Accuracy: 98.55%
34	Validation loss: 0.061921	Best loss: 0.037034	Accuracy: 98.51%
35	Validation loss: 0.054483	Best loss: 0.037034	Accuracy: 98.87%
36	Validation loss: 0.065859	Best loss: 0.037034	Accuracy: 98.67%
37	Validation loss: 0.058647	Best loss: 0.037034	Accuracy: 98.98%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function relu at 0x124366d08&gt;, total=  43.0s
[CV] n_neurons=100, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.125905	Best loss: 0.125905	Accuracy: 96.33%
1	Validation loss: 0.058323	Best loss: 0.058323	Accuracy: 98.12%
2	Validation loss: 0.067136	Best loss: 0.058323	Accuracy: 98.08%
3	Validation loss: 0.053293	Best loss: 0.053293	Accuracy: 98.40%
4	Validation loss: 0.047276	Best loss: 0.047276	Accuracy: 98.71%
5	Validation loss: 0.069889	Best loss: 0.047276	Accuracy: 98.12%
6	Validation loss: 0.061929	Best loss: 0.047276	Accuracy: 98.16%
7	Validation loss: 0.060725	Best loss: 0.047276	Accuracy: 98.44%
8	Validation loss: 0.041373	Best loss: 0.041373	Accuracy: 98.83%
9	Validation loss: 0.046689	Best loss: 0.041373	Accuracy: 98.87%
10	Validation loss: 0.038228	Best loss: 0.038228	Accuracy: 98.79%
11	Validation loss: 0.031519	Best loss: 0.031519	Accuracy: 98.94%
12	Validation loss: 0.033308	Best loss: 0.031519	Accuracy: 99.10%
13	Validation loss: 0.041676	Best loss: 0.031519	Accuracy: 98.79%
14	Validation loss: 0.048835	Best loss: 0.031519	Accuracy: 98.63%
15	Validation loss: 0.047638	Best loss: 0.031519	Accuracy: 98.67%
16	Validation loss: 0.041985	Best loss: 0.031519	Accuracy: 99.02%
17	Validation loss: 0.043972	Best loss: 0.031519	Accuracy: 98.98%
18	Validation loss: 0.078323	Best loss: 0.031519	Accuracy: 98.05%
19	Validation loss: 0.046771	Best loss: 0.031519	Accuracy: 99.02%
20	Validation loss: 0.058727	Best loss: 0.031519	Accuracy: 98.44%
21	Validation loss: 0.036517	Best loss: 0.031519	Accuracy: 99.30%
22	Validation loss: 0.045927	Best loss: 0.031519	Accuracy: 99.06%
23	Validation loss: 0.060954	Best loss: 0.031519	Accuracy: 98.67%
24	Validation loss: 0.038162	Best loss: 0.031519	Accuracy: 99.02%
25	Validation loss: 0.042558	Best loss: 0.031519	Accuracy: 99.06%
26	Validation loss: 0.049723	Best loss: 0.031519	Accuracy: 98.71%
27	Validation loss: 0.045945	Best loss: 0.031519	Accuracy: 98.98%
28	Validation loss: 0.037216	Best loss: 0.031519	Accuracy: 99.14%
29	Validation loss: 0.053610	Best loss: 0.031519	Accuracy: 98.91%
30	Validation loss: 0.045461	Best loss: 0.031519	Accuracy: 99.02%
31	Validation loss: 0.046789	Best loss: 0.031519	Accuracy: 99.02%
32	Validation loss: 0.053931	Best loss: 0.031519	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function relu at 0x124366d08&gt;, total=  37.0s
[CV] n_neurons=140, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.071146	Best loss: 0.071146	Accuracy: 98.08%
1	Validation loss: 0.066130	Best loss: 0.066130	Accuracy: 98.28%
2	Validation loss: 0.061600	Best loss: 0.061600	Accuracy: 98.12%
3	Validation loss: 0.048642	Best loss: 0.048642	Accuracy: 98.51%
4	Validation loss: 0.046672	Best loss: 0.046672	Accuracy: 98.83%
5	Validation loss: 0.040462	Best loss: 0.040462	Accuracy: 99.06%
6	Validation loss: 0.030374	Best loss: 0.030374	Accuracy: 98.79%
7	Validation loss: 0.043018	Best loss: 0.030374	Accuracy: 98.94%
8	Validation loss: 0.036435	Best loss: 0.030374	Accuracy: 99.06%
9	Validation loss: 0.065032	Best loss: 0.030374	Accuracy: 98.40%
10	Validation loss: 0.033919	Best loss: 0.030374	Accuracy: 99.06%
11	Validation loss: 0.034730	Best loss: 0.030374	Accuracy: 99.14%
12	Validation loss: 0.043391	Best loss: 0.030374	Accuracy: 98.91%
13	Validation loss: 0.042414	Best loss: 0.030374	Accuracy: 98.91%
14	Validation loss: 0.054390	Best loss: 0.030374	Accuracy: 98.71%
15	Validation loss: 0.047805	Best loss: 0.030374	Accuracy: 98.83%
16	Validation loss: 0.042435	Best loss: 0.030374	Accuracy: 99.02%
17	Validation loss: 0.035501	Best loss: 0.030374	Accuracy: 99.22%
18	Validation loss: 0.042973	Best loss: 0.030374	Accuracy: 99.10%
19	Validation loss: 0.049270	Best loss: 0.030374	Accuracy: 98.91%
20	Validation loss: 0.057322	Best loss: 0.030374	Accuracy: 98.55%
21	Validation loss: 0.048273	Best loss: 0.030374	Accuracy: 99.10%
22	Validation loss: 0.036529	Best loss: 0.030374	Accuracy: 99.14%
23	Validation loss: 0.052810	Best loss: 0.030374	Accuracy: 98.91%
24	Validation loss: 0.040306	Best loss: 0.030374	Accuracy: 99.22%
25	Validation loss: 0.043417	Best loss: 0.030374	Accuracy: 99.14%
26	Validation loss: 0.030599	Best loss: 0.030374	Accuracy: 99.22%
27	Validation loss: 0.048577	Best loss: 0.030374	Accuracy: 99.02%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total=  56.2s
[CV] n_neurons=140, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.122580	Best loss: 0.122580	Accuracy: 96.95%
1	Validation loss: 0.058885	Best loss: 0.058885	Accuracy: 98.48%
2	Validation loss: 0.065230	Best loss: 0.058885	Accuracy: 97.97%
3	Validation loss: 0.045171	Best loss: 0.045171	Accuracy: 98.55%
4	Validation loss: 0.062899	Best loss: 0.045171	Accuracy: 98.36%
5	Validation loss: 0.056584	Best loss: 0.045171	Accuracy: 98.36%
6	Validation loss: 0.079414	Best loss: 0.045171	Accuracy: 97.93%
7	Validation loss: 0.051696	Best loss: 0.045171	Accuracy: 98.44%
8	Validation loss: 0.034508	Best loss: 0.034508	Accuracy: 99.22%
9	Validation loss: 0.066415	Best loss: 0.034508	Accuracy: 98.36%
10	Validation loss: 0.036106	Best loss: 0.034508	Accuracy: 99.10%
11	Validation loss: 0.060174	Best loss: 0.034508	Accuracy: 98.40%
12	Validation loss: 0.027197	Best loss: 0.027197	Accuracy: 99.34%
13	Validation loss: 0.032708	Best loss: 0.027197	Accuracy: 99.10%
14	Validation loss: 0.044617	Best loss: 0.027197	Accuracy: 99.02%
15	Validation loss: 0.044569	Best loss: 0.027197	Accuracy: 99.06%
16	Validation loss: 0.045627	Best loss: 0.027197	Accuracy: 98.87%
17	Validation loss: 0.044217	Best loss: 0.027197	Accuracy: 99.10%
18	Validation loss: 0.041382	Best loss: 0.027197	Accuracy: 99.06%
19	Validation loss: 0.037957	Best loss: 0.027197	Accuracy: 99.02%
20	Validation loss: 0.044366	Best loss: 0.027197	Accuracy: 99.02%
21	Validation loss: 0.047095	Best loss: 0.027197	Accuracy: 99.22%
22	Validation loss: 0.032292	Best loss: 0.027197	Accuracy: 99.34%
23	Validation loss: 0.037258	Best loss: 0.027197	Accuracy: 99.02%
24	Validation loss: 0.042466	Best loss: 0.027197	Accuracy: 99.02%
25	Validation loss: 0.050847	Best loss: 0.027197	Accuracy: 99.06%
26	Validation loss: 0.041686	Best loss: 0.027197	Accuracy: 98.91%
27	Validation loss: 0.042663	Best loss: 0.027197	Accuracy: 99.14%
28	Validation loss: 0.049445	Best loss: 0.027197	Accuracy: 99.02%
29	Validation loss: 0.046338	Best loss: 0.027197	Accuracy: 99.18%
30	Validation loss: 0.062475	Best loss: 0.027197	Accuracy: 98.83%
31	Validation loss: 0.053568	Best loss: 0.027197	Accuracy: 99.10%
32	Validation loss: 0.035216	Best loss: 0.027197	Accuracy: 99.14%
33	Validation loss: 0.044862	Best loss: 0.027197	Accuracy: 98.98%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total= 1.2min
[CV] n_neurons=140, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.110634	Best loss: 0.110634	Accuracy: 97.22%
1	Validation loss: 0.041889	Best loss: 0.041889	Accuracy: 98.48%
2	Validation loss: 0.047302	Best loss: 0.041889	Accuracy: 98.48%
3	Validation loss: 0.044797	Best loss: 0.041889	Accuracy: 98.79%
4	Validation loss: 0.056283	Best loss: 0.041889	Accuracy: 98.36%
5	Validation loss: 0.032197	Best loss: 0.032197	Accuracy: 99.06%
6	Validation loss: 0.043324	Best loss: 0.032197	Accuracy: 98.67%
7	Validation loss: 0.054079	Best loss: 0.032197	Accuracy: 98.59%
8	Validation loss: 0.043875	Best loss: 0.032197	Accuracy: 99.06%
9	Validation loss: 0.039575	Best loss: 0.032197	Accuracy: 98.91%
10	Validation loss: 0.050495	Best loss: 0.032197	Accuracy: 98.87%
11	Validation loss: 0.033722	Best loss: 0.032197	Accuracy: 99.02%
12	Validation loss: 0.068227	Best loss: 0.032197	Accuracy: 98.71%
13	Validation loss: 0.040001	Best loss: 0.032197	Accuracy: 98.79%
14	Validation loss: 0.045526	Best loss: 0.032197	Accuracy: 98.79%
15	Validation loss: 0.029200	Best loss: 0.029200	Accuracy: 99.14%
16	Validation loss: 0.034036	Best loss: 0.029200	Accuracy: 99.18%
17	Validation loss: 0.032344	Best loss: 0.029200	Accuracy: 99.02%
18	Validation loss: 0.051899	Best loss: 0.029200	Accuracy: 98.87%
19	Validation loss: 0.044081	Best loss: 0.029200	Accuracy: 98.94%
20	Validation loss: 0.036844	Best loss: 0.029200	Accuracy: 99.06%
21	Validation loss: 0.028109	Best loss: 0.028109	Accuracy: 99.14%
22	Validation loss: 0.044103	Best loss: 0.028109	Accuracy: 99.14%
23	Validation loss: 0.038992	Best loss: 0.028109	Accuracy: 99.10%
24	Validation loss: 0.034632	Best loss: 0.028109	Accuracy: 99.26%
25	Validation loss: 0.029043	Best loss: 0.028109	Accuracy: 99.14%
26	Validation loss: 0.036143	Best loss: 0.028109	Accuracy: 99.14%
27	Validation loss: 0.059391	Best loss: 0.028109	Accuracy: 98.59%
28	Validation loss: 0.041075	Best loss: 0.028109	Accuracy: 98.83%
29	Validation loss: 0.039859	Best loss: 0.028109	Accuracy: 99.14%
30	Validation loss: 0.039425	Best loss: 0.028109	Accuracy: 98.94%
31	Validation loss: 0.031332	Best loss: 0.028109	Accuracy: 99.18%
32	Validation loss: 0.033408	Best loss: 0.028109	Accuracy: 99.22%
33	Validation loss: 0.038006	Best loss: 0.028109	Accuracy: 99.10%
34	Validation loss: 0.042157	Best loss: 0.028109	Accuracy: 99.22%
35	Validation loss: 0.033880	Best loss: 0.028109	Accuracy: 99.26%
36	Validation loss: 0.030146	Best loss: 0.028109	Accuracy: 99.26%
37	Validation loss: 0.026200	Best loss: 0.026200	Accuracy: 99.45%
38	Validation loss: 0.043037	Best loss: 0.026200	Accuracy: 99.02%
39	Validation loss: 0.033305	Best loss: 0.026200	Accuracy: 99.14%
40	Validation loss: 0.033903	Best loss: 0.026200	Accuracy: 99.34%
41	Validation loss: 0.027074	Best loss: 0.026200	Accuracy: 99.26%
42	Validation loss: 0.034730	Best loss: 0.026200	Accuracy: 99.06%
43	Validation loss: 0.032236	Best loss: 0.026200	Accuracy: 99.30%
44	Validation loss: 0.038330	Best loss: 0.026200	Accuracy: 99.06%
45	Validation loss: 0.032923	Best loss: 0.026200	Accuracy: 99.14%
46	Validation loss: 0.027530	Best loss: 0.026200	Accuracy: 99.34%
47	Validation loss: 0.034052	Best loss: 0.026200	Accuracy: 99.30%
48	Validation loss: 0.032505	Best loss: 0.026200	Accuracy: 99.37%
49	Validation loss: 0.046289	Best loss: 0.026200	Accuracy: 99.18%
50	Validation loss: 0.033639	Best loss: 0.026200	Accuracy: 99.34%
51	Validation loss: 0.029095	Best loss: 0.026200	Accuracy: 99.34%
52	Validation loss: 0.029973	Best loss: 0.026200	Accuracy: 99.49%
53	Validation loss: 0.037146	Best loss: 0.026200	Accuracy: 99.10%
54	Validation loss: 0.037120	Best loss: 0.026200	Accuracy: 99.37%
55	Validation loss: 0.038909	Best loss: 0.026200	Accuracy: 99.22%
56	Validation loss: 0.038567	Best loss: 0.026200	Accuracy: 99.18%
57	Validation loss: 0.035495	Best loss: 0.026200	Accuracy: 99.10%
58	Validation loss: 0.051259	Best loss: 0.026200	Accuracy: 99.02%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total= 1.9min
[CV] n_neurons=30, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.093105	Best loss: 0.093105	Accuracy: 97.03%
1	Validation loss: 0.075972	Best loss: 0.075972	Accuracy: 97.73%
2	Validation loss: 0.068748	Best loss: 0.068748	Accuracy: 98.01%
3	Validation loss: 0.054655	Best loss: 0.054655	Accuracy: 98.08%
4	Validation loss: 0.059149	Best loss: 0.054655	Accuracy: 98.08%
5	Validation loss: 0.076066	Best loss: 0.054655	Accuracy: 98.01%
6	Validation loss: 0.055431	Best loss: 0.054655	Accuracy: 98.51%
7	Validation loss: 0.060574	Best loss: 0.054655	Accuracy: 98.32%
8	Validation loss: 0.055760	Best loss: 0.054655	Accuracy: 98.28%
9	Validation loss: 0.065297	Best loss: 0.054655	Accuracy: 98.55%
10	Validation loss: 0.070719	Best loss: 0.054655	Accuracy: 98.36%
11	Validation loss: 0.072533	Best loss: 0.054655	Accuracy: 98.36%
12	Validation loss: 0.060336	Best loss: 0.054655	Accuracy: 98.44%
13	Validation loss: 0.086462	Best loss: 0.054655	Accuracy: 98.08%
14	Validation loss: 0.076613	Best loss: 0.054655	Accuracy: 98.32%
15	Validation loss: 0.101861	Best loss: 0.054655	Accuracy: 98.01%
16	Validation loss: 0.065184	Best loss: 0.054655	Accuracy: 98.59%
17	Validation loss: 0.093757	Best loss: 0.054655	Accuracy: 97.85%
18	Validation loss: 0.086553	Best loss: 0.054655	Accuracy: 98.16%
19	Validation loss: 0.073059	Best loss: 0.054655	Accuracy: 98.59%
20	Validation loss: 0.078822	Best loss: 0.054655	Accuracy: 98.44%
21	Validation loss: 0.079422	Best loss: 0.054655	Accuracy: 98.08%
22	Validation loss: 0.079785	Best loss: 0.054655	Accuracy: 98.44%
23	Validation loss: 0.078738	Best loss: 0.054655	Accuracy: 98.44%
24	Validation loss: 0.098100	Best loss: 0.054655	Accuracy: 98.44%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.3s
[CV] n_neurons=30, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.094015	Best loss: 0.094015	Accuracy: 97.22%
1	Validation loss: 0.075678	Best loss: 0.075678	Accuracy: 97.34%
2	Validation loss: 0.064904	Best loss: 0.064904	Accuracy: 97.77%
3	Validation loss: 0.065247	Best loss: 0.064904	Accuracy: 98.20%
4	Validation loss: 0.068572	Best loss: 0.064904	Accuracy: 97.89%
5	Validation loss: 0.058683	Best loss: 0.058683	Accuracy: 98.08%
6	Validation loss: 0.061405	Best loss: 0.058683	Accuracy: 98.12%
7	Validation loss: 0.063976	Best loss: 0.058683	Accuracy: 98.08%
8	Validation loss: 0.050646	Best loss: 0.050646	Accuracy: 98.67%
9	Validation loss: 0.067242	Best loss: 0.050646	Accuracy: 98.20%
10	Validation loss: 0.057316	Best loss: 0.050646	Accuracy: 98.28%
11	Validation loss: 0.071677	Best loss: 0.050646	Accuracy: 98.32%
12	Validation loss: 0.056601	Best loss: 0.050646	Accuracy: 98.59%
13	Validation loss: 0.066901	Best loss: 0.050646	Accuracy: 98.48%
14	Validation loss: 0.063264	Best loss: 0.050646	Accuracy: 98.59%
15	Validation loss: 0.052225	Best loss: 0.050646	Accuracy: 98.83%
16	Validation loss: 0.054987	Best loss: 0.050646	Accuracy: 98.71%
17	Validation loss: 0.067094	Best loss: 0.050646	Accuracy: 98.67%
18	Validation loss: 0.077637	Best loss: 0.050646	Accuracy: 98.08%
19	Validation loss: 0.072227	Best loss: 0.050646	Accuracy: 98.55%
20	Validation loss: 0.054801	Best loss: 0.050646	Accuracy: 98.79%
21	Validation loss: 0.063088	Best loss: 0.050646	Accuracy: 98.51%
22	Validation loss: 0.071846	Best loss: 0.050646	Accuracy: 98.79%
23	Validation loss: 0.105260	Best loss: 0.050646	Accuracy: 97.97%
24	Validation loss: 0.057221	Best loss: 0.050646	Accuracy: 98.36%
25	Validation loss: 0.070213	Best loss: 0.050646	Accuracy: 98.71%
26	Validation loss: 0.066411	Best loss: 0.050646	Accuracy: 98.75%
27	Validation loss: 0.063795	Best loss: 0.050646	Accuracy: 98.71%
28	Validation loss: 0.053758	Best loss: 0.050646	Accuracy: 98.87%
29	Validation loss: 0.060403	Best loss: 0.050646	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total=  10.4s
[CV] n_neurons=30, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.101024	Best loss: 0.101024	Accuracy: 96.64%
1	Validation loss: 0.073399	Best loss: 0.073399	Accuracy: 97.81%
2	Validation loss: 0.063407	Best loss: 0.063407	Accuracy: 98.01%
3	Validation loss: 0.058903	Best loss: 0.058903	Accuracy: 98.12%
4	Validation loss: 0.053816	Best loss: 0.053816	Accuracy: 98.40%
5	Validation loss: 0.076646	Best loss: 0.053816	Accuracy: 97.93%
6	Validation loss: 0.054137	Best loss: 0.053816	Accuracy: 98.40%
7	Validation loss: 0.054969	Best loss: 0.053816	Accuracy: 98.20%
8	Validation loss: 0.052863	Best loss: 0.052863	Accuracy: 98.55%
9	Validation loss: 0.049811	Best loss: 0.049811	Accuracy: 98.75%
10	Validation loss: 0.059411	Best loss: 0.049811	Accuracy: 98.51%
11	Validation loss: 0.076359	Best loss: 0.049811	Accuracy: 98.44%
12	Validation loss: 0.047200	Best loss: 0.047200	Accuracy: 98.67%
13	Validation loss: 0.059550	Best loss: 0.047200	Accuracy: 98.28%
14	Validation loss: 0.053711	Best loss: 0.047200	Accuracy: 98.48%
15	Validation loss: 0.059081	Best loss: 0.047200	Accuracy: 98.59%
16	Validation loss: 0.042547	Best loss: 0.042547	Accuracy: 98.83%
17	Validation loss: 0.061832	Best loss: 0.042547	Accuracy: 98.71%
18	Validation loss: 0.059035	Best loss: 0.042547	Accuracy: 98.63%
19	Validation loss: 0.062080	Best loss: 0.042547	Accuracy: 98.48%
20	Validation loss: 0.043773	Best loss: 0.042547	Accuracy: 98.91%
21	Validation loss: 0.048220	Best loss: 0.042547	Accuracy: 98.91%
22	Validation loss: 0.067953	Best loss: 0.042547	Accuracy: 98.59%
23	Validation loss: 0.044742	Best loss: 0.042547	Accuracy: 99.02%
24	Validation loss: 0.057121	Best loss: 0.042547	Accuracy: 98.94%
25	Validation loss: 0.053459	Best loss: 0.042547	Accuracy: 98.94%
26	Validation loss: 0.073714	Best loss: 0.042547	Accuracy: 98.48%
27	Validation loss: 0.085084	Best loss: 0.042547	Accuracy: 98.48%
28	Validation loss: 0.057860	Best loss: 0.042547	Accuracy: 98.75%
29	Validation loss: 0.054109	Best loss: 0.042547	Accuracy: 98.71%
30	Validation loss: 0.058182	Best loss: 0.042547	Accuracy: 98.63%
31	Validation loss: 0.051535	Best loss: 0.042547	Accuracy: 98.79%
32	Validation loss: 0.055727	Best loss: 0.042547	Accuracy: 98.75%
33	Validation loss: 0.050952	Best loss: 0.042547	Accuracy: 98.71%
34	Validation loss: 0.051033	Best loss: 0.042547	Accuracy: 98.75%
35	Validation loss: 0.066253	Best loss: 0.042547	Accuracy: 98.63%
36	Validation loss: 0.055847	Best loss: 0.042547	Accuracy: 98.79%
37	Validation loss: 0.054842	Best loss: 0.042547	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total=  12.5s
[CV] n_neurons=140, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.104289	Best loss: 0.104289	Accuracy: 96.76%
1	Validation loss: 0.088056	Best loss: 0.088056	Accuracy: 97.62%
2	Validation loss: 0.069316	Best loss: 0.069316	Accuracy: 97.85%
3	Validation loss: 0.052664	Best loss: 0.052664	Accuracy: 98.36%
4	Validation loss: 0.072872	Best loss: 0.052664	Accuracy: 98.16%
5	Validation loss: 0.059308	Best loss: 0.052664	Accuracy: 98.48%
6	Validation loss: 0.048302	Best loss: 0.048302	Accuracy: 98.48%
7	Validation loss: 0.042608	Best loss: 0.042608	Accuracy: 98.91%
8	Validation loss: 0.047408	Best loss: 0.042608	Accuracy: 98.87%
9	Validation loss: 0.064396	Best loss: 0.042608	Accuracy: 98.48%
10	Validation loss: 0.082636	Best loss: 0.042608	Accuracy: 98.08%
11	Validation loss: 0.039099	Best loss: 0.039099	Accuracy: 98.91%
12	Validation loss: 0.040051	Best loss: 0.039099	Accuracy: 98.94%
13	Validation loss: 0.053414	Best loss: 0.039099	Accuracy: 98.75%
14	Validation loss: 0.045692	Best loss: 0.039099	Accuracy: 98.75%
15	Validation loss: 0.083980	Best loss: 0.039099	Accuracy: 97.81%
16	Validation loss: 0.068082	Best loss: 0.039099	Accuracy: 98.67%
17	Validation loss: 0.046213	Best loss: 0.039099	Accuracy: 98.83%
18	Validation loss: 0.059548	Best loss: 0.039099	Accuracy: 98.51%
19	Validation loss: 0.048125	Best loss: 0.039099	Accuracy: 99.02%
20	Validation loss: 0.048364	Best loss: 0.039099	Accuracy: 98.94%
21	Validation loss: 0.046659	Best loss: 0.039099	Accuracy: 98.67%
22	Validation loss: 0.053233	Best loss: 0.039099	Accuracy: 98.63%
23	Validation loss: 0.047669	Best loss: 0.039099	Accuracy: 98.75%
24	Validation loss: 0.079535	Best loss: 0.039099	Accuracy: 98.05%
25	Validation loss: 0.042688	Best loss: 0.039099	Accuracy: 98.94%
26	Validation loss: 0.056169	Best loss: 0.039099	Accuracy: 98.75%
27	Validation loss: 0.046283	Best loss: 0.039099	Accuracy: 98.91%
28	Validation loss: 0.066838	Best loss: 0.039099	Accuracy: 98.87%
29	Validation loss: 0.052554	Best loss: 0.039099	Accuracy: 98.87%
30	Validation loss: 0.072931	Best loss: 0.039099	Accuracy: 98.51%
31	Validation loss: 0.064943	Best loss: 0.039099	Accuracy: 98.67%
32	Validation loss: 0.047657	Best loss: 0.039099	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt;, total=  45.2s
[CV] n_neurons=140, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.118813	Best loss: 0.118813	Accuracy: 96.40%
1	Validation loss: 0.059797	Best loss: 0.059797	Accuracy: 97.93%
2	Validation loss: 0.050497	Best loss: 0.050497	Accuracy: 98.44%
3	Validation loss: 0.083404	Best loss: 0.050497	Accuracy: 97.69%
4	Validation loss: 0.061105	Best loss: 0.050497	Accuracy: 98.05%
5	Validation loss: 0.039217	Best loss: 0.039217	Accuracy: 98.63%
6	Validation loss: 0.061757	Best loss: 0.039217	Accuracy: 98.36%
7	Validation loss: 0.076003	Best loss: 0.039217	Accuracy: 97.93%
8	Validation loss: 0.104485	Best loss: 0.039217	Accuracy: 97.15%
9	Validation loss: 0.066765	Best loss: 0.039217	Accuracy: 98.36%
10	Validation loss: 0.054654	Best loss: 0.039217	Accuracy: 98.75%
11	Validation loss: 0.061529	Best loss: 0.039217	Accuracy: 98.40%
12	Validation loss: 0.060016	Best loss: 0.039217	Accuracy: 98.63%
13	Validation loss: 0.038226	Best loss: 0.038226	Accuracy: 98.98%
14	Validation loss: 0.044906	Best loss: 0.038226	Accuracy: 98.55%
15	Validation loss: 0.049406	Best loss: 0.038226	Accuracy: 98.94%
16	Validation loss: 0.056376	Best loss: 0.038226	Accuracy: 98.63%
17	Validation loss: 0.053568	Best loss: 0.038226	Accuracy: 98.55%
18	Validation loss: 0.079940	Best loss: 0.038226	Accuracy: 98.32%
19	Validation loss: 0.053379	Best loss: 0.038226	Accuracy: 98.75%
20	Validation loss: 0.049011	Best loss: 0.038226	Accuracy: 98.83%
21	Validation loss: 0.048231	Best loss: 0.038226	Accuracy: 99.10%
22	Validation loss: 0.062535	Best loss: 0.038226	Accuracy: 98.75%
23	Validation loss: 0.047752	Best loss: 0.038226	Accuracy: 98.75%
24	Validation loss: 0.044289	Best loss: 0.038226	Accuracy: 98.91%
25	Validation loss: 0.078565	Best loss: 0.038226	Accuracy: 98.67%
26	Validation loss: 0.063282	Best loss: 0.038226	Accuracy: 98.79%
27	Validation loss: 0.054047	Best loss: 0.038226	Accuracy: 98.98%
28	Validation loss: 0.071778	Best loss: 0.038226	Accuracy: 98.63%
29	Validation loss: 0.067120	Best loss: 0.038226	Accuracy: 98.79%
30	Validation loss: 0.042561	Best loss: 0.038226	Accuracy: 99.18%
31	Validation loss: 0.057415	Best loss: 0.038226	Accuracy: 98.83%
32	Validation loss: 0.071025	Best loss: 0.038226	Accuracy: 98.51%
33	Validation loss: 0.051163	Best loss: 0.038226	Accuracy: 98.98%
34	Validation loss: 0.057397	Best loss: 0.038226	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt;, total=  46.8s
[CV] n_neurons=140, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.089265	Best loss: 0.089265	Accuracy: 97.34%
1	Validation loss: 0.064012	Best loss: 0.064012	Accuracy: 97.97%
2	Validation loss: 0.065324	Best loss: 0.064012	Accuracy: 97.81%
3	Validation loss: 0.051502	Best loss: 0.051502	Accuracy: 98.16%
4	Validation loss: 0.064422	Best loss: 0.051502	Accuracy: 98.12%
5	Validation loss: 0.044983	Best loss: 0.044983	Accuracy: 98.67%
6	Validation loss: 0.041103	Best loss: 0.041103	Accuracy: 98.71%
7	Validation loss: 0.074468	Best loss: 0.041103	Accuracy: 98.28%
8	Validation loss: 0.044166	Best loss: 0.041103	Accuracy: 98.87%
9	Validation loss: 0.062921	Best loss: 0.041103	Accuracy: 98.44%
10	Validation loss: 0.050879	Best loss: 0.041103	Accuracy: 98.67%
11	Validation loss: 0.070971	Best loss: 0.041103	Accuracy: 98.44%
12	Validation loss: 0.051346	Best loss: 0.041103	Accuracy: 98.55%
13	Validation loss: 0.049102	Best loss: 0.041103	Accuracy: 98.79%
14	Validation loss: 0.060357	Best loss: 0.041103	Accuracy: 98.59%
15	Validation loss: 0.052035	Best loss: 0.041103	Accuracy: 98.59%
16	Validation loss: 0.045060	Best loss: 0.041103	Accuracy: 98.91%
17	Validation loss: 0.071701	Best loss: 0.041103	Accuracy: 98.71%
18	Validation loss: 0.068157	Best loss: 0.041103	Accuracy: 98.44%
19	Validation loss: 0.058549	Best loss: 0.041103	Accuracy: 98.71%
20	Validation loss: 0.038526	Best loss: 0.038526	Accuracy: 98.94%
21	Validation loss: 0.040087	Best loss: 0.038526	Accuracy: 99.26%
22	Validation loss: 0.034167	Best loss: 0.034167	Accuracy: 99.26%
23	Validation loss: 0.068442	Best loss: 0.034167	Accuracy: 98.48%
24	Validation loss: 0.046294	Best loss: 0.034167	Accuracy: 98.83%
25	Validation loss: 0.063971	Best loss: 0.034167	Accuracy: 98.55%
26	Validation loss: 0.057781	Best loss: 0.034167	Accuracy: 98.94%
27	Validation loss: 0.053397	Best loss: 0.034167	Accuracy: 98.83%
28	Validation loss: 0.042017	Best loss: 0.034167	Accuracy: 99.02%
29	Validation loss: 0.053869	Best loss: 0.034167	Accuracy: 98.83%
30	Validation loss: 0.038187	Best loss: 0.034167	Accuracy: 99.02%
31	Validation loss: 0.042598	Best loss: 0.034167	Accuracy: 99.14%
32	Validation loss: 0.048955	Best loss: 0.034167	Accuracy: 98.75%
33	Validation loss: 0.061465	Best loss: 0.034167	Accuracy: 98.91%
34	Validation loss: 0.049732	Best loss: 0.034167	Accuracy: 98.98%
35	Validation loss: 0.042662	Best loss: 0.034167	Accuracy: 99.10%
36	Validation loss: 0.092186	Best loss: 0.034167	Accuracy: 98.44%
37	Validation loss: 0.040765	Best loss: 0.034167	Accuracy: 99.10%
38	Validation loss: 0.043198	Best loss: 0.034167	Accuracy: 99.02%
39	Validation loss: 0.042773	Best loss: 0.034167	Accuracy: 99.14%
40	Validation loss: 0.040589	Best loss: 0.034167	Accuracy: 99.10%
41	Validation loss: 0.053106	Best loss: 0.034167	Accuracy: 98.91%
42	Validation loss: 0.038199	Best loss: 0.034167	Accuracy: 99.37%
43	Validation loss: 0.060917	Best loss: 0.034167	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.95, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.0min
[CV] n_neurons=120, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.208597	Best loss: 0.208597	Accuracy: 95.00%
1	Validation loss: 0.081600	Best loss: 0.081600	Accuracy: 97.69%
2	Validation loss: 0.095274	Best loss: 0.081600	Accuracy: 98.20%
3	Validation loss: 0.092379	Best loss: 0.081600	Accuracy: 97.89%
4	Validation loss: 0.219729	Best loss: 0.081600	Accuracy: 96.64%
5	Validation loss: 0.081337	Best loss: 0.081337	Accuracy: 98.36%
6	Validation loss: 16.311857	Best loss: 0.081337	Accuracy: 62.67%
7	Validation loss: 0.290886	Best loss: 0.081337	Accuracy: 95.93%
8	Validation loss: 0.102508	Best loss: 0.081337	Accuracy: 98.63%
9	Validation loss: 0.076010	Best loss: 0.076010	Accuracy: 98.67%
10	Validation loss: 0.069917	Best loss: 0.069917	Accuracy: 98.75%
11	Validation loss: 0.053742	Best loss: 0.053742	Accuracy: 99.06%
12	Validation loss: 0.065901	Best loss: 0.053742	Accuracy: 98.87%
13	Validation loss: 0.091069	Best loss: 0.053742	Accuracy: 98.67%
14	Validation loss: 0.087041	Best loss: 0.053742	Accuracy: 98.28%
15	Validation loss: 7.679185	Best loss: 0.053742	Accuracy: 71.85%
16	Validation loss: 0.354720	Best loss: 0.053742	Accuracy: 96.91%
17	Validation loss: 0.271986	Best loss: 0.053742	Accuracy: 98.28%
18	Validation loss: 0.102885	Best loss: 0.053742	Accuracy: 98.87%
19	Validation loss: 0.078346	Best loss: 0.053742	Accuracy: 98.83%
20	Validation loss: 0.065964	Best loss: 0.053742	Accuracy: 98.94%
21	Validation loss: 0.077440	Best loss: 0.053742	Accuracy: 98.94%
22	Validation loss: 0.077620	Best loss: 0.053742	Accuracy: 98.79%
23	Validation loss: 4.987381	Best loss: 0.053742	Accuracy: 91.52%
24	Validation loss: 0.224194	Best loss: 0.053742	Accuracy: 98.08%
25	Validation loss: 0.126069	Best loss: 0.053742	Accuracy: 98.79%
26	Validation loss: 0.229163	Best loss: 0.053742	Accuracy: 97.85%
27	Validation loss: 0.183173	Best loss: 0.053742	Accuracy: 98.75%
28	Validation loss: 0.143440	Best loss: 0.053742	Accuracy: 98.79%
29	Validation loss: 1.842704	Best loss: 0.053742	Accuracy: 96.60%
30	Validation loss: 0.309699	Best loss: 0.053742	Accuracy: 98.63%
31	Validation loss: 0.187250	Best loss: 0.053742	Accuracy: 98.98%
32	Validation loss: 0.138224	Best loss: 0.053742	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.0min
[CV] n_neurons=120, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.195814	Best loss: 0.195814	Accuracy: 95.23%
1	Validation loss: 0.098998	Best loss: 0.098998	Accuracy: 97.50%
2	Validation loss: 0.697738	Best loss: 0.098998	Accuracy: 91.87%
3	Validation loss: 0.060113	Best loss: 0.060113	Accuracy: 98.24%
4	Validation loss: 0.076871	Best loss: 0.060113	Accuracy: 97.97%
5	Validation loss: 0.093054	Best loss: 0.060113	Accuracy: 97.65%
6	Validation loss: 0.409561	Best loss: 0.060113	Accuracy: 96.52%
7	Validation loss: 0.083020	Best loss: 0.060113	Accuracy: 98.36%
8	Validation loss: 0.059850	Best loss: 0.059850	Accuracy: 98.63%
9	Validation loss: 0.050343	Best loss: 0.050343	Accuracy: 98.67%
10	Validation loss: 0.058995	Best loss: 0.050343	Accuracy: 98.51%
11	Validation loss: 0.071519	Best loss: 0.050343	Accuracy: 98.48%
12	Validation loss: 0.226472	Best loss: 0.050343	Accuracy: 97.97%
13	Validation loss: 0.144665	Best loss: 0.050343	Accuracy: 97.54%
14	Validation loss: 0.162347	Best loss: 0.050343	Accuracy: 98.08%
15	Validation loss: 0.185027	Best loss: 0.050343	Accuracy: 97.07%
16	Validation loss: 0.257249	Best loss: 0.050343	Accuracy: 97.77%
17	Validation loss: 0.162750	Best loss: 0.050343	Accuracy: 97.65%
18	Validation loss: 0.056027	Best loss: 0.050343	Accuracy: 99.06%
19	Validation loss: 0.048273	Best loss: 0.048273	Accuracy: 99.18%
20	Validation loss: 0.047772	Best loss: 0.047772	Accuracy: 99.30%
21	Validation loss: 0.116516	Best loss: 0.047772	Accuracy: 98.83%
22	Validation loss: 1.501552	Best loss: 0.047772	Accuracy: 97.46%
23	Validation loss: 0.298164	Best loss: 0.047772	Accuracy: 98.75%
24	Validation loss: 0.246727	Best loss: 0.047772	Accuracy: 98.63%
25	Validation loss: 0.198452	Best loss: 0.047772	Accuracy: 98.87%
26	Validation loss: 0.201916	Best loss: 0.047772	Accuracy: 98.51%
27	Validation loss: 0.184520	Best loss: 0.047772	Accuracy: 99.02%
28	Validation loss: 0.149048	Best loss: 0.047772	Accuracy: 98.94%
29	Validation loss: 0.214482	Best loss: 0.047772	Accuracy: 98.83%
30	Validation loss: 0.214170	Best loss: 0.047772	Accuracy: 98.79%
31	Validation loss: 2.977505	Best loss: 0.047772	Accuracy: 96.76%
32	Validation loss: 0.365318	Best loss: 0.047772	Accuracy: 98.94%
33	Validation loss: 0.327977	Best loss: 0.047772	Accuracy: 98.71%
34	Validation loss: 0.199681	Best loss: 0.047772	Accuracy: 98.87%
35	Validation loss: 0.475344	Best loss: 0.047772	Accuracy: 98.36%
36	Validation loss: 0.271304	Best loss: 0.047772	Accuracy: 98.87%
37	Validation loss: 0.186184	Best loss: 0.047772	Accuracy: 99.14%
38	Validation loss: 0.145548	Best loss: 0.047772	Accuracy: 99.14%
39	Validation loss: 0.263914	Best loss: 0.047772	Accuracy: 98.79%
40	Validation loss: 0.182858	Best loss: 0.047772	Accuracy: 98.98%
41	Validation loss: 0.218780	Best loss: 0.047772	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.2min
[CV] n_neurons=120, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.172368	Best loss: 0.172368	Accuracy: 96.95%
1	Validation loss: 0.123661	Best loss: 0.123661	Accuracy: 96.79%
2	Validation loss: 0.074669	Best loss: 0.074669	Accuracy: 98.01%
3	Validation loss: 0.084731	Best loss: 0.074669	Accuracy: 97.81%
4	Validation loss: 0.135295	Best loss: 0.074669	Accuracy: 97.97%
5	Validation loss: 0.067530	Best loss: 0.067530	Accuracy: 97.89%
6	Validation loss: 3.061017	Best loss: 0.067530	Accuracy: 88.19%
7	Validation loss: 0.105836	Best loss: 0.067530	Accuracy: 98.24%
8	Validation loss: 0.049829	Best loss: 0.049829	Accuracy: 98.71%
9	Validation loss: 0.040800	Best loss: 0.040800	Accuracy: 98.67%
10	Validation loss: 0.063436	Best loss: 0.040800	Accuracy: 98.75%
11	Validation loss: 0.035553	Best loss: 0.035553	Accuracy: 98.87%
12	Validation loss: 0.312587	Best loss: 0.035553	Accuracy: 97.97%
13	Validation loss: 0.452123	Best loss: 0.035553	Accuracy: 98.28%
14	Validation loss: 0.108411	Best loss: 0.035553	Accuracy: 98.87%
15	Validation loss: 0.063586	Best loss: 0.035553	Accuracy: 98.91%
16	Validation loss: 0.068366	Best loss: 0.035553	Accuracy: 98.91%
17	Validation loss: 0.085080	Best loss: 0.035553	Accuracy: 98.67%
18	Validation loss: 0.084521	Best loss: 0.035553	Accuracy: 98.91%
19	Validation loss: 0.088844	Best loss: 0.035553	Accuracy: 98.87%
20	Validation loss: 0.101631	Best loss: 0.035553	Accuracy: 98.55%
21	Validation loss: 0.112827	Best loss: 0.035553	Accuracy: 98.63%
22	Validation loss: 1.498221	Best loss: 0.035553	Accuracy: 96.40%
23	Validation loss: 0.249195	Best loss: 0.035553	Accuracy: 98.44%
24	Validation loss: 0.107377	Best loss: 0.035553	Accuracy: 98.94%
25	Validation loss: 0.110524	Best loss: 0.035553	Accuracy: 98.87%
26	Validation loss: 0.063842	Best loss: 0.035553	Accuracy: 99.22%
27	Validation loss: 0.093711	Best loss: 0.035553	Accuracy: 98.94%
28	Validation loss: 0.184349	Best loss: 0.035553	Accuracy: 98.55%
29	Validation loss: 0.306320	Best loss: 0.035553	Accuracy: 98.71%
30	Validation loss: 0.330934	Best loss: 0.035553	Accuracy: 98.48%
31	Validation loss: 0.129814	Best loss: 0.035553	Accuracy: 98.98%
32	Validation loss: 0.161528	Best loss: 0.035553	Accuracy: 98.91%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.0min
[CV] n_neurons=10, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.264158	Best loss: 0.264158	Accuracy: 92.57%
1	Validation loss: 0.165056	Best loss: 0.165056	Accuracy: 95.23%
2	Validation loss: 0.145583	Best loss: 0.145583	Accuracy: 95.39%
3	Validation loss: 0.149098	Best loss: 0.145583	Accuracy: 95.43%
4	Validation loss: 0.292341	Best loss: 0.145583	Accuracy: 89.05%
5	Validation loss: 0.169418	Best loss: 0.145583	Accuracy: 95.04%
6	Validation loss: 0.085891	Best loss: 0.085891	Accuracy: 97.50%
7	Validation loss: 0.127434	Best loss: 0.085891	Accuracy: 96.40%
8	Validation loss: 0.207583	Best loss: 0.085891	Accuracy: 94.84%
9	Validation loss: 0.078138	Best loss: 0.078138	Accuracy: 97.93%
10	Validation loss: 0.118284	Best loss: 0.078138	Accuracy: 96.95%
11	Validation loss: 0.094467	Best loss: 0.078138	Accuracy: 97.65%
12	Validation loss: 0.104557	Best loss: 0.078138	Accuracy: 97.50%
13	Validation loss: 0.133229	Best loss: 0.078138	Accuracy: 96.52%
14	Validation loss: 0.095809	Best loss: 0.078138	Accuracy: 97.77%
15	Validation loss: 0.119083	Best loss: 0.078138	Accuracy: 97.46%
16	Validation loss: 0.102711	Best loss: 0.078138	Accuracy: 97.50%
17	Validation loss: 0.110628	Best loss: 0.078138	Accuracy: 97.73%
18	Validation loss: 0.114217	Best loss: 0.078138	Accuracy: 97.15%
19	Validation loss: 0.133385	Best loss: 0.078138	Accuracy: 97.38%
20	Validation loss: 0.084075	Best loss: 0.078138	Accuracy: 98.28%
21	Validation loss: 0.122834	Best loss: 0.078138	Accuracy: 97.26%
22	Validation loss: 0.104705	Best loss: 0.078138	Accuracy: 97.73%
23	Validation loss: 0.143729	Best loss: 0.078138	Accuracy: 97.34%
24	Validation loss: 0.128479	Best loss: 0.078138	Accuracy: 97.69%
25	Validation loss: 0.119445	Best loss: 0.078138	Accuracy: 97.42%
26	Validation loss: 0.117927	Best loss: 0.078138	Accuracy: 97.77%
27	Validation loss: 0.112870	Best loss: 0.078138	Accuracy: 97.73%
28	Validation loss: 0.117787	Best loss: 0.078138	Accuracy: 97.62%
29	Validation loss: 0.202802	Best loss: 0.078138	Accuracy: 95.78%
30	Validation loss: 0.091533	Best loss: 0.078138	Accuracy: 97.97%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total=   8.4s
[CV] n_neurons=10, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.206499	Best loss: 0.206499	Accuracy: 93.59%
1	Validation loss: 0.175375	Best loss: 0.175375	Accuracy: 94.25%
2	Validation loss: 0.122817	Best loss: 0.122817	Accuracy: 96.64%
3	Validation loss: 0.131442	Best loss: 0.122817	Accuracy: 95.86%
4	Validation loss: 0.095623	Best loss: 0.095623	Accuracy: 96.87%
5	Validation loss: 0.100492	Best loss: 0.095623	Accuracy: 96.72%
6	Validation loss: 0.098175	Best loss: 0.095623	Accuracy: 97.62%
7	Validation loss: 0.139268	Best loss: 0.095623	Accuracy: 96.05%
8	Validation loss: 0.103013	Best loss: 0.095623	Accuracy: 96.91%
9	Validation loss: 0.103680	Best loss: 0.095623	Accuracy: 96.99%
10	Validation loss: 0.106068	Best loss: 0.095623	Accuracy: 97.19%
11	Validation loss: 0.117420	Best loss: 0.095623	Accuracy: 97.07%
12	Validation loss: 0.104611	Best loss: 0.095623	Accuracy: 97.50%
13	Validation loss: 0.121955	Best loss: 0.095623	Accuracy: 97.38%
14	Validation loss: 0.082500	Best loss: 0.082500	Accuracy: 97.85%
15	Validation loss: 0.087211	Best loss: 0.082500	Accuracy: 97.77%
16	Validation loss: 0.086095	Best loss: 0.082500	Accuracy: 98.05%
17	Validation loss: 0.102151	Best loss: 0.082500	Accuracy: 97.62%
18	Validation loss: 0.093085	Best loss: 0.082500	Accuracy: 98.01%
19	Validation loss: 0.110342	Best loss: 0.082500	Accuracy: 97.97%
20	Validation loss: 0.093387	Best loss: 0.082500	Accuracy: 98.01%
21	Validation loss: 0.110124	Best loss: 0.082500	Accuracy: 97.42%
22	Validation loss: 0.096059	Best loss: 0.082500	Accuracy: 97.73%
23	Validation loss: 0.125697	Best loss: 0.082500	Accuracy: 97.73%
24	Validation loss: 0.089741	Best loss: 0.082500	Accuracy: 98.05%
25	Validation loss: 0.111203	Best loss: 0.082500	Accuracy: 97.81%
26	Validation loss: 0.119076	Best loss: 0.082500	Accuracy: 97.65%
27	Validation loss: 0.127893	Best loss: 0.082500	Accuracy: 97.77%
28	Validation loss: 0.093530	Best loss: 0.082500	Accuracy: 98.16%
29	Validation loss: 0.095806	Best loss: 0.082500	Accuracy: 98.24%
30	Validation loss: 0.165915	Best loss: 0.082500	Accuracy: 97.46%
31	Validation loss: 0.109842	Best loss: 0.082500	Accuracy: 97.81%
32	Validation loss: 0.121489	Best loss: 0.082500	Accuracy: 97.85%
33	Validation loss: 0.142889	Best loss: 0.082500	Accuracy: 97.19%
34	Validation loss: 0.118985	Best loss: 0.082500	Accuracy: 97.85%
35	Validation loss: 0.156430	Best loss: 0.082500	Accuracy: 97.11%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total=   9.6s
[CV] n_neurons=10, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.229925	Best loss: 0.229925	Accuracy: 93.00%
1	Validation loss: 0.233114	Best loss: 0.229925	Accuracy: 91.56%
2	Validation loss: 0.245124	Best loss: 0.229925	Accuracy: 91.91%
3	Validation loss: 0.190087	Best loss: 0.190087	Accuracy: 93.78%
4	Validation loss: 0.092742	Best loss: 0.092742	Accuracy: 96.87%
5	Validation loss: 0.098815	Best loss: 0.092742	Accuracy: 96.56%
6	Validation loss: 0.080619	Best loss: 0.080619	Accuracy: 97.11%
7	Validation loss: 0.080649	Best loss: 0.080619	Accuracy: 97.50%
8	Validation loss: 0.090933	Best loss: 0.080619	Accuracy: 97.50%
9	Validation loss: 0.072076	Best loss: 0.072076	Accuracy: 98.05%
10	Validation loss: 0.076817	Best loss: 0.072076	Accuracy: 97.85%
11	Validation loss: 0.083120	Best loss: 0.072076	Accuracy: 97.54%
12	Validation loss: 0.084334	Best loss: 0.072076	Accuracy: 97.77%
13	Validation loss: 0.075972	Best loss: 0.072076	Accuracy: 97.42%
14	Validation loss: 0.120680	Best loss: 0.072076	Accuracy: 96.91%
15	Validation loss: 0.086632	Best loss: 0.072076	Accuracy: 97.89%
16	Validation loss: 0.094271	Best loss: 0.072076	Accuracy: 97.81%
17	Validation loss: 0.101450	Best loss: 0.072076	Accuracy: 97.54%
18	Validation loss: 0.079982	Best loss: 0.072076	Accuracy: 97.73%
19	Validation loss: 0.105291	Best loss: 0.072076	Accuracy: 97.19%
20	Validation loss: 0.074564	Best loss: 0.072076	Accuracy: 97.93%
21	Validation loss: 0.121468	Best loss: 0.072076	Accuracy: 97.34%
22	Validation loss: 0.105508	Best loss: 0.072076	Accuracy: 97.89%
23	Validation loss: 0.076992	Best loss: 0.072076	Accuracy: 98.36%
24	Validation loss: 0.097925	Best loss: 0.072076	Accuracy: 97.81%
25	Validation loss: 0.079487	Best loss: 0.072076	Accuracy: 98.32%
26	Validation loss: 0.081278	Best loss: 0.072076	Accuracy: 98.20%
27	Validation loss: 0.074155	Best loss: 0.072076	Accuracy: 97.97%
28	Validation loss: 0.061651	Best loss: 0.061651	Accuracy: 98.40%
29	Validation loss: 0.071272	Best loss: 0.061651	Accuracy: 98.12%
30	Validation loss: 0.067605	Best loss: 0.061651	Accuracy: 98.44%
31	Validation loss: 0.076863	Best loss: 0.061651	Accuracy: 98.36%
32	Validation loss: 0.078108	Best loss: 0.061651	Accuracy: 98.08%
33	Validation loss: 0.098633	Best loss: 0.061651	Accuracy: 98.05%
34	Validation loss: 0.101053	Best loss: 0.061651	Accuracy: 98.08%
35	Validation loss: 0.126628	Best loss: 0.061651	Accuracy: 97.26%
36	Validation loss: 0.099204	Best loss: 0.061651	Accuracy: 97.81%
37	Validation loss: 0.111017	Best loss: 0.061651	Accuracy: 97.73%
38	Validation loss: 0.086819	Best loss: 0.061651	Accuracy: 98.24%
39	Validation loss: 0.108722	Best loss: 0.061651	Accuracy: 97.81%
40	Validation loss: 0.098566	Best loss: 0.061651	Accuracy: 98.05%
41	Validation loss: 0.094402	Best loss: 0.061651	Accuracy: 97.81%
42	Validation loss: 0.124804	Best loss: 0.061651	Accuracy: 97.69%
43	Validation loss: 0.107611	Best loss: 0.061651	Accuracy: 98.01%
44	Validation loss: 0.071693	Best loss: 0.061651	Accuracy: 98.28%
45	Validation loss: 0.079425	Best loss: 0.061651	Accuracy: 98.24%
46	Validation loss: 0.080142	Best loss: 0.061651	Accuracy: 98.20%
47	Validation loss: 0.080008	Best loss: 0.061651	Accuracy: 98.40%
48	Validation loss: 0.078455	Best loss: 0.061651	Accuracy: 98.16%
49	Validation loss: 0.088361	Best loss: 0.061651	Accuracy: 98.20%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total=  12.4s
[CV] n_neurons=160, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.122967	Best loss: 0.122967	Accuracy: 96.91%
1	Validation loss: 0.073627	Best loss: 0.073627	Accuracy: 97.93%
2	Validation loss: 0.063534	Best loss: 0.063534	Accuracy: 98.05%
3	Validation loss: 0.073978	Best loss: 0.063534	Accuracy: 97.89%
4	Validation loss: 0.079793	Best loss: 0.063534	Accuracy: 97.69%
5	Validation loss: 0.073930	Best loss: 0.063534	Accuracy: 98.32%
6	Validation loss: 0.055183	Best loss: 0.055183	Accuracy: 98.40%
7	Validation loss: 0.082039	Best loss: 0.055183	Accuracy: 98.01%
8	Validation loss: 0.049445	Best loss: 0.049445	Accuracy: 98.67%
9	Validation loss: 0.102642	Best loss: 0.049445	Accuracy: 97.65%
10	Validation loss: 0.232736	Best loss: 0.049445	Accuracy: 97.11%
11	Validation loss: 0.070800	Best loss: 0.049445	Accuracy: 98.51%
12	Validation loss: 0.072289	Best loss: 0.049445	Accuracy: 98.51%
13	Validation loss: 0.052375	Best loss: 0.049445	Accuracy: 98.51%
14	Validation loss: 0.140682	Best loss: 0.049445	Accuracy: 97.89%
15	Validation loss: 0.069690	Best loss: 0.049445	Accuracy: 98.67%
16	Validation loss: 0.050640	Best loss: 0.049445	Accuracy: 98.83%
17	Validation loss: 0.049798	Best loss: 0.049445	Accuracy: 98.67%
18	Validation loss: 0.051507	Best loss: 0.049445	Accuracy: 98.83%
19	Validation loss: 0.057364	Best loss: 0.049445	Accuracy: 98.83%
20	Validation loss: 0.062877	Best loss: 0.049445	Accuracy: 98.67%
21	Validation loss: 0.103777	Best loss: 0.049445	Accuracy: 97.93%
22	Validation loss: 0.079505	Best loss: 0.049445	Accuracy: 98.91%
23	Validation loss: 0.190004	Best loss: 0.049445	Accuracy: 97.97%
24	Validation loss: 0.217494	Best loss: 0.049445	Accuracy: 97.97%
25	Validation loss: 0.086267	Best loss: 0.049445	Accuracy: 98.79%
26	Validation loss: 0.066676	Best loss: 0.049445	Accuracy: 99.10%
27	Validation loss: 0.098446	Best loss: 0.049445	Accuracy: 98.91%
28	Validation loss: 0.072718	Best loss: 0.049445	Accuracy: 98.98%
29	Validation loss: 0.137616	Best loss: 0.049445	Accuracy: 98.44%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.1min
[CV] n_neurons=160, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.151226	Best loss: 0.151226	Accuracy: 95.31%
1	Validation loss: 0.098008	Best loss: 0.098008	Accuracy: 97.46%
2	Validation loss: 0.085549	Best loss: 0.085549	Accuracy: 97.77%
3	Validation loss: 0.075072	Best loss: 0.075072	Accuracy: 98.20%
4	Validation loss: 0.094640	Best loss: 0.075072	Accuracy: 97.69%
5	Validation loss: 0.074115	Best loss: 0.074115	Accuracy: 97.89%
6	Validation loss: 0.177250	Best loss: 0.074115	Accuracy: 97.19%
7	Validation loss: 0.063545	Best loss: 0.063545	Accuracy: 98.12%
8	Validation loss: 0.071201	Best loss: 0.063545	Accuracy: 98.51%
9	Validation loss: 0.247579	Best loss: 0.063545	Accuracy: 95.35%
10	Validation loss: 0.083471	Best loss: 0.063545	Accuracy: 98.08%
11	Validation loss: 0.063273	Best loss: 0.063273	Accuracy: 98.75%
12	Validation loss: 0.063759	Best loss: 0.063273	Accuracy: 98.32%
13	Validation loss: 0.117726	Best loss: 0.063273	Accuracy: 98.32%
14	Validation loss: 0.094766	Best loss: 0.063273	Accuracy: 98.36%
15	Validation loss: 0.087246	Best loss: 0.063273	Accuracy: 98.32%
16	Validation loss: 0.202709	Best loss: 0.063273	Accuracy: 97.62%
17	Validation loss: 0.052333	Best loss: 0.052333	Accuracy: 98.71%
18	Validation loss: 0.066740	Best loss: 0.052333	Accuracy: 98.24%
19	Validation loss: 0.052763	Best loss: 0.052333	Accuracy: 98.87%
20	Validation loss: 0.063512	Best loss: 0.052333	Accuracy: 98.79%
21	Validation loss: 0.060496	Best loss: 0.052333	Accuracy: 98.75%
22	Validation loss: 0.061719	Best loss: 0.052333	Accuracy: 98.83%
23	Validation loss: 0.795140	Best loss: 0.052333	Accuracy: 93.08%
24	Validation loss: 0.067781	Best loss: 0.052333	Accuracy: 98.91%
25	Validation loss: 0.054745	Best loss: 0.052333	Accuracy: 98.91%
26	Validation loss: 0.090014	Best loss: 0.052333	Accuracy: 98.51%
27	Validation loss: 0.073005	Best loss: 0.052333	Accuracy: 98.83%
28	Validation loss: 0.083469	Best loss: 0.052333	Accuracy: 98.51%
29	Validation loss: 0.088510	Best loss: 0.052333	Accuracy: 98.83%
30	Validation loss: 0.082744	Best loss: 0.052333	Accuracy: 98.87%
31	Validation loss: 0.084615	Best loss: 0.052333	Accuracy: 98.94%
32	Validation loss: 0.059368	Best loss: 0.052333	Accuracy: 98.87%
33	Validation loss: 0.142276	Best loss: 0.052333	Accuracy: 98.44%
34	Validation loss: 0.182582	Best loss: 0.052333	Accuracy: 98.05%
35	Validation loss: 0.139780	Best loss: 0.052333	Accuracy: 98.40%
36	Validation loss: 0.174733	Best loss: 0.052333	Accuracy: 98.59%
37	Validation loss: 0.105882	Best loss: 0.052333	Accuracy: 98.94%
38	Validation loss: 0.142444	Best loss: 0.052333	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.4min
[CV] n_neurons=160, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.222095	Best loss: 0.222095	Accuracy: 94.76%
1	Validation loss: 0.101757	Best loss: 0.101757	Accuracy: 97.34%
2	Validation loss: 0.104883	Best loss: 0.101757	Accuracy: 97.50%
3	Validation loss: 0.064137	Best loss: 0.064137	Accuracy: 98.36%
4	Validation loss: 0.055809	Best loss: 0.055809	Accuracy: 98.51%
5	Validation loss: 0.060122	Best loss: 0.055809	Accuracy: 98.44%
6	Validation loss: 0.072848	Best loss: 0.055809	Accuracy: 98.12%
7	Validation loss: 0.107401	Best loss: 0.055809	Accuracy: 98.05%
8	Validation loss: 0.080405	Best loss: 0.055809	Accuracy: 98.32%
9	Validation loss: 0.124217	Best loss: 0.055809	Accuracy: 97.73%
10	Validation loss: 0.060520	Best loss: 0.055809	Accuracy: 98.59%
11	Validation loss: 0.077576	Best loss: 0.055809	Accuracy: 98.59%
12	Validation loss: 0.207449	Best loss: 0.055809	Accuracy: 97.07%
13	Validation loss: 0.075013	Best loss: 0.055809	Accuracy: 98.44%
14	Validation loss: 0.076189	Best loss: 0.055809	Accuracy: 98.32%
15	Validation loss: 0.067657	Best loss: 0.055809	Accuracy: 98.67%
16	Validation loss: 0.054832	Best loss: 0.054832	Accuracy: 98.79%
17	Validation loss: 0.052812	Best loss: 0.052812	Accuracy: 98.94%
18	Validation loss: 0.094212	Best loss: 0.052812	Accuracy: 98.16%
19	Validation loss: 0.081820	Best loss: 0.052812	Accuracy: 98.87%
20	Validation loss: 0.073493	Best loss: 0.052812	Accuracy: 98.12%
21	Validation loss: 0.195306	Best loss: 0.052812	Accuracy: 96.95%
22	Validation loss: 0.052546	Best loss: 0.052546	Accuracy: 99.26%
23	Validation loss: 0.045754	Best loss: 0.045754	Accuracy: 99.14%
24	Validation loss: 0.058634	Best loss: 0.045754	Accuracy: 98.83%
25	Validation loss: 0.247625	Best loss: 0.045754	Accuracy: 97.81%
26	Validation loss: 0.085616	Best loss: 0.045754	Accuracy: 98.55%
27	Validation loss: 0.062754	Best loss: 0.045754	Accuracy: 98.91%
28	Validation loss: 0.078805	Best loss: 0.045754	Accuracy: 98.87%
29	Validation loss: 0.059283	Best loss: 0.045754	Accuracy: 99.26%
30	Validation loss: 0.097100	Best loss: 0.045754	Accuracy: 98.91%
31	Validation loss: 0.174069	Best loss: 0.045754	Accuracy: 97.81%
32	Validation loss: 0.162395	Best loss: 0.045754	Accuracy: 98.94%
33	Validation loss: 0.162474	Best loss: 0.045754	Accuracy: 98.51%
34	Validation loss: 0.080810	Best loss: 0.045754	Accuracy: 99.18%
35	Validation loss: 0.097336	Best loss: 0.045754	Accuracy: 99.06%
36	Validation loss: 0.085217	Best loss: 0.045754	Accuracy: 99.06%
37	Validation loss: 0.073161	Best loss: 0.045754	Accuracy: 98.83%
38	Validation loss: 0.112265	Best loss: 0.045754	Accuracy: 98.83%
39	Validation loss: 0.089030	Best loss: 0.045754	Accuracy: 98.94%
40	Validation loss: 0.079981	Best loss: 0.045754	Accuracy: 98.94%
41	Validation loss: 0.084542	Best loss: 0.045754	Accuracy: 98.87%
42	Validation loss: 0.070743	Best loss: 0.045754	Accuracy: 99.10%
43	Validation loss: 0.110518	Best loss: 0.045754	Accuracy: 98.79%
44	Validation loss: 0.147208	Best loss: 0.045754	Accuracy: 98.32%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.7min
[CV] n_neurons=100, learning_rate=0.02, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.081641	Best loss: 0.081641	Accuracy: 97.65%
1	Validation loss: 0.081765	Best loss: 0.081641	Accuracy: 98.05%
2	Validation loss: 0.059867	Best loss: 0.059867	Accuracy: 98.28%
3	Validation loss: 0.077532	Best loss: 0.059867	Accuracy: 97.81%
4	Validation loss: 0.042854	Best loss: 0.042854	Accuracy: 98.79%
5	Validation loss: 0.043285	Best loss: 0.042854	Accuracy: 98.75%
6	Validation loss: 0.045207	Best loss: 0.042854	Accuracy: 98.63%
7	Validation loss: 0.077551	Best loss: 0.042854	Accuracy: 98.08%
8	Validation loss: 0.041684	Best loss: 0.041684	Accuracy: 98.94%
9	Validation loss: 0.057161	Best loss: 0.041684	Accuracy: 98.75%
10	Validation loss: 0.065073	Best loss: 0.041684	Accuracy: 98.44%
11	Validation loss: 0.038072	Best loss: 0.038072	Accuracy: 98.91%
12	Validation loss: 0.045490	Best loss: 0.038072	Accuracy: 98.79%
13	Validation loss: 0.046958	Best loss: 0.038072	Accuracy: 98.87%
14	Validation loss: 0.041406	Best loss: 0.038072	Accuracy: 98.71%
15	Validation loss: 0.067975	Best loss: 0.038072	Accuracy: 98.79%
16	Validation loss: 0.054126	Best loss: 0.038072	Accuracy: 98.98%
17	Validation loss: 0.055849	Best loss: 0.038072	Accuracy: 98.75%
18	Validation loss: 0.083338	Best loss: 0.038072	Accuracy: 98.44%
19	Validation loss: 0.041440	Best loss: 0.038072	Accuracy: 98.91%
20	Validation loss: 0.057241	Best loss: 0.038072	Accuracy: 98.71%
21	Validation loss: 0.050788	Best loss: 0.038072	Accuracy: 98.83%
22	Validation loss: 0.045337	Best loss: 0.038072	Accuracy: 98.98%
23	Validation loss: 0.050156	Best loss: 0.038072	Accuracy: 98.94%
24	Validation loss: 0.069666	Best loss: 0.038072	Accuracy: 98.63%
25	Validation loss: 0.084721	Best loss: 0.038072	Accuracy: 98.51%
26	Validation loss: 0.061969	Best loss: 0.038072	Accuracy: 98.67%
27	Validation loss: 0.042215	Best loss: 0.038072	Accuracy: 99.02%
28	Validation loss: 0.043448	Best loss: 0.038072	Accuracy: 98.91%
29	Validation loss: 0.038929	Best loss: 0.038072	Accuracy: 98.94%
30	Validation loss: 0.029923	Best loss: 0.029923	Accuracy: 99.30%
31	Validation loss: 0.051790	Best loss: 0.029923	Accuracy: 99.14%
32	Validation loss: 0.043670	Best loss: 0.029923	Accuracy: 98.98%
33	Validation loss: 0.052030	Best loss: 0.029923	Accuracy: 98.83%
34	Validation loss: 0.057517	Best loss: 0.029923	Accuracy: 98.75%
35	Validation loss: 0.047425	Best loss: 0.029923	Accuracy: 99.02%
36	Validation loss: 0.043833	Best loss: 0.029923	Accuracy: 98.94%
37	Validation loss: 0.068761	Best loss: 0.029923	Accuracy: 98.91%
38	Validation loss: 0.041963	Best loss: 0.029923	Accuracy: 98.79%
39	Validation loss: 0.046108	Best loss: 0.029923	Accuracy: 99.02%
40	Validation loss: 0.042722	Best loss: 0.029923	Accuracy: 99.22%
41	Validation loss: 0.053982	Best loss: 0.029923	Accuracy: 99.10%
42	Validation loss: 0.041650	Best loss: 0.029923	Accuracy: 99.10%
43	Validation loss: 0.057837	Best loss: 0.029923	Accuracy: 98.91%
44	Validation loss: 0.048458	Best loss: 0.029923	Accuracy: 98.87%
45	Validation loss: 0.056824	Best loss: 0.029923	Accuracy: 99.10%
46	Validation loss: 0.079573	Best loss: 0.029923	Accuracy: 98.87%
47	Validation loss: 0.059263	Best loss: 0.029923	Accuracy: 98.91%
48	Validation loss: 0.058559	Best loss: 0.029923	Accuracy: 98.87%
49	Validation loss: 0.060767	Best loss: 0.029923	Accuracy: 98.91%
50	Validation loss: 0.055259	Best loss: 0.029923	Accuracy: 99.06%
51	Validation loss: 0.058950	Best loss: 0.029923	Accuracy: 98.91%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 1.5min
[CV] n_neurons=100, learning_rate=0.02, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.170721	Best loss: 0.170721	Accuracy: 95.93%
1	Validation loss: 0.057687	Best loss: 0.057687	Accuracy: 98.44%
2	Validation loss: 0.074510	Best loss: 0.057687	Accuracy: 97.73%
3	Validation loss: 0.052779	Best loss: 0.052779	Accuracy: 98.40%
4	Validation loss: 0.059723	Best loss: 0.052779	Accuracy: 98.40%
5	Validation loss: 0.048356	Best loss: 0.048356	Accuracy: 98.44%
6	Validation loss: 0.073801	Best loss: 0.048356	Accuracy: 98.24%
7	Validation loss: 0.039320	Best loss: 0.039320	Accuracy: 98.94%
8	Validation loss: 0.053885	Best loss: 0.039320	Accuracy: 98.75%
9	Validation loss: 0.036899	Best loss: 0.036899	Accuracy: 98.91%
10	Validation loss: 0.040653	Best loss: 0.036899	Accuracy: 98.94%
11	Validation loss: 0.040231	Best loss: 0.036899	Accuracy: 99.10%
12	Validation loss: 0.034707	Best loss: 0.034707	Accuracy: 99.14%
13	Validation loss: 0.036974	Best loss: 0.034707	Accuracy: 99.10%
14	Validation loss: 0.050080	Best loss: 0.034707	Accuracy: 98.94%
15	Validation loss: 0.046201	Best loss: 0.034707	Accuracy: 98.87%
16	Validation loss: 0.042763	Best loss: 0.034707	Accuracy: 98.83%
17	Validation loss: 0.051950	Best loss: 0.034707	Accuracy: 98.79%
18	Validation loss: 0.046055	Best loss: 0.034707	Accuracy: 99.02%
19	Validation loss: 0.057691	Best loss: 0.034707	Accuracy: 98.83%
20	Validation loss: 0.032049	Best loss: 0.032049	Accuracy: 99.06%
21	Validation loss: 0.041172	Best loss: 0.032049	Accuracy: 99.06%
22	Validation loss: 0.038552	Best loss: 0.032049	Accuracy: 99.06%
23	Validation loss: 0.039943	Best loss: 0.032049	Accuracy: 99.06%
24	Validation loss: 0.032353	Best loss: 0.032049	Accuracy: 99.26%
25	Validation loss: 0.046782	Best loss: 0.032049	Accuracy: 98.98%
26	Validation loss: 0.040495	Best loss: 0.032049	Accuracy: 99.06%
27	Validation loss: 0.044069	Best loss: 0.032049	Accuracy: 98.98%
28	Validation loss: 0.040132	Best loss: 0.032049	Accuracy: 99.14%
29	Validation loss: 0.040437	Best loss: 0.032049	Accuracy: 99.18%
30	Validation loss: 0.047339	Best loss: 0.032049	Accuracy: 99.06%
31	Validation loss: 0.059535	Best loss: 0.032049	Accuracy: 98.94%
32	Validation loss: 0.052938	Best loss: 0.032049	Accuracy: 98.94%
33	Validation loss: 0.043328	Best loss: 0.032049	Accuracy: 99.14%
34	Validation loss: 0.045217	Best loss: 0.032049	Accuracy: 99.22%
35	Validation loss: 0.044891	Best loss: 0.032049	Accuracy: 99.18%
36	Validation loss: 0.056891	Best loss: 0.032049	Accuracy: 98.94%
37	Validation loss: 0.047159	Best loss: 0.032049	Accuracy: 99.18%
38	Validation loss: 0.054491	Best loss: 0.032049	Accuracy: 98.91%
39	Validation loss: 0.048081	Best loss: 0.032049	Accuracy: 99.18%
40	Validation loss: 0.051562	Best loss: 0.032049	Accuracy: 99.06%
41	Validation loss: 0.054951	Best loss: 0.032049	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 1.3min
[CV] n_neurons=100, learning_rate=0.02, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.137297	Best loss: 0.137297	Accuracy: 96.91%
1	Validation loss: 0.059595	Best loss: 0.059595	Accuracy: 98.12%
2	Validation loss: 0.055230	Best loss: 0.055230	Accuracy: 98.44%
3	Validation loss: 0.046490	Best loss: 0.046490	Accuracy: 98.32%
4	Validation loss: 0.053932	Best loss: 0.046490	Accuracy: 98.44%
5	Validation loss: 0.053661	Best loss: 0.046490	Accuracy: 98.63%
6	Validation loss: 0.062709	Best loss: 0.046490	Accuracy: 98.16%
7	Validation loss: 0.057249	Best loss: 0.046490	Accuracy: 98.63%
8	Validation loss: 0.036392	Best loss: 0.036392	Accuracy: 98.98%
9	Validation loss: 0.037300	Best loss: 0.036392	Accuracy: 98.79%
10	Validation loss: 0.045085	Best loss: 0.036392	Accuracy: 98.71%
11	Validation loss: 0.032390	Best loss: 0.032390	Accuracy: 99.02%
12	Validation loss: 0.053661	Best loss: 0.032390	Accuracy: 98.40%
13	Validation loss: 0.030703	Best loss: 0.030703	Accuracy: 99.02%
14	Validation loss: 0.059192	Best loss: 0.030703	Accuracy: 98.79%
15	Validation loss: 0.062814	Best loss: 0.030703	Accuracy: 98.63%
16	Validation loss: 0.042820	Best loss: 0.030703	Accuracy: 98.83%
17	Validation loss: 0.052612	Best loss: 0.030703	Accuracy: 98.51%
18	Validation loss: 0.054798	Best loss: 0.030703	Accuracy: 98.44%
19	Validation loss: 0.043479	Best loss: 0.030703	Accuracy: 99.10%
20	Validation loss: 0.054672	Best loss: 0.030703	Accuracy: 98.63%
21	Validation loss: 0.038383	Best loss: 0.030703	Accuracy: 99.10%
22	Validation loss: 0.047668	Best loss: 0.030703	Accuracy: 98.79%
23	Validation loss: 0.041960	Best loss: 0.030703	Accuracy: 99.06%
24	Validation loss: 0.040125	Best loss: 0.030703	Accuracy: 98.98%
25	Validation loss: 0.043831	Best loss: 0.030703	Accuracy: 98.98%
26	Validation loss: 0.040185	Best loss: 0.030703	Accuracy: 98.98%
27	Validation loss: 0.038450	Best loss: 0.030703	Accuracy: 98.87%
28	Validation loss: 0.051992	Best loss: 0.030703	Accuracy: 98.83%
29	Validation loss: 0.035053	Best loss: 0.030703	Accuracy: 98.94%
30	Validation loss: 0.060337	Best loss: 0.030703	Accuracy: 98.87%
31	Validation loss: 0.065633	Best loss: 0.030703	Accuracy: 98.63%
32	Validation loss: 0.026218	Best loss: 0.026218	Accuracy: 99.18%
33	Validation loss: 0.032578	Best loss: 0.026218	Accuracy: 99.14%
34	Validation loss: 0.036654	Best loss: 0.026218	Accuracy: 99.26%
35	Validation loss: 0.038328	Best loss: 0.026218	Accuracy: 99.18%
36	Validation loss: 0.052935	Best loss: 0.026218	Accuracy: 98.94%
37	Validation loss: 0.036116	Best loss: 0.026218	Accuracy: 99.37%
38	Validation loss: 0.057692	Best loss: 0.026218	Accuracy: 98.87%
39	Validation loss: 0.031628	Best loss: 0.026218	Accuracy: 99.10%
40	Validation loss: 0.043644	Best loss: 0.026218	Accuracy: 99.14%
41	Validation loss: 0.051122	Best loss: 0.026218	Accuracy: 98.75%
42	Validation loss: 0.053577	Best loss: 0.026218	Accuracy: 98.94%
43	Validation loss: 0.056929	Best loss: 0.026218	Accuracy: 98.75%
44	Validation loss: 0.065919	Best loss: 0.026218	Accuracy: 98.94%
45	Validation loss: 0.036891	Best loss: 0.026218	Accuracy: 99.37%
46	Validation loss: 0.044042	Best loss: 0.026218	Accuracy: 99.22%
47	Validation loss: 0.062056	Best loss: 0.026218	Accuracy: 98.98%
48	Validation loss: 0.032063	Best loss: 0.026218	Accuracy: 99.37%
49	Validation loss: 0.043325	Best loss: 0.026218	Accuracy: 99.30%
50	Validation loss: 0.036455	Best loss: 0.026218	Accuracy: 99.26%
51	Validation loss: 0.037328	Best loss: 0.026218	Accuracy: 99.14%
52	Validation loss: 0.047625	Best loss: 0.026218	Accuracy: 98.98%
53	Validation loss: 0.040500	Best loss: 0.026218	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, batch_size=50, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 1.7min
[CV] n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.067455	Best loss: 0.067455	Accuracy: 98.05%
1	Validation loss: 0.061855	Best loss: 0.061855	Accuracy: 98.24%
2	Validation loss: 0.047230	Best loss: 0.047230	Accuracy: 98.79%
3	Validation loss: 0.063804	Best loss: 0.047230	Accuracy: 98.08%
4	Validation loss: 0.034730	Best loss: 0.034730	Accuracy: 98.91%
5	Validation loss: 0.040584	Best loss: 0.034730	Accuracy: 98.91%
6	Validation loss: 0.030964	Best loss: 0.030964	Accuracy: 98.98%
7	Validation loss: 0.042575	Best loss: 0.030964	Accuracy: 98.91%
8	Validation loss: 0.040437	Best loss: 0.030964	Accuracy: 99.02%
9	Validation loss: 0.034803	Best loss: 0.030964	Accuracy: 99.10%
10	Validation loss: 0.034514	Best loss: 0.030964	Accuracy: 98.94%
11	Validation loss: 0.060066	Best loss: 0.030964	Accuracy: 98.44%
12	Validation loss: 0.034739	Best loss: 0.030964	Accuracy: 99.26%
13	Validation loss: 0.035592	Best loss: 0.030964	Accuracy: 98.98%
14	Validation loss: 0.047036	Best loss: 0.030964	Accuracy: 99.06%
15	Validation loss: 0.062472	Best loss: 0.030964	Accuracy: 98.79%
16	Validation loss: 0.042419	Best loss: 0.030964	Accuracy: 99.02%
17	Validation loss: 0.044851	Best loss: 0.030964	Accuracy: 98.98%
18	Validation loss: 0.057798	Best loss: 0.030964	Accuracy: 98.75%
19	Validation loss: 0.045748	Best loss: 0.030964	Accuracy: 99.02%
20	Validation loss: 0.056114	Best loss: 0.030964	Accuracy: 98.91%
21	Validation loss: 0.043950	Best loss: 0.030964	Accuracy: 99.14%
22	Validation loss: 0.043575	Best loss: 0.030964	Accuracy: 98.83%
23	Validation loss: 0.051284	Best loss: 0.030964	Accuracy: 98.98%
24	Validation loss: 0.070433	Best loss: 0.030964	Accuracy: 98.51%
25	Validation loss: 0.050912	Best loss: 0.030964	Accuracy: 98.98%
26	Validation loss: 0.041273	Best loss: 0.030964	Accuracy: 99.06%
27	Validation loss: 0.044195	Best loss: 0.030964	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  34.7s
[CV] n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.081618	Best loss: 0.081618	Accuracy: 97.58%
1	Validation loss: 0.047220	Best loss: 0.047220	Accuracy: 98.48%
2	Validation loss: 0.044672	Best loss: 0.044672	Accuracy: 98.59%
3	Validation loss: 0.046910	Best loss: 0.044672	Accuracy: 98.59%
4	Validation loss: 0.047807	Best loss: 0.044672	Accuracy: 98.40%
5	Validation loss: 0.048082	Best loss: 0.044672	Accuracy: 98.36%
6	Validation loss: 0.049191	Best loss: 0.044672	Accuracy: 98.71%
7	Validation loss: 0.057325	Best loss: 0.044672	Accuracy: 98.55%
8	Validation loss: 0.039913	Best loss: 0.039913	Accuracy: 98.87%
9	Validation loss: 0.047506	Best loss: 0.039913	Accuracy: 98.83%
10	Validation loss: 0.056892	Best loss: 0.039913	Accuracy: 98.71%
11	Validation loss: 0.041382	Best loss: 0.039913	Accuracy: 98.87%
12	Validation loss: 0.034376	Best loss: 0.034376	Accuracy: 99.06%
13	Validation loss: 0.034061	Best loss: 0.034061	Accuracy: 99.06%
14	Validation loss: 0.041159	Best loss: 0.034061	Accuracy: 98.98%
15	Validation loss: 0.046244	Best loss: 0.034061	Accuracy: 99.06%
16	Validation loss: 0.041082	Best loss: 0.034061	Accuracy: 99.02%
17	Validation loss: 0.087970	Best loss: 0.034061	Accuracy: 98.24%
18	Validation loss: 0.040601	Best loss: 0.034061	Accuracy: 98.87%
19	Validation loss: 0.049411	Best loss: 0.034061	Accuracy: 98.91%
20	Validation loss: 0.040935	Best loss: 0.034061	Accuracy: 99.10%
21	Validation loss: 0.049134	Best loss: 0.034061	Accuracy: 99.18%
22	Validation loss: 0.044857	Best loss: 0.034061	Accuracy: 98.91%
23	Validation loss: 0.054938	Best loss: 0.034061	Accuracy: 98.87%
24	Validation loss: 0.038926	Best loss: 0.034061	Accuracy: 99.10%
25	Validation loss: 0.047377	Best loss: 0.034061	Accuracy: 99.06%
26	Validation loss: 0.037424	Best loss: 0.034061	Accuracy: 99.26%
27	Validation loss: 0.046100	Best loss: 0.034061	Accuracy: 99.02%
28	Validation loss: 0.048655	Best loss: 0.034061	Accuracy: 98.98%
29	Validation loss: 0.056440	Best loss: 0.034061	Accuracy: 99.06%
30	Validation loss: 0.044988	Best loss: 0.034061	Accuracy: 99.26%
31	Validation loss: 0.047036	Best loss: 0.034061	Accuracy: 99.02%
32	Validation loss: 0.051043	Best loss: 0.034061	Accuracy: 98.98%
33	Validation loss: 0.055008	Best loss: 0.034061	Accuracy: 98.91%
34	Validation loss: 0.045203	Best loss: 0.034061	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  42.4s
[CV] n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.064416	Best loss: 0.064416	Accuracy: 98.16%
1	Validation loss: 0.050405	Best loss: 0.050405	Accuracy: 98.40%
2	Validation loss: 0.056791	Best loss: 0.050405	Accuracy: 98.16%
3	Validation loss: 0.040951	Best loss: 0.040951	Accuracy: 98.91%
4	Validation loss: 0.042159	Best loss: 0.040951	Accuracy: 98.63%
5	Validation loss: 0.033673	Best loss: 0.033673	Accuracy: 98.67%
6	Validation loss: 0.037937	Best loss: 0.033673	Accuracy: 98.91%
7	Validation loss: 0.040383	Best loss: 0.033673	Accuracy: 98.91%
8	Validation loss: 0.029133	Best loss: 0.029133	Accuracy: 99.02%
9	Validation loss: 0.031293	Best loss: 0.029133	Accuracy: 99.22%
10	Validation loss: 0.037224	Best loss: 0.029133	Accuracy: 98.94%
11	Validation loss: 0.046777	Best loss: 0.029133	Accuracy: 98.87%
12	Validation loss: 0.034861	Best loss: 0.029133	Accuracy: 99.06%
13	Validation loss: 0.036934	Best loss: 0.029133	Accuracy: 99.06%
14	Validation loss: 0.037278	Best loss: 0.029133	Accuracy: 98.98%
15	Validation loss: 0.042614	Best loss: 0.029133	Accuracy: 99.02%
16	Validation loss: 0.032502	Best loss: 0.029133	Accuracy: 99.14%
17	Validation loss: 0.033310	Best loss: 0.029133	Accuracy: 98.94%
18	Validation loss: 0.048315	Best loss: 0.029133	Accuracy: 98.91%
19	Validation loss: 0.031671	Best loss: 0.029133	Accuracy: 99.18%
20	Validation loss: 0.041543	Best loss: 0.029133	Accuracy: 99.14%
21	Validation loss: 0.035281	Best loss: 0.029133	Accuracy: 98.98%
22	Validation loss: 0.037397	Best loss: 0.029133	Accuracy: 98.91%
23	Validation loss: 0.033223	Best loss: 0.029133	Accuracy: 99.10%
24	Validation loss: 0.039809	Best loss: 0.029133	Accuracy: 98.98%
25	Validation loss: 0.039518	Best loss: 0.029133	Accuracy: 98.91%
26	Validation loss: 0.049248	Best loss: 0.029133	Accuracy: 98.87%
27	Validation loss: 0.048837	Best loss: 0.029133	Accuracy: 98.98%
28	Validation loss: 0.030682	Best loss: 0.029133	Accuracy: 99.34%
29	Validation loss: 0.058448	Best loss: 0.029133	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  34.7s
[CV] n_neurons=90, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 2.494629	Best loss: 2.494629	Accuracy: 94.33%
1	Validation loss: 0.503220	Best loss: 0.503220	Accuracy: 94.96%
2	Validation loss: 0.152014	Best loss: 0.152014	Accuracy: 97.65%
3	Validation loss: 0.104971	Best loss: 0.104971	Accuracy: 98.20%
4	Validation loss: 0.085086	Best loss: 0.085086	Accuracy: 98.40%
5	Validation loss: 0.131101	Best loss: 0.085086	Accuracy: 97.73%
6	Validation loss: 0.128493	Best loss: 0.085086	Accuracy: 97.46%
7	Validation loss: 0.098243	Best loss: 0.085086	Accuracy: 98.20%
8	Validation loss: 0.072220	Best loss: 0.072220	Accuracy: 98.67%
9	Validation loss: 0.070263	Best loss: 0.070263	Accuracy: 98.71%
10	Validation loss: 0.061565	Best loss: 0.061565	Accuracy: 98.71%
11	Validation loss: 0.093822	Best loss: 0.061565	Accuracy: 98.28%
12	Validation loss: 0.104020	Best loss: 0.061565	Accuracy: 98.28%
13	Validation loss: 0.070264	Best loss: 0.061565	Accuracy: 98.63%
14	Validation loss: 0.068082	Best loss: 0.061565	Accuracy: 98.71%
15	Validation loss: 0.096528	Best loss: 0.061565	Accuracy: 98.44%
16	Validation loss: 0.091198	Best loss: 0.061565	Accuracy: 98.55%
17	Validation loss: 0.090263	Best loss: 0.061565	Accuracy: 98.40%
18	Validation loss: 0.068980	Best loss: 0.061565	Accuracy: 98.48%
19	Validation loss: 0.079091	Best loss: 0.061565	Accuracy: 98.48%
20	Validation loss: 0.074983	Best loss: 0.061565	Accuracy: 98.71%
21	Validation loss: 0.078237	Best loss: 0.061565	Accuracy: 98.83%
22	Validation loss: 0.098658	Best loss: 0.061565	Accuracy: 98.24%
23	Validation loss: 0.056878	Best loss: 0.056878	Accuracy: 98.87%
24	Validation loss: 0.084435	Best loss: 0.056878	Accuracy: 98.83%
25	Validation loss: 0.059834	Best loss: 0.056878	Accuracy: 98.98%
26	Validation loss: 0.075904	Best loss: 0.056878	Accuracy: 98.59%
27	Validation loss: 0.079252	Best loss: 0.056878	Accuracy: 98.59%
28	Validation loss: 0.066225	Best loss: 0.056878	Accuracy: 98.79%
29	Validation loss: 0.086149	Best loss: 0.056878	Accuracy: 98.59%
30	Validation loss: 0.060643	Best loss: 0.056878	Accuracy: 99.06%
31	Validation loss: 0.059942	Best loss: 0.056878	Accuracy: 98.98%
32	Validation loss: 0.089972	Best loss: 0.056878	Accuracy: 98.28%
33	Validation loss: 0.066644	Best loss: 0.056878	Accuracy: 98.83%
34	Validation loss: 0.080575	Best loss: 0.056878	Accuracy: 98.36%
35	Validation loss: 0.057695	Best loss: 0.056878	Accuracy: 99.02%
36	Validation loss: 0.080141	Best loss: 0.056878	Accuracy: 98.75%
37	Validation loss: 0.053771	Best loss: 0.053771	Accuracy: 98.98%
38	Validation loss: 0.095993	Best loss: 0.053771	Accuracy: 98.63%
39	Validation loss: 0.089760	Best loss: 0.053771	Accuracy: 98.48%
40	Validation loss: 0.062467	Best loss: 0.053771	Accuracy: 98.75%
41	Validation loss: 0.052675	Best loss: 0.052675	Accuracy: 99.06%
42	Validation loss: 0.056606	Best loss: 0.052675	Accuracy: 98.91%
43	Validation loss: 0.057342	Best loss: 0.052675	Accuracy: 99.14%
44	Validation loss: 0.056563	Best loss: 0.052675	Accuracy: 98.94%
45	Validation loss: 0.053628	Best loss: 0.052675	Accuracy: 98.98%
46	Validation loss: 0.041741	Best loss: 0.041741	Accuracy: 99.06%
47	Validation loss: 0.054042	Best loss: 0.041741	Accuracy: 98.71%
48	Validation loss: 0.065447	Best loss: 0.041741	Accuracy: 98.75%
49	Validation loss: 0.056949	Best loss: 0.041741	Accuracy: 99.02%
50	Validation loss: 0.052567	Best loss: 0.041741	Accuracy: 99.02%
51	Validation loss: 0.069476	Best loss: 0.041741	Accuracy: 98.87%
52	Validation loss: 0.067438	Best loss: 0.041741	Accuracy: 99.02%
53	Validation loss: 0.091859	Best loss: 0.041741	Accuracy: 98.12%
54	Validation loss: 0.085583	Best loss: 0.041741	Accuracy: 98.91%
55	Validation loss: 0.076337	Best loss: 0.041741	Accuracy: 98.75%
56	Validation loss: 0.090951	Best loss: 0.041741	Accuracy: 98.63%
57	Validation loss: 0.066225	Best loss: 0.041741	Accuracy: 98.91%
58	Validation loss: 0.083922	Best loss: 0.041741	Accuracy: 98.83%
59	Validation loss: 0.078272	Best loss: 0.041741	Accuracy: 98.71%
60	Validation loss: 0.091007	Best loss: 0.041741	Accuracy: 98.83%
61	Validation loss: 0.086030	Best loss: 0.041741	Accuracy: 98.87%
62	Validation loss: 0.080125	Best loss: 0.041741	Accuracy: 98.87%
63	Validation loss: 0.073479	Best loss: 0.041741	Accuracy: 98.94%
64	Validation loss: 0.084742	Best loss: 0.041741	Accuracy: 98.79%
65	Validation loss: 0.119411	Best loss: 0.041741	Accuracy: 98.36%
66	Validation loss: 0.111773	Best loss: 0.041741	Accuracy: 98.32%
67	Validation loss: 0.098525	Best loss: 0.041741	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  34.7s
[CV] n_neurons=90, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 3.978338	Best loss: 3.978338	Accuracy: 86.16%
1	Validation loss: 0.265931	Best loss: 0.265931	Accuracy: 97.26%
2	Validation loss: 0.180193	Best loss: 0.180193	Accuracy: 97.22%
3	Validation loss: 0.083981	Best loss: 0.083981	Accuracy: 98.12%
4	Validation loss: 0.100750	Best loss: 0.083981	Accuracy: 97.85%
5	Validation loss: 0.114881	Best loss: 0.083981	Accuracy: 97.73%
6	Validation loss: 0.082133	Best loss: 0.082133	Accuracy: 98.08%
7	Validation loss: 0.100432	Best loss: 0.082133	Accuracy: 97.89%
8	Validation loss: 0.083724	Best loss: 0.082133	Accuracy: 98.32%
9	Validation loss: 0.061167	Best loss: 0.061167	Accuracy: 98.63%
10	Validation loss: 0.123583	Best loss: 0.061167	Accuracy: 97.58%
11	Validation loss: 0.098896	Best loss: 0.061167	Accuracy: 97.97%
12	Validation loss: 0.061722	Best loss: 0.061167	Accuracy: 98.55%
13	Validation loss: 0.058387	Best loss: 0.058387	Accuracy: 98.71%
14	Validation loss: 0.078543	Best loss: 0.058387	Accuracy: 98.51%
15	Validation loss: 0.101230	Best loss: 0.058387	Accuracy: 97.81%
16	Validation loss: 0.052972	Best loss: 0.052972	Accuracy: 98.87%
17	Validation loss: 0.054512	Best loss: 0.052972	Accuracy: 98.79%
18	Validation loss: 0.062178	Best loss: 0.052972	Accuracy: 98.83%
19	Validation loss: 0.056760	Best loss: 0.052972	Accuracy: 98.94%
20	Validation loss: 0.065630	Best loss: 0.052972	Accuracy: 98.94%
21	Validation loss: 0.077693	Best loss: 0.052972	Accuracy: 98.83%
22	Validation loss: 0.093537	Best loss: 0.052972	Accuracy: 98.59%
23	Validation loss: 0.116835	Best loss: 0.052972	Accuracy: 98.32%
24	Validation loss: 0.101107	Best loss: 0.052972	Accuracy: 98.24%
25	Validation loss: 0.076626	Best loss: 0.052972	Accuracy: 98.51%
26	Validation loss: 0.057264	Best loss: 0.052972	Accuracy: 98.94%
27	Validation loss: 0.089223	Best loss: 0.052972	Accuracy: 98.55%
28	Validation loss: 0.085751	Best loss: 0.052972	Accuracy: 98.51%
29	Validation loss: 0.072792	Best loss: 0.052972	Accuracy: 98.67%
30	Validation loss: 0.061170	Best loss: 0.052972	Accuracy: 98.79%
31	Validation loss: 0.066812	Best loss: 0.052972	Accuracy: 98.79%
32	Validation loss: 0.071692	Best loss: 0.052972	Accuracy: 98.91%
33	Validation loss: 0.058448	Best loss: 0.052972	Accuracy: 98.98%
34	Validation loss: 0.092221	Best loss: 0.052972	Accuracy: 98.63%
35	Validation loss: 0.086051	Best loss: 0.052972	Accuracy: 98.67%
36	Validation loss: 0.075259	Best loss: 0.052972	Accuracy: 98.79%
37	Validation loss: 0.070491	Best loss: 0.052972	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  20.9s
[CV] n_neurons=90, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 1.529439	Best loss: 1.529439	Accuracy: 93.20%
1	Validation loss: 0.282826	Best loss: 0.282826	Accuracy: 97.26%
2	Validation loss: 0.111064	Best loss: 0.111064	Accuracy: 98.12%
3	Validation loss: 0.078270	Best loss: 0.078270	Accuracy: 98.40%
4	Validation loss: 0.093882	Best loss: 0.078270	Accuracy: 98.08%
5	Validation loss: 0.078679	Best loss: 0.078270	Accuracy: 98.24%
6	Validation loss: 0.074446	Best loss: 0.074446	Accuracy: 98.44%
7	Validation loss: 0.082382	Best loss: 0.074446	Accuracy: 98.32%
8	Validation loss: 0.095863	Best loss: 0.074446	Accuracy: 97.81%
9	Validation loss: 0.088476	Best loss: 0.074446	Accuracy: 98.67%
10	Validation loss: 0.069425	Best loss: 0.069425	Accuracy: 98.67%
11	Validation loss: 0.058241	Best loss: 0.058241	Accuracy: 98.98%
12	Validation loss: 0.067493	Best loss: 0.058241	Accuracy: 98.91%
13	Validation loss: 0.064761	Best loss: 0.058241	Accuracy: 98.59%
14	Validation loss: 0.054534	Best loss: 0.054534	Accuracy: 98.83%
15	Validation loss: 0.059020	Best loss: 0.054534	Accuracy: 98.71%
16	Validation loss: 0.054274	Best loss: 0.054274	Accuracy: 98.94%
17	Validation loss: 0.052861	Best loss: 0.052861	Accuracy: 99.02%
18	Validation loss: 0.053715	Best loss: 0.052861	Accuracy: 98.83%
19	Validation loss: 0.072851	Best loss: 0.052861	Accuracy: 98.63%
20	Validation loss: 0.061401	Best loss: 0.052861	Accuracy: 98.59%
21	Validation loss: 0.084828	Best loss: 0.052861	Accuracy: 98.55%
22	Validation loss: 0.111556	Best loss: 0.052861	Accuracy: 98.01%
23	Validation loss: 0.064398	Best loss: 0.052861	Accuracy: 98.79%
24	Validation loss: 0.043110	Best loss: 0.043110	Accuracy: 98.91%
25	Validation loss: 0.061933	Best loss: 0.043110	Accuracy: 98.79%
26	Validation loss: 0.061814	Best loss: 0.043110	Accuracy: 98.83%
27	Validation loss: 0.048223	Best loss: 0.043110	Accuracy: 99.14%
28	Validation loss: 0.048825	Best loss: 0.043110	Accuracy: 98.75%
29	Validation loss: 0.078679	Best loss: 0.043110	Accuracy: 98.71%
30	Validation loss: 0.075303	Best loss: 0.043110	Accuracy: 98.71%
31	Validation loss: 0.043754	Best loss: 0.043110	Accuracy: 99.18%
32	Validation loss: 0.071316	Best loss: 0.043110	Accuracy: 98.87%
33	Validation loss: 0.050971	Best loss: 0.043110	Accuracy: 99.06%
34	Validation loss: 0.078167	Best loss: 0.043110	Accuracy: 98.51%
35	Validation loss: 0.058390	Best loss: 0.043110	Accuracy: 98.91%
36	Validation loss: 0.052882	Best loss: 0.043110	Accuracy: 99.18%
37	Validation loss: 0.060057	Best loss: 0.043110	Accuracy: 98.87%
38	Validation loss: 0.068648	Best loss: 0.043110	Accuracy: 98.87%
39	Validation loss: 0.067266	Best loss: 0.043110	Accuracy: 98.83%
40	Validation loss: 0.157483	Best loss: 0.043110	Accuracy: 97.34%
41	Validation loss: 0.069620	Best loss: 0.043110	Accuracy: 98.75%
42	Validation loss: 0.054127	Best loss: 0.043110	Accuracy: 99.06%
43	Validation loss: 0.064419	Best loss: 0.043110	Accuracy: 99.02%
44	Validation loss: 0.070194	Best loss: 0.043110	Accuracy: 99.02%
45	Validation loss: 0.083295	Best loss: 0.043110	Accuracy: 98.63%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  24.0s
[CV] n_neurons=50, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.100595	Best loss: 0.100595	Accuracy: 97.26%
1	Validation loss: 0.095895	Best loss: 0.095895	Accuracy: 96.91%
2	Validation loss: 0.061779	Best loss: 0.061779	Accuracy: 98.12%
3	Validation loss: 0.070577	Best loss: 0.061779	Accuracy: 97.46%
4	Validation loss: 0.056120	Best loss: 0.056120	Accuracy: 97.97%
5	Validation loss: 0.071178	Best loss: 0.056120	Accuracy: 97.81%
6	Validation loss: 0.052559	Best loss: 0.052559	Accuracy: 98.44%
7	Validation loss: 0.044357	Best loss: 0.044357	Accuracy: 98.71%
8	Validation loss: 0.044527	Best loss: 0.044357	Accuracy: 98.75%
9	Validation loss: 0.055905	Best loss: 0.044357	Accuracy: 98.05%
10	Validation loss: 0.064669	Best loss: 0.044357	Accuracy: 98.24%
11	Validation loss: 0.043023	Best loss: 0.043023	Accuracy: 98.59%
12	Validation loss: 0.043289	Best loss: 0.043023	Accuracy: 98.79%
13	Validation loss: 0.039926	Best loss: 0.039926	Accuracy: 98.83%
14	Validation loss: 0.049284	Best loss: 0.039926	Accuracy: 98.63%
15	Validation loss: 0.043407	Best loss: 0.039926	Accuracy: 98.83%
16	Validation loss: 0.052809	Best loss: 0.039926	Accuracy: 98.51%
17	Validation loss: 0.039077	Best loss: 0.039077	Accuracy: 98.98%
18	Validation loss: 0.037640	Best loss: 0.037640	Accuracy: 99.02%
19	Validation loss: 0.039611	Best loss: 0.037640	Accuracy: 98.98%
20	Validation loss: 0.038917	Best loss: 0.037640	Accuracy: 98.71%
21	Validation loss: 0.040693	Best loss: 0.037640	Accuracy: 98.63%
22	Validation loss: 0.038136	Best loss: 0.037640	Accuracy: 99.10%
23	Validation loss: 0.039849	Best loss: 0.037640	Accuracy: 98.75%
24	Validation loss: 0.037226	Best loss: 0.037226	Accuracy: 98.91%
25	Validation loss: 0.038470	Best loss: 0.037226	Accuracy: 99.06%
26	Validation loss: 0.045993	Best loss: 0.037226	Accuracy: 98.87%
27	Validation loss: 0.044776	Best loss: 0.037226	Accuracy: 98.79%
28	Validation loss: 0.044658	Best loss: 0.037226	Accuracy: 98.75%
29	Validation loss: 0.039367	Best loss: 0.037226	Accuracy: 98.87%
30	Validation loss: 0.050046	Best loss: 0.037226	Accuracy: 98.75%
31	Validation loss: 0.042515	Best loss: 0.037226	Accuracy: 98.91%
32	Validation loss: 0.043270	Best loss: 0.037226	Accuracy: 98.98%
33	Validation loss: 0.041328	Best loss: 0.037226	Accuracy: 99.14%
34	Validation loss: 0.044210	Best loss: 0.037226	Accuracy: 98.91%
35	Validation loss: 0.042112	Best loss: 0.037226	Accuracy: 98.87%
36	Validation loss: 0.036303	Best loss: 0.036303	Accuracy: 98.83%
37	Validation loss: 0.042055	Best loss: 0.036303	Accuracy: 98.83%
38	Validation loss: 0.042645	Best loss: 0.036303	Accuracy: 98.67%
39	Validation loss: 0.044121	Best loss: 0.036303	Accuracy: 98.71%
40	Validation loss: 0.039258	Best loss: 0.036303	Accuracy: 99.06%
41	Validation loss: 0.043027	Best loss: 0.036303	Accuracy: 98.75%
42	Validation loss: 0.039724	Best loss: 0.036303	Accuracy: 98.83%
43	Validation loss: 0.042223	Best loss: 0.036303	Accuracy: 98.87%
44	Validation loss: 0.050481	Best loss: 0.036303	Accuracy: 98.40%
45	Validation loss: 0.041733	Best loss: 0.036303	Accuracy: 99.10%
46	Validation loss: 0.040309	Best loss: 0.036303	Accuracy: 98.98%
47	Validation loss: 0.065615	Best loss: 0.036303	Accuracy: 98.55%
48	Validation loss: 0.053182	Best loss: 0.036303	Accuracy: 98.67%
49	Validation loss: 0.046389	Best loss: 0.036303	Accuracy: 98.91%
50	Validation loss: 0.035410	Best loss: 0.035410	Accuracy: 99.02%
51	Validation loss: 0.041622	Best loss: 0.035410	Accuracy: 99.14%
52	Validation loss: 0.044744	Best loss: 0.035410	Accuracy: 98.94%
53	Validation loss: 0.047848	Best loss: 0.035410	Accuracy: 98.67%
54	Validation loss: 0.041385	Best loss: 0.035410	Accuracy: 98.87%
55	Validation loss: 0.042619	Best loss: 0.035410	Accuracy: 98.91%
56	Validation loss: 0.048150	Best loss: 0.035410	Accuracy: 98.67%
57	Validation loss: 0.044195	Best loss: 0.035410	Accuracy: 98.79%
58	Validation loss: 0.047240	Best loss: 0.035410	Accuracy: 98.83%
59	Validation loss: 0.053924	Best loss: 0.035410	Accuracy: 98.83%
60	Validation loss: 0.046515	Best loss: 0.035410	Accuracy: 98.83%
61	Validation loss: 0.059205	Best loss: 0.035410	Accuracy: 98.51%
62	Validation loss: 0.048847	Best loss: 0.035410	Accuracy: 98.94%
63	Validation loss: 0.050100	Best loss: 0.035410	Accuracy: 98.40%
64	Validation loss: 0.052520	Best loss: 0.035410	Accuracy: 98.75%
65	Validation loss: 0.055122	Best loss: 0.035410	Accuracy: 98.63%
66	Validation loss: 0.050645	Best loss: 0.035410	Accuracy: 98.79%
67	Validation loss: 0.049379	Best loss: 0.035410	Accuracy: 98.98%
68	Validation loss: 0.048641	Best loss: 0.035410	Accuracy: 98.67%
69	Validation loss: 0.048497	Best loss: 0.035410	Accuracy: 98.94%
70	Validation loss: 0.037817	Best loss: 0.035410	Accuracy: 98.94%
71	Validation loss: 0.048165	Best loss: 0.035410	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total= 6.0min
[CV] n_neurons=50, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.121906	Best loss: 0.121906	Accuracy: 96.29%
1	Validation loss: 0.092225	Best loss: 0.092225	Accuracy: 97.15%
2	Validation loss: 0.087873	Best loss: 0.087873	Accuracy: 97.11%
3	Validation loss: 0.060295	Best loss: 0.060295	Accuracy: 98.16%
4	Validation loss: 0.057565	Best loss: 0.057565	Accuracy: 98.05%
5	Validation loss: 0.060639	Best loss: 0.057565	Accuracy: 97.81%
6	Validation loss: 0.048507	Best loss: 0.048507	Accuracy: 98.44%
7	Validation loss: 0.044518	Best loss: 0.044518	Accuracy: 98.24%
8	Validation loss: 0.048295	Best loss: 0.044518	Accuracy: 98.48%
9	Validation loss: 0.049610	Best loss: 0.044518	Accuracy: 98.24%
10	Validation loss: 0.048838	Best loss: 0.044518	Accuracy: 98.44%
11	Validation loss: 0.039460	Best loss: 0.039460	Accuracy: 98.75%
12	Validation loss: 0.042967	Best loss: 0.039460	Accuracy: 98.59%
13	Validation loss: 0.056272	Best loss: 0.039460	Accuracy: 98.48%
14	Validation loss: 0.056239	Best loss: 0.039460	Accuracy: 98.12%
15	Validation loss: 0.045734	Best loss: 0.039460	Accuracy: 98.79%
16	Validation loss: 0.059110	Best loss: 0.039460	Accuracy: 98.20%
17	Validation loss: 0.047091	Best loss: 0.039460	Accuracy: 98.67%
18	Validation loss: 0.041967	Best loss: 0.039460	Accuracy: 98.67%
19	Validation loss: 0.049736	Best loss: 0.039460	Accuracy: 98.44%
20	Validation loss: 0.037346	Best loss: 0.037346	Accuracy: 98.79%
21	Validation loss: 0.042449	Best loss: 0.037346	Accuracy: 98.87%
22	Validation loss: 0.045480	Best loss: 0.037346	Accuracy: 98.51%
23	Validation loss: 0.041657	Best loss: 0.037346	Accuracy: 98.94%
24	Validation loss: 0.033494	Best loss: 0.033494	Accuracy: 98.83%
25	Validation loss: 0.035208	Best loss: 0.033494	Accuracy: 98.98%
26	Validation loss: 0.039657	Best loss: 0.033494	Accuracy: 98.98%
27	Validation loss: 0.039805	Best loss: 0.033494	Accuracy: 98.55%
28	Validation loss: 0.038200	Best loss: 0.033494	Accuracy: 98.98%
29	Validation loss: 0.045005	Best loss: 0.033494	Accuracy: 98.79%
30	Validation loss: 0.053015	Best loss: 0.033494	Accuracy: 98.71%
31	Validation loss: 0.038865	Best loss: 0.033494	Accuracy: 98.91%
32	Validation loss: 0.041077	Best loss: 0.033494	Accuracy: 98.87%
33	Validation loss: 0.042964	Best loss: 0.033494	Accuracy: 98.91%
34	Validation loss: 0.052072	Best loss: 0.033494	Accuracy: 98.71%
35	Validation loss: 0.037688	Best loss: 0.033494	Accuracy: 98.83%
36	Validation loss: 0.065556	Best loss: 0.033494	Accuracy: 98.44%
37	Validation loss: 0.047555	Best loss: 0.033494	Accuracy: 98.51%
38	Validation loss: 0.043908	Best loss: 0.033494	Accuracy: 98.94%
39	Validation loss: 0.045998	Best loss: 0.033494	Accuracy: 98.87%
40	Validation loss: 0.052119	Best loss: 0.033494	Accuracy: 98.44%
41	Validation loss: 0.038229	Best loss: 0.033494	Accuracy: 98.87%
42	Validation loss: 0.041530	Best loss: 0.033494	Accuracy: 98.98%
43	Validation loss: 0.049109	Best loss: 0.033494	Accuracy: 98.94%
44	Validation loss: 0.045037	Best loss: 0.033494	Accuracy: 98.79%
45	Validation loss: 0.045640	Best loss: 0.033494	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total= 4.0min
[CV] n_neurons=50, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.100013	Best loss: 0.100013	Accuracy: 97.19%
1	Validation loss: 0.070638	Best loss: 0.070638	Accuracy: 98.01%
2	Validation loss: 0.064852	Best loss: 0.064852	Accuracy: 98.01%
3	Validation loss: 0.058966	Best loss: 0.058966	Accuracy: 98.28%
4	Validation loss: 0.047295	Best loss: 0.047295	Accuracy: 98.32%
5	Validation loss: 0.057662	Best loss: 0.047295	Accuracy: 98.01%
6	Validation loss: 0.088988	Best loss: 0.047295	Accuracy: 97.03%
7	Validation loss: 0.041926	Best loss: 0.041926	Accuracy: 98.67%
8	Validation loss: 0.037131	Best loss: 0.037131	Accuracy: 98.87%
9	Validation loss: 0.038428	Best loss: 0.037131	Accuracy: 98.87%
10	Validation loss: 0.041628	Best loss: 0.037131	Accuracy: 98.83%
11	Validation loss: 0.039087	Best loss: 0.037131	Accuracy: 98.79%
12	Validation loss: 0.044222	Best loss: 0.037131	Accuracy: 98.79%
13	Validation loss: 0.039484	Best loss: 0.037131	Accuracy: 98.79%
14	Validation loss: 0.039265	Best loss: 0.037131	Accuracy: 98.91%
15	Validation loss: 0.039257	Best loss: 0.037131	Accuracy: 98.79%
16	Validation loss: 0.029849	Best loss: 0.029849	Accuracy: 99.14%
17	Validation loss: 0.036373	Best loss: 0.029849	Accuracy: 99.14%
18	Validation loss: 0.032524	Best loss: 0.029849	Accuracy: 99.02%
19	Validation loss: 0.032400	Best loss: 0.029849	Accuracy: 99.10%
20	Validation loss: 0.044645	Best loss: 0.029849	Accuracy: 98.75%
21	Validation loss: 0.029993	Best loss: 0.029849	Accuracy: 99.02%
22	Validation loss: 0.038522	Best loss: 0.029849	Accuracy: 98.87%
23	Validation loss: 0.033407	Best loss: 0.029849	Accuracy: 98.98%
24	Validation loss: 0.041830	Best loss: 0.029849	Accuracy: 98.94%
25	Validation loss: 0.043537	Best loss: 0.029849	Accuracy: 98.71%
26	Validation loss: 0.038705	Best loss: 0.029849	Accuracy: 98.79%
27	Validation loss: 0.034490	Best loss: 0.029849	Accuracy: 98.98%
28	Validation loss: 0.038400	Best loss: 0.029849	Accuracy: 98.94%
29	Validation loss: 0.056032	Best loss: 0.029849	Accuracy: 98.28%
30	Validation loss: 0.038661	Best loss: 0.029849	Accuracy: 98.91%
31	Validation loss: 0.034591	Best loss: 0.029849	Accuracy: 98.98%
32	Validation loss: 0.031363	Best loss: 0.029849	Accuracy: 98.91%
33	Validation loss: 0.041452	Best loss: 0.029849	Accuracy: 98.63%
34	Validation loss: 0.029311	Best loss: 0.029311	Accuracy: 99.10%
35	Validation loss: 0.040532	Best loss: 0.029311	Accuracy: 98.83%
36	Validation loss: 0.040827	Best loss: 0.029311	Accuracy: 98.87%
37	Validation loss: 0.036921	Best loss: 0.029311	Accuracy: 98.98%
38	Validation loss: 0.041434	Best loss: 0.029311	Accuracy: 98.91%
39	Validation loss: 0.036792	Best loss: 0.029311	Accuracy: 98.91%
40	Validation loss: 0.039983	Best loss: 0.029311	Accuracy: 98.79%
41	Validation loss: 0.050822	Best loss: 0.029311	Accuracy: 98.75%
42	Validation loss: 0.041773	Best loss: 0.029311	Accuracy: 98.91%
43	Validation loss: 0.033060	Best loss: 0.029311	Accuracy: 99.06%
44	Validation loss: 0.034004	Best loss: 0.029311	Accuracy: 99.06%
45	Validation loss: 0.037782	Best loss: 0.029311	Accuracy: 98.91%
46	Validation loss: 0.031645	Best loss: 0.029311	Accuracy: 99.06%
47	Validation loss: 0.031378	Best loss: 0.029311	Accuracy: 99.10%
48	Validation loss: 0.047951	Best loss: 0.029311	Accuracy: 98.51%
49	Validation loss: 0.064227	Best loss: 0.029311	Accuracy: 98.44%
50	Validation loss: 0.040477	Best loss: 0.029311	Accuracy: 99.06%
51	Validation loss: 0.049310	Best loss: 0.029311	Accuracy: 98.71%
52	Validation loss: 0.035838	Best loss: 0.029311	Accuracy: 98.98%
53	Validation loss: 0.044307	Best loss: 0.029311	Accuracy: 98.94%
54	Validation loss: 0.046143	Best loss: 0.029311	Accuracy: 98.94%
55	Validation loss: 0.040823	Best loss: 0.029311	Accuracy: 98.98%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function elu at 0x1243639d8&gt;, total= 4.5min
[CV] n_neurons=120, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.123825	Best loss: 0.123825	Accuracy: 96.33%
1	Validation loss: 0.084884	Best loss: 0.084884	Accuracy: 97.73%
2	Validation loss: 0.067555	Best loss: 0.067555	Accuracy: 97.69%
3	Validation loss: 0.105620	Best loss: 0.067555	Accuracy: 96.52%
4	Validation loss: 0.054726	Best loss: 0.054726	Accuracy: 98.24%
5	Validation loss: 0.090107	Best loss: 0.054726	Accuracy: 97.19%
6	Validation loss: 0.062008	Best loss: 0.054726	Accuracy: 98.16%
7	Validation loss: 0.059261	Best loss: 0.054726	Accuracy: 98.16%
8	Validation loss: 0.054531	Best loss: 0.054531	Accuracy: 98.48%
9	Validation loss: 0.085960	Best loss: 0.054531	Accuracy: 97.15%
10	Validation loss: 0.069660	Best loss: 0.054531	Accuracy: 98.05%
11	Validation loss: 0.046646	Best loss: 0.046646	Accuracy: 98.79%
12	Validation loss: 0.056042	Best loss: 0.046646	Accuracy: 98.44%
13	Validation loss: 0.049468	Best loss: 0.046646	Accuracy: 98.48%
14	Validation loss: 0.060885	Best loss: 0.046646	Accuracy: 98.59%
15	Validation loss: 0.038463	Best loss: 0.038463	Accuracy: 98.79%
16	Validation loss: 0.044009	Best loss: 0.038463	Accuracy: 98.79%
17	Validation loss: 0.056901	Best loss: 0.038463	Accuracy: 98.32%
18	Validation loss: 0.048803	Best loss: 0.038463	Accuracy: 98.51%
19	Validation loss: 0.069247	Best loss: 0.038463	Accuracy: 98.01%
20	Validation loss: 0.066339	Best loss: 0.038463	Accuracy: 98.51%
21	Validation loss: 0.044909	Best loss: 0.038463	Accuracy: 98.63%
22	Validation loss: 0.041964	Best loss: 0.038463	Accuracy: 98.87%
23	Validation loss: 0.043043	Best loss: 0.038463	Accuracy: 98.91%
24	Validation loss: 0.045733	Best loss: 0.038463	Accuracy: 98.79%
25	Validation loss: 0.060295	Best loss: 0.038463	Accuracy: 98.63%
26	Validation loss: 0.037586	Best loss: 0.037586	Accuracy: 99.06%
27	Validation loss: 0.070585	Best loss: 0.037586	Accuracy: 98.59%
28	Validation loss: 0.052415	Best loss: 0.037586	Accuracy: 98.79%
29	Validation loss: 0.063530	Best loss: 0.037586	Accuracy: 98.55%
30	Validation loss: 0.042331	Best loss: 0.037586	Accuracy: 98.79%
31	Validation loss: 0.051761	Best loss: 0.037586	Accuracy: 98.87%
32	Validation loss: 0.047452	Best loss: 0.037586	Accuracy: 98.91%
33	Validation loss: 0.073161	Best loss: 0.037586	Accuracy: 98.12%
34	Validation loss: 0.034479	Best loss: 0.034479	Accuracy: 99.18%
35	Validation loss: 0.044199	Best loss: 0.034479	Accuracy: 98.94%
36	Validation loss: 0.046020	Best loss: 0.034479	Accuracy: 99.02%
37	Validation loss: 0.044112	Best loss: 0.034479	Accuracy: 98.67%
38	Validation loss: 0.049762	Best loss: 0.034479	Accuracy: 98.83%
39	Validation loss: 0.039458	Best loss: 0.034479	Accuracy: 98.91%
40	Validation loss: 0.045460	Best loss: 0.034479	Accuracy: 98.83%
41	Validation loss: 0.055182	Best loss: 0.034479	Accuracy: 98.71%
42	Validation loss: 0.074848	Best loss: 0.034479	Accuracy: 98.36%
43	Validation loss: 0.060645	Best loss: 0.034479	Accuracy: 98.79%
44	Validation loss: 0.058045	Best loss: 0.034479	Accuracy: 98.87%
45	Validation loss: 0.051305	Best loss: 0.034479	Accuracy: 98.67%
46	Validation loss: 0.048828	Best loss: 0.034479	Accuracy: 98.94%
47	Validation loss: 0.086640	Best loss: 0.034479	Accuracy: 97.93%
48	Validation loss: 0.049706	Best loss: 0.034479	Accuracy: 98.71%
49	Validation loss: 0.054777	Best loss: 0.034479	Accuracy: 98.71%
50	Validation loss: 0.047047	Best loss: 0.034479	Accuracy: 98.94%
51	Validation loss: 0.056074	Best loss: 0.034479	Accuracy: 98.83%
52	Validation loss: 0.066507	Best loss: 0.034479	Accuracy: 98.94%
53	Validation loss: 0.055797	Best loss: 0.034479	Accuracy: 98.83%
54	Validation loss: 0.052879	Best loss: 0.034479	Accuracy: 98.94%
55	Validation loss: 0.059412	Best loss: 0.034479	Accuracy: 98.91%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 6.0min
[CV] n_neurons=120, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.136878	Best loss: 0.136878	Accuracy: 95.66%
1	Validation loss: 0.093404	Best loss: 0.093404	Accuracy: 97.30%
2	Validation loss: 0.076188	Best loss: 0.076188	Accuracy: 97.46%
3	Validation loss: 0.064224	Best loss: 0.064224	Accuracy: 97.93%
4	Validation loss: 0.079487	Best loss: 0.064224	Accuracy: 97.85%
5	Validation loss: 0.055363	Best loss: 0.055363	Accuracy: 98.51%
6	Validation loss: 0.059573	Best loss: 0.055363	Accuracy: 98.28%
7	Validation loss: 0.058908	Best loss: 0.055363	Accuracy: 98.16%
8	Validation loss: 0.072359	Best loss: 0.055363	Accuracy: 98.24%
9	Validation loss: 0.068875	Best loss: 0.055363	Accuracy: 98.08%
10	Validation loss: 0.106549	Best loss: 0.055363	Accuracy: 97.22%
11	Validation loss: 0.073292	Best loss: 0.055363	Accuracy: 98.40%
12	Validation loss: 0.054298	Best loss: 0.054298	Accuracy: 98.75%
13	Validation loss: 0.099789	Best loss: 0.054298	Accuracy: 97.42%
14	Validation loss: 0.057112	Best loss: 0.054298	Accuracy: 98.48%
15	Validation loss: 0.048464	Best loss: 0.048464	Accuracy: 98.87%
16	Validation loss: 0.068171	Best loss: 0.048464	Accuracy: 98.44%
17	Validation loss: 0.054086	Best loss: 0.048464	Accuracy: 98.83%
18	Validation loss: 0.047413	Best loss: 0.047413	Accuracy: 98.63%
19	Validation loss: 0.066903	Best loss: 0.047413	Accuracy: 98.83%
20	Validation loss: 0.038194	Best loss: 0.038194	Accuracy: 98.83%
21	Validation loss: 0.045286	Best loss: 0.038194	Accuracy: 99.14%
22	Validation loss: 0.042729	Best loss: 0.038194	Accuracy: 98.75%
23	Validation loss: 0.044298	Best loss: 0.038194	Accuracy: 98.79%
24	Validation loss: 0.050357	Best loss: 0.038194	Accuracy: 98.55%
25	Validation loss: 0.061629	Best loss: 0.038194	Accuracy: 98.79%
26	Validation loss: 0.048766	Best loss: 0.038194	Accuracy: 98.67%
27	Validation loss: 0.088362	Best loss: 0.038194	Accuracy: 97.73%
28	Validation loss: 0.051985	Best loss: 0.038194	Accuracy: 98.79%
29	Validation loss: 0.105548	Best loss: 0.038194	Accuracy: 97.58%
30	Validation loss: 0.071587	Best loss: 0.038194	Accuracy: 98.51%
31	Validation loss: 0.117037	Best loss: 0.038194	Accuracy: 98.05%
32	Validation loss: 0.132990	Best loss: 0.038194	Accuracy: 96.91%
33	Validation loss: 0.065094	Best loss: 0.038194	Accuracy: 98.59%
34	Validation loss: 0.051338	Best loss: 0.038194	Accuracy: 98.79%
35	Validation loss: 0.053349	Best loss: 0.038194	Accuracy: 98.63%
36	Validation loss: 0.068112	Best loss: 0.038194	Accuracy: 98.91%
37	Validation loss: 0.079548	Best loss: 0.038194	Accuracy: 98.44%
38	Validation loss: 0.044267	Best loss: 0.038194	Accuracy: 98.71%
39	Validation loss: 0.065318	Best loss: 0.038194	Accuracy: 98.79%
40	Validation loss: 0.063799	Best loss: 0.038194	Accuracy: 98.51%
41	Validation loss: 0.058696	Best loss: 0.038194	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 4.5min
[CV] n_neurons=120, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.108625	Best loss: 0.108625	Accuracy: 96.99%
1	Validation loss: 0.087525	Best loss: 0.087525	Accuracy: 97.85%
2	Validation loss: 0.070137	Best loss: 0.070137	Accuracy: 97.77%
3	Validation loss: 0.061422	Best loss: 0.061422	Accuracy: 98.28%
4	Validation loss: 0.067661	Best loss: 0.061422	Accuracy: 97.93%
5	Validation loss: 0.047263	Best loss: 0.047263	Accuracy: 98.28%
6	Validation loss: 0.049160	Best loss: 0.047263	Accuracy: 98.28%
7	Validation loss: 0.047559	Best loss: 0.047263	Accuracy: 98.40%
8	Validation loss: 0.059745	Best loss: 0.047263	Accuracy: 98.28%
9	Validation loss: 0.102115	Best loss: 0.047263	Accuracy: 96.91%
10	Validation loss: 0.050192	Best loss: 0.047263	Accuracy: 98.48%
11	Validation loss: 0.037212	Best loss: 0.037212	Accuracy: 98.91%
12	Validation loss: 0.046663	Best loss: 0.037212	Accuracy: 98.51%
13	Validation loss: 0.066726	Best loss: 0.037212	Accuracy: 98.48%
14	Validation loss: 0.046079	Best loss: 0.037212	Accuracy: 98.55%
15	Validation loss: 0.058561	Best loss: 0.037212	Accuracy: 98.51%
16	Validation loss: 0.039028	Best loss: 0.037212	Accuracy: 98.98%
17	Validation loss: 0.056588	Best loss: 0.037212	Accuracy: 98.32%
18	Validation loss: 0.054187	Best loss: 0.037212	Accuracy: 98.79%
19	Validation loss: 0.056614	Best loss: 0.037212	Accuracy: 98.67%
20	Validation loss: 0.105252	Best loss: 0.037212	Accuracy: 97.97%
21	Validation loss: 0.036948	Best loss: 0.036948	Accuracy: 98.67%
22	Validation loss: 0.038973	Best loss: 0.036948	Accuracy: 99.02%
23	Validation loss: 0.059925	Best loss: 0.036948	Accuracy: 98.32%
24	Validation loss: 0.033077	Best loss: 0.033077	Accuracy: 99.10%
25	Validation loss: 0.052479	Best loss: 0.033077	Accuracy: 98.55%
26	Validation loss: 0.034609	Best loss: 0.033077	Accuracy: 98.87%
27	Validation loss: 0.040041	Best loss: 0.033077	Accuracy: 98.79%
28	Validation loss: 0.032149	Best loss: 0.032149	Accuracy: 99.06%
29	Validation loss: 0.041336	Best loss: 0.032149	Accuracy: 98.83%
30	Validation loss: 0.050972	Best loss: 0.032149	Accuracy: 98.79%
31	Validation loss: 0.035925	Best loss: 0.032149	Accuracy: 99.06%
32	Validation loss: 0.025965	Best loss: 0.025965	Accuracy: 99.22%
33	Validation loss: 0.030233	Best loss: 0.025965	Accuracy: 98.98%
34	Validation loss: 0.037292	Best loss: 0.025965	Accuracy: 99.18%
35	Validation loss: 0.055943	Best loss: 0.025965	Accuracy: 98.79%
36	Validation loss: 0.041362	Best loss: 0.025965	Accuracy: 98.83%
37	Validation loss: 0.045605	Best loss: 0.025965	Accuracy: 98.59%
38	Validation loss: 0.039593	Best loss: 0.025965	Accuracy: 99.02%
39	Validation loss: 0.620307	Best loss: 0.025965	Accuracy: 90.38%
40	Validation loss: 0.044739	Best loss: 0.025965	Accuracy: 98.94%
41	Validation loss: 0.047004	Best loss: 0.025965	Accuracy: 98.79%
42	Validation loss: 0.053682	Best loss: 0.025965	Accuracy: 98.75%
43	Validation loss: 0.051632	Best loss: 0.025965	Accuracy: 98.79%
44	Validation loss: 0.052040	Best loss: 0.025965	Accuracy: 99.02%
45	Validation loss: 0.051449	Best loss: 0.025965	Accuracy: 98.75%
46	Validation loss: 0.075772	Best loss: 0.025965	Accuracy: 98.44%
47	Validation loss: 0.031296	Best loss: 0.025965	Accuracy: 99.22%
48	Validation loss: 0.040220	Best loss: 0.025965	Accuracy: 99.10%
49	Validation loss: 0.103711	Best loss: 0.025965	Accuracy: 98.12%
50	Validation loss: 0.057651	Best loss: 0.025965	Accuracy: 98.44%
51	Validation loss: 0.070801	Best loss: 0.025965	Accuracy: 98.55%
52	Validation loss: 0.046563	Best loss: 0.025965	Accuracy: 98.87%
53	Validation loss: 0.042997	Best loss: 0.025965	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 5.8min
[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.091994	Best loss: 0.091994	Accuracy: 97.85%
1	Validation loss: 0.077401	Best loss: 0.077401	Accuracy: 98.05%
2	Validation loss: 0.066100	Best loss: 0.066100	Accuracy: 98.16%
3	Validation loss: 0.059657	Best loss: 0.059657	Accuracy: 98.44%
4	Validation loss: 0.039586	Best loss: 0.039586	Accuracy: 98.75%
5	Validation loss: 0.059896	Best loss: 0.039586	Accuracy: 98.48%
6	Validation loss: 0.032596	Best loss: 0.032596	Accuracy: 98.91%
7	Validation loss: 0.043546	Best loss: 0.032596	Accuracy: 98.79%
8	Validation loss: 0.051633	Best loss: 0.032596	Accuracy: 98.67%
9	Validation loss: 0.076750	Best loss: 0.032596	Accuracy: 98.20%
10	Validation loss: 0.037433	Best loss: 0.032596	Accuracy: 98.83%
11	Validation loss: 0.031594	Best loss: 0.031594	Accuracy: 99.22%
12	Validation loss: 0.048795	Best loss: 0.031594	Accuracy: 98.94%
13	Validation loss: 0.047674	Best loss: 0.031594	Accuracy: 98.79%
14	Validation loss: 0.055937	Best loss: 0.031594	Accuracy: 98.59%
15	Validation loss: 0.030819	Best loss: 0.030819	Accuracy: 99.10%
16	Validation loss: 0.039774	Best loss: 0.030819	Accuracy: 99.02%
17	Validation loss: 0.047590	Best loss: 0.030819	Accuracy: 98.94%
18	Validation loss: 0.043514	Best loss: 0.030819	Accuracy: 98.98%
19	Validation loss: 0.051101	Best loss: 0.030819	Accuracy: 98.75%
20	Validation loss: 0.046454	Best loss: 0.030819	Accuracy: 98.98%
21	Validation loss: 0.039496	Best loss: 0.030819	Accuracy: 99.18%
22	Validation loss: 0.030317	Best loss: 0.030317	Accuracy: 99.14%
23	Validation loss: 0.049209	Best loss: 0.030317	Accuracy: 99.06%
24	Validation loss: 0.049387	Best loss: 0.030317	Accuracy: 99.10%
25	Validation loss: 0.031946	Best loss: 0.030317	Accuracy: 99.18%
26	Validation loss: 0.033996	Best loss: 0.030317	Accuracy: 99.37%
27	Validation loss: 0.035605	Best loss: 0.030317	Accuracy: 99.26%
28	Validation loss: 0.048123	Best loss: 0.030317	Accuracy: 99.22%
29	Validation loss: 0.046970	Best loss: 0.030317	Accuracy: 99.02%
30	Validation loss: 0.036453	Best loss: 0.030317	Accuracy: 99.22%
31	Validation loss: 0.043329	Best loss: 0.030317	Accuracy: 99.02%
32	Validation loss: 0.045326	Best loss: 0.030317	Accuracy: 99.10%
33	Validation loss: 0.037111	Best loss: 0.030317	Accuracy: 98.91%
34	Validation loss: 0.038421	Best loss: 0.030317	Accuracy: 98.94%
35	Validation loss: 0.049628	Best loss: 0.030317	Accuracy: 99.02%
36	Validation loss: 0.036847	Best loss: 0.030317	Accuracy: 99.30%
37	Validation loss: 0.065398	Best loss: 0.030317	Accuracy: 98.75%
38	Validation loss: 0.033793	Best loss: 0.030317	Accuracy: 99.22%
39	Validation loss: 0.048902	Best loss: 0.030317	Accuracy: 99.18%
40	Validation loss: 0.052773	Best loss: 0.030317	Accuracy: 98.98%
41	Validation loss: 0.048201	Best loss: 0.030317	Accuracy: 99.18%
42	Validation loss: 0.053327	Best loss: 0.030317	Accuracy: 98.91%
43	Validation loss: 0.052139	Best loss: 0.030317	Accuracy: 99.26%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 1.4min
[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.124103	Best loss: 0.124103	Accuracy: 96.99%
1	Validation loss: 0.057861	Best loss: 0.057861	Accuracy: 98.48%
2	Validation loss: 0.055079	Best loss: 0.055079	Accuracy: 98.59%
3	Validation loss: 0.051125	Best loss: 0.051125	Accuracy: 98.48%
4	Validation loss: 0.063491	Best loss: 0.051125	Accuracy: 98.44%
5	Validation loss: 0.047547	Best loss: 0.047547	Accuracy: 98.55%
6	Validation loss: 0.037371	Best loss: 0.037371	Accuracy: 98.83%
7	Validation loss: 0.041101	Best loss: 0.037371	Accuracy: 98.87%
8	Validation loss: 0.034764	Best loss: 0.034764	Accuracy: 99.18%
9	Validation loss: 0.046881	Best loss: 0.034764	Accuracy: 98.83%
10	Validation loss: 0.053321	Best loss: 0.034764	Accuracy: 98.87%
11	Validation loss: 0.056025	Best loss: 0.034764	Accuracy: 98.87%
12	Validation loss: 0.047676	Best loss: 0.034764	Accuracy: 98.79%
13	Validation loss: 0.047984	Best loss: 0.034764	Accuracy: 98.75%
14	Validation loss: 0.059355	Best loss: 0.034764	Accuracy: 98.75%
15	Validation loss: 0.047879	Best loss: 0.034764	Accuracy: 98.91%
16	Validation loss: 0.058693	Best loss: 0.034764	Accuracy: 98.94%
17	Validation loss: 0.056088	Best loss: 0.034764	Accuracy: 99.02%
18	Validation loss: 0.058010	Best loss: 0.034764	Accuracy: 98.94%
19	Validation loss: 0.047840	Best loss: 0.034764	Accuracy: 98.98%
20	Validation loss: 0.061129	Best loss: 0.034764	Accuracy: 98.94%
21	Validation loss: 0.059679	Best loss: 0.034764	Accuracy: 98.44%
22	Validation loss: 0.055015	Best loss: 0.034764	Accuracy: 98.94%
23	Validation loss: 0.038731	Best loss: 0.034764	Accuracy: 99.06%
24	Validation loss: 0.051488	Best loss: 0.034764	Accuracy: 98.63%
25	Validation loss: 0.043849	Best loss: 0.034764	Accuracy: 99.06%
26	Validation loss: 0.041040	Best loss: 0.034764	Accuracy: 99.02%
27	Validation loss: 0.041548	Best loss: 0.034764	Accuracy: 99.06%
28	Validation loss: 0.061545	Best loss: 0.034764	Accuracy: 98.98%
29	Validation loss: 0.055505	Best loss: 0.034764	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  55.7s
[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.100815	Best loss: 0.100815	Accuracy: 97.77%
1	Validation loss: 0.067951	Best loss: 0.067951	Accuracy: 98.05%
2	Validation loss: 0.062436	Best loss: 0.062436	Accuracy: 98.08%
3	Validation loss: 0.047428	Best loss: 0.047428	Accuracy: 98.67%
4	Validation loss: 0.059736	Best loss: 0.047428	Accuracy: 98.51%
5	Validation loss: 0.045159	Best loss: 0.045159	Accuracy: 98.67%
6	Validation loss: 0.045980	Best loss: 0.045159	Accuracy: 98.48%
7	Validation loss: 0.062375	Best loss: 0.045159	Accuracy: 98.55%
8	Validation loss: 0.044834	Best loss: 0.044834	Accuracy: 98.75%
9	Validation loss: 0.043244	Best loss: 0.043244	Accuracy: 98.79%
10	Validation loss: 0.043621	Best loss: 0.043244	Accuracy: 98.91%
11	Validation loss: 0.043831	Best loss: 0.043244	Accuracy: 98.87%
12	Validation loss: 0.039420	Best loss: 0.039420	Accuracy: 99.02%
13	Validation loss: 0.039213	Best loss: 0.039213	Accuracy: 99.02%
14	Validation loss: 0.049590	Best loss: 0.039213	Accuracy: 98.59%
15	Validation loss: 0.060579	Best loss: 0.039213	Accuracy: 98.79%
16	Validation loss: 0.063496	Best loss: 0.039213	Accuracy: 98.55%
17	Validation loss: 0.050539	Best loss: 0.039213	Accuracy: 98.94%
18	Validation loss: 0.046187	Best loss: 0.039213	Accuracy: 98.79%
19	Validation loss: 0.045801	Best loss: 0.039213	Accuracy: 98.79%
20	Validation loss: 0.047014	Best loss: 0.039213	Accuracy: 98.91%
21	Validation loss: 0.033935	Best loss: 0.033935	Accuracy: 99.18%
22	Validation loss: 0.047300	Best loss: 0.033935	Accuracy: 99.18%
23	Validation loss: 0.043987	Best loss: 0.033935	Accuracy: 99.10%
24	Validation loss: 0.049854	Best loss: 0.033935	Accuracy: 98.75%
25	Validation loss: 0.037232	Best loss: 0.033935	Accuracy: 99.06%
26	Validation loss: 0.047521	Best loss: 0.033935	Accuracy: 99.14%
27	Validation loss: 0.039534	Best loss: 0.033935	Accuracy: 99.06%
28	Validation loss: 0.043549	Best loss: 0.033935	Accuracy: 99.22%
29	Validation loss: 0.044265	Best loss: 0.033935	Accuracy: 99.14%
30	Validation loss: 0.051416	Best loss: 0.033935	Accuracy: 99.06%
31	Validation loss: 0.055587	Best loss: 0.033935	Accuracy: 99.02%
32	Validation loss: 0.035721	Best loss: 0.033935	Accuracy: 99.14%
33	Validation loss: 0.063536	Best loss: 0.033935	Accuracy: 98.75%
34	Validation loss: 0.055287	Best loss: 0.033935	Accuracy: 98.94%
35	Validation loss: 0.043586	Best loss: 0.033935	Accuracy: 99.26%
36	Validation loss: 0.037531	Best loss: 0.033935	Accuracy: 99.18%
37	Validation loss: 0.050345	Best loss: 0.033935	Accuracy: 98.87%
38	Validation loss: 0.029880	Best loss: 0.029880	Accuracy: 99.34%
39	Validation loss: 0.036530	Best loss: 0.029880	Accuracy: 99.22%
40	Validation loss: 0.041946	Best loss: 0.029880	Accuracy: 98.94%
41	Validation loss: 0.044278	Best loss: 0.029880	Accuracy: 99.26%
42	Validation loss: 0.045010	Best loss: 0.029880	Accuracy: 99.30%
43	Validation loss: 0.047724	Best loss: 0.029880	Accuracy: 99.26%
44	Validation loss: 0.040583	Best loss: 0.029880	Accuracy: 99.22%
45	Validation loss: 0.041361	Best loss: 0.029880	Accuracy: 99.18%
46	Validation loss: 0.054680	Best loss: 0.029880	Accuracy: 99.02%
47	Validation loss: 0.049275	Best loss: 0.029880	Accuracy: 99.06%
48	Validation loss: 0.050439	Best loss: 0.029880	Accuracy: 99.10%
49	Validation loss: 0.051532	Best loss: 0.029880	Accuracy: 99.18%
50	Validation loss: 0.042001	Best loss: 0.029880	Accuracy: 99.14%
51	Validation loss: 0.051966	Best loss: 0.029880	Accuracy: 99.02%
52	Validation loss: 0.056011	Best loss: 0.029880	Accuracy: 99.22%
53	Validation loss: 0.065224	Best loss: 0.029880	Accuracy: 98.98%
54	Validation loss: 0.037825	Best loss: 0.029880	Accuracy: 99.14%
55	Validation loss: 0.050700	Best loss: 0.029880	Accuracy: 99.26%
56	Validation loss: 0.055467	Best loss: 0.029880	Accuracy: 98.83%
57	Validation loss: 0.047299	Best loss: 0.029880	Accuracy: 99.22%
58	Validation loss: 0.048068	Best loss: 0.029880	Accuracy: 98.98%
59	Validation loss: 0.038924	Best loss: 0.029880	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 1.9min
[CV] n_neurons=140, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1010.262695	Best loss: 1010.262695	Accuracy: 67.28%
1	Validation loss: 144.453400	Best loss: 144.453400	Accuracy: 75.61%
2	Validation loss: 100.987091	Best loss: 100.987091	Accuracy: 62.71%
3	Validation loss: 14.717021	Best loss: 14.717021	Accuracy: 79.79%
4	Validation loss: 11.684780	Best loss: 11.684780	Accuracy: 79.44%
5	Validation loss: 11.460147	Best loss: 11.460147	Accuracy: 81.47%
6	Validation loss: 3.946227	Best loss: 3.946227	Accuracy: 91.32%
7	Validation loss: 1.968313	Best loss: 1.968313	Accuracy: 94.84%
8	Validation loss: 0.843098	Best loss: 0.843098	Accuracy: 97.50%
9	Validation loss: 0.870373	Best loss: 0.843098	Accuracy: 97.42%
10	Validation loss: 0.690358	Best loss: 0.690358	Accuracy: 97.65%
11	Validation loss: 0.880898	Best loss: 0.690358	Accuracy: 96.33%
12	Validation loss: 0.560758	Best loss: 0.560758	Accuracy: 97.77%
13	Validation loss: 0.484655	Best loss: 0.484655	Accuracy: 98.20%
14	Validation loss: 0.481961	Best loss: 0.481961	Accuracy: 98.16%
15	Validation loss: 0.426978	Best loss: 0.426978	Accuracy: 98.40%
16	Validation loss: 0.387208	Best loss: 0.387208	Accuracy: 98.36%
17	Validation loss: 0.332560	Best loss: 0.332560	Accuracy: 98.63%
18	Validation loss: 0.255948	Best loss: 0.255948	Accuracy: 98.59%
19	Validation loss: 0.380991	Best loss: 0.255948	Accuracy: 98.08%
20	Validation loss: 0.256541	Best loss: 0.255948	Accuracy: 98.48%
21	Validation loss: 0.265262	Best loss: 0.255948	Accuracy: 98.75%
22	Validation loss: 0.344277	Best loss: 0.255948	Accuracy: 98.44%
23	Validation loss: 0.306479	Best loss: 0.255948	Accuracy: 98.87%
24	Validation loss: 0.247901	Best loss: 0.247901	Accuracy: 98.55%
25	Validation loss: 0.261921	Best loss: 0.247901	Accuracy: 98.51%
26	Validation loss: 0.296687	Best loss: 0.247901	Accuracy: 98.40%
27	Validation loss: 0.346350	Best loss: 0.247901	Accuracy: 97.77%
28	Validation loss: 0.197023	Best loss: 0.197023	Accuracy: 98.79%
29	Validation loss: 0.171636	Best loss: 0.171636	Accuracy: 98.75%
30	Validation loss: 0.156869	Best loss: 0.156869	Accuracy: 99.10%
31	Validation loss: 0.160130	Best loss: 0.156869	Accuracy: 98.91%
32	Validation loss: 0.223076	Best loss: 0.156869	Accuracy: 98.71%
33	Validation loss: 0.179636	Best loss: 0.156869	Accuracy: 98.98%
34	Validation loss: 0.165218	Best loss: 0.156869	Accuracy: 98.91%
35	Validation loss: 0.255987	Best loss: 0.156869	Accuracy: 98.44%
36	Validation loss: 0.269476	Best loss: 0.156869	Accuracy: 98.05%
37	Validation loss: 0.180571	Best loss: 0.156869	Accuracy: 98.36%
38	Validation loss: 0.215131	Best loss: 0.156869	Accuracy: 98.51%
39	Validation loss: 0.165682	Best loss: 0.156869	Accuracy: 98.94%
40	Validation loss: 0.168180	Best loss: 0.156869	Accuracy: 98.98%
41	Validation loss: 0.143138	Best loss: 0.143138	Accuracy: 98.98%
42	Validation loss: 0.128105	Best loss: 0.128105	Accuracy: 99.14%
43	Validation loss: 0.137288	Best loss: 0.128105	Accuracy: 98.94%
44	Validation loss: 0.133305	Best loss: 0.128105	Accuracy: 99.06%
45	Validation loss: 0.135677	Best loss: 0.128105	Accuracy: 98.87%
46	Validation loss: 0.178018	Best loss: 0.128105	Accuracy: 98.83%
47	Validation loss: 0.280256	Best loss: 0.128105	Accuracy: 97.93%
48	Validation loss: 0.331224	Best loss: 0.128105	Accuracy: 98.05%
49	Validation loss: 0.302551	Best loss: 0.128105	Accuracy: 98.28%
50	Validation loss: 0.207310	Best loss: 0.128105	Accuracy: 98.55%
51	Validation loss: 0.260842	Best loss: 0.128105	Accuracy: 98.59%
52	Validation loss: 0.229423	Best loss: 0.128105	Accuracy: 98.71%
53	Validation loss: 0.197241	Best loss: 0.128105	Accuracy: 98.83%
54	Validation loss: 0.176000	Best loss: 0.128105	Accuracy: 98.87%
55	Validation loss: 0.169123	Best loss: 0.128105	Accuracy: 99.10%
56	Validation loss: 0.158717	Best loss: 0.128105	Accuracy: 98.94%
57	Validation loss: 0.194570	Best loss: 0.128105	Accuracy: 98.71%
58	Validation loss: 0.185369	Best loss: 0.128105	Accuracy: 98.91%
59	Validation loss: 0.140681	Best loss: 0.128105	Accuracy: 98.87%
60	Validation loss: 0.198094	Best loss: 0.128105	Accuracy: 98.71%
61	Validation loss: 0.220810	Best loss: 0.128105	Accuracy: 98.71%
62	Validation loss: 0.270742	Best loss: 0.128105	Accuracy: 98.05%
63	Validation loss: 0.272724	Best loss: 0.128105	Accuracy: 98.28%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total=  40.6s
[CV] n_neurons=140, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 235.111755	Best loss: 235.111755	Accuracy: 85.93%
1	Validation loss: 57.942741	Best loss: 57.942741	Accuracy: 85.57%
2	Validation loss: 28.906513	Best loss: 28.906513	Accuracy: 82.96%
3	Validation loss: 6.398335	Best loss: 6.398335	Accuracy: 90.19%
4	Validation loss: 16.094091	Best loss: 6.398335	Accuracy: 75.84%
5	Validation loss: 12.174726	Best loss: 6.398335	Accuracy: 81.78%
6	Validation loss: 4.967026	Best loss: 4.967026	Accuracy: 88.55%
7	Validation loss: 1.996646	Best loss: 1.996646	Accuracy: 93.86%
8	Validation loss: 1.519768	Best loss: 1.519768	Accuracy: 95.00%
9	Validation loss: 1.121775	Best loss: 1.121775	Accuracy: 96.36%
10	Validation loss: 1.488512	Best loss: 1.121775	Accuracy: 95.19%
11	Validation loss: 0.928413	Best loss: 0.928413	Accuracy: 96.95%
12	Validation loss: 0.586088	Best loss: 0.586088	Accuracy: 98.01%
13	Validation loss: 0.905647	Best loss: 0.586088	Accuracy: 96.79%
14	Validation loss: 0.851747	Best loss: 0.586088	Accuracy: 96.91%
15	Validation loss: 0.794948	Best loss: 0.586088	Accuracy: 96.95%
16	Validation loss: 0.792012	Best loss: 0.586088	Accuracy: 96.79%
17	Validation loss: 0.827557	Best loss: 0.586088	Accuracy: 96.64%
18	Validation loss: 0.632945	Best loss: 0.586088	Accuracy: 97.30%
19	Validation loss: 0.580913	Best loss: 0.580913	Accuracy: 97.54%
20	Validation loss: 0.431651	Best loss: 0.431651	Accuracy: 98.16%
21	Validation loss: 1.453736	Best loss: 0.431651	Accuracy: 94.37%
22	Validation loss: 0.376645	Best loss: 0.376645	Accuracy: 98.48%
23	Validation loss: 0.453764	Best loss: 0.376645	Accuracy: 97.93%
24	Validation loss: 0.484460	Best loss: 0.376645	Accuracy: 97.89%
25	Validation loss: 0.547588	Best loss: 0.376645	Accuracy: 97.65%
26	Validation loss: 0.325945	Best loss: 0.325945	Accuracy: 98.48%
27	Validation loss: 0.226946	Best loss: 0.226946	Accuracy: 98.83%
28	Validation loss: 0.423299	Best loss: 0.226946	Accuracy: 98.16%
29	Validation loss: 0.373125	Best loss: 0.226946	Accuracy: 98.05%
30	Validation loss: 0.340415	Best loss: 0.226946	Accuracy: 98.20%
31	Validation loss: 0.359679	Best loss: 0.226946	Accuracy: 98.24%
32	Validation loss: 0.361349	Best loss: 0.226946	Accuracy: 98.01%
33	Validation loss: 0.289950	Best loss: 0.226946	Accuracy: 98.51%
34	Validation loss: 0.310665	Best loss: 0.226946	Accuracy: 98.28%
35	Validation loss: 0.283097	Best loss: 0.226946	Accuracy: 98.40%
36	Validation loss: 0.321661	Best loss: 0.226946	Accuracy: 98.51%
37	Validation loss: 0.279656	Best loss: 0.226946	Accuracy: 98.16%
38	Validation loss: 0.280870	Best loss: 0.226946	Accuracy: 98.51%
39	Validation loss: 0.226781	Best loss: 0.226781	Accuracy: 98.63%
40	Validation loss: 0.209528	Best loss: 0.209528	Accuracy: 98.63%
41	Validation loss: 0.312415	Best loss: 0.209528	Accuracy: 98.05%
42	Validation loss: 0.289184	Best loss: 0.209528	Accuracy: 98.40%
43	Validation loss: 0.360916	Best loss: 0.209528	Accuracy: 98.44%
44	Validation loss: 0.312380	Best loss: 0.209528	Accuracy: 98.40%
45	Validation loss: 0.378211	Best loss: 0.209528	Accuracy: 98.48%
46	Validation loss: 0.350550	Best loss: 0.209528	Accuracy: 98.36%
47	Validation loss: 0.314208	Best loss: 0.209528	Accuracy: 98.55%
48	Validation loss: 0.229020	Best loss: 0.209528	Accuracy: 98.71%
49	Validation loss: 0.250724	Best loss: 0.209528	Accuracy: 98.83%
50	Validation loss: 0.217479	Best loss: 0.209528	Accuracy: 98.87%
51	Validation loss: 0.280926	Best loss: 0.209528	Accuracy: 98.44%
52	Validation loss: 0.266293	Best loss: 0.209528	Accuracy: 98.20%
53	Validation loss: 0.189214	Best loss: 0.189214	Accuracy: 98.75%
54	Validation loss: 0.224082	Best loss: 0.189214	Accuracy: 98.28%
55	Validation loss: 0.229383	Best loss: 0.189214	Accuracy: 98.79%
56	Validation loss: 0.346378	Best loss: 0.189214	Accuracy: 98.01%
57	Validation loss: 0.295625	Best loss: 0.189214	Accuracy: 98.24%
58	Validation loss: 0.245355	Best loss: 0.189214	Accuracy: 98.59%
59	Validation loss: 0.265176	Best loss: 0.189214	Accuracy: 98.55%
60	Validation loss: 0.246912	Best loss: 0.189214	Accuracy: 98.83%
61	Validation loss: 0.271165	Best loss: 0.189214	Accuracy: 98.32%
62	Validation loss: 0.229907	Best loss: 0.189214	Accuracy: 98.48%
63	Validation loss: 0.301847	Best loss: 0.189214	Accuracy: 98.51%
64	Validation loss: 0.201143	Best loss: 0.189214	Accuracy: 98.75%
65	Validation loss: 0.214698	Best loss: 0.189214	Accuracy: 98.55%
66	Validation loss: 0.153761	Best loss: 0.153761	Accuracy: 98.91%
67	Validation loss: 0.288648	Best loss: 0.153761	Accuracy: 98.55%
68	Validation loss: 0.254417	Best loss: 0.153761	Accuracy: 98.59%
69	Validation loss: 0.209526	Best loss: 0.153761	Accuracy: 98.63%
70	Validation loss: 0.235270	Best loss: 0.153761	Accuracy: 98.67%
71	Validation loss: 0.153060	Best loss: 0.153060	Accuracy: 99.02%
72	Validation loss: 0.201096	Best loss: 0.153060	Accuracy: 98.79%
73	Validation loss: 0.208837	Best loss: 0.153060	Accuracy: 98.79%
74	Validation loss: 0.177941	Best loss: 0.153060	Accuracy: 98.83%
75	Validation loss: 0.179212	Best loss: 0.153060	Accuracy: 98.91%
76	Validation loss: 0.168732	Best loss: 0.153060	Accuracy: 98.94%
77	Validation loss: 0.158038	Best loss: 0.153060	Accuracy: 99.02%
78	Validation loss: 0.153049	Best loss: 0.153049	Accuracy: 99.06%
79	Validation loss: 0.148290	Best loss: 0.148290	Accuracy: 99.02%
80	Validation loss: 0.144956	Best loss: 0.144956	Accuracy: 99.02%
81	Validation loss: 0.159383	Best loss: 0.144956	Accuracy: 98.91%
82	Validation loss: 0.127870	Best loss: 0.127870	Accuracy: 98.98%
83	Validation loss: 0.167651	Best loss: 0.127870	Accuracy: 98.71%
84	Validation loss: 0.278859	Best loss: 0.127870	Accuracy: 97.58%
85	Validation loss: 0.262323	Best loss: 0.127870	Accuracy: 98.08%
86	Validation loss: 0.226733	Best loss: 0.127870	Accuracy: 98.51%
87	Validation loss: 0.297905	Best loss: 0.127870	Accuracy: 97.89%
88	Validation loss: 0.387580	Best loss: 0.127870	Accuracy: 98.05%
89	Validation loss: 0.382480	Best loss: 0.127870	Accuracy: 97.85%
90	Validation loss: 0.315523	Best loss: 0.127870	Accuracy: 98.36%
91	Validation loss: 0.238573	Best loss: 0.127870	Accuracy: 98.75%
92	Validation loss: 0.483640	Best loss: 0.127870	Accuracy: 98.12%
93	Validation loss: 0.242890	Best loss: 0.127870	Accuracy: 98.67%
94	Validation loss: 0.285536	Best loss: 0.127870	Accuracy: 98.51%
95	Validation loss: 0.216588	Best loss: 0.127870	Accuracy: 98.94%
96	Validation loss: 0.214024	Best loss: 0.127870	Accuracy: 98.87%
97	Validation loss: 0.239130	Best loss: 0.127870	Accuracy: 98.55%
98	Validation loss: 0.228078	Best loss: 0.127870	Accuracy: 98.83%
99	Validation loss: 0.211395	Best loss: 0.127870	Accuracy: 98.87%
100	Validation loss: 0.228820	Best loss: 0.127870	Accuracy: 98.67%
101	Validation loss: 0.168795	Best loss: 0.127870	Accuracy: 98.91%
102	Validation loss: 0.162763	Best loss: 0.127870	Accuracy: 99.02%
103	Validation loss: 0.151288	Best loss: 0.127870	Accuracy: 98.98%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.2min
[CV] n_neurons=140, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 456.005676	Best loss: 456.005676	Accuracy: 73.34%
1	Validation loss: 115.984009	Best loss: 115.984009	Accuracy: 72.63%
2	Validation loss: 25.850719	Best loss: 25.850719	Accuracy: 77.72%
3	Validation loss: 12.628258	Best loss: 12.628258	Accuracy: 80.88%
4	Validation loss: 4.413403	Best loss: 4.413403	Accuracy: 89.99%
5	Validation loss: 4.715781	Best loss: 4.413403	Accuracy: 87.33%
6	Validation loss: 2.190699	Best loss: 2.190699	Accuracy: 92.53%
7	Validation loss: 1.236642	Best loss: 1.236642	Accuracy: 95.54%
8	Validation loss: 1.262013	Best loss: 1.236642	Accuracy: 95.54%
9	Validation loss: 0.964106	Best loss: 0.964106	Accuracy: 96.64%
10	Validation loss: 1.148598	Best loss: 0.964106	Accuracy: 95.23%
11	Validation loss: 0.666253	Best loss: 0.666253	Accuracy: 97.58%
12	Validation loss: 0.640893	Best loss: 0.640893	Accuracy: 97.11%
13	Validation loss: 0.735222	Best loss: 0.640893	Accuracy: 97.15%
14	Validation loss: 0.399933	Best loss: 0.399933	Accuracy: 97.93%
15	Validation loss: 0.450907	Best loss: 0.399933	Accuracy: 97.97%
16	Validation loss: 0.490338	Best loss: 0.399933	Accuracy: 97.65%
17	Validation loss: 0.421327	Best loss: 0.399933	Accuracy: 98.01%
18	Validation loss: 0.303890	Best loss: 0.303890	Accuracy: 98.16%
19	Validation loss: 0.448353	Best loss: 0.303890	Accuracy: 96.99%
20	Validation loss: 0.268190	Best loss: 0.268190	Accuracy: 98.51%
21	Validation loss: 0.417404	Best loss: 0.268190	Accuracy: 97.38%
22	Validation loss: 0.552773	Best loss: 0.268190	Accuracy: 97.50%
23	Validation loss: 0.349593	Best loss: 0.268190	Accuracy: 98.16%
24	Validation loss: 0.267916	Best loss: 0.267916	Accuracy: 98.71%
25	Validation loss: 0.284662	Best loss: 0.267916	Accuracy: 98.67%
26	Validation loss: 0.230945	Best loss: 0.230945	Accuracy: 98.63%
27	Validation loss: 0.259463	Best loss: 0.230945	Accuracy: 98.48%
28	Validation loss: 0.320797	Best loss: 0.230945	Accuracy: 98.28%
29	Validation loss: 0.233044	Best loss: 0.230945	Accuracy: 98.63%
30	Validation loss: 0.234554	Best loss: 0.230945	Accuracy: 98.79%
31	Validation loss: 0.247916	Best loss: 0.230945	Accuracy: 98.67%
32	Validation loss: 0.194864	Best loss: 0.194864	Accuracy: 98.44%
33	Validation loss: 0.245021	Best loss: 0.194864	Accuracy: 98.20%
34	Validation loss: 0.218333	Best loss: 0.194864	Accuracy: 98.55%
35	Validation loss: 0.198878	Best loss: 0.194864	Accuracy: 98.48%
36	Validation loss: 0.179360	Best loss: 0.179360	Accuracy: 98.63%
37	Validation loss: 0.180437	Best loss: 0.179360	Accuracy: 98.67%
38	Validation loss: 0.198978	Best loss: 0.179360	Accuracy: 98.44%
39	Validation loss: 0.193968	Best loss: 0.179360	Accuracy: 98.59%
40	Validation loss: 0.178002	Best loss: 0.178002	Accuracy: 98.59%
41	Validation loss: 0.180426	Best loss: 0.178002	Accuracy: 98.79%
42	Validation loss: 0.212111	Best loss: 0.178002	Accuracy: 98.67%
43	Validation loss: 0.123629	Best loss: 0.123629	Accuracy: 99.02%
44	Validation loss: 0.393297	Best loss: 0.123629	Accuracy: 98.32%
45	Validation loss: 0.258936	Best loss: 0.123629	Accuracy: 98.20%
46	Validation loss: 0.265760	Best loss: 0.123629	Accuracy: 97.89%
47	Validation loss: 0.332206	Best loss: 0.123629	Accuracy: 97.97%
48	Validation loss: 0.187891	Best loss: 0.123629	Accuracy: 98.59%
49	Validation loss: 0.203717	Best loss: 0.123629	Accuracy: 98.55%
50	Validation loss: 0.245932	Best loss: 0.123629	Accuracy: 98.67%
51	Validation loss: 0.193937	Best loss: 0.123629	Accuracy: 98.87%
52	Validation loss: 0.146436	Best loss: 0.123629	Accuracy: 98.87%
53	Validation loss: 0.115593	Best loss: 0.115593	Accuracy: 99.06%
54	Validation loss: 0.148484	Best loss: 0.115593	Accuracy: 98.67%
55	Validation loss: 0.151718	Best loss: 0.115593	Accuracy: 98.59%
56	Validation loss: 0.130414	Best loss: 0.115593	Accuracy: 98.87%
57	Validation loss: 0.129625	Best loss: 0.115593	Accuracy: 99.06%
58	Validation loss: 0.365078	Best loss: 0.115593	Accuracy: 96.72%
59	Validation loss: 0.226114	Best loss: 0.115593	Accuracy: 98.44%
60	Validation loss: 0.155043	Best loss: 0.115593	Accuracy: 98.83%
61	Validation loss: 0.158950	Best loss: 0.115593	Accuracy: 98.91%
62	Validation loss: 0.171274	Best loss: 0.115593	Accuracy: 98.79%
63	Validation loss: 0.155317	Best loss: 0.115593	Accuracy: 98.87%
64	Validation loss: 0.179365	Best loss: 0.115593	Accuracy: 98.83%
65	Validation loss: 0.167025	Best loss: 0.115593	Accuracy: 98.87%
66	Validation loss: 0.140986	Best loss: 0.115593	Accuracy: 98.98%
67	Validation loss: 0.218438	Best loss: 0.115593	Accuracy: 98.83%
68	Validation loss: 0.153991	Best loss: 0.115593	Accuracy: 98.94%
69	Validation loss: 0.356531	Best loss: 0.115593	Accuracy: 98.05%
70	Validation loss: 0.328445	Best loss: 0.115593	Accuracy: 98.28%
71	Validation loss: 0.228211	Best loss: 0.115593	Accuracy: 98.20%
72	Validation loss: 0.250758	Best loss: 0.115593	Accuracy: 98.51%
73	Validation loss: 0.275111	Best loss: 0.115593	Accuracy: 98.40%
74	Validation loss: 0.250182	Best loss: 0.115593	Accuracy: 98.48%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total=  51.3s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.078876	Best loss: 0.078876	Accuracy: 98.08%
1	Validation loss: 0.056712	Best loss: 0.056712	Accuracy: 98.20%
2	Validation loss: 0.046939	Best loss: 0.046939	Accuracy: 98.63%
3	Validation loss: 0.046216	Best loss: 0.046216	Accuracy: 98.83%
4	Validation loss: 0.041195	Best loss: 0.041195	Accuracy: 98.67%
5	Validation loss: 0.073479	Best loss: 0.041195	Accuracy: 98.28%
6	Validation loss: 0.046468	Best loss: 0.041195	Accuracy: 98.75%
7	Validation loss: 0.049180	Best loss: 0.041195	Accuracy: 98.75%
8	Validation loss: 0.043404	Best loss: 0.041195	Accuracy: 98.71%
9	Validation loss: 0.045552	Best loss: 0.041195	Accuracy: 98.94%
10	Validation loss: 0.051279	Best loss: 0.041195	Accuracy: 98.71%
11	Validation loss: 0.035188	Best loss: 0.035188	Accuracy: 99.18%
12	Validation loss: 0.034845	Best loss: 0.034845	Accuracy: 98.91%
13	Validation loss: 0.032996	Best loss: 0.032996	Accuracy: 99.30%
14	Validation loss: 0.055423	Best loss: 0.032996	Accuracy: 98.87%
15	Validation loss: 0.047672	Best loss: 0.032996	Accuracy: 98.79%
16	Validation loss: 0.056303	Best loss: 0.032996	Accuracy: 98.71%
17	Validation loss: 0.033355	Best loss: 0.032996	Accuracy: 98.98%
18	Validation loss: 0.045322	Best loss: 0.032996	Accuracy: 98.83%
19	Validation loss: 0.038134	Best loss: 0.032996	Accuracy: 99.10%
20	Validation loss: 0.033528	Best loss: 0.032996	Accuracy: 99.18%
21	Validation loss: 0.037493	Best loss: 0.032996	Accuracy: 99.26%
22	Validation loss: 0.043256	Best loss: 0.032996	Accuracy: 99.10%
23	Validation loss: 0.045506	Best loss: 0.032996	Accuracy: 98.98%
24	Validation loss: 0.043555	Best loss: 0.032996	Accuracy: 99.06%
25	Validation loss: 0.042822	Best loss: 0.032996	Accuracy: 99.02%
26	Validation loss: 0.047716	Best loss: 0.032996	Accuracy: 99.10%
27	Validation loss: 0.046718	Best loss: 0.032996	Accuracy: 98.94%
28	Validation loss: 0.047197	Best loss: 0.032996	Accuracy: 98.91%
29	Validation loss: 0.060583	Best loss: 0.032996	Accuracy: 99.02%
30	Validation loss: 0.052026	Best loss: 0.032996	Accuracy: 99.06%
31	Validation loss: 0.047766	Best loss: 0.032996	Accuracy: 99.18%
32	Validation loss: 0.057404	Best loss: 0.032996	Accuracy: 99.14%
33	Validation loss: 0.043628	Best loss: 0.032996	Accuracy: 99.06%
34	Validation loss: 0.071960	Best loss: 0.032996	Accuracy: 98.55%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  25.6s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.082168	Best loss: 0.082168	Accuracy: 97.34%
1	Validation loss: 0.051098	Best loss: 0.051098	Accuracy: 98.28%
2	Validation loss: 0.051679	Best loss: 0.051098	Accuracy: 98.48%
3	Validation loss: 0.045230	Best loss: 0.045230	Accuracy: 98.44%
4	Validation loss: 0.047593	Best loss: 0.045230	Accuracy: 98.36%
5	Validation loss: 0.059022	Best loss: 0.045230	Accuracy: 98.67%
6	Validation loss: 0.043610	Best loss: 0.043610	Accuracy: 98.98%
7	Validation loss: 0.051487	Best loss: 0.043610	Accuracy: 98.91%
8	Validation loss: 0.045693	Best loss: 0.043610	Accuracy: 98.67%
9	Validation loss: 0.048815	Best loss: 0.043610	Accuracy: 98.67%
10	Validation loss: 0.056813	Best loss: 0.043610	Accuracy: 98.67%
11	Validation loss: 0.050574	Best loss: 0.043610	Accuracy: 98.91%
12	Validation loss: 0.043188	Best loss: 0.043188	Accuracy: 98.94%
13	Validation loss: 0.047806	Best loss: 0.043188	Accuracy: 98.91%
14	Validation loss: 0.047105	Best loss: 0.043188	Accuracy: 98.79%
15	Validation loss: 0.058158	Best loss: 0.043188	Accuracy: 98.63%
16	Validation loss: 0.054051	Best loss: 0.043188	Accuracy: 98.63%
17	Validation loss: 0.054006	Best loss: 0.043188	Accuracy: 98.59%
18	Validation loss: 0.048703	Best loss: 0.043188	Accuracy: 99.10%
19	Validation loss: 0.060426	Best loss: 0.043188	Accuracy: 98.63%
20	Validation loss: 0.046003	Best loss: 0.043188	Accuracy: 99.06%
21	Validation loss: 0.050408	Best loss: 0.043188	Accuracy: 98.83%
22	Validation loss: 0.046403	Best loss: 0.043188	Accuracy: 99.02%
23	Validation loss: 0.044458	Best loss: 0.043188	Accuracy: 98.91%
24	Validation loss: 0.043897	Best loss: 0.043188	Accuracy: 99.10%
25	Validation loss: 0.044602	Best loss: 0.043188	Accuracy: 98.98%
26	Validation loss: 0.043229	Best loss: 0.043188	Accuracy: 99.18%
27	Validation loss: 0.044652	Best loss: 0.043188	Accuracy: 99.18%
28	Validation loss: 0.046893	Best loss: 0.043188	Accuracy: 99.14%
29	Validation loss: 0.046755	Best loss: 0.043188	Accuracy: 99.18%
30	Validation loss: 0.047210	Best loss: 0.043188	Accuracy: 99.14%
31	Validation loss: 0.049847	Best loss: 0.043188	Accuracy: 99.10%
32	Validation loss: 0.049563	Best loss: 0.043188	Accuracy: 99.10%
33	Validation loss: 0.049600	Best loss: 0.043188	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  24.0s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.064962	Best loss: 0.064962	Accuracy: 98.24%
1	Validation loss: 0.060293	Best loss: 0.060293	Accuracy: 98.16%
2	Validation loss: 0.046117	Best loss: 0.046117	Accuracy: 98.59%
3	Validation loss: 0.059410	Best loss: 0.046117	Accuracy: 98.32%
4	Validation loss: 0.042186	Best loss: 0.042186	Accuracy: 98.94%
5	Validation loss: 0.055272	Best loss: 0.042186	Accuracy: 98.55%
6	Validation loss: 0.050348	Best loss: 0.042186	Accuracy: 98.71%
7	Validation loss: 0.041787	Best loss: 0.041787	Accuracy: 98.91%
8	Validation loss: 0.050610	Best loss: 0.041787	Accuracy: 98.63%
9	Validation loss: 0.044881	Best loss: 0.041787	Accuracy: 98.83%
10	Validation loss: 0.066271	Best loss: 0.041787	Accuracy: 98.51%
11	Validation loss: 0.052963	Best loss: 0.041787	Accuracy: 98.75%
12	Validation loss: 0.050259	Best loss: 0.041787	Accuracy: 98.75%
13	Validation loss: 0.041554	Best loss: 0.041554	Accuracy: 98.94%
14	Validation loss: 0.043167	Best loss: 0.041554	Accuracy: 99.02%
15	Validation loss: 0.055819	Best loss: 0.041554	Accuracy: 98.94%
16	Validation loss: 0.048977	Best loss: 0.041554	Accuracy: 98.75%
17	Validation loss: 0.048346	Best loss: 0.041554	Accuracy: 98.91%
18	Validation loss: 0.052003	Best loss: 0.041554	Accuracy: 98.71%
19	Validation loss: 0.044987	Best loss: 0.041554	Accuracy: 98.94%
20	Validation loss: 0.049548	Best loss: 0.041554	Accuracy: 98.83%
21	Validation loss: 0.060401	Best loss: 0.041554	Accuracy: 98.79%
22	Validation loss: 0.055342	Best loss: 0.041554	Accuracy: 98.79%
23	Validation loss: 0.045361	Best loss: 0.041554	Accuracy: 99.10%
24	Validation loss: 0.046271	Best loss: 0.041554	Accuracy: 99.10%
25	Validation loss: 0.065587	Best loss: 0.041554	Accuracy: 98.51%
26	Validation loss: 0.052211	Best loss: 0.041554	Accuracy: 98.59%
27	Validation loss: 0.050506	Best loss: 0.041554	Accuracy: 98.98%
28	Validation loss: 0.041275	Best loss: 0.041275	Accuracy: 98.79%
29	Validation loss: 0.050416	Best loss: 0.041275	Accuracy: 98.79%
30	Validation loss: 0.034063	Best loss: 0.034063	Accuracy: 99.22%
31	Validation loss: 0.040858	Best loss: 0.034063	Accuracy: 99.02%
32	Validation loss: 0.054221	Best loss: 0.034063	Accuracy: 98.94%
33	Validation loss: 0.047288	Best loss: 0.034063	Accuracy: 98.98%
34	Validation loss: 0.052692	Best loss: 0.034063	Accuracy: 98.98%
35	Validation loss: 0.057165	Best loss: 0.034063	Accuracy: 98.94%
36	Validation loss: 0.051008	Best loss: 0.034063	Accuracy: 98.87%
37	Validation loss: 0.048851	Best loss: 0.034063	Accuracy: 98.91%
38	Validation loss: 0.050859	Best loss: 0.034063	Accuracy: 98.94%
39	Validation loss: 0.044318	Best loss: 0.034063	Accuracy: 99.02%
40	Validation loss: 0.049807	Best loss: 0.034063	Accuracy: 99.02%
41	Validation loss: 0.045093	Best loss: 0.034063	Accuracy: 98.98%
42	Validation loss: 0.039914	Best loss: 0.034063	Accuracy: 99.02%
43	Validation loss: 0.053983	Best loss: 0.034063	Accuracy: 98.79%
44	Validation loss: 0.046860	Best loss: 0.034063	Accuracy: 99.02%
45	Validation loss: 0.062489	Best loss: 0.034063	Accuracy: 98.79%
46	Validation loss: 0.054294	Best loss: 0.034063	Accuracy: 98.79%
47	Validation loss: 0.051460	Best loss: 0.034063	Accuracy: 98.91%
48	Validation loss: 0.044896	Best loss: 0.034063	Accuracy: 99.14%
49	Validation loss: 0.042952	Best loss: 0.034063	Accuracy: 99.10%
50	Validation loss: 0.054324	Best loss: 0.034063	Accuracy: 98.87%
51	Validation loss: 0.058379	Best loss: 0.034063	Accuracy: 99.14%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  34.9s
[CV] n_neurons=100, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.123537	Best loss: 0.123537	Accuracy: 96.64%
1	Validation loss: 0.073752	Best loss: 0.073752	Accuracy: 97.93%
2	Validation loss: 0.072326	Best loss: 0.072326	Accuracy: 97.77%
3	Validation loss: 0.074413	Best loss: 0.072326	Accuracy: 97.65%
4	Validation loss: 0.074122	Best loss: 0.072326	Accuracy: 98.12%
5	Validation loss: 0.060716	Best loss: 0.060716	Accuracy: 98.59%
6	Validation loss: 0.061736	Best loss: 0.060716	Accuracy: 98.48%
7	Validation loss: 0.076347	Best loss: 0.060716	Accuracy: 98.36%
8	Validation loss: 0.049794	Best loss: 0.049794	Accuracy: 98.91%
9	Validation loss: 0.693048	Best loss: 0.049794	Accuracy: 81.78%
10	Validation loss: 0.093161	Best loss: 0.049794	Accuracy: 98.51%
11	Validation loss: 0.203613	Best loss: 0.049794	Accuracy: 95.50%
12	Validation loss: 0.085830	Best loss: 0.049794	Accuracy: 98.63%
13	Validation loss: 0.062732	Best loss: 0.049794	Accuracy: 98.79%
14	Validation loss: 0.076846	Best loss: 0.049794	Accuracy: 98.59%
15	Validation loss: 0.076075	Best loss: 0.049794	Accuracy: 98.94%
16	Validation loss: 0.039839	Best loss: 0.039839	Accuracy: 99.02%
17	Validation loss: 0.060033	Best loss: 0.039839	Accuracy: 98.59%
18	Validation loss: 0.296747	Best loss: 0.039839	Accuracy: 97.03%
19	Validation loss: 0.065417	Best loss: 0.039839	Accuracy: 98.51%
20	Validation loss: 0.051826	Best loss: 0.039839	Accuracy: 98.98%
21	Validation loss: 0.069617	Best loss: 0.039839	Accuracy: 98.48%
22	Validation loss: 0.062578	Best loss: 0.039839	Accuracy: 98.79%
23	Validation loss: 0.063582	Best loss: 0.039839	Accuracy: 98.59%
24	Validation loss: 0.056986	Best loss: 0.039839	Accuracy: 98.91%
25	Validation loss: 0.060577	Best loss: 0.039839	Accuracy: 98.87%
26	Validation loss: 0.060750	Best loss: 0.039839	Accuracy: 98.98%
27	Validation loss: 0.085646	Best loss: 0.039839	Accuracy: 98.67%
28	Validation loss: 0.059484	Best loss: 0.039839	Accuracy: 98.94%
29	Validation loss: 0.064931	Best loss: 0.039839	Accuracy: 98.63%
30	Validation loss: 0.098738	Best loss: 0.039839	Accuracy: 98.48%
31	Validation loss: 0.110535	Best loss: 0.039839	Accuracy: 98.67%
32	Validation loss: 0.072899	Best loss: 0.039839	Accuracy: 98.91%
33	Validation loss: 0.095180	Best loss: 0.039839	Accuracy: 98.75%
34	Validation loss: 0.064761	Best loss: 0.039839	Accuracy: 98.94%
35	Validation loss: 0.082305	Best loss: 0.039839	Accuracy: 98.75%
36	Validation loss: 0.454887	Best loss: 0.039839	Accuracy: 96.64%
37	Validation loss: 0.111336	Best loss: 0.039839	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.0min
[CV] n_neurons=100, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.104364	Best loss: 0.104364	Accuracy: 96.91%
1	Validation loss: 0.080972	Best loss: 0.080972	Accuracy: 97.89%
2	Validation loss: 0.096146	Best loss: 0.080972	Accuracy: 97.54%
3	Validation loss: 0.059701	Best loss: 0.059701	Accuracy: 98.12%
4	Validation loss: 0.173862	Best loss: 0.059701	Accuracy: 97.03%
5	Validation loss: 0.084840	Best loss: 0.059701	Accuracy: 98.01%
6	Validation loss: 0.067868	Best loss: 0.059701	Accuracy: 98.44%
7	Validation loss: 0.071421	Best loss: 0.059701	Accuracy: 98.20%
8	Validation loss: 0.522578	Best loss: 0.059701	Accuracy: 95.90%
9	Validation loss: 0.094840	Best loss: 0.059701	Accuracy: 97.69%
10	Validation loss: 0.073480	Best loss: 0.059701	Accuracy: 98.32%
11	Validation loss: 0.055963	Best loss: 0.055963	Accuracy: 98.79%
12	Validation loss: 0.122096	Best loss: 0.055963	Accuracy: 97.73%
13	Validation loss: 0.078617	Best loss: 0.055963	Accuracy: 98.20%
14	Validation loss: 0.091261	Best loss: 0.055963	Accuracy: 97.65%
15	Validation loss: 0.051258	Best loss: 0.051258	Accuracy: 98.79%
16	Validation loss: 0.053641	Best loss: 0.051258	Accuracy: 98.79%
17	Validation loss: 0.090333	Best loss: 0.051258	Accuracy: 98.44%
18	Validation loss: 0.155528	Best loss: 0.051258	Accuracy: 97.97%
19	Validation loss: 0.043701	Best loss: 0.043701	Accuracy: 98.94%
20	Validation loss: 0.059728	Best loss: 0.043701	Accuracy: 98.79%
21	Validation loss: 0.127701	Best loss: 0.043701	Accuracy: 98.40%
22	Validation loss: 0.056345	Best loss: 0.043701	Accuracy: 98.75%
23	Validation loss: 0.069329	Best loss: 0.043701	Accuracy: 98.75%
24	Validation loss: 0.076169	Best loss: 0.043701	Accuracy: 98.83%
25	Validation loss: 0.068900	Best loss: 0.043701	Accuracy: 98.67%
26	Validation loss: 0.070629	Best loss: 0.043701	Accuracy: 98.63%
27	Validation loss: 0.101960	Best loss: 0.043701	Accuracy: 98.44%
28	Validation loss: 0.073116	Best loss: 0.043701	Accuracy: 98.63%
29	Validation loss: 0.065871	Best loss: 0.043701	Accuracy: 98.83%
30	Validation loss: 0.092967	Best loss: 0.043701	Accuracy: 98.59%
31	Validation loss: 0.228094	Best loss: 0.043701	Accuracy: 96.09%
32	Validation loss: 0.089107	Best loss: 0.043701	Accuracy: 98.48%
33	Validation loss: 0.077791	Best loss: 0.043701	Accuracy: 98.79%
34	Validation loss: 0.068848	Best loss: 0.043701	Accuracy: 98.87%
35	Validation loss: 0.092546	Best loss: 0.043701	Accuracy: 98.48%
36	Validation loss: 0.180698	Best loss: 0.043701	Accuracy: 97.38%
37	Validation loss: 0.146919	Best loss: 0.043701	Accuracy: 98.24%
38	Validation loss: 0.090600	Best loss: 0.043701	Accuracy: 98.79%
39	Validation loss: 0.100416	Best loss: 0.043701	Accuracy: 98.63%
40	Validation loss: 0.083163	Best loss: 0.043701	Accuracy: 98.63%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.1min
[CV] n_neurons=100, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.133446	Best loss: 0.133446	Accuracy: 96.79%
1	Validation loss: 0.087364	Best loss: 0.087364	Accuracy: 97.65%
2	Validation loss: 0.068559	Best loss: 0.068559	Accuracy: 97.81%
3	Validation loss: 0.082555	Best loss: 0.068559	Accuracy: 97.89%
4	Validation loss: 0.092360	Best loss: 0.068559	Accuracy: 98.12%
5	Validation loss: 0.128144	Best loss: 0.068559	Accuracy: 96.87%
6	Validation loss: 0.211972	Best loss: 0.068559	Accuracy: 96.48%
7	Validation loss: 0.076146	Best loss: 0.068559	Accuracy: 98.24%
8	Validation loss: 0.048098	Best loss: 0.048098	Accuracy: 98.63%
9	Validation loss: 0.081195	Best loss: 0.048098	Accuracy: 98.01%
10	Validation loss: 0.054632	Best loss: 0.048098	Accuracy: 98.51%
11	Validation loss: 0.053635	Best loss: 0.048098	Accuracy: 98.51%
12	Validation loss: 0.094354	Best loss: 0.048098	Accuracy: 97.93%
13	Validation loss: 0.065380	Best loss: 0.048098	Accuracy: 98.59%
14	Validation loss: 0.107818	Best loss: 0.048098	Accuracy: 98.44%
15	Validation loss: 0.099018	Best loss: 0.048098	Accuracy: 98.24%
16	Validation loss: 0.055654	Best loss: 0.048098	Accuracy: 98.75%
17	Validation loss: 0.068179	Best loss: 0.048098	Accuracy: 98.44%
18	Validation loss: 0.124455	Best loss: 0.048098	Accuracy: 98.28%
19	Validation loss: 0.069154	Best loss: 0.048098	Accuracy: 98.63%
20	Validation loss: 0.150264	Best loss: 0.048098	Accuracy: 97.34%
21	Validation loss: 0.082510	Best loss: 0.048098	Accuracy: 98.55%
22	Validation loss: 0.086791	Best loss: 0.048098	Accuracy: 98.24%
23	Validation loss: 0.079486	Best loss: 0.048098	Accuracy: 98.67%
24	Validation loss: 0.108375	Best loss: 0.048098	Accuracy: 98.83%
25	Validation loss: 0.055798	Best loss: 0.048098	Accuracy: 99.02%
26	Validation loss: 0.060690	Best loss: 0.048098	Accuracy: 98.79%
27	Validation loss: 0.102401	Best loss: 0.048098	Accuracy: 98.32%
28	Validation loss: 0.060589	Best loss: 0.048098	Accuracy: 98.75%
29	Validation loss: 0.136522	Best loss: 0.048098	Accuracy: 98.40%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  49.7s
[CV] n_neurons=50, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.141660	Best loss: 0.141660	Accuracy: 96.99%
1	Validation loss: 0.066854	Best loss: 0.066854	Accuracy: 98.20%
2	Validation loss: 0.055005	Best loss: 0.055005	Accuracy: 98.44%
3	Validation loss: 0.059371	Best loss: 0.055005	Accuracy: 98.48%
4	Validation loss: 0.050114	Best loss: 0.050114	Accuracy: 98.55%
5	Validation loss: 0.090925	Best loss: 0.050114	Accuracy: 98.12%
6	Validation loss: 0.056699	Best loss: 0.050114	Accuracy: 98.71%
7	Validation loss: 0.093463	Best loss: 0.050114	Accuracy: 98.08%
8	Validation loss: 0.059431	Best loss: 0.050114	Accuracy: 98.51%
9	Validation loss: 0.064317	Best loss: 0.050114	Accuracy: 98.59%
10	Validation loss: 0.046294	Best loss: 0.046294	Accuracy: 98.67%
11	Validation loss: 0.068748	Best loss: 0.046294	Accuracy: 98.71%
12	Validation loss: 0.057709	Best loss: 0.046294	Accuracy: 98.83%
13	Validation loss: 0.061938	Best loss: 0.046294	Accuracy: 98.67%
14	Validation loss: 0.068541	Best loss: 0.046294	Accuracy: 98.55%
15	Validation loss: 0.042854	Best loss: 0.042854	Accuracy: 99.06%
16	Validation loss: 0.086663	Best loss: 0.042854	Accuracy: 98.24%
17	Validation loss: 0.077516	Best loss: 0.042854	Accuracy: 98.59%
18	Validation loss: 0.059279	Best loss: 0.042854	Accuracy: 98.67%
19	Validation loss: 0.063672	Best loss: 0.042854	Accuracy: 98.59%
20	Validation loss: 0.067542	Best loss: 0.042854	Accuracy: 98.59%
21	Validation loss: 0.055585	Best loss: 0.042854	Accuracy: 98.71%
22	Validation loss: 0.052076	Best loss: 0.042854	Accuracy: 98.79%
23	Validation loss: 0.078357	Best loss: 0.042854	Accuracy: 98.28%
24	Validation loss: 0.062811	Best loss: 0.042854	Accuracy: 98.55%
25	Validation loss: 0.051672	Best loss: 0.042854	Accuracy: 98.83%
26	Validation loss: 0.083543	Best loss: 0.042854	Accuracy: 98.36%
27	Validation loss: 0.056505	Best loss: 0.042854	Accuracy: 98.75%
28	Validation loss: 0.077053	Best loss: 0.042854	Accuracy: 98.28%
29	Validation loss: 0.062063	Best loss: 0.042854	Accuracy: 98.71%
30	Validation loss: 0.064400	Best loss: 0.042854	Accuracy: 98.63%
31	Validation loss: 0.060949	Best loss: 0.042854	Accuracy: 98.79%
32	Validation loss: 0.076138	Best loss: 0.042854	Accuracy: 98.55%
33	Validation loss: 0.063748	Best loss: 0.042854	Accuracy: 98.63%
34	Validation loss: 0.060579	Best loss: 0.042854	Accuracy: 98.71%
35	Validation loss: 0.048157	Best loss: 0.042854	Accuracy: 99.02%
36	Validation loss: 0.059800	Best loss: 0.042854	Accuracy: 98.63%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  14.3s
[CV] n_neurons=50, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.278813	Best loss: 0.278813	Accuracy: 95.15%
1	Validation loss: 0.096892	Best loss: 0.096892	Accuracy: 97.46%
2	Validation loss: 0.064883	Best loss: 0.064883	Accuracy: 98.16%
3	Validation loss: 0.047234	Best loss: 0.047234	Accuracy: 98.94%
4	Validation loss: 0.056586	Best loss: 0.047234	Accuracy: 98.40%
5	Validation loss: 0.061590	Best loss: 0.047234	Accuracy: 98.51%
6	Validation loss: 0.062485	Best loss: 0.047234	Accuracy: 98.48%
7	Validation loss: 0.060283	Best loss: 0.047234	Accuracy: 98.20%
8	Validation loss: 0.107746	Best loss: 0.047234	Accuracy: 96.95%
9	Validation loss: 0.058258	Best loss: 0.047234	Accuracy: 98.48%
10	Validation loss: 0.051665	Best loss: 0.047234	Accuracy: 98.98%
11	Validation loss: 0.060088	Best loss: 0.047234	Accuracy: 98.67%
12	Validation loss: 0.058748	Best loss: 0.047234	Accuracy: 98.67%
13	Validation loss: 0.047664	Best loss: 0.047234	Accuracy: 98.91%
14	Validation loss: 0.052167	Best loss: 0.047234	Accuracy: 98.87%
15	Validation loss: 0.055946	Best loss: 0.047234	Accuracy: 98.67%
16	Validation loss: 0.061044	Best loss: 0.047234	Accuracy: 98.48%
17	Validation loss: 0.065067	Best loss: 0.047234	Accuracy: 98.67%
18	Validation loss: 0.054944	Best loss: 0.047234	Accuracy: 98.94%
19	Validation loss: 0.056579	Best loss: 0.047234	Accuracy: 98.75%
20	Validation loss: 0.094971	Best loss: 0.047234	Accuracy: 98.12%
21	Validation loss: 0.074664	Best loss: 0.047234	Accuracy: 98.59%
22	Validation loss: 0.069759	Best loss: 0.047234	Accuracy: 98.63%
23	Validation loss: 0.062378	Best loss: 0.047234	Accuracy: 98.87%
24	Validation loss: 0.066169	Best loss: 0.047234	Accuracy: 98.91%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  10.6s
[CV] n_neurons=50, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.182792	Best loss: 0.182792	Accuracy: 96.25%
1	Validation loss: 0.070966	Best loss: 0.070966	Accuracy: 98.24%
2	Validation loss: 0.060627	Best loss: 0.060627	Accuracy: 98.36%
3	Validation loss: 0.058714	Best loss: 0.058714	Accuracy: 98.44%
4	Validation loss: 0.055321	Best loss: 0.055321	Accuracy: 98.40%
5	Validation loss: 0.072657	Best loss: 0.055321	Accuracy: 98.32%
6	Validation loss: 0.070765	Best loss: 0.055321	Accuracy: 98.05%
7	Validation loss: 0.041366	Best loss: 0.041366	Accuracy: 98.79%
8	Validation loss: 0.052635	Best loss: 0.041366	Accuracy: 98.63%
9	Validation loss: 0.054334	Best loss: 0.041366	Accuracy: 98.63%
10	Validation loss: 0.084421	Best loss: 0.041366	Accuracy: 98.24%
11	Validation loss: 0.056673	Best loss: 0.041366	Accuracy: 98.79%
12	Validation loss: 0.047882	Best loss: 0.041366	Accuracy: 98.79%
13	Validation loss: 0.062549	Best loss: 0.041366	Accuracy: 98.51%
14	Validation loss: 0.057946	Best loss: 0.041366	Accuracy: 98.79%
15	Validation loss: 0.051036	Best loss: 0.041366	Accuracy: 98.63%
16	Validation loss: 0.072028	Best loss: 0.041366	Accuracy: 98.71%
17	Validation loss: 0.064259	Best loss: 0.041366	Accuracy: 98.75%
18	Validation loss: 0.050846	Best loss: 0.041366	Accuracy: 98.75%
19	Validation loss: 0.063595	Best loss: 0.041366	Accuracy: 98.55%
20	Validation loss: 0.059850	Best loss: 0.041366	Accuracy: 98.79%
21	Validation loss: 0.058835	Best loss: 0.041366	Accuracy: 98.59%
22	Validation loss: 0.055085	Best loss: 0.041366	Accuracy: 98.83%
23	Validation loss: 0.040476	Best loss: 0.040476	Accuracy: 98.98%
24	Validation loss: 0.065192	Best loss: 0.040476	Accuracy: 98.59%
25	Validation loss: 0.077349	Best loss: 0.040476	Accuracy: 98.20%
26	Validation loss: 0.067726	Best loss: 0.040476	Accuracy: 98.67%
27	Validation loss: 0.054139	Best loss: 0.040476	Accuracy: 98.75%
28	Validation loss: 0.071474	Best loss: 0.040476	Accuracy: 98.67%
29	Validation loss: 0.043188	Best loss: 0.040476	Accuracy: 99.06%
30	Validation loss: 0.041043	Best loss: 0.040476	Accuracy: 99.14%
31	Validation loss: 0.059632	Best loss: 0.040476	Accuracy: 98.75%
32	Validation loss: 0.057023	Best loss: 0.040476	Accuracy: 98.59%
33	Validation loss: 0.049743	Best loss: 0.040476	Accuracy: 98.79%
34	Validation loss: 0.048816	Best loss: 0.040476	Accuracy: 98.91%
35	Validation loss: 0.051961	Best loss: 0.040476	Accuracy: 99.02%
36	Validation loss: 0.034386	Best loss: 0.034386	Accuracy: 98.94%
37	Validation loss: 0.035620	Best loss: 0.034386	Accuracy: 99.10%
38	Validation loss: 0.066471	Best loss: 0.034386	Accuracy: 98.87%
39	Validation loss: 0.056240	Best loss: 0.034386	Accuracy: 98.98%
40	Validation loss: 0.053034	Best loss: 0.034386	Accuracy: 98.79%
41	Validation loss: 0.054373	Best loss: 0.034386	Accuracy: 98.87%
42	Validation loss: 0.052229	Best loss: 0.034386	Accuracy: 98.98%
43	Validation loss: 0.054474	Best loss: 0.034386	Accuracy: 98.83%
44	Validation loss: 0.075843	Best loss: 0.034386	Accuracy: 98.32%
45	Validation loss: 0.046048	Best loss: 0.034386	Accuracy: 98.94%
46	Validation loss: 0.071775	Best loss: 0.034386	Accuracy: 98.71%
47	Validation loss: 0.059033	Best loss: 0.034386	Accuracy: 98.83%
48	Validation loss: 0.072409	Best loss: 0.034386	Accuracy: 98.75%
49	Validation loss: 0.042949	Best loss: 0.034386	Accuracy: 98.98%
50	Validation loss: 0.045587	Best loss: 0.034386	Accuracy: 99.22%
51	Validation loss: 0.061916	Best loss: 0.034386	Accuracy: 98.59%
52	Validation loss: 0.048847	Best loss: 0.034386	Accuracy: 98.75%
53	Validation loss: 0.036385	Best loss: 0.034386	Accuracy: 99.34%
54	Validation loss: 0.047762	Best loss: 0.034386	Accuracy: 98.94%
55	Validation loss: 0.061693	Best loss: 0.034386	Accuracy: 98.71%
56	Validation loss: 0.050977	Best loss: 0.034386	Accuracy: 98.94%
57	Validation loss: 0.051635	Best loss: 0.034386	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  21.5s
[CV] n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.116514	Best loss: 0.116514	Accuracy: 97.50%
1	Validation loss: 0.099710	Best loss: 0.099710	Accuracy: 97.62%
2	Validation loss: 0.056391	Best loss: 0.056391	Accuracy: 98.83%
3	Validation loss: 0.051758	Best loss: 0.051758	Accuracy: 98.63%
4	Validation loss: 0.059827	Best loss: 0.051758	Accuracy: 98.83%
5	Validation loss: 0.046800	Best loss: 0.046800	Accuracy: 98.91%
6	Validation loss: 0.042221	Best loss: 0.042221	Accuracy: 99.06%
7	Validation loss: 0.061158	Best loss: 0.042221	Accuracy: 98.67%
8	Validation loss: 0.046278	Best loss: 0.042221	Accuracy: 98.83%
9	Validation loss: 0.057002	Best loss: 0.042221	Accuracy: 98.75%
10	Validation loss: 0.041915	Best loss: 0.041915	Accuracy: 98.94%
11	Validation loss: 0.032358	Best loss: 0.032358	Accuracy: 99.18%
12	Validation loss: 0.046701	Best loss: 0.032358	Accuracy: 98.98%
13	Validation loss: 0.051075	Best loss: 0.032358	Accuracy: 98.55%
14	Validation loss: 0.048930	Best loss: 0.032358	Accuracy: 98.71%
15	Validation loss: 0.053422	Best loss: 0.032358	Accuracy: 98.98%
16	Validation loss: 0.064345	Best loss: 0.032358	Accuracy: 98.87%
17	Validation loss: 0.036504	Best loss: 0.032358	Accuracy: 99.18%
18	Validation loss: 0.055241	Best loss: 0.032358	Accuracy: 98.83%
19	Validation loss: 0.041707	Best loss: 0.032358	Accuracy: 99.14%
20	Validation loss: 0.049611	Best loss: 0.032358	Accuracy: 99.10%
21	Validation loss: 0.059956	Best loss: 0.032358	Accuracy: 98.83%
22	Validation loss: 0.043484	Best loss: 0.032358	Accuracy: 99.18%
23	Validation loss: 0.048078	Best loss: 0.032358	Accuracy: 99.18%
24	Validation loss: 0.052088	Best loss: 0.032358	Accuracy: 99.02%
25	Validation loss: 0.051097	Best loss: 0.032358	Accuracy: 99.02%
26	Validation loss: 0.060255	Best loss: 0.032358	Accuracy: 99.02%
27	Validation loss: 0.048136	Best loss: 0.032358	Accuracy: 98.98%
28	Validation loss: 0.053418	Best loss: 0.032358	Accuracy: 98.98%
29	Validation loss: 0.072119	Best loss: 0.032358	Accuracy: 98.63%
30	Validation loss: 0.062158	Best loss: 0.032358	Accuracy: 98.83%
31	Validation loss: 0.051003	Best loss: 0.032358	Accuracy: 99.02%
32	Validation loss: 0.045557	Best loss: 0.032358	Accuracy: 99.02%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  44.5s
[CV] n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.211174	Best loss: 0.211174	Accuracy: 96.13%
1	Validation loss: 0.057385	Best loss: 0.057385	Accuracy: 98.36%
2	Validation loss: 0.072042	Best loss: 0.057385	Accuracy: 98.32%
3	Validation loss: 0.060702	Best loss: 0.057385	Accuracy: 98.28%
4	Validation loss: 0.107034	Best loss: 0.057385	Accuracy: 97.73%
5	Validation loss: 0.035410	Best loss: 0.035410	Accuracy: 99.14%
6	Validation loss: 0.052527	Best loss: 0.035410	Accuracy: 98.48%
7	Validation loss: 0.053398	Best loss: 0.035410	Accuracy: 98.83%
8	Validation loss: 0.058467	Best loss: 0.035410	Accuracy: 98.83%
9	Validation loss: 0.067449	Best loss: 0.035410	Accuracy: 98.63%
10	Validation loss: 0.038820	Best loss: 0.035410	Accuracy: 99.10%
11	Validation loss: 0.064173	Best loss: 0.035410	Accuracy: 98.63%
12	Validation loss: 0.058955	Best loss: 0.035410	Accuracy: 98.59%
13	Validation loss: 0.044251	Best loss: 0.035410	Accuracy: 99.06%
14	Validation loss: 0.056686	Best loss: 0.035410	Accuracy: 99.02%
15	Validation loss: 0.050160	Best loss: 0.035410	Accuracy: 98.94%
16	Validation loss: 0.035574	Best loss: 0.035410	Accuracy: 99.06%
17	Validation loss: 0.045552	Best loss: 0.035410	Accuracy: 99.10%
18	Validation loss: 0.044656	Best loss: 0.035410	Accuracy: 99.06%
19	Validation loss: 0.045261	Best loss: 0.035410	Accuracy: 98.91%
20	Validation loss: 0.053261	Best loss: 0.035410	Accuracy: 98.71%
21	Validation loss: 0.060446	Best loss: 0.035410	Accuracy: 98.67%
22	Validation loss: 0.050718	Best loss: 0.035410	Accuracy: 98.79%
23	Validation loss: 0.056768	Best loss: 0.035410	Accuracy: 98.83%
24	Validation loss: 0.060569	Best loss: 0.035410	Accuracy: 98.67%
25	Validation loss: 0.044758	Best loss: 0.035410	Accuracy: 99.22%
26	Validation loss: 0.076718	Best loss: 0.035410	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  36.4s
[CV] n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.123306	Best loss: 0.123306	Accuracy: 97.42%
1	Validation loss: 0.077991	Best loss: 0.077991	Accuracy: 97.85%
2	Validation loss: 0.068677	Best loss: 0.068677	Accuracy: 98.12%
3	Validation loss: 0.124616	Best loss: 0.068677	Accuracy: 96.36%
4	Validation loss: 0.048621	Best loss: 0.048621	Accuracy: 98.83%
5	Validation loss: 0.030188	Best loss: 0.030188	Accuracy: 98.98%
6	Validation loss: 0.044800	Best loss: 0.030188	Accuracy: 98.87%
7	Validation loss: 0.054267	Best loss: 0.030188	Accuracy: 98.67%
8	Validation loss: 0.052462	Best loss: 0.030188	Accuracy: 98.87%
9	Validation loss: 0.063691	Best loss: 0.030188	Accuracy: 98.83%
10	Validation loss: 0.074539	Best loss: 0.030188	Accuracy: 98.40%
11	Validation loss: 0.047847	Best loss: 0.030188	Accuracy: 98.83%
12	Validation loss: 0.043005	Best loss: 0.030188	Accuracy: 98.94%
13	Validation loss: 0.037351	Best loss: 0.030188	Accuracy: 99.06%
14	Validation loss: 0.050495	Best loss: 0.030188	Accuracy: 98.71%
15	Validation loss: 0.031152	Best loss: 0.030188	Accuracy: 99.18%
16	Validation loss: 0.043937	Best loss: 0.030188	Accuracy: 99.02%
17	Validation loss: 0.030990	Best loss: 0.030188	Accuracy: 99.14%
18	Validation loss: 0.044670	Best loss: 0.030188	Accuracy: 98.83%
19	Validation loss: 0.040396	Best loss: 0.030188	Accuracy: 99.02%
20	Validation loss: 0.044905	Best loss: 0.030188	Accuracy: 98.98%
21	Validation loss: 0.031613	Best loss: 0.030188	Accuracy: 99.37%
22	Validation loss: 0.042731	Best loss: 0.030188	Accuracy: 99.02%
23	Validation loss: 0.059879	Best loss: 0.030188	Accuracy: 98.63%
24	Validation loss: 0.046299	Best loss: 0.030188	Accuracy: 99.06%
25	Validation loss: 0.034537	Best loss: 0.030188	Accuracy: 99.14%
26	Validation loss: 0.040966	Best loss: 0.030188	Accuracy: 99.14%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  36.2s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.171066	Best loss: 0.171066	Accuracy: 95.15%
1	Validation loss: 0.088664	Best loss: 0.088664	Accuracy: 97.58%
2	Validation loss: 0.090961	Best loss: 0.088664	Accuracy: 97.22%
3	Validation loss: 0.073116	Best loss: 0.073116	Accuracy: 97.73%
4	Validation loss: 0.066910	Best loss: 0.066910	Accuracy: 98.01%
5	Validation loss: 0.110713	Best loss: 0.066910	Accuracy: 96.44%
6	Validation loss: 0.064214	Best loss: 0.064214	Accuracy: 98.12%
7	Validation loss: 0.069763	Best loss: 0.064214	Accuracy: 98.05%
8	Validation loss: 0.069283	Best loss: 0.064214	Accuracy: 97.73%
9	Validation loss: 0.063549	Best loss: 0.063549	Accuracy: 98.12%
10	Validation loss: 0.066037	Best loss: 0.063549	Accuracy: 98.24%
11	Validation loss: 0.080538	Best loss: 0.063549	Accuracy: 97.85%
12	Validation loss: 0.060026	Best loss: 0.060026	Accuracy: 98.51%
13	Validation loss: 0.061918	Best loss: 0.060026	Accuracy: 98.32%
14	Validation loss: 0.064869	Best loss: 0.060026	Accuracy: 98.20%
15	Validation loss: 0.072180	Best loss: 0.060026	Accuracy: 98.24%
16	Validation loss: 0.062992	Best loss: 0.060026	Accuracy: 98.36%
17	Validation loss: 0.077529	Best loss: 0.060026	Accuracy: 98.16%
18	Validation loss: 0.077546	Best loss: 0.060026	Accuracy: 98.08%
19	Validation loss: 0.083041	Best loss: 0.060026	Accuracy: 98.08%
20	Validation loss: 0.059054	Best loss: 0.059054	Accuracy: 98.40%
21	Validation loss: 0.125535	Best loss: 0.059054	Accuracy: 97.38%
22	Validation loss: 0.080131	Best loss: 0.059054	Accuracy: 98.36%
23	Validation loss: 0.083994	Best loss: 0.059054	Accuracy: 98.16%
24	Validation loss: 0.084316	Best loss: 0.059054	Accuracy: 97.97%
25	Validation loss: 0.089007	Best loss: 0.059054	Accuracy: 98.16%
26	Validation loss: 0.084105	Best loss: 0.059054	Accuracy: 98.28%
27	Validation loss: 0.080244	Best loss: 0.059054	Accuracy: 98.32%
28	Validation loss: 0.104437	Best loss: 0.059054	Accuracy: 97.81%
29	Validation loss: 0.080838	Best loss: 0.059054	Accuracy: 98.63%
30	Validation loss: 0.097385	Best loss: 0.059054	Accuracy: 98.20%
31	Validation loss: 0.088903	Best loss: 0.059054	Accuracy: 98.08%
32	Validation loss: 0.081199	Best loss: 0.059054	Accuracy: 98.55%
33	Validation loss: 0.101421	Best loss: 0.059054	Accuracy: 98.28%
34	Validation loss: 0.091159	Best loss: 0.059054	Accuracy: 98.16%
35	Validation loss: 0.085419	Best loss: 0.059054	Accuracy: 98.40%
36	Validation loss: 0.094407	Best loss: 0.059054	Accuracy: 98.36%
37	Validation loss: 0.123925	Best loss: 0.059054	Accuracy: 97.81%
38	Validation loss: 0.099048	Best loss: 0.059054	Accuracy: 98.16%
39	Validation loss: 0.103560	Best loss: 0.059054	Accuracy: 98.08%
40	Validation loss: 0.076357	Best loss: 0.059054	Accuracy: 98.40%
41	Validation loss: 0.093477	Best loss: 0.059054	Accuracy: 98.32%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total=  15.9s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.184550	Best loss: 0.184550	Accuracy: 94.64%
1	Validation loss: 0.093942	Best loss: 0.093942	Accuracy: 97.42%
2	Validation loss: 0.069969	Best loss: 0.069969	Accuracy: 97.81%
3	Validation loss: 0.093000	Best loss: 0.069969	Accuracy: 96.95%
4	Validation loss: 0.066980	Best loss: 0.066980	Accuracy: 97.81%
5	Validation loss: 0.064864	Best loss: 0.064864	Accuracy: 97.77%
6	Validation loss: 0.072242	Best loss: 0.064864	Accuracy: 97.97%
7	Validation loss: 0.068729	Best loss: 0.064864	Accuracy: 97.77%
8	Validation loss: 0.084949	Best loss: 0.064864	Accuracy: 97.34%
9	Validation loss: 0.089993	Best loss: 0.064864	Accuracy: 98.12%
10	Validation loss: 0.075443	Best loss: 0.064864	Accuracy: 98.16%
11	Validation loss: 0.061366	Best loss: 0.061366	Accuracy: 98.51%
12	Validation loss: 0.060621	Best loss: 0.060621	Accuracy: 98.63%
13	Validation loss: 0.067448	Best loss: 0.060621	Accuracy: 98.48%
14	Validation loss: 0.065844	Best loss: 0.060621	Accuracy: 98.44%
15	Validation loss: 0.093175	Best loss: 0.060621	Accuracy: 97.81%
16	Validation loss: 0.067065	Best loss: 0.060621	Accuracy: 98.55%
17	Validation loss: 0.056238	Best loss: 0.056238	Accuracy: 98.67%
18	Validation loss: 0.069304	Best loss: 0.056238	Accuracy: 98.36%
19	Validation loss: 0.065897	Best loss: 0.056238	Accuracy: 98.32%
20	Validation loss: 0.107495	Best loss: 0.056238	Accuracy: 98.05%
21	Validation loss: 0.070009	Best loss: 0.056238	Accuracy: 98.05%
22	Validation loss: 0.072273	Best loss: 0.056238	Accuracy: 98.44%
23	Validation loss: 0.065230	Best loss: 0.056238	Accuracy: 98.48%
24	Validation loss: 0.071800	Best loss: 0.056238	Accuracy: 98.51%
25	Validation loss: 0.064755	Best loss: 0.056238	Accuracy: 98.75%
26	Validation loss: 0.083139	Best loss: 0.056238	Accuracy: 98.51%
27	Validation loss: 0.069894	Best loss: 0.056238	Accuracy: 98.51%
28	Validation loss: 0.079640	Best loss: 0.056238	Accuracy: 98.63%
29	Validation loss: 0.074552	Best loss: 0.056238	Accuracy: 98.40%
30	Validation loss: 0.076228	Best loss: 0.056238	Accuracy: 98.55%
31	Validation loss: 0.098068	Best loss: 0.056238	Accuracy: 98.12%
32	Validation loss: 0.090923	Best loss: 0.056238	Accuracy: 98.12%
33	Validation loss: 0.074083	Best loss: 0.056238	Accuracy: 98.48%
34	Validation loss: 0.063096	Best loss: 0.056238	Accuracy: 98.94%
35	Validation loss: 0.061363	Best loss: 0.056238	Accuracy: 98.83%
36	Validation loss: 0.069268	Best loss: 0.056238	Accuracy: 98.98%
37	Validation loss: 0.076186	Best loss: 0.056238	Accuracy: 98.71%
38	Validation loss: 0.094377	Best loss: 0.056238	Accuracy: 98.48%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total=  14.9s
[CV] n_neurons=50, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.161249	Best loss: 0.161249	Accuracy: 95.19%
1	Validation loss: 0.091973	Best loss: 0.091973	Accuracy: 97.30%
2	Validation loss: 0.078096	Best loss: 0.078096	Accuracy: 97.69%
3	Validation loss: 0.071715	Best loss: 0.071715	Accuracy: 97.77%
4	Validation loss: 0.081359	Best loss: 0.071715	Accuracy: 97.42%
5	Validation loss: 0.066094	Best loss: 0.066094	Accuracy: 97.89%
6	Validation loss: 0.058252	Best loss: 0.058252	Accuracy: 98.48%
7	Validation loss: 0.073372	Best loss: 0.058252	Accuracy: 97.65%
8	Validation loss: 0.055853	Best loss: 0.055853	Accuracy: 98.16%
9	Validation loss: 0.054771	Best loss: 0.054771	Accuracy: 98.36%
10	Validation loss: 0.059667	Best loss: 0.054771	Accuracy: 98.32%
11	Validation loss: 0.063073	Best loss: 0.054771	Accuracy: 98.12%
12	Validation loss: 0.059985	Best loss: 0.054771	Accuracy: 98.55%
13	Validation loss: 0.080081	Best loss: 0.054771	Accuracy: 98.08%
14	Validation loss: 0.051510	Best loss: 0.051510	Accuracy: 98.63%
15	Validation loss: 0.087952	Best loss: 0.051510	Accuracy: 98.28%
16	Validation loss: 0.052098	Best loss: 0.051510	Accuracy: 98.75%
17	Validation loss: 0.072124	Best loss: 0.051510	Accuracy: 98.01%
18	Validation loss: 0.064043	Best loss: 0.051510	Accuracy: 98.48%
19	Validation loss: 0.060394	Best loss: 0.051510	Accuracy: 98.55%
20	Validation loss: 0.066065	Best loss: 0.051510	Accuracy: 98.79%
21	Validation loss: 0.092798	Best loss: 0.051510	Accuracy: 98.08%
22	Validation loss: 0.076726	Best loss: 0.051510	Accuracy: 98.40%
23	Validation loss: 0.067376	Best loss: 0.051510	Accuracy: 98.79%
24	Validation loss: 0.073936	Best loss: 0.051510	Accuracy: 98.44%
25	Validation loss: 0.060849	Best loss: 0.051510	Accuracy: 98.55%
26	Validation loss: 0.066845	Best loss: 0.051510	Accuracy: 98.59%
27	Validation loss: 0.069450	Best loss: 0.051510	Accuracy: 98.48%
28	Validation loss: 0.089021	Best loss: 0.051510	Accuracy: 98.40%
29	Validation loss: 0.075647	Best loss: 0.051510	Accuracy: 98.28%
30	Validation loss: 0.051962	Best loss: 0.051510	Accuracy: 98.67%
31	Validation loss: 0.060325	Best loss: 0.051510	Accuracy: 98.75%
32	Validation loss: 0.068531	Best loss: 0.051510	Accuracy: 98.67%
33	Validation loss: 0.060917	Best loss: 0.051510	Accuracy: 98.83%
34	Validation loss: 0.067596	Best loss: 0.051510	Accuracy: 98.67%
35	Validation loss: 0.081629	Best loss: 0.051510	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function elu at 0x1243639d8&gt;, total=  13.9s
[CV] n_neurons=90, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.142671	Best loss: 0.142671	Accuracy: 95.86%
1	Validation loss: 0.082055	Best loss: 0.082055	Accuracy: 97.42%
2	Validation loss: 0.063823	Best loss: 0.063823	Accuracy: 97.77%
3	Validation loss: 0.062725	Best loss: 0.062725	Accuracy: 98.01%
4	Validation loss: 0.055056	Best loss: 0.055056	Accuracy: 98.36%
5	Validation loss: 0.078346	Best loss: 0.055056	Accuracy: 97.42%
6	Validation loss: 0.055660	Best loss: 0.055056	Accuracy: 98.05%
7	Validation loss: 0.064935	Best loss: 0.055056	Accuracy: 98.20%
8	Validation loss: 0.046714	Best loss: 0.046714	Accuracy: 98.51%
9	Validation loss: 0.060292	Best loss: 0.046714	Accuracy: 98.40%
10	Validation loss: 0.053140	Best loss: 0.046714	Accuracy: 98.75%
11	Validation loss: 0.088338	Best loss: 0.046714	Accuracy: 98.12%
12	Validation loss: 0.064179	Best loss: 0.046714	Accuracy: 98.44%
13	Validation loss: 0.069692	Best loss: 0.046714	Accuracy: 98.20%
14	Validation loss: 0.058352	Best loss: 0.046714	Accuracy: 98.55%
15	Validation loss: 0.057790	Best loss: 0.046714	Accuracy: 98.48%
16	Validation loss: 0.060518	Best loss: 0.046714	Accuracy: 98.59%
17	Validation loss: 0.045784	Best loss: 0.045784	Accuracy: 98.75%
18	Validation loss: 0.056225	Best loss: 0.045784	Accuracy: 98.79%
19	Validation loss: 0.073498	Best loss: 0.045784	Accuracy: 98.59%
20	Validation loss: 0.078935	Best loss: 0.045784	Accuracy: 98.36%
21	Validation loss: 0.062208	Best loss: 0.045784	Accuracy: 98.32%
22	Validation loss: 0.074145	Best loss: 0.045784	Accuracy: 98.36%
23	Validation loss: 0.047015	Best loss: 0.045784	Accuracy: 98.71%
24	Validation loss: 0.084137	Best loss: 0.045784	Accuracy: 98.51%
25	Validation loss: 0.086225	Best loss: 0.045784	Accuracy: 98.36%
26	Validation loss: 0.076027	Best loss: 0.045784	Accuracy: 98.40%
27	Validation loss: 0.077176	Best loss: 0.045784	Accuracy: 98.48%
28	Validation loss: 0.079490	Best loss: 0.045784	Accuracy: 98.51%
29	Validation loss: 0.100609	Best loss: 0.045784	Accuracy: 98.51%
30	Validation loss: 0.073364	Best loss: 0.045784	Accuracy: 98.63%
31	Validation loss: 0.072471	Best loss: 0.045784	Accuracy: 98.75%
32	Validation loss: 0.078343	Best loss: 0.045784	Accuracy: 98.75%
33	Validation loss: 0.082363	Best loss: 0.045784	Accuracy: 98.40%
34	Validation loss: 0.055649	Best loss: 0.045784	Accuracy: 98.71%
35	Validation loss: 0.067460	Best loss: 0.045784	Accuracy: 98.79%
36	Validation loss: 0.073799	Best loss: 0.045784	Accuracy: 98.71%
37	Validation loss: 0.098881	Best loss: 0.045784	Accuracy: 98.05%
38	Validation loss: 0.067196	Best loss: 0.045784	Accuracy: 98.40%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  20.1s
[CV] n_neurons=90, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.131974	Best loss: 0.131974	Accuracy: 96.01%
1	Validation loss: 0.077312	Best loss: 0.077312	Accuracy: 97.54%
2	Validation loss: 0.069160	Best loss: 0.069160	Accuracy: 97.97%
3	Validation loss: 0.058308	Best loss: 0.058308	Accuracy: 98.44%
4	Validation loss: 0.065315	Best loss: 0.058308	Accuracy: 98.16%
5	Validation loss: 0.066662	Best loss: 0.058308	Accuracy: 97.85%
6	Validation loss: 0.055816	Best loss: 0.055816	Accuracy: 98.59%
7	Validation loss: 0.063027	Best loss: 0.055816	Accuracy: 98.12%
8	Validation loss: 0.047907	Best loss: 0.047907	Accuracy: 98.55%
9	Validation loss: 0.065852	Best loss: 0.047907	Accuracy: 98.16%
10	Validation loss: 0.056590	Best loss: 0.047907	Accuracy: 98.63%
11	Validation loss: 0.063016	Best loss: 0.047907	Accuracy: 98.44%
12	Validation loss: 0.057161	Best loss: 0.047907	Accuracy: 98.59%
13	Validation loss: 0.065622	Best loss: 0.047907	Accuracy: 98.63%
14	Validation loss: 0.078786	Best loss: 0.047907	Accuracy: 98.32%
15	Validation loss: 0.080158	Best loss: 0.047907	Accuracy: 98.28%
16	Validation loss: 0.054734	Best loss: 0.047907	Accuracy: 98.67%
17	Validation loss: 0.090661	Best loss: 0.047907	Accuracy: 98.20%
18	Validation loss: 0.065532	Best loss: 0.047907	Accuracy: 98.79%
19	Validation loss: 0.081188	Best loss: 0.047907	Accuracy: 98.32%
20	Validation loss: 0.066814	Best loss: 0.047907	Accuracy: 98.63%
21	Validation loss: 0.066974	Best loss: 0.047907	Accuracy: 98.44%
22	Validation loss: 0.087046	Best loss: 0.047907	Accuracy: 98.28%
23	Validation loss: 0.061944	Best loss: 0.047907	Accuracy: 98.79%
24	Validation loss: 0.063376	Best loss: 0.047907	Accuracy: 98.87%
25	Validation loss: 0.069778	Best loss: 0.047907	Accuracy: 98.40%
26	Validation loss: 0.067635	Best loss: 0.047907	Accuracy: 98.59%
27	Validation loss: 0.071639	Best loss: 0.047907	Accuracy: 98.40%
28	Validation loss: 0.063041	Best loss: 0.047907	Accuracy: 98.71%
29	Validation loss: 0.054198	Best loss: 0.047907	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  15.9s
[CV] n_neurons=90, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.124579	Best loss: 0.124579	Accuracy: 96.01%
1	Validation loss: 0.070235	Best loss: 0.070235	Accuracy: 97.69%
2	Validation loss: 0.055345	Best loss: 0.055345	Accuracy: 98.32%
3	Validation loss: 0.054439	Best loss: 0.054439	Accuracy: 98.40%
4	Validation loss: 0.042199	Best loss: 0.042199	Accuracy: 98.83%
5	Validation loss: 0.058649	Best loss: 0.042199	Accuracy: 98.12%
6	Validation loss: 0.047031	Best loss: 0.042199	Accuracy: 98.63%
7	Validation loss: 0.048121	Best loss: 0.042199	Accuracy: 98.28%
8	Validation loss: 0.061970	Best loss: 0.042199	Accuracy: 98.20%
9	Validation loss: 0.039378	Best loss: 0.039378	Accuracy: 99.02%
10	Validation loss: 0.063771	Best loss: 0.039378	Accuracy: 98.24%
11	Validation loss: 0.047200	Best loss: 0.039378	Accuracy: 98.67%
12	Validation loss: 0.052598	Best loss: 0.039378	Accuracy: 98.79%
13	Validation loss: 0.057120	Best loss: 0.039378	Accuracy: 98.83%
14	Validation loss: 0.066008	Best loss: 0.039378	Accuracy: 98.08%
15	Validation loss: 0.057681	Best loss: 0.039378	Accuracy: 98.63%
16	Validation loss: 0.058276	Best loss: 0.039378	Accuracy: 98.83%
17	Validation loss: 0.068966	Best loss: 0.039378	Accuracy: 98.55%
18	Validation loss: 0.059692	Best loss: 0.039378	Accuracy: 98.59%
19	Validation loss: 0.074158	Best loss: 0.039378	Accuracy: 98.40%
20	Validation loss: 0.053155	Best loss: 0.039378	Accuracy: 98.51%
21	Validation loss: 0.050926	Best loss: 0.039378	Accuracy: 98.79%
22	Validation loss: 0.055532	Best loss: 0.039378	Accuracy: 98.63%
23	Validation loss: 0.061710	Best loss: 0.039378	Accuracy: 98.44%
24	Validation loss: 0.056511	Best loss: 0.039378	Accuracy: 98.91%
25	Validation loss: 0.041661	Best loss: 0.039378	Accuracy: 99.06%
26	Validation loss: 0.057083	Best loss: 0.039378	Accuracy: 98.63%
27	Validation loss: 0.032506	Best loss: 0.032506	Accuracy: 99.02%
28	Validation loss: 0.055774	Best loss: 0.032506	Accuracy: 98.87%
29	Validation loss: 0.082362	Best loss: 0.032506	Accuracy: 98.48%
30	Validation loss: 0.042115	Best loss: 0.032506	Accuracy: 99.06%
31	Validation loss: 0.056243	Best loss: 0.032506	Accuracy: 98.79%
32	Validation loss: 0.053082	Best loss: 0.032506	Accuracy: 98.91%
33	Validation loss: 0.050908	Best loss: 0.032506	Accuracy: 98.71%
34	Validation loss: 0.071894	Best loss: 0.032506	Accuracy: 98.71%
35	Validation loss: 0.042555	Best loss: 0.032506	Accuracy: 99.10%
36	Validation loss: 0.046700	Best loss: 0.032506	Accuracy: 99.02%
37	Validation loss: 0.054115	Best loss: 0.032506	Accuracy: 99.06%
38	Validation loss: 0.071741	Best loss: 0.032506	Accuracy: 98.71%
39	Validation loss: 0.061903	Best loss: 0.032506	Accuracy: 98.79%
40	Validation loss: 0.075316	Best loss: 0.032506	Accuracy: 98.63%
41	Validation loss: 0.069101	Best loss: 0.032506	Accuracy: 98.75%
42	Validation loss: 0.054373	Best loss: 0.032506	Accuracy: 98.91%
43	Validation loss: 0.060497	Best loss: 0.032506	Accuracy: 99.06%
44	Validation loss: 0.054458	Best loss: 0.032506	Accuracy: 98.98%
45	Validation loss: 0.068454	Best loss: 0.032506	Accuracy: 98.79%
46	Validation loss: 0.049827	Best loss: 0.032506	Accuracy: 98.83%
47	Validation loss: 0.062940	Best loss: 0.032506	Accuracy: 98.91%
48	Validation loss: 0.070959	Best loss: 0.032506	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  24.4s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.062970	Best loss: 0.062970	Accuracy: 98.28%
1	Validation loss: 0.049347	Best loss: 0.049347	Accuracy: 98.48%
2	Validation loss: 0.048980	Best loss: 0.048980	Accuracy: 98.59%
3	Validation loss: 0.055675	Best loss: 0.048980	Accuracy: 98.44%
4	Validation loss: 0.031562	Best loss: 0.031562	Accuracy: 98.91%
5	Validation loss: 0.052759	Best loss: 0.031562	Accuracy: 98.75%
6	Validation loss: 0.042463	Best loss: 0.031562	Accuracy: 98.87%
7	Validation loss: 0.056963	Best loss: 0.031562	Accuracy: 98.44%
8	Validation loss: 0.048591	Best loss: 0.031562	Accuracy: 98.67%
9	Validation loss: 0.042107	Best loss: 0.031562	Accuracy: 98.87%
10	Validation loss: 0.041448	Best loss: 0.031562	Accuracy: 98.91%
11	Validation loss: 0.060694	Best loss: 0.031562	Accuracy: 98.63%
12	Validation loss: 0.040650	Best loss: 0.031562	Accuracy: 98.75%
13	Validation loss: 0.054244	Best loss: 0.031562	Accuracy: 98.83%
14	Validation loss: 0.054577	Best loss: 0.031562	Accuracy: 98.91%
15	Validation loss: 0.052831	Best loss: 0.031562	Accuracy: 98.75%
16	Validation loss: 0.055654	Best loss: 0.031562	Accuracy: 98.59%
17	Validation loss: 0.054106	Best loss: 0.031562	Accuracy: 98.98%
18	Validation loss: 0.058704	Best loss: 0.031562	Accuracy: 98.63%
19	Validation loss: 0.049628	Best loss: 0.031562	Accuracy: 98.94%
20	Validation loss: 0.039498	Best loss: 0.031562	Accuracy: 99.14%
21	Validation loss: 0.067366	Best loss: 0.031562	Accuracy: 98.79%
22	Validation loss: 0.044049	Best loss: 0.031562	Accuracy: 98.94%
23	Validation loss: 0.036566	Best loss: 0.031562	Accuracy: 99.14%
24	Validation loss: 0.046937	Best loss: 0.031562	Accuracy: 99.06%
25	Validation loss: 0.041144	Best loss: 0.031562	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function relu at 0x124366d08&gt;, total=  16.1s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.064760	Best loss: 0.064760	Accuracy: 98.01%
1	Validation loss: 0.047554	Best loss: 0.047554	Accuracy: 98.59%
2	Validation loss: 0.044101	Best loss: 0.044101	Accuracy: 98.71%
3	Validation loss: 0.040178	Best loss: 0.040178	Accuracy: 98.51%
4	Validation loss: 0.039474	Best loss: 0.039474	Accuracy: 98.79%
5	Validation loss: 0.050695	Best loss: 0.039474	Accuracy: 98.59%
6	Validation loss: 0.042233	Best loss: 0.039474	Accuracy: 98.87%
7	Validation loss: 0.054776	Best loss: 0.039474	Accuracy: 98.59%
8	Validation loss: 0.050740	Best loss: 0.039474	Accuracy: 98.91%
9	Validation loss: 0.041034	Best loss: 0.039474	Accuracy: 98.98%
10	Validation loss: 0.043248	Best loss: 0.039474	Accuracy: 98.91%
11	Validation loss: 0.053137	Best loss: 0.039474	Accuracy: 98.71%
12	Validation loss: 0.053686	Best loss: 0.039474	Accuracy: 98.83%
13	Validation loss: 0.043087	Best loss: 0.039474	Accuracy: 98.98%
14	Validation loss: 0.047614	Best loss: 0.039474	Accuracy: 98.83%
15	Validation loss: 0.047167	Best loss: 0.039474	Accuracy: 98.83%
16	Validation loss: 0.039667	Best loss: 0.039474	Accuracy: 99.26%
17	Validation loss: 0.042540	Best loss: 0.039474	Accuracy: 99.06%
18	Validation loss: 0.046792	Best loss: 0.039474	Accuracy: 99.06%
19	Validation loss: 0.037240	Best loss: 0.037240	Accuracy: 99.06%
20	Validation loss: 0.038321	Best loss: 0.037240	Accuracy: 98.94%
21	Validation loss: 0.063090	Best loss: 0.037240	Accuracy: 98.87%
22	Validation loss: 0.037704	Best loss: 0.037240	Accuracy: 99.10%
23	Validation loss: 0.045800	Best loss: 0.037240	Accuracy: 98.98%
24	Validation loss: 0.033781	Best loss: 0.033781	Accuracy: 99.14%
25	Validation loss: 0.038720	Best loss: 0.033781	Accuracy: 99.10%
26	Validation loss: 0.038354	Best loss: 0.033781	Accuracy: 99.22%
27	Validation loss: 0.035255	Best loss: 0.033781	Accuracy: 99.34%
28	Validation loss: 0.035117	Best loss: 0.033781	Accuracy: 99.41%
29	Validation loss: 0.034871	Best loss: 0.033781	Accuracy: 99.45%
30	Validation loss: 0.037357	Best loss: 0.033781	Accuracy: 99.41%
31	Validation loss: 0.036804	Best loss: 0.033781	Accuracy: 99.45%
32	Validation loss: 0.036029	Best loss: 0.033781	Accuracy: 99.41%
33	Validation loss: 0.037109	Best loss: 0.033781	Accuracy: 99.34%
34	Validation loss: 0.037982	Best loss: 0.033781	Accuracy: 99.30%
35	Validation loss: 0.037559	Best loss: 0.033781	Accuracy: 99.37%
36	Validation loss: 0.039609	Best loss: 0.033781	Accuracy: 99.37%
37	Validation loss: 0.038677	Best loss: 0.033781	Accuracy: 99.41%
38	Validation loss: 0.037917	Best loss: 0.033781	Accuracy: 99.37%
39	Validation loss: 0.038955	Best loss: 0.033781	Accuracy: 99.34%
40	Validation loss: 0.038351	Best loss: 0.033781	Accuracy: 99.30%
41	Validation loss: 0.038693	Best loss: 0.033781	Accuracy: 99.26%
42	Validation loss: 0.038217	Best loss: 0.033781	Accuracy: 99.41%
43	Validation loss: 0.040236	Best loss: 0.033781	Accuracy: 99.37%
44	Validation loss: 0.039917	Best loss: 0.033781	Accuracy: 99.34%
45	Validation loss: 0.040390	Best loss: 0.033781	Accuracy: 99.34%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function relu at 0x124366d08&gt;, total=  27.2s
[CV] n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.064403	Best loss: 0.064403	Accuracy: 97.85%
1	Validation loss: 0.052393	Best loss: 0.052393	Accuracy: 98.32%
2	Validation loss: 0.035763	Best loss: 0.035763	Accuracy: 99.06%
3	Validation loss: 0.038318	Best loss: 0.035763	Accuracy: 98.59%
4	Validation loss: 0.037001	Best loss: 0.035763	Accuracy: 98.94%
5	Validation loss: 0.050290	Best loss: 0.035763	Accuracy: 98.59%
6	Validation loss: 0.027322	Best loss: 0.027322	Accuracy: 99.06%
7	Validation loss: 0.043043	Best loss: 0.027322	Accuracy: 98.87%
8	Validation loss: 0.047759	Best loss: 0.027322	Accuracy: 98.94%
9	Validation loss: 0.057266	Best loss: 0.027322	Accuracy: 98.67%
10	Validation loss: 0.048572	Best loss: 0.027322	Accuracy: 98.83%
11	Validation loss: 0.047205	Best loss: 0.027322	Accuracy: 98.75%
12	Validation loss: 0.041259	Best loss: 0.027322	Accuracy: 98.87%
13	Validation loss: 0.039104	Best loss: 0.027322	Accuracy: 98.67%
14	Validation loss: 0.033377	Best loss: 0.027322	Accuracy: 99.02%
15	Validation loss: 0.035937	Best loss: 0.027322	Accuracy: 99.18%
16	Validation loss: 0.038107	Best loss: 0.027322	Accuracy: 99.10%
17	Validation loss: 0.034762	Best loss: 0.027322	Accuracy: 99.26%
18	Validation loss: 0.038912	Best loss: 0.027322	Accuracy: 99.14%
19	Validation loss: 0.047085	Best loss: 0.027322	Accuracy: 99.06%
20	Validation loss: 0.050832	Best loss: 0.027322	Accuracy: 98.94%
21	Validation loss: 0.051059	Best loss: 0.027322	Accuracy: 98.71%
22	Validation loss: 0.054538	Best loss: 0.027322	Accuracy: 98.87%
23	Validation loss: 0.043137	Best loss: 0.027322	Accuracy: 99.02%
24	Validation loss: 0.034122	Best loss: 0.027322	Accuracy: 99.30%
25	Validation loss: 0.058238	Best loss: 0.027322	Accuracy: 98.75%
26	Validation loss: 0.042851	Best loss: 0.027322	Accuracy: 98.98%
27	Validation loss: 0.033167	Best loss: 0.027322	Accuracy: 99.26%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function relu at 0x124366d08&gt;, total=  17.4s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.068986	Best loss: 0.068986	Accuracy: 98.24%
1	Validation loss: 0.057057	Best loss: 0.057057	Accuracy: 98.08%
2	Validation loss: 0.044101	Best loss: 0.044101	Accuracy: 98.83%
3	Validation loss: 0.045915	Best loss: 0.044101	Accuracy: 98.67%
4	Validation loss: 0.044016	Best loss: 0.044016	Accuracy: 98.79%
5	Validation loss: 0.085545	Best loss: 0.044016	Accuracy: 98.24%
6	Validation loss: 0.050668	Best loss: 0.044016	Accuracy: 98.51%
7	Validation loss: 0.053975	Best loss: 0.044016	Accuracy: 98.87%
8	Validation loss: 0.045320	Best loss: 0.044016	Accuracy: 98.79%
9	Validation loss: 0.038218	Best loss: 0.038218	Accuracy: 99.06%
10	Validation loss: 0.058289	Best loss: 0.038218	Accuracy: 98.75%
11	Validation loss: 0.065204	Best loss: 0.038218	Accuracy: 98.24%
12	Validation loss: 0.049027	Best loss: 0.038218	Accuracy: 98.94%
13	Validation loss: 0.045820	Best loss: 0.038218	Accuracy: 98.79%
14	Validation loss: 0.056839	Best loss: 0.038218	Accuracy: 98.79%
15	Validation loss: 0.041784	Best loss: 0.038218	Accuracy: 99.02%
16	Validation loss: 0.053760	Best loss: 0.038218	Accuracy: 98.94%
17	Validation loss: 0.039558	Best loss: 0.038218	Accuracy: 99.06%
18	Validation loss: 0.050676	Best loss: 0.038218	Accuracy: 99.10%
19	Validation loss: 0.044523	Best loss: 0.038218	Accuracy: 99.18%
20	Validation loss: 0.047179	Best loss: 0.038218	Accuracy: 99.06%
21	Validation loss: 0.045531	Best loss: 0.038218	Accuracy: 99.22%
22	Validation loss: 0.054464	Best loss: 0.038218	Accuracy: 98.94%
23	Validation loss: 0.062073	Best loss: 0.038218	Accuracy: 98.87%
24	Validation loss: 0.052883	Best loss: 0.038218	Accuracy: 98.75%
25	Validation loss: 0.053771	Best loss: 0.038218	Accuracy: 98.83%
26	Validation loss: 0.041284	Best loss: 0.038218	Accuracy: 99.02%
27	Validation loss: 0.047685	Best loss: 0.038218	Accuracy: 98.94%
28	Validation loss: 0.048663	Best loss: 0.038218	Accuracy: 99.06%
29	Validation loss: 0.044777	Best loss: 0.038218	Accuracy: 99.02%
30	Validation loss: 0.048360	Best loss: 0.038218	Accuracy: 99.10%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  21.4s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.061907	Best loss: 0.061907	Accuracy: 98.01%
1	Validation loss: 0.045662	Best loss: 0.045662	Accuracy: 98.44%
2	Validation loss: 0.053346	Best loss: 0.045662	Accuracy: 98.40%
3	Validation loss: 0.039772	Best loss: 0.039772	Accuracy: 98.71%
4	Validation loss: 0.038869	Best loss: 0.038869	Accuracy: 98.91%
5	Validation loss: 0.043452	Best loss: 0.038869	Accuracy: 98.71%
6	Validation loss: 0.043379	Best loss: 0.038869	Accuracy: 98.71%
7	Validation loss: 0.057232	Best loss: 0.038869	Accuracy: 98.63%
8	Validation loss: 0.046677	Best loss: 0.038869	Accuracy: 98.59%
9	Validation loss: 0.049054	Best loss: 0.038869	Accuracy: 98.91%
10	Validation loss: 0.053303	Best loss: 0.038869	Accuracy: 98.63%
11	Validation loss: 0.038820	Best loss: 0.038820	Accuracy: 98.98%
12	Validation loss: 0.041413	Best loss: 0.038820	Accuracy: 99.14%
13	Validation loss: 0.038993	Best loss: 0.038820	Accuracy: 99.06%
14	Validation loss: 0.074475	Best loss: 0.038820	Accuracy: 98.36%
15	Validation loss: 0.053864	Best loss: 0.038820	Accuracy: 98.51%
16	Validation loss: 0.035415	Best loss: 0.035415	Accuracy: 99.06%
17	Validation loss: 0.038997	Best loss: 0.035415	Accuracy: 98.98%
18	Validation loss: 0.046991	Best loss: 0.035415	Accuracy: 98.83%
19	Validation loss: 0.044733	Best loss: 0.035415	Accuracy: 98.94%
20	Validation loss: 0.070395	Best loss: 0.035415	Accuracy: 98.36%
21	Validation loss: 0.049718	Best loss: 0.035415	Accuracy: 98.87%
22	Validation loss: 0.065168	Best loss: 0.035415	Accuracy: 98.51%
23	Validation loss: 0.064985	Best loss: 0.035415	Accuracy: 98.71%
24	Validation loss: 0.057378	Best loss: 0.035415	Accuracy: 98.83%
25	Validation loss: 0.056261	Best loss: 0.035415	Accuracy: 98.91%
26	Validation loss: 0.051500	Best loss: 0.035415	Accuracy: 98.83%
27	Validation loss: 0.068900	Best loss: 0.035415	Accuracy: 98.75%
28	Validation loss: 0.058811	Best loss: 0.035415	Accuracy: 98.91%
29	Validation loss: 0.043477	Best loss: 0.035415	Accuracy: 98.83%
30	Validation loss: 0.040436	Best loss: 0.035415	Accuracy: 99.10%
31	Validation loss: 0.039846	Best loss: 0.035415	Accuracy: 99.18%
32	Validation loss: 0.052552	Best loss: 0.035415	Accuracy: 99.02%
33	Validation loss: 0.052853	Best loss: 0.035415	Accuracy: 99.02%
34	Validation loss: 0.051740	Best loss: 0.035415	Accuracy: 98.98%
35	Validation loss: 0.061483	Best loss: 0.035415	Accuracy: 98.91%
36	Validation loss: 0.046068	Best loss: 0.035415	Accuracy: 98.94%
37	Validation loss: 0.041208	Best loss: 0.035415	Accuracy: 99.22%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  25.6s
[CV] n_neurons=140, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.056071	Best loss: 0.056071	Accuracy: 98.20%
1	Validation loss: 0.050929	Best loss: 0.050929	Accuracy: 98.40%
2	Validation loss: 0.049077	Best loss: 0.049077	Accuracy: 98.32%
3	Validation loss: 0.055934	Best loss: 0.049077	Accuracy: 98.48%
4	Validation loss: 0.036707	Best loss: 0.036707	Accuracy: 98.87%
5	Validation loss: 0.068132	Best loss: 0.036707	Accuracy: 98.05%
6	Validation loss: 0.041808	Best loss: 0.036707	Accuracy: 98.71%
7	Validation loss: 0.042538	Best loss: 0.036707	Accuracy: 98.75%
8	Validation loss: 0.059260	Best loss: 0.036707	Accuracy: 98.71%
9	Validation loss: 0.042434	Best loss: 0.036707	Accuracy: 98.83%
10	Validation loss: 0.048804	Best loss: 0.036707	Accuracy: 98.75%
11	Validation loss: 0.045167	Best loss: 0.036707	Accuracy: 98.87%
12	Validation loss: 0.048727	Best loss: 0.036707	Accuracy: 98.98%
13	Validation loss: 0.041552	Best loss: 0.036707	Accuracy: 99.02%
14	Validation loss: 0.051907	Best loss: 0.036707	Accuracy: 98.91%
15	Validation loss: 0.049868	Best loss: 0.036707	Accuracy: 99.02%
16	Validation loss: 0.041858	Best loss: 0.036707	Accuracy: 99.10%
17	Validation loss: 0.045949	Best loss: 0.036707	Accuracy: 98.75%
18	Validation loss: 0.046992	Best loss: 0.036707	Accuracy: 98.75%
19	Validation loss: 0.049529	Best loss: 0.036707	Accuracy: 98.91%
20	Validation loss: 0.044298	Best loss: 0.036707	Accuracy: 99.02%
21	Validation loss: 0.039166	Best loss: 0.036707	Accuracy: 99.06%
22	Validation loss: 0.039637	Best loss: 0.036707	Accuracy: 99.06%
23	Validation loss: 0.035904	Best loss: 0.035904	Accuracy: 99.06%
24	Validation loss: 0.044795	Best loss: 0.035904	Accuracy: 99.10%
25	Validation loss: 0.036988	Best loss: 0.035904	Accuracy: 99.02%
26	Validation loss: 0.039572	Best loss: 0.035904	Accuracy: 98.83%
27	Validation loss: 0.039856	Best loss: 0.035904	Accuracy: 98.94%
28	Validation loss: 0.056018	Best loss: 0.035904	Accuracy: 98.94%
29	Validation loss: 0.092519	Best loss: 0.035904	Accuracy: 98.01%
30	Validation loss: 0.062936	Best loss: 0.035904	Accuracy: 98.75%
31	Validation loss: 0.038471	Best loss: 0.035904	Accuracy: 99.06%
32	Validation loss: 0.041914	Best loss: 0.035904	Accuracy: 99.14%
33	Validation loss: 0.042861	Best loss: 0.035904	Accuracy: 99.02%
34	Validation loss: 0.045383	Best loss: 0.035904	Accuracy: 99.18%
35	Validation loss: 0.038884	Best loss: 0.035904	Accuracy: 99.30%
36	Validation loss: 0.037020	Best loss: 0.035904	Accuracy: 99.14%
37	Validation loss: 0.035637	Best loss: 0.035637	Accuracy: 99.26%
38	Validation loss: 0.032902	Best loss: 0.032902	Accuracy: 99.18%
39	Validation loss: 0.039940	Best loss: 0.032902	Accuracy: 99.06%
40	Validation loss: 0.039040	Best loss: 0.032902	Accuracy: 99.10%
41	Validation loss: 0.037188	Best loss: 0.032902	Accuracy: 99.22%
42	Validation loss: 0.045574	Best loss: 0.032902	Accuracy: 99.10%
43	Validation loss: 0.043486	Best loss: 0.032902	Accuracy: 99.10%
44	Validation loss: 0.051896	Best loss: 0.032902	Accuracy: 98.79%
45	Validation loss: 0.053980	Best loss: 0.032902	Accuracy: 98.91%
46	Validation loss: 0.091338	Best loss: 0.032902	Accuracy: 98.40%
47	Validation loss: 0.041826	Best loss: 0.032902	Accuracy: 98.94%
48	Validation loss: 0.039590	Best loss: 0.032902	Accuracy: 99.10%
49	Validation loss: 0.054006	Best loss: 0.032902	Accuracy: 98.98%
50	Validation loss: 0.039090	Best loss: 0.032902	Accuracy: 98.94%
51	Validation loss: 0.034941	Best loss: 0.032902	Accuracy: 99.14%
52	Validation loss: 0.048104	Best loss: 0.032902	Accuracy: 98.98%
53	Validation loss: 0.036921	Best loss: 0.032902	Accuracy: 99.02%
54	Validation loss: 0.037573	Best loss: 0.032902	Accuracy: 99.18%
55	Validation loss: 0.033128	Best loss: 0.032902	Accuracy: 99.18%
56	Validation loss: 0.044930	Best loss: 0.032902	Accuracy: 99.06%
57	Validation loss: 0.037901	Best loss: 0.032902	Accuracy: 99.02%
58	Validation loss: 0.038008	Best loss: 0.032902	Accuracy: 99.18%
59	Validation loss: 0.039796	Best loss: 0.032902	Accuracy: 99.22%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  39.0s
[CV] n_neurons=160, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 2.300488	Best loss: 2.300488	Accuracy: 91.83%
1	Validation loss: 1.851069	Best loss: 1.851069	Accuracy: 85.93%
2	Validation loss: 0.667068	Best loss: 0.667068	Accuracy: 96.72%
3	Validation loss: 0.179180	Best loss: 0.179180	Accuracy: 97.65%
4	Validation loss: 1.061817	Best loss: 0.179180	Accuracy: 86.36%
5	Validation loss: 0.502553	Best loss: 0.179180	Accuracy: 96.44%
6	Validation loss: 0.304625	Best loss: 0.179180	Accuracy: 95.82%
7	Validation loss: 0.245393	Best loss: 0.179180	Accuracy: 97.65%
8	Validation loss: 0.255433	Best loss: 0.179180	Accuracy: 97.81%
9	Validation loss: 0.293631	Best loss: 0.179180	Accuracy: 97.97%
10	Validation loss: 0.307028	Best loss: 0.179180	Accuracy: 96.64%
11	Validation loss: 0.228172	Best loss: 0.179180	Accuracy: 97.07%
12	Validation loss: 0.229030	Best loss: 0.179180	Accuracy: 97.58%
13	Validation loss: 0.327047	Best loss: 0.179180	Accuracy: 96.13%
14	Validation loss: 0.246481	Best loss: 0.179180	Accuracy: 98.16%
15	Validation loss: 0.203085	Best loss: 0.179180	Accuracy: 97.77%
16	Validation loss: 0.183067	Best loss: 0.179180	Accuracy: 98.48%
17	Validation loss: 0.199080	Best loss: 0.179180	Accuracy: 97.97%
18	Validation loss: 1.714361	Best loss: 0.179180	Accuracy: 93.12%
19	Validation loss: 0.201118	Best loss: 0.179180	Accuracy: 98.36%
20	Validation loss: 0.163877	Best loss: 0.163877	Accuracy: 98.01%
21	Validation loss: 0.230531	Best loss: 0.163877	Accuracy: 97.81%
22	Validation loss: 0.362572	Best loss: 0.163877	Accuracy: 97.69%
23	Validation loss: 0.173123	Best loss: 0.163877	Accuracy: 98.51%
24	Validation loss: 0.233759	Best loss: 0.163877	Accuracy: 98.44%
25	Validation loss: 0.592181	Best loss: 0.163877	Accuracy: 96.48%
26	Validation loss: 0.292294	Best loss: 0.163877	Accuracy: 98.28%
27	Validation loss: 0.480207	Best loss: 0.163877	Accuracy: 96.79%
28	Validation loss: 1.000808	Best loss: 0.163877	Accuracy: 92.65%
29	Validation loss: 0.450910	Best loss: 0.163877	Accuracy: 97.42%
30	Validation loss: 0.408930	Best loss: 0.163877	Accuracy: 97.85%
31	Validation loss: 0.370641	Best loss: 0.163877	Accuracy: 98.40%
32	Validation loss: 0.291398	Best loss: 0.163877	Accuracy: 98.24%
33	Validation loss: 0.291863	Best loss: 0.163877	Accuracy: 98.51%
34	Validation loss: 0.344686	Best loss: 0.163877	Accuracy: 98.08%
35	Validation loss: 0.294546	Best loss: 0.163877	Accuracy: 98.32%
36	Validation loss: 0.169077	Best loss: 0.163877	Accuracy: 98.63%
37	Validation loss: 0.450494	Best loss: 0.163877	Accuracy: 98.01%
38	Validation loss: 0.282459	Best loss: 0.163877	Accuracy: 98.40%
39	Validation loss: 0.226547	Best loss: 0.163877	Accuracy: 98.71%
40	Validation loss: 0.343172	Best loss: 0.163877	Accuracy: 98.32%
41	Validation loss: 0.454132	Best loss: 0.163877	Accuracy: 97.89%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 4.7min
[CV] n_neurons=160, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 18.157303	Best loss: 18.157303	Accuracy: 74.71%
1	Validation loss: 2.285331	Best loss: 2.285331	Accuracy: 91.13%
2	Validation loss: 10.936816	Best loss: 2.285331	Accuracy: 78.30%
3	Validation loss: 0.835381	Best loss: 0.835381	Accuracy: 93.12%
4	Validation loss: 2.518460	Best loss: 0.835381	Accuracy: 79.83%
5	Validation loss: 9.370196	Best loss: 0.835381	Accuracy: 54.81%
6	Validation loss: 0.491397	Best loss: 0.491397	Accuracy: 96.56%
7	Validation loss: 0.177617	Best loss: 0.177617	Accuracy: 97.85%
8	Validation loss: 0.749180	Best loss: 0.177617	Accuracy: 95.78%
9	Validation loss: 1.032494	Best loss: 0.177617	Accuracy: 94.84%
10	Validation loss: 0.861525	Best loss: 0.177617	Accuracy: 90.58%
11	Validation loss: 0.414580	Best loss: 0.177617	Accuracy: 96.83%
12	Validation loss: 0.176810	Best loss: 0.176810	Accuracy: 98.28%
13	Validation loss: 0.134487	Best loss: 0.134487	Accuracy: 98.24%
14	Validation loss: 0.245141	Best loss: 0.134487	Accuracy: 97.58%
15	Validation loss: 0.482226	Best loss: 0.134487	Accuracy: 97.19%
16	Validation loss: 0.342680	Best loss: 0.134487	Accuracy: 97.69%
17	Validation loss: 0.673193	Best loss: 0.134487	Accuracy: 95.86%
18	Validation loss: 0.464125	Best loss: 0.134487	Accuracy: 97.11%
19	Validation loss: 0.424649	Best loss: 0.134487	Accuracy: 98.01%
20	Validation loss: 0.249642	Best loss: 0.134487	Accuracy: 98.51%
21	Validation loss: 0.193154	Best loss: 0.134487	Accuracy: 98.55%
22	Validation loss: 0.544063	Best loss: 0.134487	Accuracy: 95.97%
23	Validation loss: 0.285671	Best loss: 0.134487	Accuracy: 98.05%
24	Validation loss: 0.389124	Best loss: 0.134487	Accuracy: 97.54%
25	Validation loss: 0.204427	Best loss: 0.134487	Accuracy: 98.67%
26	Validation loss: 0.270836	Best loss: 0.134487	Accuracy: 97.15%
27	Validation loss: 0.273170	Best loss: 0.134487	Accuracy: 98.28%
28	Validation loss: 0.416163	Best loss: 0.134487	Accuracy: 98.16%
29	Validation loss: 0.252708	Best loss: 0.134487	Accuracy: 98.67%
30	Validation loss: 0.449781	Best loss: 0.134487	Accuracy: 97.46%
31	Validation loss: 0.246104	Best loss: 0.134487	Accuracy: 98.55%
32	Validation loss: 4.768617	Best loss: 0.134487	Accuracy: 89.09%
33	Validation loss: 0.259410	Best loss: 0.134487	Accuracy: 98.08%
34	Validation loss: 0.282520	Best loss: 0.134487	Accuracy: 98.71%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 3.9min
[CV] n_neurons=160, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 3.325453	Best loss: 3.325453	Accuracy: 85.89%
1	Validation loss: 3.581776	Best loss: 3.325453	Accuracy: 88.35%
2	Validation loss: 0.415021	Best loss: 0.415021	Accuracy: 94.06%
3	Validation loss: 0.599101	Best loss: 0.415021	Accuracy: 92.61%
4	Validation loss: 0.304752	Best loss: 0.304752	Accuracy: 97.97%
5	Validation loss: 0.487871	Best loss: 0.304752	Accuracy: 88.04%
6	Validation loss: 0.513810	Best loss: 0.304752	Accuracy: 96.87%
7	Validation loss: 0.144699	Best loss: 0.144699	Accuracy: 97.62%
8	Validation loss: 0.312479	Best loss: 0.144699	Accuracy: 96.91%
9	Validation loss: 0.444137	Best loss: 0.144699	Accuracy: 94.41%
10	Validation loss: 0.241456	Best loss: 0.144699	Accuracy: 97.50%
11	Validation loss: 0.146936	Best loss: 0.144699	Accuracy: 97.46%
12	Validation loss: 0.310078	Best loss: 0.144699	Accuracy: 97.03%
13	Validation loss: 0.287209	Best loss: 0.144699	Accuracy: 98.40%
14	Validation loss: 0.122329	Best loss: 0.122329	Accuracy: 98.20%
15	Validation loss: 1.019554	Best loss: 0.122329	Accuracy: 92.18%
16	Validation loss: 0.227403	Best loss: 0.122329	Accuracy: 98.08%
17	Validation loss: 0.129448	Best loss: 0.122329	Accuracy: 98.63%
18	Validation loss: 0.127651	Best loss: 0.122329	Accuracy: 98.40%
19	Validation loss: 0.417211	Best loss: 0.122329	Accuracy: 96.13%
20	Validation loss: 0.319147	Best loss: 0.122329	Accuracy: 96.99%
21	Validation loss: 0.261847	Best loss: 0.122329	Accuracy: 98.08%
22	Validation loss: 0.149629	Best loss: 0.122329	Accuracy: 98.67%
23	Validation loss: 0.200507	Best loss: 0.122329	Accuracy: 98.01%
24	Validation loss: 0.234281	Best loss: 0.122329	Accuracy: 97.54%
25	Validation loss: 0.495076	Best loss: 0.122329	Accuracy: 97.58%
26	Validation loss: 0.171854	Best loss: 0.122329	Accuracy: 98.55%
27	Validation loss: 0.356671	Best loss: 0.122329	Accuracy: 95.66%
28	Validation loss: 0.203489	Best loss: 0.122329	Accuracy: 98.67%
29	Validation loss: 0.277103	Best loss: 0.122329	Accuracy: 98.48%
30	Validation loss: 0.487818	Best loss: 0.122329	Accuracy: 96.36%
31	Validation loss: 0.971606	Best loss: 0.122329	Accuracy: 95.62%
32	Validation loss: 0.243590	Best loss: 0.122329	Accuracy: 98.36%
33	Validation loss: 0.182903	Best loss: 0.122329	Accuracy: 98.83%
34	Validation loss: 0.451444	Best loss: 0.122329	Accuracy: 97.58%
35	Validation loss: 0.765352	Best loss: 0.122329	Accuracy: 94.18%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 4.0min
[CV] n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.116881	Best loss: 0.116881	Accuracy: 97.38%
1	Validation loss: 0.113301	Best loss: 0.113301	Accuracy: 97.58%
2	Validation loss: 0.067235	Best loss: 0.067235	Accuracy: 98.20%
3	Validation loss: 0.064038	Best loss: 0.064038	Accuracy: 98.24%
4	Validation loss: 0.055909	Best loss: 0.055909	Accuracy: 98.51%
5	Validation loss: 0.057928	Best loss: 0.055909	Accuracy: 98.40%
6	Validation loss: 0.050541	Best loss: 0.050541	Accuracy: 98.67%
7	Validation loss: 0.041605	Best loss: 0.041605	Accuracy: 98.91%
8	Validation loss: 0.044189	Best loss: 0.041605	Accuracy: 98.79%
9	Validation loss: 0.050768	Best loss: 0.041605	Accuracy: 98.98%
10	Validation loss: 0.058314	Best loss: 0.041605	Accuracy: 98.67%
11	Validation loss: 0.066489	Best loss: 0.041605	Accuracy: 98.87%
12	Validation loss: 0.036958	Best loss: 0.036958	Accuracy: 99.22%
13	Validation loss: 0.072282	Best loss: 0.036958	Accuracy: 98.36%
14	Validation loss: 0.060847	Best loss: 0.036958	Accuracy: 98.48%
15	Validation loss: 0.054519	Best loss: 0.036958	Accuracy: 98.59%
16	Validation loss: 0.043179	Best loss: 0.036958	Accuracy: 99.10%
17	Validation loss: 0.063270	Best loss: 0.036958	Accuracy: 98.59%
18	Validation loss: 0.045353	Best loss: 0.036958	Accuracy: 99.10%
19	Validation loss: 0.086362	Best loss: 0.036958	Accuracy: 98.44%
20	Validation loss: 0.058158	Best loss: 0.036958	Accuracy: 99.06%
21	Validation loss: 0.073636	Best loss: 0.036958	Accuracy: 98.28%
22	Validation loss: 0.057806	Best loss: 0.036958	Accuracy: 98.59%
23	Validation loss: 0.057847	Best loss: 0.036958	Accuracy: 98.59%
24	Validation loss: 0.049136	Best loss: 0.036958	Accuracy: 98.98%
25	Validation loss: 0.037014	Best loss: 0.036958	Accuracy: 99.26%
26	Validation loss: 0.057429	Best loss: 0.036958	Accuracy: 98.94%
27	Validation loss: 0.049060	Best loss: 0.036958	Accuracy: 99.14%
28	Validation loss: 0.049746	Best loss: 0.036958	Accuracy: 99.10%
29	Validation loss: 0.058774	Best loss: 0.036958	Accuracy: 98.75%
30	Validation loss: 0.063465	Best loss: 0.036958	Accuracy: 98.71%
31	Validation loss: 0.077810	Best loss: 0.036958	Accuracy: 98.51%
32	Validation loss: 0.046624	Best loss: 0.036958	Accuracy: 98.94%
33	Validation loss: 0.046560	Best loss: 0.036958	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  38.6s
[CV] n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.159744	Best loss: 0.159744	Accuracy: 96.05%
1	Validation loss: 0.059483	Best loss: 0.059483	Accuracy: 98.48%
2	Validation loss: 0.061590	Best loss: 0.059483	Accuracy: 98.28%
3	Validation loss: 0.082989	Best loss: 0.059483	Accuracy: 97.77%
4	Validation loss: 0.055550	Best loss: 0.055550	Accuracy: 98.71%
5	Validation loss: 0.050351	Best loss: 0.050351	Accuracy: 98.79%
6	Validation loss: 0.057822	Best loss: 0.050351	Accuracy: 98.59%
7	Validation loss: 0.055778	Best loss: 0.050351	Accuracy: 98.63%
8	Validation loss: 0.060553	Best loss: 0.050351	Accuracy: 98.79%
9	Validation loss: 0.044725	Best loss: 0.044725	Accuracy: 99.06%
10	Validation loss: 0.042214	Best loss: 0.042214	Accuracy: 98.98%
11	Validation loss: 0.043474	Best loss: 0.042214	Accuracy: 98.75%
12	Validation loss: 0.041426	Best loss: 0.041426	Accuracy: 99.06%
13	Validation loss: 0.041252	Best loss: 0.041252	Accuracy: 98.91%
14	Validation loss: 0.049815	Best loss: 0.041252	Accuracy: 99.10%
15	Validation loss: 0.046722	Best loss: 0.041252	Accuracy: 99.02%
16	Validation loss: 0.050094	Best loss: 0.041252	Accuracy: 98.83%
17	Validation loss: 0.076182	Best loss: 0.041252	Accuracy: 98.40%
18	Validation loss: 0.054175	Best loss: 0.041252	Accuracy: 98.63%
19	Validation loss: 0.048481	Best loss: 0.041252	Accuracy: 98.91%
20	Validation loss: 0.052542	Best loss: 0.041252	Accuracy: 98.79%
21	Validation loss: 0.059503	Best loss: 0.041252	Accuracy: 98.55%
22	Validation loss: 0.052550	Best loss: 0.041252	Accuracy: 98.91%
23	Validation loss: 0.056401	Best loss: 0.041252	Accuracy: 98.83%
24	Validation loss: 0.034443	Best loss: 0.034443	Accuracy: 99.14%
25	Validation loss: 0.081657	Best loss: 0.034443	Accuracy: 98.63%
26	Validation loss: 0.049939	Best loss: 0.034443	Accuracy: 99.18%
27	Validation loss: 0.051103	Best loss: 0.034443	Accuracy: 98.87%
28	Validation loss: 0.052843	Best loss: 0.034443	Accuracy: 98.83%
29	Validation loss: 0.070888	Best loss: 0.034443	Accuracy: 98.59%
30	Validation loss: 0.050819	Best loss: 0.034443	Accuracy: 99.14%
31	Validation loss: 0.051518	Best loss: 0.034443	Accuracy: 98.91%
32	Validation loss: 0.067261	Best loss: 0.034443	Accuracy: 98.71%
33	Validation loss: 0.039147	Best loss: 0.034443	Accuracy: 99.37%
34	Validation loss: 0.054557	Best loss: 0.034443	Accuracy: 98.98%
35	Validation loss: 0.041452	Best loss: 0.034443	Accuracy: 99.18%
36	Validation loss: 0.046691	Best loss: 0.034443	Accuracy: 99.06%
37	Validation loss: 0.051975	Best loss: 0.034443	Accuracy: 99.06%
38	Validation loss: 0.048794	Best loss: 0.034443	Accuracy: 98.94%
39	Validation loss: 0.058887	Best loss: 0.034443	Accuracy: 98.91%
40	Validation loss: 0.078316	Best loss: 0.034443	Accuracy: 98.36%
41	Validation loss: 0.058714	Best loss: 0.034443	Accuracy: 99.02%
42	Validation loss: 0.052793	Best loss: 0.034443	Accuracy: 98.83%
43	Validation loss: 0.082276	Best loss: 0.034443	Accuracy: 98.59%
44	Validation loss: 0.055957	Best loss: 0.034443	Accuracy: 99.10%
45	Validation loss: 0.067005	Best loss: 0.034443	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  53.7s
[CV] n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.177274	Best loss: 0.177274	Accuracy: 96.48%
1	Validation loss: 0.079565	Best loss: 0.079565	Accuracy: 98.08%
2	Validation loss: 0.046937	Best loss: 0.046937	Accuracy: 98.55%
3	Validation loss: 0.066213	Best loss: 0.046937	Accuracy: 98.28%
4	Validation loss: 0.047079	Best loss: 0.046937	Accuracy: 98.87%
5	Validation loss: 0.054665	Best loss: 0.046937	Accuracy: 98.59%
6	Validation loss: 0.057793	Best loss: 0.046937	Accuracy: 98.79%
7	Validation loss: 0.048028	Best loss: 0.046937	Accuracy: 98.67%
8	Validation loss: 0.064783	Best loss: 0.046937	Accuracy: 98.24%
9	Validation loss: 0.056563	Best loss: 0.046937	Accuracy: 98.71%
10	Validation loss: 0.046672	Best loss: 0.046672	Accuracy: 98.83%
11	Validation loss: 0.059406	Best loss: 0.046672	Accuracy: 98.51%
12	Validation loss: 0.042801	Best loss: 0.042801	Accuracy: 98.91%
13	Validation loss: 0.058400	Best loss: 0.042801	Accuracy: 98.67%
14	Validation loss: 0.054672	Best loss: 0.042801	Accuracy: 98.94%
15	Validation loss: 0.040994	Best loss: 0.040994	Accuracy: 99.10%
16	Validation loss: 0.049519	Best loss: 0.040994	Accuracy: 98.98%
17	Validation loss: 0.076864	Best loss: 0.040994	Accuracy: 98.20%
18	Validation loss: 0.045311	Best loss: 0.040994	Accuracy: 99.02%
19	Validation loss: 0.061308	Best loss: 0.040994	Accuracy: 98.67%
20	Validation loss: 0.099838	Best loss: 0.040994	Accuracy: 98.48%
21	Validation loss: 0.069589	Best loss: 0.040994	Accuracy: 98.59%
22	Validation loss: 0.060117	Best loss: 0.040994	Accuracy: 98.91%
23	Validation loss: 0.051194	Best loss: 0.040994	Accuracy: 98.98%
24	Validation loss: 0.060593	Best loss: 0.040994	Accuracy: 98.71%
25	Validation loss: 0.052369	Best loss: 0.040994	Accuracy: 98.98%
26	Validation loss: 0.036876	Best loss: 0.036876	Accuracy: 99.22%
27	Validation loss: 0.070400	Best loss: 0.036876	Accuracy: 98.67%
28	Validation loss: 0.049478	Best loss: 0.036876	Accuracy: 99.02%
29	Validation loss: 0.062496	Best loss: 0.036876	Accuracy: 98.87%
30	Validation loss: 0.051598	Best loss: 0.036876	Accuracy: 99.18%
31	Validation loss: 0.053588	Best loss: 0.036876	Accuracy: 98.91%
32	Validation loss: 0.043699	Best loss: 0.036876	Accuracy: 99.02%
33	Validation loss: 0.049017	Best loss: 0.036876	Accuracy: 98.79%
34	Validation loss: 0.059701	Best loss: 0.036876	Accuracy: 98.91%
35	Validation loss: 0.045645	Best loss: 0.036876	Accuracy: 98.91%
36	Validation loss: 0.042123	Best loss: 0.036876	Accuracy: 99.06%
37	Validation loss: 0.037727	Best loss: 0.036876	Accuracy: 99.10%
38	Validation loss: 0.053799	Best loss: 0.036876	Accuracy: 98.83%
39	Validation loss: 0.044970	Best loss: 0.036876	Accuracy: 99.02%
40	Validation loss: 0.047377	Best loss: 0.036876	Accuracy: 99.14%
41	Validation loss: 0.042084	Best loss: 0.036876	Accuracy: 98.98%
42	Validation loss: 0.048833	Best loss: 0.036876	Accuracy: 98.87%
43	Validation loss: 0.055677	Best loss: 0.036876	Accuracy: 99.10%
44	Validation loss: 0.042032	Best loss: 0.036876	Accuracy: 99.06%
45	Validation loss: 0.051592	Best loss: 0.036876	Accuracy: 98.94%
46	Validation loss: 0.062324	Best loss: 0.036876	Accuracy: 98.94%
47	Validation loss: 0.034839	Best loss: 0.034839	Accuracy: 99.02%
48	Validation loss: 0.070517	Best loss: 0.034839	Accuracy: 98.67%
49	Validation loss: 0.040662	Best loss: 0.034839	Accuracy: 99.10%
50	Validation loss: 0.050308	Best loss: 0.034839	Accuracy: 98.87%
51	Validation loss: 0.041680	Best loss: 0.034839	Accuracy: 99.02%
52	Validation loss: 0.044566	Best loss: 0.034839	Accuracy: 99.02%
53	Validation loss: 0.040470	Best loss: 0.034839	Accuracy: 99.06%
54	Validation loss: 0.053002	Best loss: 0.034839	Accuracy: 98.94%
55	Validation loss: 0.042057	Best loss: 0.034839	Accuracy: 99.10%
56	Validation loss: 0.047180	Best loss: 0.034839	Accuracy: 98.94%
57	Validation loss: 0.043889	Best loss: 0.034839	Accuracy: 99.06%
58	Validation loss: 0.042835	Best loss: 0.034839	Accuracy: 99.02%
59	Validation loss: 0.058223	Best loss: 0.034839	Accuracy: 98.91%
60	Validation loss: 0.046671	Best loss: 0.034839	Accuracy: 98.87%
61	Validation loss: 0.042566	Best loss: 0.034839	Accuracy: 99.02%
62	Validation loss: 0.049150	Best loss: 0.034839	Accuracy: 99.02%
63	Validation loss: 0.061433	Best loss: 0.034839	Accuracy: 98.98%
64	Validation loss: 0.172256	Best loss: 0.034839	Accuracy: 98.16%
65	Validation loss: 0.040733	Best loss: 0.034839	Accuracy: 99.18%
66	Validation loss: 0.055265	Best loss: 0.034839	Accuracy: 98.87%
67	Validation loss: 0.038646	Best loss: 0.034839	Accuracy: 99.30%
68	Validation loss: 0.069608	Best loss: 0.034839	Accuracy: 98.55%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.3min
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.193002	Best loss: 0.193002	Accuracy: 93.82%
1	Validation loss: 0.124403	Best loss: 0.124403	Accuracy: 95.97%
2	Validation loss: 0.143254	Best loss: 0.124403	Accuracy: 96.21%
3	Validation loss: 0.134828	Best loss: 0.124403	Accuracy: 96.25%
4	Validation loss: 0.098745	Best loss: 0.098745	Accuracy: 97.15%
5	Validation loss: 0.128652	Best loss: 0.098745	Accuracy: 96.01%
6	Validation loss: 0.107418	Best loss: 0.098745	Accuracy: 97.26%
7	Validation loss: 0.069652	Best loss: 0.069652	Accuracy: 98.05%
8	Validation loss: 0.225805	Best loss: 0.069652	Accuracy: 94.25%
9	Validation loss: 0.076507	Best loss: 0.069652	Accuracy: 97.85%
10	Validation loss: 0.066040	Best loss: 0.066040	Accuracy: 97.97%
11	Validation loss: 0.052393	Best loss: 0.052393	Accuracy: 98.28%
12	Validation loss: 0.112249	Best loss: 0.052393	Accuracy: 96.99%
13	Validation loss: 0.064390	Best loss: 0.052393	Accuracy: 98.01%
14	Validation loss: 0.053466	Best loss: 0.052393	Accuracy: 98.48%
15	Validation loss: 0.066361	Best loss: 0.052393	Accuracy: 98.55%
16	Validation loss: 0.075517	Best loss: 0.052393	Accuracy: 98.08%
17	Validation loss: 0.098161	Best loss: 0.052393	Accuracy: 97.85%
18	Validation loss: 0.061083	Best loss: 0.052393	Accuracy: 98.32%
19	Validation loss: 0.045010	Best loss: 0.045010	Accuracy: 98.83%
20	Validation loss: 0.117220	Best loss: 0.045010	Accuracy: 97.11%
21	Validation loss: 0.070771	Best loss: 0.045010	Accuracy: 98.32%
22	Validation loss: 0.064627	Best loss: 0.045010	Accuracy: 98.59%
23	Validation loss: 0.075540	Best loss: 0.045010	Accuracy: 98.51%
24	Validation loss: 0.093279	Best loss: 0.045010	Accuracy: 97.22%
25	Validation loss: 0.107271	Best loss: 0.045010	Accuracy: 97.81%
26	Validation loss: 0.060603	Best loss: 0.045010	Accuracy: 98.83%
27	Validation loss: 0.092732	Best loss: 0.045010	Accuracy: 98.20%
28	Validation loss: 0.073431	Best loss: 0.045010	Accuracy: 98.51%
29	Validation loss: 0.080709	Best loss: 0.045010	Accuracy: 98.28%
30	Validation loss: 0.064300	Best loss: 0.045010	Accuracy: 98.36%
31	Validation loss: 0.069587	Best loss: 0.045010	Accuracy: 98.24%
32	Validation loss: 0.056186	Best loss: 0.045010	Accuracy: 98.67%
33	Validation loss: 0.047607	Best loss: 0.045010	Accuracy: 98.83%
34	Validation loss: 0.049625	Best loss: 0.045010	Accuracy: 98.91%
35	Validation loss: 0.063654	Best loss: 0.045010	Accuracy: 98.55%
36	Validation loss: 0.064788	Best loss: 0.045010	Accuracy: 98.40%
37	Validation loss: 0.084687	Best loss: 0.045010	Accuracy: 98.05%
38	Validation loss: 0.060310	Best loss: 0.045010	Accuracy: 98.67%
39	Validation loss: 0.069918	Best loss: 0.045010	Accuracy: 98.24%
40	Validation loss: 0.063707	Best loss: 0.045010	Accuracy: 98.44%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 3.8min
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.215571	Best loss: 0.215571	Accuracy: 92.96%
1	Validation loss: 0.121244	Best loss: 0.121244	Accuracy: 96.56%
2	Validation loss: 0.090933	Best loss: 0.090933	Accuracy: 97.22%
3	Validation loss: 0.068561	Best loss: 0.068561	Accuracy: 97.65%
4	Validation loss: 0.079919	Best loss: 0.068561	Accuracy: 97.97%
5	Validation loss: 0.112067	Best loss: 0.068561	Accuracy: 97.42%
6	Validation loss: 0.079749	Best loss: 0.068561	Accuracy: 97.69%
7	Validation loss: 0.087834	Best loss: 0.068561	Accuracy: 97.81%
8	Validation loss: 0.083439	Best loss: 0.068561	Accuracy: 97.85%
9	Validation loss: 0.061827	Best loss: 0.061827	Accuracy: 98.32%
10	Validation loss: 0.072085	Best loss: 0.061827	Accuracy: 97.93%
11	Validation loss: 0.070991	Best loss: 0.061827	Accuracy: 98.44%
12	Validation loss: 0.096150	Best loss: 0.061827	Accuracy: 97.93%
13	Validation loss: 0.063694	Best loss: 0.061827	Accuracy: 98.08%
14	Validation loss: 0.136559	Best loss: 0.061827	Accuracy: 97.07%
15	Validation loss: 0.085911	Best loss: 0.061827	Accuracy: 97.58%
16	Validation loss: 0.095746	Best loss: 0.061827	Accuracy: 97.46%
17	Validation loss: 0.097060	Best loss: 0.061827	Accuracy: 98.24%
18	Validation loss: 0.096296	Best loss: 0.061827	Accuracy: 98.05%
19	Validation loss: 0.060182	Best loss: 0.060182	Accuracy: 98.55%
20	Validation loss: 0.131692	Best loss: 0.060182	Accuracy: 97.38%
21	Validation loss: 0.077866	Best loss: 0.060182	Accuracy: 98.12%
22	Validation loss: 0.076401	Best loss: 0.060182	Accuracy: 97.65%
23	Validation loss: 0.088864	Best loss: 0.060182	Accuracy: 97.93%
24	Validation loss: 0.160973	Best loss: 0.060182	Accuracy: 96.83%
25	Validation loss: 0.064515	Best loss: 0.060182	Accuracy: 98.44%
26	Validation loss: 0.073214	Best loss: 0.060182	Accuracy: 98.16%
27	Validation loss: 0.081900	Best loss: 0.060182	Accuracy: 98.05%
28	Validation loss: 0.108812	Best loss: 0.060182	Accuracy: 97.15%
29	Validation loss: 0.065056	Best loss: 0.060182	Accuracy: 98.71%
30	Validation loss: 0.125545	Best loss: 0.060182	Accuracy: 97.07%
31	Validation loss: 0.153506	Best loss: 0.060182	Accuracy: 97.65%
32	Validation loss: 0.073483	Best loss: 0.060182	Accuracy: 98.79%
33	Validation loss: 0.065086	Best loss: 0.060182	Accuracy: 98.51%
34	Validation loss: 0.065474	Best loss: 0.060182	Accuracy: 98.79%
35	Validation loss: 0.084384	Best loss: 0.060182	Accuracy: 98.16%
36	Validation loss: 0.074254	Best loss: 0.060182	Accuracy: 98.40%
37	Validation loss: 0.085009	Best loss: 0.060182	Accuracy: 98.12%
38	Validation loss: 0.080351	Best loss: 0.060182	Accuracy: 98.44%
39	Validation loss: 0.059750	Best loss: 0.059750	Accuracy: 98.75%
40	Validation loss: 0.078271	Best loss: 0.059750	Accuracy: 98.36%
41	Validation loss: 0.189304	Best loss: 0.059750	Accuracy: 97.11%
42	Validation loss: 0.067018	Best loss: 0.059750	Accuracy: 98.40%
43	Validation loss: 0.048036	Best loss: 0.048036	Accuracy: 98.63%
44	Validation loss: 0.094523	Best loss: 0.048036	Accuracy: 97.46%
45	Validation loss: 0.110803	Best loss: 0.048036	Accuracy: 97.46%
46	Validation loss: 0.066838	Best loss: 0.048036	Accuracy: 98.67%
47	Validation loss: 0.054518	Best loss: 0.048036	Accuracy: 98.83%
48	Validation loss: 0.073333	Best loss: 0.048036	Accuracy: 98.71%
49	Validation loss: 0.063787	Best loss: 0.048036	Accuracy: 98.67%
50	Validation loss: 0.066133	Best loss: 0.048036	Accuracy: 98.71%
51	Validation loss: 0.049425	Best loss: 0.048036	Accuracy: 98.91%
52	Validation loss: 0.053663	Best loss: 0.048036	Accuracy: 98.48%
53	Validation loss: 0.059297	Best loss: 0.048036	Accuracy: 98.55%
54	Validation loss: 0.090946	Best loss: 0.048036	Accuracy: 97.65%
55	Validation loss: 0.068615	Best loss: 0.048036	Accuracy: 98.01%
56	Validation loss: 0.068171	Best loss: 0.048036	Accuracy: 98.28%
57	Validation loss: 0.088919	Best loss: 0.048036	Accuracy: 98.08%
58	Validation loss: 0.055852	Best loss: 0.048036	Accuracy: 98.94%
59	Validation loss: 0.067493	Best loss: 0.048036	Accuracy: 98.51%
60	Validation loss: 0.060790	Best loss: 0.048036	Accuracy: 98.71%
61	Validation loss: 0.072283	Best loss: 0.048036	Accuracy: 98.59%
62	Validation loss: 0.071341	Best loss: 0.048036	Accuracy: 98.75%
63	Validation loss: 0.061646	Best loss: 0.048036	Accuracy: 98.71%
64	Validation loss: 0.076704	Best loss: 0.048036	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 6.0min
[CV] n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.118095	Best loss: 0.118095	Accuracy: 96.33%
1	Validation loss: 0.091915	Best loss: 0.091915	Accuracy: 97.07%
2	Validation loss: 0.098832	Best loss: 0.091915	Accuracy: 97.46%
3	Validation loss: 0.083666	Best loss: 0.083666	Accuracy: 97.58%
4	Validation loss: 0.416584	Best loss: 0.083666	Accuracy: 85.26%
5	Validation loss: 0.076916	Best loss: 0.076916	Accuracy: 97.77%
6	Validation loss: 0.082147	Best loss: 0.076916	Accuracy: 97.81%
7	Validation loss: 0.064736	Best loss: 0.064736	Accuracy: 98.32%
8	Validation loss: 0.065548	Best loss: 0.064736	Accuracy: 98.12%
9	Validation loss: 0.070329	Best loss: 0.064736	Accuracy: 98.36%
10	Validation loss: 0.129267	Best loss: 0.064736	Accuracy: 96.56%
11	Validation loss: 0.072291	Best loss: 0.064736	Accuracy: 98.20%
12	Validation loss: 0.057347	Best loss: 0.057347	Accuracy: 98.36%
13	Validation loss: 0.068222	Best loss: 0.057347	Accuracy: 98.51%
14	Validation loss: 0.073819	Best loss: 0.057347	Accuracy: 98.24%
15	Validation loss: 0.062313	Best loss: 0.057347	Accuracy: 98.24%
16	Validation loss: 0.068477	Best loss: 0.057347	Accuracy: 98.71%
17	Validation loss: 0.063981	Best loss: 0.057347	Accuracy: 98.20%
18	Validation loss: 0.054725	Best loss: 0.054725	Accuracy: 98.59%
19	Validation loss: 0.168288	Best loss: 0.054725	Accuracy: 96.87%
20	Validation loss: 0.066909	Best loss: 0.054725	Accuracy: 98.20%
21	Validation loss: 0.086144	Best loss: 0.054725	Accuracy: 97.85%
22	Validation loss: 0.048573	Best loss: 0.048573	Accuracy: 98.44%
23	Validation loss: 0.066691	Best loss: 0.048573	Accuracy: 98.48%
24	Validation loss: 0.084009	Best loss: 0.048573	Accuracy: 98.32%
25	Validation loss: 0.069358	Best loss: 0.048573	Accuracy: 98.40%
26	Validation loss: 0.042341	Best loss: 0.042341	Accuracy: 98.94%
27	Validation loss: 0.040652	Best loss: 0.040652	Accuracy: 98.75%
28	Validation loss: 0.059000	Best loss: 0.040652	Accuracy: 98.98%
29	Validation loss: 0.061555	Best loss: 0.040652	Accuracy: 98.36%
30	Validation loss: 0.080103	Best loss: 0.040652	Accuracy: 98.28%
31	Validation loss: 0.064120	Best loss: 0.040652	Accuracy: 98.32%
32	Validation loss: 0.105966	Best loss: 0.040652	Accuracy: 97.42%
33	Validation loss: 0.077815	Best loss: 0.040652	Accuracy: 97.97%
34	Validation loss: 0.116279	Best loss: 0.040652	Accuracy: 97.58%
35	Validation loss: 0.064545	Best loss: 0.040652	Accuracy: 98.32%
36	Validation loss: 0.070368	Best loss: 0.040652	Accuracy: 98.08%
37	Validation loss: 0.044276	Best loss: 0.040652	Accuracy: 98.75%
38	Validation loss: 0.068963	Best loss: 0.040652	Accuracy: 98.71%
39	Validation loss: 0.041946	Best loss: 0.040652	Accuracy: 98.79%
40	Validation loss: 0.050144	Best loss: 0.040652	Accuracy: 98.67%
41	Validation loss: 0.176306	Best loss: 0.040652	Accuracy: 98.16%
42	Validation loss: 0.251599	Best loss: 0.040652	Accuracy: 97.03%
43	Validation loss: 0.036810	Best loss: 0.036810	Accuracy: 98.98%
44	Validation loss: 0.042096	Best loss: 0.036810	Accuracy: 98.83%
45	Validation loss: 0.062362	Best loss: 0.036810	Accuracy: 98.48%
46	Validation loss: 0.070111	Best loss: 0.036810	Accuracy: 98.36%
47	Validation loss: 0.039598	Best loss: 0.036810	Accuracy: 99.10%
48	Validation loss: 0.061490	Best loss: 0.036810	Accuracy: 98.79%
49	Validation loss: 0.067361	Best loss: 0.036810	Accuracy: 98.55%
50	Validation loss: 0.908360	Best loss: 0.036810	Accuracy: 91.28%
51	Validation loss: 0.041864	Best loss: 0.036810	Accuracy: 98.94%
52	Validation loss: 0.059423	Best loss: 0.036810	Accuracy: 98.36%
53	Validation loss: 0.070846	Best loss: 0.036810	Accuracy: 98.36%
54	Validation loss: 0.058700	Best loss: 0.036810	Accuracy: 98.67%
55	Validation loss: 0.059769	Best loss: 0.036810	Accuracy: 98.63%
56	Validation loss: 0.066742	Best loss: 0.036810	Accuracy: 98.63%
57	Validation loss: 0.052277	Best loss: 0.036810	Accuracy: 98.55%
58	Validation loss: 0.065629	Best loss: 0.036810	Accuracy: 98.63%
59	Validation loss: 0.040338	Best loss: 0.036810	Accuracy: 99.02%
60	Validation loss: 0.063694	Best loss: 0.036810	Accuracy: 98.40%
61	Validation loss: 0.052377	Best loss: 0.036810	Accuracy: 98.87%
62	Validation loss: 0.047924	Best loss: 0.036810	Accuracy: 98.98%
63	Validation loss: 0.065418	Best loss: 0.036810	Accuracy: 98.91%
64	Validation loss: 0.052296	Best loss: 0.036810	Accuracy: 99.06%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.9, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 6.0min
[CV] n_neurons=140, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.415261	Best loss: 0.415261	Accuracy: 96.68%
1	Validation loss: 0.570930	Best loss: 0.415261	Accuracy: 91.71%
2	Validation loss: 0.276308	Best loss: 0.276308	Accuracy: 95.39%
3	Validation loss: 0.154553	Best loss: 0.154553	Accuracy: 96.09%
4	Validation loss: 1.460882	Best loss: 0.154553	Accuracy: 84.64%
5	Validation loss: 0.260863	Best loss: 0.154553	Accuracy: 93.71%
6	Validation loss: 0.080811	Best loss: 0.080811	Accuracy: 97.85%
7	Validation loss: 0.081522	Best loss: 0.080811	Accuracy: 97.97%
8	Validation loss: 0.714614	Best loss: 0.080811	Accuracy: 89.99%
9	Validation loss: 0.078034	Best loss: 0.078034	Accuracy: 98.32%
10	Validation loss: 0.072744	Best loss: 0.072744	Accuracy: 98.24%
11	Validation loss: 0.215586	Best loss: 0.072744	Accuracy: 96.87%
12	Validation loss: 0.360639	Best loss: 0.072744	Accuracy: 96.64%
13	Validation loss: 0.128829	Best loss: 0.072744	Accuracy: 97.77%
14	Validation loss: 0.086723	Best loss: 0.072744	Accuracy: 98.48%
15	Validation loss: 0.106032	Best loss: 0.072744	Accuracy: 98.48%
16	Validation loss: 0.410081	Best loss: 0.072744	Accuracy: 96.01%
17	Validation loss: 0.089236	Best loss: 0.072744	Accuracy: 98.67%
18	Validation loss: 0.210535	Best loss: 0.072744	Accuracy: 97.50%
19	Validation loss: 0.076903	Best loss: 0.072744	Accuracy: 98.59%
20	Validation loss: 0.165345	Best loss: 0.072744	Accuracy: 98.36%
21	Validation loss: 0.232497	Best loss: 0.072744	Accuracy: 97.19%
22	Validation loss: 0.137862	Best loss: 0.072744	Accuracy: 98.28%
23	Validation loss: 0.137300	Best loss: 0.072744	Accuracy: 98.63%
24	Validation loss: 0.220550	Best loss: 0.072744	Accuracy: 96.76%
25	Validation loss: 0.181100	Best loss: 0.072744	Accuracy: 98.20%
26	Validation loss: 0.162892	Best loss: 0.072744	Accuracy: 98.32%
27	Validation loss: 0.479828	Best loss: 0.072744	Accuracy: 95.39%
28	Validation loss: 0.176256	Best loss: 0.072744	Accuracy: 97.77%
29	Validation loss: 0.170178	Best loss: 0.072744	Accuracy: 98.75%
30	Validation loss: 0.143798	Best loss: 0.072744	Accuracy: 98.79%
31	Validation loss: 0.553292	Best loss: 0.072744	Accuracy: 96.87%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total= 3.3min
[CV] n_neurons=140, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.513155	Best loss: 0.513155	Accuracy: 92.57%
1	Validation loss: 0.873042	Best loss: 0.513155	Accuracy: 94.45%
2	Validation loss: 0.428420	Best loss: 0.428420	Accuracy: 93.55%
3	Validation loss: 0.155861	Best loss: 0.155861	Accuracy: 97.38%
4	Validation loss: 0.837525	Best loss: 0.155861	Accuracy: 90.27%
5	Validation loss: 0.104965	Best loss: 0.104965	Accuracy: 97.42%
6	Validation loss: 0.098041	Best loss: 0.098041	Accuracy: 97.93%
7	Validation loss: 0.950818	Best loss: 0.098041	Accuracy: 93.28%
8	Validation loss: 0.074752	Best loss: 0.074752	Accuracy: 98.08%
9	Validation loss: 0.524033	Best loss: 0.074752	Accuracy: 94.49%
10	Validation loss: 0.158932	Best loss: 0.074752	Accuracy: 96.64%
11	Validation loss: 0.200370	Best loss: 0.074752	Accuracy: 97.30%
12	Validation loss: 0.116892	Best loss: 0.074752	Accuracy: 97.07%
13	Validation loss: 0.576169	Best loss: 0.074752	Accuracy: 95.27%
14	Validation loss: 0.099032	Best loss: 0.074752	Accuracy: 98.48%
15	Validation loss: 0.070267	Best loss: 0.070267	Accuracy: 98.51%
16	Validation loss: 0.291958	Best loss: 0.070267	Accuracy: 97.85%
17	Validation loss: 0.164603	Best loss: 0.070267	Accuracy: 97.69%
18	Validation loss: 0.220426	Best loss: 0.070267	Accuracy: 97.97%
19	Validation loss: 0.078389	Best loss: 0.070267	Accuracy: 98.71%
20	Validation loss: 0.174444	Best loss: 0.070267	Accuracy: 98.51%
21	Validation loss: 0.157587	Best loss: 0.070267	Accuracy: 97.89%
22	Validation loss: 0.119377	Best loss: 0.070267	Accuracy: 98.40%
23	Validation loss: 0.110607	Best loss: 0.070267	Accuracy: 98.87%
24	Validation loss: 0.128837	Best loss: 0.070267	Accuracy: 98.05%
25	Validation loss: 0.217792	Best loss: 0.070267	Accuracy: 97.46%
26	Validation loss: 0.089811	Best loss: 0.070267	Accuracy: 98.94%
27	Validation loss: 0.108602	Best loss: 0.070267	Accuracy: 98.67%
28	Validation loss: 0.149226	Best loss: 0.070267	Accuracy: 98.83%
29	Validation loss: 0.161176	Best loss: 0.070267	Accuracy: 98.20%
30	Validation loss: 0.136269	Best loss: 0.070267	Accuracy: 98.51%
31	Validation loss: 0.146481	Best loss: 0.070267	Accuracy: 98.48%
32	Validation loss: 0.179100	Best loss: 0.070267	Accuracy: 98.83%
33	Validation loss: 0.126016	Best loss: 0.070267	Accuracy: 98.75%
34	Validation loss: 0.160589	Best loss: 0.070267	Accuracy: 98.71%
35	Validation loss: 0.245412	Best loss: 0.070267	Accuracy: 98.08%
36	Validation loss: 0.135640	Best loss: 0.070267	Accuracy: 98.51%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total= 3.8min
[CV] n_neurons=140, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.049897	Best loss: 1.049897	Accuracy: 92.06%
1	Validation loss: 0.170493	Best loss: 0.170493	Accuracy: 97.93%
2	Validation loss: 0.160819	Best loss: 0.160819	Accuracy: 97.58%
3	Validation loss: 0.241414	Best loss: 0.160819	Accuracy: 94.33%
4	Validation loss: 0.328955	Best loss: 0.160819	Accuracy: 95.27%
5	Validation loss: 0.224514	Best loss: 0.160819	Accuracy: 97.42%
6	Validation loss: 0.170313	Best loss: 0.160819	Accuracy: 97.42%
7	Validation loss: 0.074568	Best loss: 0.074568	Accuracy: 97.89%
8	Validation loss: 0.118622	Best loss: 0.074568	Accuracy: 98.32%
9	Validation loss: 0.968420	Best loss: 0.074568	Accuracy: 91.13%
10	Validation loss: 0.064215	Best loss: 0.064215	Accuracy: 98.36%
11	Validation loss: 0.384438	Best loss: 0.064215	Accuracy: 95.04%
12	Validation loss: 0.104874	Best loss: 0.064215	Accuracy: 97.81%
13	Validation loss: 0.152910	Best loss: 0.064215	Accuracy: 98.05%
14	Validation loss: 0.053005	Best loss: 0.053005	Accuracy: 98.79%
15	Validation loss: 0.167332	Best loss: 0.053005	Accuracy: 97.97%
16	Validation loss: 0.183113	Best loss: 0.053005	Accuracy: 98.20%
17	Validation loss: 0.140135	Best loss: 0.053005	Accuracy: 98.59%
18	Validation loss: 0.115059	Best loss: 0.053005	Accuracy: 97.93%
19	Validation loss: 0.121940	Best loss: 0.053005	Accuracy: 98.75%
20	Validation loss: 0.122597	Best loss: 0.053005	Accuracy: 98.36%
21	Validation loss: 0.118940	Best loss: 0.053005	Accuracy: 98.32%
22	Validation loss: 0.116297	Best loss: 0.053005	Accuracy: 98.75%
23	Validation loss: 0.155661	Best loss: 0.053005	Accuracy: 98.24%
24	Validation loss: 0.098028	Best loss: 0.053005	Accuracy: 98.63%
25	Validation loss: 0.153276	Best loss: 0.053005	Accuracy: 98.63%
26	Validation loss: 0.171779	Best loss: 0.053005	Accuracy: 98.05%
27	Validation loss: 1.145944	Best loss: 0.053005	Accuracy: 92.49%
28	Validation loss: 0.107044	Best loss: 0.053005	Accuracy: 98.51%
29	Validation loss: 0.134024	Best loss: 0.053005	Accuracy: 98.71%
30	Validation loss: 0.282598	Best loss: 0.053005	Accuracy: 98.36%
31	Validation loss: 0.111580	Best loss: 0.053005	Accuracy: 98.48%
32	Validation loss: 0.199585	Best loss: 0.053005	Accuracy: 98.59%
33	Validation loss: 0.148757	Best loss: 0.053005	Accuracy: 98.63%
34	Validation loss: 0.174496	Best loss: 0.053005	Accuracy: 98.20%
35	Validation loss: 0.427654	Best loss: 0.053005	Accuracy: 97.50%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total= 3.7min
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.110982	Best loss: 0.110982	Accuracy: 97.15%
1	Validation loss: 0.084843	Best loss: 0.084843	Accuracy: 97.42%
2	Validation loss: 0.099838	Best loss: 0.084843	Accuracy: 97.15%
3	Validation loss: 0.082484	Best loss: 0.082484	Accuracy: 97.62%
4	Validation loss: 0.067287	Best loss: 0.067287	Accuracy: 97.89%
5	Validation loss: 0.085584	Best loss: 0.067287	Accuracy: 97.97%
6	Validation loss: 0.073282	Best loss: 0.067287	Accuracy: 98.05%
7	Validation loss: 0.060410	Best loss: 0.060410	Accuracy: 98.40%
8	Validation loss: 0.054438	Best loss: 0.054438	Accuracy: 98.67%
9	Validation loss: 0.055330	Best loss: 0.054438	Accuracy: 98.75%
10	Validation loss: 0.052301	Best loss: 0.052301	Accuracy: 98.67%
11	Validation loss: 0.092222	Best loss: 0.052301	Accuracy: 98.08%
12	Validation loss: 0.082966	Best loss: 0.052301	Accuracy: 98.05%
13	Validation loss: 0.071627	Best loss: 0.052301	Accuracy: 98.51%
14	Validation loss: 0.067144	Best loss: 0.052301	Accuracy: 98.32%
15	Validation loss: 0.053035	Best loss: 0.052301	Accuracy: 98.63%
16	Validation loss: 0.042437	Best loss: 0.042437	Accuracy: 98.71%
17	Validation loss: 0.108896	Best loss: 0.042437	Accuracy: 97.77%
18	Validation loss: 0.062855	Best loss: 0.042437	Accuracy: 98.48%
19	Validation loss: 0.063409	Best loss: 0.042437	Accuracy: 98.63%
20	Validation loss: 0.079936	Best loss: 0.042437	Accuracy: 98.20%
21	Validation loss: 0.056405	Best loss: 0.042437	Accuracy: 98.94%
22	Validation loss: 0.054752	Best loss: 0.042437	Accuracy: 98.91%
23	Validation loss: 0.074390	Best loss: 0.042437	Accuracy: 98.71%
24	Validation loss: 0.058468	Best loss: 0.042437	Accuracy: 98.71%
25	Validation loss: 0.054830	Best loss: 0.042437	Accuracy: 98.79%
26	Validation loss: 0.083510	Best loss: 0.042437	Accuracy: 98.71%
27	Validation loss: 0.096775	Best loss: 0.042437	Accuracy: 98.05%
28	Validation loss: 0.122001	Best loss: 0.042437	Accuracy: 98.59%
29	Validation loss: 0.116749	Best loss: 0.042437	Accuracy: 98.48%
30	Validation loss: 0.089304	Best loss: 0.042437	Accuracy: 98.91%
31	Validation loss: 0.147034	Best loss: 0.042437	Accuracy: 98.12%
32	Validation loss: 0.086915	Best loss: 0.042437	Accuracy: 98.98%
33	Validation loss: 0.063703	Best loss: 0.042437	Accuracy: 98.83%
34	Validation loss: 0.076464	Best loss: 0.042437	Accuracy: 98.94%
35	Validation loss: 0.086421	Best loss: 0.042437	Accuracy: 99.06%
36	Validation loss: 0.138172	Best loss: 0.042437	Accuracy: 98.44%
37	Validation loss: 0.173732	Best loss: 0.042437	Accuracy: 97.58%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  56.1s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.133658	Best loss: 0.133658	Accuracy: 96.52%
1	Validation loss: 0.076068	Best loss: 0.076068	Accuracy: 97.97%
2	Validation loss: 0.065323	Best loss: 0.065323	Accuracy: 98.20%
3	Validation loss: 0.068069	Best loss: 0.065323	Accuracy: 98.36%
4	Validation loss: 0.073531	Best loss: 0.065323	Accuracy: 97.89%
5	Validation loss: 0.073018	Best loss: 0.065323	Accuracy: 98.01%
6	Validation loss: 0.069221	Best loss: 0.065323	Accuracy: 98.01%
7	Validation loss: 0.057620	Best loss: 0.057620	Accuracy: 98.40%
8	Validation loss: 0.114972	Best loss: 0.057620	Accuracy: 98.24%
9	Validation loss: 0.069582	Best loss: 0.057620	Accuracy: 98.28%
10	Validation loss: 0.089828	Best loss: 0.057620	Accuracy: 98.44%
11	Validation loss: 0.070050	Best loss: 0.057620	Accuracy: 98.71%
12	Validation loss: 0.052776	Best loss: 0.052776	Accuracy: 98.71%
13	Validation loss: 0.049031	Best loss: 0.049031	Accuracy: 98.98%
14	Validation loss: 0.128271	Best loss: 0.049031	Accuracy: 97.93%
15	Validation loss: 0.064884	Best loss: 0.049031	Accuracy: 98.44%
16	Validation loss: 0.080694	Best loss: 0.049031	Accuracy: 98.05%
17	Validation loss: 0.069157	Best loss: 0.049031	Accuracy: 98.36%
18	Validation loss: 0.052500	Best loss: 0.049031	Accuracy: 98.48%
19	Validation loss: 0.070261	Best loss: 0.049031	Accuracy: 98.63%
20	Validation loss: 0.085204	Best loss: 0.049031	Accuracy: 98.44%
21	Validation loss: 0.073157	Best loss: 0.049031	Accuracy: 98.59%
22	Validation loss: 0.147126	Best loss: 0.049031	Accuracy: 97.81%
23	Validation loss: 0.065970	Best loss: 0.049031	Accuracy: 98.59%
24	Validation loss: 0.063033	Best loss: 0.049031	Accuracy: 98.28%
25	Validation loss: 0.093381	Best loss: 0.049031	Accuracy: 98.40%
26	Validation loss: 0.068343	Best loss: 0.049031	Accuracy: 98.67%
27	Validation loss: 0.134098	Best loss: 0.049031	Accuracy: 98.12%
28	Validation loss: 0.094203	Best loss: 0.049031	Accuracy: 98.40%
29	Validation loss: 0.081290	Best loss: 0.049031	Accuracy: 98.55%
30	Validation loss: 0.065889	Best loss: 0.049031	Accuracy: 98.71%
31	Validation loss: 0.071898	Best loss: 0.049031	Accuracy: 98.79%
32	Validation loss: 0.116652	Best loss: 0.049031	Accuracy: 98.08%
33	Validation loss: 0.082098	Best loss: 0.049031	Accuracy: 98.67%
34	Validation loss: 0.061130	Best loss: 0.049031	Accuracy: 98.79%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  51.5s
[CV] n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.214963	Best loss: 0.214963	Accuracy: 95.19%
1	Validation loss: 0.134794	Best loss: 0.134794	Accuracy: 95.97%
2	Validation loss: 0.082646	Best loss: 0.082646	Accuracy: 98.01%
3	Validation loss: 0.077641	Best loss: 0.077641	Accuracy: 98.24%
4	Validation loss: 0.086207	Best loss: 0.077641	Accuracy: 97.81%
5	Validation loss: 0.090990	Best loss: 0.077641	Accuracy: 97.69%
6	Validation loss: 0.064629	Best loss: 0.064629	Accuracy: 98.16%
7	Validation loss: 0.071361	Best loss: 0.064629	Accuracy: 98.12%
8	Validation loss: 0.052273	Best loss: 0.052273	Accuracy: 98.55%
9	Validation loss: 0.060891	Best loss: 0.052273	Accuracy: 97.97%
10	Validation loss: 0.184019	Best loss: 0.052273	Accuracy: 94.33%
11	Validation loss: 0.054753	Best loss: 0.052273	Accuracy: 98.48%
12	Validation loss: 0.081386	Best loss: 0.052273	Accuracy: 97.85%
13	Validation loss: 0.049612	Best loss: 0.049612	Accuracy: 98.71%
14	Validation loss: 0.073842	Best loss: 0.049612	Accuracy: 98.59%
15	Validation loss: 0.067237	Best loss: 0.049612	Accuracy: 98.40%
16	Validation loss: 0.044686	Best loss: 0.044686	Accuracy: 98.83%
17	Validation loss: 0.433918	Best loss: 0.044686	Accuracy: 95.50%
18	Validation loss: 0.057049	Best loss: 0.044686	Accuracy: 98.63%
19	Validation loss: 0.068373	Best loss: 0.044686	Accuracy: 98.48%
20	Validation loss: 0.062525	Best loss: 0.044686	Accuracy: 98.55%
21	Validation loss: 0.047392	Best loss: 0.044686	Accuracy: 98.79%
22	Validation loss: 0.045192	Best loss: 0.044686	Accuracy: 98.83%
23	Validation loss: 0.079287	Best loss: 0.044686	Accuracy: 98.51%
24	Validation loss: 0.076172	Best loss: 0.044686	Accuracy: 98.91%
25	Validation loss: 0.067482	Best loss: 0.044686	Accuracy: 99.18%
26	Validation loss: 0.049023	Best loss: 0.044686	Accuracy: 99.10%
27	Validation loss: 0.072351	Best loss: 0.044686	Accuracy: 98.71%
28	Validation loss: 0.057924	Best loss: 0.044686	Accuracy: 98.79%
29	Validation loss: 0.101906	Best loss: 0.044686	Accuracy: 97.93%
30	Validation loss: 0.062793	Best loss: 0.044686	Accuracy: 98.79%
31	Validation loss: 0.104876	Best loss: 0.044686	Accuracy: 98.20%
32	Validation loss: 0.097390	Best loss: 0.044686	Accuracy: 98.55%
33	Validation loss: 0.081423	Best loss: 0.044686	Accuracy: 98.79%
34	Validation loss: 0.078818	Best loss: 0.044686	Accuracy: 98.94%
35	Validation loss: 0.059748	Best loss: 0.044686	Accuracy: 99.02%
36	Validation loss: 0.053149	Best loss: 0.044686	Accuracy: 99.02%
37	Validation loss: 0.062208	Best loss: 0.044686	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, batch_norm_momentum=0.95, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  55.8s
[CV] n_neurons=30, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.294298	Best loss: 0.294298	Accuracy: 91.40%
1	Validation loss: 0.752486	Best loss: 0.294298	Accuracy: 83.58%
2	Validation loss: 0.293903	Best loss: 0.293903	Accuracy: 93.43%
3	Validation loss: 0.197769	Best loss: 0.197769	Accuracy: 95.07%
4	Validation loss: 0.214177	Best loss: 0.197769	Accuracy: 94.80%
5	Validation loss: 0.365923	Best loss: 0.197769	Accuracy: 93.59%
6	Validation loss: 0.139113	Best loss: 0.139113	Accuracy: 97.42%
7	Validation loss: 0.160554	Best loss: 0.139113	Accuracy: 97.22%
8	Validation loss: 0.114384	Best loss: 0.114384	Accuracy: 97.93%
9	Validation loss: 0.135031	Best loss: 0.114384	Accuracy: 97.97%
10	Validation loss: 0.129849	Best loss: 0.114384	Accuracy: 98.08%
11	Validation loss: 0.250061	Best loss: 0.114384	Accuracy: 96.91%
12	Validation loss: 0.213072	Best loss: 0.114384	Accuracy: 96.76%
13	Validation loss: 0.243454	Best loss: 0.114384	Accuracy: 96.87%
14	Validation loss: 0.132142	Best loss: 0.114384	Accuracy: 98.36%
15	Validation loss: 0.166676	Best loss: 0.114384	Accuracy: 98.16%
16	Validation loss: 0.180868	Best loss: 0.114384	Accuracy: 98.08%
17	Validation loss: 0.182265	Best loss: 0.114384	Accuracy: 98.08%
18	Validation loss: 0.140331	Best loss: 0.114384	Accuracy: 98.48%
19	Validation loss: 0.193152	Best loss: 0.114384	Accuracy: 97.97%
20	Validation loss: 0.169704	Best loss: 0.114384	Accuracy: 98.01%
21	Validation loss: 0.184061	Best loss: 0.114384	Accuracy: 97.73%
22	Validation loss: 0.197661	Best loss: 0.114384	Accuracy: 98.16%
23	Validation loss: 0.168208	Best loss: 0.114384	Accuracy: 98.20%
24	Validation loss: 0.128910	Best loss: 0.114384	Accuracy: 98.63%
25	Validation loss: 0.140031	Best loss: 0.114384	Accuracy: 98.55%
26	Validation loss: 0.176253	Best loss: 0.114384	Accuracy: 98.12%
27	Validation loss: 0.127439	Best loss: 0.114384	Accuracy: 98.51%
28	Validation loss: 0.115914	Best loss: 0.114384	Accuracy: 98.48%
29	Validation loss: 0.108862	Best loss: 0.108862	Accuracy: 98.48%
30	Validation loss: 0.103952	Best loss: 0.103952	Accuracy: 98.48%
31	Validation loss: 0.094839	Best loss: 0.094839	Accuracy: 98.55%
32	Validation loss: 0.088668	Best loss: 0.088668	Accuracy: 98.67%
33	Validation loss: 0.087368	Best loss: 0.087368	Accuracy: 98.71%
34	Validation loss: 0.081643	Best loss: 0.081643	Accuracy: 98.67%
35	Validation loss: 0.077683	Best loss: 0.077683	Accuracy: 98.75%
36	Validation loss: 0.074824	Best loss: 0.074824	Accuracy: 98.79%
37	Validation loss: 0.076480	Best loss: 0.074824	Accuracy: 98.83%
38	Validation loss: 0.072225	Best loss: 0.072225	Accuracy: 98.79%
39	Validation loss: 0.069885	Best loss: 0.069885	Accuracy: 98.79%
40	Validation loss: 0.067338	Best loss: 0.067338	Accuracy: 98.79%
41	Validation loss: 0.068459	Best loss: 0.067338	Accuracy: 98.83%
42	Validation loss: 0.069424	Best loss: 0.067338	Accuracy: 98.79%
43	Validation loss: 0.064697	Best loss: 0.064697	Accuracy: 98.91%
44	Validation loss: 0.068128	Best loss: 0.064697	Accuracy: 98.87%
45	Validation loss: 0.067115	Best loss: 0.064697	Accuracy: 98.75%
46	Validation loss: 0.064942	Best loss: 0.064697	Accuracy: 98.79%
47	Validation loss: 0.062438	Best loss: 0.062438	Accuracy: 98.83%
48	Validation loss: 0.061357	Best loss: 0.061357	Accuracy: 98.94%
49	Validation loss: 0.066527	Best loss: 0.061357	Accuracy: 98.83%
50	Validation loss: 0.060896	Best loss: 0.060896	Accuracy: 98.94%
51	Validation loss: 0.061038	Best loss: 0.060896	Accuracy: 98.87%
52	Validation loss: 0.060538	Best loss: 0.060538	Accuracy: 98.91%
53	Validation loss: 0.060832	Best loss: 0.060538	Accuracy: 98.87%
54	Validation loss: 0.060229	Best loss: 0.060229	Accuracy: 98.98%
55	Validation loss: 0.058288	Best loss: 0.058288	Accuracy: 99.06%
56	Validation loss: 0.058201	Best loss: 0.058201	Accuracy: 98.83%
57	Validation loss: 0.103934	Best loss: 0.058201	Accuracy: 98.63%
58	Validation loss: 0.253657	Best loss: 0.058201	Accuracy: 97.85%
59	Validation loss: 0.306538	Best loss: 0.058201	Accuracy: 97.54%
60	Validation loss: 0.234402	Best loss: 0.058201	Accuracy: 98.36%
61	Validation loss: 0.257918	Best loss: 0.058201	Accuracy: 98.20%
62	Validation loss: 0.260105	Best loss: 0.058201	Accuracy: 98.36%
63	Validation loss: 0.215743	Best loss: 0.058201	Accuracy: 98.51%
64	Validation loss: 0.197259	Best loss: 0.058201	Accuracy: 98.48%
65	Validation loss: 0.181054	Best loss: 0.058201	Accuracy: 98.63%
66	Validation loss: 0.176981	Best loss: 0.058201	Accuracy: 98.63%
67	Validation loss: 0.133481	Best loss: 0.058201	Accuracy: 98.67%
68	Validation loss: 0.137972	Best loss: 0.058201	Accuracy: 98.51%
69	Validation loss: 0.135499	Best loss: 0.058201	Accuracy: 98.63%
70	Validation loss: 0.127383	Best loss: 0.058201	Accuracy: 98.67%
71	Validation loss: 0.122017	Best loss: 0.058201	Accuracy: 98.79%
72	Validation loss: 0.107827	Best loss: 0.058201	Accuracy: 98.71%
73	Validation loss: 0.108117	Best loss: 0.058201	Accuracy: 98.79%
74	Validation loss: 0.115652	Best loss: 0.058201	Accuracy: 98.91%
75	Validation loss: 0.113560	Best loss: 0.058201	Accuracy: 98.94%
76	Validation loss: 0.105849	Best loss: 0.058201	Accuracy: 98.94%
77	Validation loss: 0.101488	Best loss: 0.058201	Accuracy: 98.83%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total=  22.8s
[CV] n_neurons=30, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.353982	Best loss: 0.353982	Accuracy: 89.60%
1	Validation loss: 0.169617	Best loss: 0.169617	Accuracy: 95.66%
2	Validation loss: 0.143946	Best loss: 0.143946	Accuracy: 96.33%
3	Validation loss: 0.107143	Best loss: 0.107143	Accuracy: 97.58%
4	Validation loss: 0.144079	Best loss: 0.107143	Accuracy: 97.11%
5	Validation loss: 0.188320	Best loss: 0.107143	Accuracy: 96.76%
6	Validation loss: 0.149578	Best loss: 0.107143	Accuracy: 97.34%
7	Validation loss: 0.128809	Best loss: 0.107143	Accuracy: 97.89%
8	Validation loss: 0.173233	Best loss: 0.107143	Accuracy: 97.42%
9	Validation loss: 0.298742	Best loss: 0.107143	Accuracy: 96.05%
10	Validation loss: 0.225341	Best loss: 0.107143	Accuracy: 97.07%
11	Validation loss: 0.215188	Best loss: 0.107143	Accuracy: 97.42%
12	Validation loss: 0.228636	Best loss: 0.107143	Accuracy: 97.11%
13	Validation loss: 0.193793	Best loss: 0.107143	Accuracy: 97.73%
14	Validation loss: 0.191241	Best loss: 0.107143	Accuracy: 97.93%
15	Validation loss: 0.195274	Best loss: 0.107143	Accuracy: 97.89%
16	Validation loss: 0.168824	Best loss: 0.107143	Accuracy: 98.08%
17	Validation loss: 0.252509	Best loss: 0.107143	Accuracy: 97.46%
18	Validation loss: 0.192688	Best loss: 0.107143	Accuracy: 98.01%
19	Validation loss: 0.181034	Best loss: 0.107143	Accuracy: 98.24%
20	Validation loss: 0.176362	Best loss: 0.107143	Accuracy: 98.12%
21	Validation loss: 0.172303	Best loss: 0.107143	Accuracy: 98.24%
22	Validation loss: 0.227675	Best loss: 0.107143	Accuracy: 97.73%
23	Validation loss: 0.183743	Best loss: 0.107143	Accuracy: 98.24%
24	Validation loss: 0.270343	Best loss: 0.107143	Accuracy: 97.58%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total=   8.8s
[CV] n_neurons=30, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.746810	Best loss: 0.746810	Accuracy: 80.41%
1	Validation loss: 0.211506	Best loss: 0.211506	Accuracy: 94.45%
2	Validation loss: 0.226148	Best loss: 0.211506	Accuracy: 94.68%
3	Validation loss: 0.128550	Best loss: 0.128550	Accuracy: 96.25%
4	Validation loss: 0.130737	Best loss: 0.128550	Accuracy: 96.76%
5	Validation loss: 0.159819	Best loss: 0.128550	Accuracy: 96.60%
6	Validation loss: 0.136910	Best loss: 0.128550	Accuracy: 97.46%
7	Validation loss: 0.132961	Best loss: 0.128550	Accuracy: 97.93%
8	Validation loss: 0.157380	Best loss: 0.128550	Accuracy: 97.81%
9	Validation loss: 0.133178	Best loss: 0.128550	Accuracy: 97.93%
10	Validation loss: 0.206140	Best loss: 0.128550	Accuracy: 97.15%
11	Validation loss: 0.228862	Best loss: 0.128550	Accuracy: 97.30%
12	Validation loss: 0.164116	Best loss: 0.128550	Accuracy: 97.81%
13	Validation loss: 0.133181	Best loss: 0.128550	Accuracy: 98.20%
14	Validation loss: 0.124732	Best loss: 0.124732	Accuracy: 98.20%
15	Validation loss: 0.177460	Best loss: 0.124732	Accuracy: 98.12%
16	Validation loss: 0.172059	Best loss: 0.124732	Accuracy: 98.12%
17	Validation loss: 0.143853	Best loss: 0.124732	Accuracy: 98.20%
18	Validation loss: 0.137077	Best loss: 0.124732	Accuracy: 98.48%
19	Validation loss: 0.143570	Best loss: 0.124732	Accuracy: 98.71%
20	Validation loss: 0.185397	Best loss: 0.124732	Accuracy: 98.20%
21	Validation loss: 0.124079	Best loss: 0.124079	Accuracy: 98.75%
22	Validation loss: 0.160290	Best loss: 0.124079	Accuracy: 98.32%
23	Validation loss: 0.157496	Best loss: 0.124079	Accuracy: 98.67%
24	Validation loss: 0.167720	Best loss: 0.124079	Accuracy: 98.40%
25	Validation loss: 0.165905	Best loss: 0.124079	Accuracy: 98.36%
26	Validation loss: 0.178057	Best loss: 0.124079	Accuracy: 98.48%
27	Validation loss: 0.186379	Best loss: 0.124079	Accuracy: 98.08%
28	Validation loss: 0.172759	Best loss: 0.124079	Accuracy: 98.08%
29	Validation loss: 0.167760	Best loss: 0.124079	Accuracy: 98.51%
30	Validation loss: 0.149223	Best loss: 0.124079	Accuracy: 98.51%
31	Validation loss: 0.149767	Best loss: 0.124079	Accuracy: 98.51%
32	Validation loss: 0.122459	Best loss: 0.122459	Accuracy: 98.71%
33	Validation loss: 0.129875	Best loss: 0.122459	Accuracy: 98.75%
34	Validation loss: 0.093565	Best loss: 0.093565	Accuracy: 98.94%
35	Validation loss: 0.119215	Best loss: 0.093565	Accuracy: 98.94%
36	Validation loss: 0.088689	Best loss: 0.088689	Accuracy: 99.06%
37	Validation loss: 0.091032	Best loss: 0.088689	Accuracy: 98.98%
38	Validation loss: 0.081189	Best loss: 0.081189	Accuracy: 98.98%
39	Validation loss: 0.082660	Best loss: 0.081189	Accuracy: 99.02%
40	Validation loss: 0.078190	Best loss: 0.078190	Accuracy: 98.98%
41	Validation loss: 0.074604	Best loss: 0.074604	Accuracy: 99.06%
42	Validation loss: 0.096114	Best loss: 0.074604	Accuracy: 98.87%
43	Validation loss: 0.076503	Best loss: 0.074604	Accuracy: 98.83%
44	Validation loss: 0.109511	Best loss: 0.074604	Accuracy: 98.55%
45	Validation loss: 0.107890	Best loss: 0.074604	Accuracy: 98.48%
46	Validation loss: 0.156227	Best loss: 0.074604	Accuracy: 98.12%
47	Validation loss: 0.151931	Best loss: 0.074604	Accuracy: 98.08%
48	Validation loss: 0.310134	Best loss: 0.074604	Accuracy: 97.22%
49	Validation loss: 0.163584	Best loss: 0.074604	Accuracy: 98.79%
50	Validation loss: 0.140772	Best loss: 0.074604	Accuracy: 98.71%
51	Validation loss: 0.121726	Best loss: 0.074604	Accuracy: 99.06%
52	Validation loss: 0.100908	Best loss: 0.074604	Accuracy: 99.06%
53	Validation loss: 0.090968	Best loss: 0.074604	Accuracy: 99.14%
54	Validation loss: 0.085285	Best loss: 0.074604	Accuracy: 99.26%
55	Validation loss: 0.083588	Best loss: 0.074604	Accuracy: 99.22%
56	Validation loss: 0.079100	Best loss: 0.074604	Accuracy: 99.22%
57	Validation loss: 0.075518	Best loss: 0.074604	Accuracy: 99.18%
58	Validation loss: 0.072389	Best loss: 0.072389	Accuracy: 99.06%
59	Validation loss: 0.069305	Best loss: 0.069305	Accuracy: 99.18%
60	Validation loss: 0.066409	Best loss: 0.066409	Accuracy: 99.18%
61	Validation loss: 0.063425	Best loss: 0.063425	Accuracy: 99.18%
62	Validation loss: 0.061978	Best loss: 0.061978	Accuracy: 99.22%
63	Validation loss: 0.059105	Best loss: 0.059105	Accuracy: 99.18%
64	Validation loss: 0.057440	Best loss: 0.057440	Accuracy: 99.18%
65	Validation loss: 0.056114	Best loss: 0.056114	Accuracy: 99.18%
66	Validation loss: 0.054989	Best loss: 0.054989	Accuracy: 99.22%
67	Validation loss: 0.054069	Best loss: 0.054069	Accuracy: 99.18%
68	Validation loss: 0.053154	Best loss: 0.053154	Accuracy: 99.22%
69	Validation loss: 0.051354	Best loss: 0.051354	Accuracy: 99.22%
70	Validation loss: 0.048962	Best loss: 0.048962	Accuracy: 99.22%
71	Validation loss: 0.048123	Best loss: 0.048123	Accuracy: 99.22%
72	Validation loss: 0.047134	Best loss: 0.047134	Accuracy: 99.22%
73	Validation loss: 0.046227	Best loss: 0.046227	Accuracy: 99.22%
74	Validation loss: 0.045135	Best loss: 0.045135	Accuracy: 99.30%
75	Validation loss: 0.044777	Best loss: 0.044777	Accuracy: 99.18%
76	Validation loss: 0.044507	Best loss: 0.044507	Accuracy: 99.18%
77	Validation loss: 0.044145	Best loss: 0.044145	Accuracy: 99.18%
78	Validation loss: 0.043912	Best loss: 0.043912	Accuracy: 99.18%
79	Validation loss: 0.043773	Best loss: 0.043773	Accuracy: 99.18%
80	Validation loss: 0.043860	Best loss: 0.043773	Accuracy: 99.18%
81	Validation loss: 0.042616	Best loss: 0.042616	Accuracy: 99.22%
82	Validation loss: 0.042472	Best loss: 0.042472	Accuracy: 99.18%
83	Validation loss: 0.041786	Best loss: 0.041786	Accuracy: 99.18%
84	Validation loss: 0.043367	Best loss: 0.041786	Accuracy: 99.14%
85	Validation loss: 0.043005	Best loss: 0.041786	Accuracy: 99.22%
86	Validation loss: 0.042201	Best loss: 0.041786	Accuracy: 99.18%
87	Validation loss: 0.041717	Best loss: 0.041717	Accuracy: 99.18%
88	Validation loss: 0.041379	Best loss: 0.041379	Accuracy: 99.18%
89	Validation loss: 0.041618	Best loss: 0.041379	Accuracy: 99.18%
90	Validation loss: 0.040749	Best loss: 0.040749	Accuracy: 99.18%
91	Validation loss: 0.041235	Best loss: 0.040749	Accuracy: 99.18%
92	Validation loss: 0.040444	Best loss: 0.040444	Accuracy: 99.22%
93	Validation loss: 0.040077	Best loss: 0.040077	Accuracy: 99.22%
94	Validation loss: 0.039169	Best loss: 0.039169	Accuracy: 99.30%
95	Validation loss: 0.039916	Best loss: 0.039169	Accuracy: 99.30%
96	Validation loss: 0.039445	Best loss: 0.039169	Accuracy: 99.26%
97	Validation loss: 0.039218	Best loss: 0.039169	Accuracy: 99.26%
98	Validation loss: 0.039162	Best loss: 0.039162	Accuracy: 99.26%
99	Validation loss: 0.038785	Best loss: 0.038785	Accuracy: 99.22%
100	Validation loss: 0.038533	Best loss: 0.038533	Accuracy: 99.26%
101	Validation loss: 0.038464	Best loss: 0.038464	Accuracy: 99.30%
102	Validation loss: 0.038754	Best loss: 0.038464	Accuracy: 99.26%
103	Validation loss: 0.038673	Best loss: 0.038464	Accuracy: 99.22%
104	Validation loss: 0.038284	Best loss: 0.038284	Accuracy: 99.26%
105	Validation loss: 0.136525	Best loss: 0.038284	Accuracy: 98.08%
106	Validation loss: 0.257348	Best loss: 0.038284	Accuracy: 97.30%
107	Validation loss: 0.195889	Best loss: 0.038284	Accuracy: 97.93%
108	Validation loss: 0.239910	Best loss: 0.038284	Accuracy: 98.08%
109	Validation loss: 0.229297	Best loss: 0.038284	Accuracy: 98.59%
110	Validation loss: 0.177576	Best loss: 0.038284	Accuracy: 98.55%
111	Validation loss: 0.234767	Best loss: 0.038284	Accuracy: 98.55%
112	Validation loss: 0.209353	Best loss: 0.038284	Accuracy: 98.67%
113	Validation loss: 0.154116	Best loss: 0.038284	Accuracy: 98.71%
114	Validation loss: 0.198105	Best loss: 0.038284	Accuracy: 98.59%
115	Validation loss: 0.136635	Best loss: 0.038284	Accuracy: 98.83%
116	Validation loss: 0.121680	Best loss: 0.038284	Accuracy: 99.02%
117	Validation loss: 0.126716	Best loss: 0.038284	Accuracy: 98.83%
118	Validation loss: 0.127976	Best loss: 0.038284	Accuracy: 98.91%
119	Validation loss: 0.151240	Best loss: 0.038284	Accuracy: 98.79%
120	Validation loss: 0.169287	Best loss: 0.038284	Accuracy: 98.67%
121	Validation loss: 0.135671	Best loss: 0.038284	Accuracy: 98.79%
122	Validation loss: 0.110826	Best loss: 0.038284	Accuracy: 99.06%
123	Validation loss: 0.121109	Best loss: 0.038284	Accuracy: 98.79%
124	Validation loss: 0.133410	Best loss: 0.038284	Accuracy: 98.91%
125	Validation loss: 0.157750	Best loss: 0.038284	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, batch_size=500, batch_norm_momentum=0.999, activation=&lt;function elu at 0x1243639d8&gt;, total=  36.2s
[CV] n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.479649	Best loss: 0.479649	Accuracy: 94.68%
1	Validation loss: 0.144776	Best loss: 0.144776	Accuracy: 97.30%
2	Validation loss: 0.102974	Best loss: 0.102974	Accuracy: 97.85%
3	Validation loss: 0.092139	Best loss: 0.092139	Accuracy: 97.65%
4	Validation loss: 0.082264	Best loss: 0.082264	Accuracy: 97.81%
5	Validation loss: 0.098235	Best loss: 0.082264	Accuracy: 97.07%
6	Validation loss: 0.082386	Best loss: 0.082264	Accuracy: 97.38%
7	Validation loss: 0.194785	Best loss: 0.082264	Accuracy: 94.76%
8	Validation loss: 0.089543	Best loss: 0.082264	Accuracy: 97.30%
9	Validation loss: 0.274500	Best loss: 0.082264	Accuracy: 91.95%
10	Validation loss: 0.061369	Best loss: 0.061369	Accuracy: 98.40%
11	Validation loss: 0.065475	Best loss: 0.061369	Accuracy: 98.05%
12	Validation loss: 0.059116	Best loss: 0.059116	Accuracy: 98.71%
13	Validation loss: 0.068018	Best loss: 0.059116	Accuracy: 98.20%
14	Validation loss: 0.080096	Best loss: 0.059116	Accuracy: 97.77%
15	Validation loss: 0.053109	Best loss: 0.053109	Accuracy: 98.67%
16	Validation loss: 0.057565	Best loss: 0.053109	Accuracy: 98.51%
17	Validation loss: 0.056701	Best loss: 0.053109	Accuracy: 98.48%
18	Validation loss: 0.061863	Best loss: 0.053109	Accuracy: 97.97%
19	Validation loss: 0.042285	Best loss: 0.042285	Accuracy: 99.10%
20	Validation loss: 0.043108	Best loss: 0.042285	Accuracy: 98.87%
21	Validation loss: 0.064355	Best loss: 0.042285	Accuracy: 98.48%
22	Validation loss: 0.048680	Best loss: 0.042285	Accuracy: 99.02%
23	Validation loss: 0.052093	Best loss: 0.042285	Accuracy: 98.40%
24	Validation loss: 0.089036	Best loss: 0.042285	Accuracy: 97.65%
25	Validation loss: 0.041303	Best loss: 0.041303	Accuracy: 99.06%
26	Validation loss: 0.052047	Best loss: 0.041303	Accuracy: 98.71%
27	Validation loss: 0.067956	Best loss: 0.041303	Accuracy: 98.32%
28	Validation loss: 0.057155	Best loss: 0.041303	Accuracy: 98.48%
29	Validation loss: 0.091271	Best loss: 0.041303	Accuracy: 98.08%
30	Validation loss: 0.052641	Best loss: 0.041303	Accuracy: 98.75%
31	Validation loss: 0.083270	Best loss: 0.041303	Accuracy: 97.62%
32	Validation loss: 0.057963	Best loss: 0.041303	Accuracy: 98.55%
33	Validation loss: 0.046727	Best loss: 0.041303	Accuracy: 98.75%
34	Validation loss: 0.052546	Best loss: 0.041303	Accuracy: 98.91%
35	Validation loss: 0.048113	Best loss: 0.041303	Accuracy: 98.91%
36	Validation loss: 0.046982	Best loss: 0.041303	Accuracy: 98.79%
37	Validation loss: 0.047240	Best loss: 0.041303	Accuracy: 98.51%
38	Validation loss: 0.058517	Best loss: 0.041303	Accuracy: 98.59%
39	Validation loss: 0.054159	Best loss: 0.041303	Accuracy: 98.87%
40	Validation loss: 0.042830	Best loss: 0.041303	Accuracy: 99.02%
41	Validation loss: 0.050257	Best loss: 0.041303	Accuracy: 98.71%
42	Validation loss: 0.044598	Best loss: 0.041303	Accuracy: 98.87%
43	Validation loss: 0.048760	Best loss: 0.041303	Accuracy: 98.94%
44	Validation loss: 0.054572	Best loss: 0.041303	Accuracy: 98.75%
45	Validation loss: 0.052111	Best loss: 0.041303	Accuracy: 98.51%
46	Validation loss: 0.036785	Best loss: 0.036785	Accuracy: 99.18%
47	Validation loss: 0.055371	Best loss: 0.036785	Accuracy: 98.28%
48	Validation loss: 0.047584	Best loss: 0.036785	Accuracy: 98.79%
49	Validation loss: 0.071217	Best loss: 0.036785	Accuracy: 98.44%
50	Validation loss: 0.038141	Best loss: 0.036785	Accuracy: 99.22%
51	Validation loss: 0.038992	Best loss: 0.036785	Accuracy: 98.87%
52	Validation loss: 0.049312	Best loss: 0.036785	Accuracy: 99.02%
53	Validation loss: 0.043136	Best loss: 0.036785	Accuracy: 99.02%
54	Validation loss: 0.043329	Best loss: 0.036785	Accuracy: 98.83%
55	Validation loss: 0.038253	Best loss: 0.036785	Accuracy: 98.94%
56	Validation loss: 0.042012	Best loss: 0.036785	Accuracy: 98.91%
57	Validation loss: 0.044629	Best loss: 0.036785	Accuracy: 98.87%
58	Validation loss: 0.053710	Best loss: 0.036785	Accuracy: 98.83%
59	Validation loss: 0.051263	Best loss: 0.036785	Accuracy: 98.63%
60	Validation loss: 0.054077	Best loss: 0.036785	Accuracy: 98.71%
61	Validation loss: 0.050665	Best loss: 0.036785	Accuracy: 98.79%
62	Validation loss: 0.038629	Best loss: 0.036785	Accuracy: 98.94%
63	Validation loss: 0.057098	Best loss: 0.036785	Accuracy: 98.71%
64	Validation loss: 0.072470	Best loss: 0.036785	Accuracy: 98.40%
65	Validation loss: 0.053356	Best loss: 0.036785	Accuracy: 98.51%
66	Validation loss: 0.060044	Best loss: 0.036785	Accuracy: 98.91%
67	Validation loss: 0.069258	Best loss: 0.036785	Accuracy: 98.59%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 6.1min
[CV] n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.289929	Best loss: 0.289929	Accuracy: 96.48%
1	Validation loss: 0.159975	Best loss: 0.159975	Accuracy: 97.19%
2	Validation loss: 0.235973	Best loss: 0.159975	Accuracy: 95.43%
3	Validation loss: 0.087163	Best loss: 0.087163	Accuracy: 98.20%
4	Validation loss: 0.103453	Best loss: 0.087163	Accuracy: 97.73%
5	Validation loss: 0.092765	Best loss: 0.087163	Accuracy: 97.03%
6	Validation loss: 0.077898	Best loss: 0.077898	Accuracy: 97.73%
7	Validation loss: 0.081277	Best loss: 0.077898	Accuracy: 98.01%
8	Validation loss: 0.052591	Best loss: 0.052591	Accuracy: 98.40%
9	Validation loss: 0.103746	Best loss: 0.052591	Accuracy: 97.19%
10	Validation loss: 0.085667	Best loss: 0.052591	Accuracy: 97.50%
11	Validation loss: 0.047084	Best loss: 0.047084	Accuracy: 98.71%
12	Validation loss: 0.077702	Best loss: 0.047084	Accuracy: 97.50%
13	Validation loss: 0.091584	Best loss: 0.047084	Accuracy: 97.89%
14	Validation loss: 0.047694	Best loss: 0.047084	Accuracy: 98.67%
15	Validation loss: 0.082575	Best loss: 0.047084	Accuracy: 97.65%
16	Validation loss: 0.068383	Best loss: 0.047084	Accuracy: 98.24%
17	Validation loss: 0.122952	Best loss: 0.047084	Accuracy: 96.36%
18	Validation loss: 0.056115	Best loss: 0.047084	Accuracy: 98.32%
19	Validation loss: 0.056916	Best loss: 0.047084	Accuracy: 98.51%
20	Validation loss: 0.043387	Best loss: 0.043387	Accuracy: 98.71%
21	Validation loss: 0.044566	Best loss: 0.043387	Accuracy: 98.98%
22	Validation loss: 0.040203	Best loss: 0.040203	Accuracy: 98.98%
23	Validation loss: 0.060764	Best loss: 0.040203	Accuracy: 98.16%
24	Validation loss: 0.064622	Best loss: 0.040203	Accuracy: 98.59%
25	Validation loss: 0.039149	Best loss: 0.039149	Accuracy: 99.02%
26	Validation loss: 0.048557	Best loss: 0.039149	Accuracy: 98.83%
27	Validation loss: 0.037799	Best loss: 0.037799	Accuracy: 99.02%
28	Validation loss: 0.033830	Best loss: 0.033830	Accuracy: 99.22%
29	Validation loss: 0.038375	Best loss: 0.033830	Accuracy: 98.94%
30	Validation loss: 0.054736	Best loss: 0.033830	Accuracy: 98.44%
31	Validation loss: 0.043322	Best loss: 0.033830	Accuracy: 98.83%
32	Validation loss: 0.065105	Best loss: 0.033830	Accuracy: 98.28%
33	Validation loss: 0.042361	Best loss: 0.033830	Accuracy: 99.06%
34	Validation loss: 0.051139	Best loss: 0.033830	Accuracy: 98.75%
35	Validation loss: 0.040747	Best loss: 0.033830	Accuracy: 98.87%
36	Validation loss: 0.044760	Best loss: 0.033830	Accuracy: 98.87%
37	Validation loss: 0.044928	Best loss: 0.033830	Accuracy: 98.79%
38	Validation loss: 0.039026	Best loss: 0.033830	Accuracy: 99.02%
39	Validation loss: 0.038748	Best loss: 0.033830	Accuracy: 98.98%
40	Validation loss: 0.045940	Best loss: 0.033830	Accuracy: 98.75%
41	Validation loss: 0.046085	Best loss: 0.033830	Accuracy: 99.02%
42	Validation loss: 0.046957	Best loss: 0.033830	Accuracy: 98.98%
43	Validation loss: 0.055385	Best loss: 0.033830	Accuracy: 98.91%
44	Validation loss: 0.036177	Best loss: 0.033830	Accuracy: 99.22%
45	Validation loss: 0.052287	Best loss: 0.033830	Accuracy: 98.75%
46	Validation loss: 0.053410	Best loss: 0.033830	Accuracy: 98.83%
47	Validation loss: 0.041630	Best loss: 0.033830	Accuracy: 98.83%
48	Validation loss: 0.049837	Best loss: 0.033830	Accuracy: 98.83%
49	Validation loss: 0.041890	Best loss: 0.033830	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 4.5min
[CV] n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.327616	Best loss: 0.327616	Accuracy: 95.58%
1	Validation loss: 0.125495	Best loss: 0.125495	Accuracy: 98.01%
2	Validation loss: 0.100842	Best loss: 0.100842	Accuracy: 97.54%
3	Validation loss: 0.125459	Best loss: 0.100842	Accuracy: 96.99%
4	Validation loss: 0.066817	Best loss: 0.066817	Accuracy: 98.51%
5	Validation loss: 0.101651	Best loss: 0.066817	Accuracy: 97.34%
6	Validation loss: 0.087132	Best loss: 0.066817	Accuracy: 97.69%
7	Validation loss: 0.111056	Best loss: 0.066817	Accuracy: 96.87%
8	Validation loss: 0.063863	Best loss: 0.063863	Accuracy: 98.08%
9	Validation loss: 0.054105	Best loss: 0.054105	Accuracy: 98.40%
10	Validation loss: 0.061465	Best loss: 0.054105	Accuracy: 98.44%
11	Validation loss: 0.049010	Best loss: 0.049010	Accuracy: 98.79%
12	Validation loss: 0.071759	Best loss: 0.049010	Accuracy: 97.89%
13	Validation loss: 0.083757	Best loss: 0.049010	Accuracy: 98.05%
14	Validation loss: 0.119217	Best loss: 0.049010	Accuracy: 96.60%
15	Validation loss: 0.072841	Best loss: 0.049010	Accuracy: 98.05%
16	Validation loss: 0.043677	Best loss: 0.043677	Accuracy: 98.67%
17	Validation loss: 0.113254	Best loss: 0.043677	Accuracy: 97.03%
18	Validation loss: 0.060092	Best loss: 0.043677	Accuracy: 98.28%
19	Validation loss: 0.043183	Best loss: 0.043183	Accuracy: 98.67%
20	Validation loss: 0.039936	Best loss: 0.039936	Accuracy: 98.83%
21	Validation loss: 0.031623	Best loss: 0.031623	Accuracy: 99.30%
22	Validation loss: 0.051989	Best loss: 0.031623	Accuracy: 98.55%
23	Validation loss: 0.052413	Best loss: 0.031623	Accuracy: 98.55%
24	Validation loss: 0.032297	Best loss: 0.031623	Accuracy: 99.10%
25	Validation loss: 0.035449	Best loss: 0.031623	Accuracy: 99.10%
26	Validation loss: 0.053211	Best loss: 0.031623	Accuracy: 98.20%
27	Validation loss: 0.100505	Best loss: 0.031623	Accuracy: 97.26%
28	Validation loss: 0.071530	Best loss: 0.031623	Accuracy: 98.05%
29	Validation loss: 0.041658	Best loss: 0.031623	Accuracy: 98.79%
30	Validation loss: 0.050593	Best loss: 0.031623	Accuracy: 98.79%
31	Validation loss: 0.058479	Best loss: 0.031623	Accuracy: 98.55%
32	Validation loss: 0.055368	Best loss: 0.031623	Accuracy: 98.59%
33	Validation loss: 0.041384	Best loss: 0.031623	Accuracy: 98.79%
34	Validation loss: 0.045531	Best loss: 0.031623	Accuracy: 98.71%
35	Validation loss: 0.034353	Best loss: 0.031623	Accuracy: 98.98%
36	Validation loss: 0.041204	Best loss: 0.031623	Accuracy: 98.83%
37	Validation loss: 0.045357	Best loss: 0.031623	Accuracy: 98.83%
38	Validation loss: 0.042574	Best loss: 0.031623	Accuracy: 98.94%
39	Validation loss: 0.075479	Best loss: 0.031623	Accuracy: 98.48%
40	Validation loss: 0.041234	Best loss: 0.031623	Accuracy: 99.02%
41	Validation loss: 0.071734	Best loss: 0.031623	Accuracy: 98.59%
42	Validation loss: 0.036476	Best loss: 0.031623	Accuracy: 98.94%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.999, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total= 3.9min
[CV] n_neurons=70, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.087257	Best loss: 0.087257	Accuracy: 97.62%
1	Validation loss: 0.073945	Best loss: 0.073945	Accuracy: 98.16%
2	Validation loss: 0.085632	Best loss: 0.073945	Accuracy: 97.93%
3	Validation loss: 0.060651	Best loss: 0.060651	Accuracy: 98.12%
4	Validation loss: 0.043236	Best loss: 0.043236	Accuracy: 98.87%
5	Validation loss: 0.053301	Best loss: 0.043236	Accuracy: 98.55%
6	Validation loss: 0.063026	Best loss: 0.043236	Accuracy: 98.28%
7	Validation loss: 0.049803	Best loss: 0.043236	Accuracy: 98.51%
8	Validation loss: 0.045335	Best loss: 0.043236	Accuracy: 98.91%
9	Validation loss: 0.038025	Best loss: 0.038025	Accuracy: 98.83%
10	Validation loss: 0.050086	Best loss: 0.038025	Accuracy: 98.87%
11	Validation loss: 0.056872	Best loss: 0.038025	Accuracy: 98.67%
12	Validation loss: 0.066849	Best loss: 0.038025	Accuracy: 98.28%
13	Validation loss: 0.048423	Best loss: 0.038025	Accuracy: 98.91%
14	Validation loss: 0.058523	Best loss: 0.038025	Accuracy: 98.79%
15	Validation loss: 0.057660	Best loss: 0.038025	Accuracy: 98.87%
16	Validation loss: 0.048447	Best loss: 0.038025	Accuracy: 99.06%
17	Validation loss: 0.048265	Best loss: 0.038025	Accuracy: 98.94%
18	Validation loss: 0.036759	Best loss: 0.036759	Accuracy: 99.06%
19	Validation loss: 0.051970	Best loss: 0.036759	Accuracy: 98.98%
20	Validation loss: 0.054322	Best loss: 0.036759	Accuracy: 98.87%
21	Validation loss: 0.042382	Best loss: 0.036759	Accuracy: 98.87%
22	Validation loss: 0.047200	Best loss: 0.036759	Accuracy: 98.79%
23	Validation loss: 0.041053	Best loss: 0.036759	Accuracy: 98.91%
24	Validation loss: 0.041928	Best loss: 0.036759	Accuracy: 99.14%
25	Validation loss: 0.046657	Best loss: 0.036759	Accuracy: 98.87%
26	Validation loss: 0.054153	Best loss: 0.036759	Accuracy: 98.94%
27	Validation loss: 0.035682	Best loss: 0.035682	Accuracy: 99.26%
28	Validation loss: 0.048919	Best loss: 0.035682	Accuracy: 98.91%
29	Validation loss: 0.037750	Best loss: 0.035682	Accuracy: 99.10%
30	Validation loss: 0.046824	Best loss: 0.035682	Accuracy: 99.14%
31	Validation loss: 0.053477	Best loss: 0.035682	Accuracy: 98.91%
32	Validation loss: 0.044626	Best loss: 0.035682	Accuracy: 99.10%
33	Validation loss: 0.047375	Best loss: 0.035682	Accuracy: 98.83%
34	Validation loss: 0.052665	Best loss: 0.035682	Accuracy: 98.79%
35	Validation loss: 0.055408	Best loss: 0.035682	Accuracy: 98.79%
36	Validation loss: 0.041836	Best loss: 0.035682	Accuracy: 99.10%
37	Validation loss: 0.056768	Best loss: 0.035682	Accuracy: 99.02%
38	Validation loss: 0.043488	Best loss: 0.035682	Accuracy: 99.18%
39	Validation loss: 0.044905	Best loss: 0.035682	Accuracy: 99.06%
40	Validation loss: 0.063269	Best loss: 0.035682	Accuracy: 98.98%
41	Validation loss: 0.058254	Best loss: 0.035682	Accuracy: 98.67%
42	Validation loss: 0.049971	Best loss: 0.035682	Accuracy: 98.94%
43	Validation loss: 0.053169	Best loss: 0.035682	Accuracy: 99.14%
44	Validation loss: 0.053602	Best loss: 0.035682	Accuracy: 98.94%
45	Validation loss: 0.046721	Best loss: 0.035682	Accuracy: 98.87%
46	Validation loss: 0.049413	Best loss: 0.035682	Accuracy: 99.06%
47	Validation loss: 0.058028	Best loss: 0.035682	Accuracy: 98.98%
48	Validation loss: 0.065007	Best loss: 0.035682	Accuracy: 98.75%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  47.0s
[CV] n_neurons=70, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.132145	Best loss: 0.132145	Accuracy: 95.82%
1	Validation loss: 0.046092	Best loss: 0.046092	Accuracy: 98.63%
2	Validation loss: 0.078817	Best loss: 0.046092	Accuracy: 97.89%
3	Validation loss: 0.054445	Best loss: 0.046092	Accuracy: 98.51%
4	Validation loss: 0.063496	Best loss: 0.046092	Accuracy: 98.28%
5	Validation loss: 0.049609	Best loss: 0.046092	Accuracy: 98.83%
6	Validation loss: 0.046967	Best loss: 0.046092	Accuracy: 98.75%
7	Validation loss: 0.058224	Best loss: 0.046092	Accuracy: 98.67%
8	Validation loss: 0.047252	Best loss: 0.046092	Accuracy: 98.79%
9	Validation loss: 0.043742	Best loss: 0.043742	Accuracy: 98.71%
10	Validation loss: 0.039135	Best loss: 0.039135	Accuracy: 98.94%
11	Validation loss: 0.053366	Best loss: 0.039135	Accuracy: 98.59%
12	Validation loss: 0.037113	Best loss: 0.037113	Accuracy: 99.22%
13	Validation loss: 0.052111	Best loss: 0.037113	Accuracy: 98.87%
14	Validation loss: 0.033986	Best loss: 0.033986	Accuracy: 99.18%
15	Validation loss: 0.058620	Best loss: 0.033986	Accuracy: 99.02%
16	Validation loss: 0.051977	Best loss: 0.033986	Accuracy: 98.98%
17	Validation loss: 0.033808	Best loss: 0.033808	Accuracy: 99.10%
18	Validation loss: 0.048091	Best loss: 0.033808	Accuracy: 98.94%
19	Validation loss: 0.042656	Best loss: 0.033808	Accuracy: 99.02%
20	Validation loss: 0.038375	Best loss: 0.033808	Accuracy: 99.02%
21	Validation loss: 0.042453	Best loss: 0.033808	Accuracy: 99.06%
22	Validation loss: 0.045476	Best loss: 0.033808	Accuracy: 98.87%
23	Validation loss: 0.056626	Best loss: 0.033808	Accuracy: 98.87%
24	Validation loss: 0.034293	Best loss: 0.033808	Accuracy: 99.30%
25	Validation loss: 0.038202	Best loss: 0.033808	Accuracy: 99.02%
26	Validation loss: 0.052137	Best loss: 0.033808	Accuracy: 98.98%
27	Validation loss: 0.046042	Best loss: 0.033808	Accuracy: 98.94%
28	Validation loss: 0.032958	Best loss: 0.032958	Accuracy: 99.18%
29	Validation loss: 0.047305	Best loss: 0.032958	Accuracy: 98.94%
30	Validation loss: 0.030460	Best loss: 0.030460	Accuracy: 99.18%
31	Validation loss: 0.035790	Best loss: 0.030460	Accuracy: 99.34%
32	Validation loss: 0.032813	Best loss: 0.030460	Accuracy: 99.37%
33	Validation loss: 0.036043	Best loss: 0.030460	Accuracy: 99.18%
34	Validation loss: 0.039116	Best loss: 0.030460	Accuracy: 99.41%
35	Validation loss: 0.032012	Best loss: 0.030460	Accuracy: 99.26%
36	Validation loss: 0.037949	Best loss: 0.030460	Accuracy: 99.10%
37	Validation loss: 0.037853	Best loss: 0.030460	Accuracy: 99.30%
38	Validation loss: 0.058177	Best loss: 0.030460	Accuracy: 98.71%
39	Validation loss: 0.050497	Best loss: 0.030460	Accuracy: 98.94%
40	Validation loss: 0.047098	Best loss: 0.030460	Accuracy: 98.98%
41	Validation loss: 0.043367	Best loss: 0.030460	Accuracy: 99.06%
42	Validation loss: 0.051403	Best loss: 0.030460	Accuracy: 99.02%
43	Validation loss: 0.059338	Best loss: 0.030460	Accuracy: 98.91%
44	Validation loss: 0.035210	Best loss: 0.030460	Accuracy: 99.37%
45	Validation loss: 0.044627	Best loss: 0.030460	Accuracy: 99.22%
46	Validation loss: 0.072364	Best loss: 0.030460	Accuracy: 98.98%
47	Validation loss: 0.039092	Best loss: 0.030460	Accuracy: 99.26%
48	Validation loss: 0.048290	Best loss: 0.030460	Accuracy: 99.22%
49	Validation loss: 0.040805	Best loss: 0.030460	Accuracy: 99.14%
50	Validation loss: 0.034447	Best loss: 0.030460	Accuracy: 99.18%
51	Validation loss: 0.036173	Best loss: 0.030460	Accuracy: 99.30%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  48.2s
[CV] n_neurons=70, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt; 
0	Validation loss: 0.103886	Best loss: 0.103886	Accuracy: 97.42%
1	Validation loss: 0.063013	Best loss: 0.063013	Accuracy: 98.16%
2	Validation loss: 0.059406	Best loss: 0.059406	Accuracy: 98.05%
3	Validation loss: 0.040742	Best loss: 0.040742	Accuracy: 98.83%
4	Validation loss: 0.037489	Best loss: 0.037489	Accuracy: 98.79%
5	Validation loss: 0.039350	Best loss: 0.037489	Accuracy: 98.87%
6	Validation loss: 0.053654	Best loss: 0.037489	Accuracy: 98.79%
7	Validation loss: 0.047396	Best loss: 0.037489	Accuracy: 98.71%
8	Validation loss: 0.041734	Best loss: 0.037489	Accuracy: 99.02%
9	Validation loss: 0.040156	Best loss: 0.037489	Accuracy: 98.75%
10	Validation loss: 0.052603	Best loss: 0.037489	Accuracy: 98.83%
11	Validation loss: 0.052343	Best loss: 0.037489	Accuracy: 99.06%
12	Validation loss: 0.043791	Best loss: 0.037489	Accuracy: 98.75%
13	Validation loss: 0.042373	Best loss: 0.037489	Accuracy: 98.87%
14	Validation loss: 0.040559	Best loss: 0.037489	Accuracy: 99.06%
15	Validation loss: 0.039487	Best loss: 0.037489	Accuracy: 98.94%
16	Validation loss: 0.037904	Best loss: 0.037489	Accuracy: 99.18%
17	Validation loss: 0.054451	Best loss: 0.037489	Accuracy: 98.83%
18	Validation loss: 0.039928	Best loss: 0.037489	Accuracy: 98.94%
19	Validation loss: 0.044322	Best loss: 0.037489	Accuracy: 99.02%
20	Validation loss: 0.058098	Best loss: 0.037489	Accuracy: 98.67%
21	Validation loss: 0.051576	Best loss: 0.037489	Accuracy: 98.87%
22	Validation loss: 0.048221	Best loss: 0.037489	Accuracy: 98.83%
23	Validation loss: 0.040653	Best loss: 0.037489	Accuracy: 99.22%
24	Validation loss: 0.033029	Best loss: 0.033029	Accuracy: 99.10%
25	Validation loss: 0.037656	Best loss: 0.033029	Accuracy: 99.26%
26	Validation loss: 0.029581	Best loss: 0.029581	Accuracy: 99.26%
27	Validation loss: 0.050754	Best loss: 0.029581	Accuracy: 98.94%
28	Validation loss: 0.056092	Best loss: 0.029581	Accuracy: 98.59%
29	Validation loss: 0.052593	Best loss: 0.029581	Accuracy: 98.87%
30	Validation loss: 0.034522	Best loss: 0.029581	Accuracy: 99.10%
31	Validation loss: 0.038023	Best loss: 0.029581	Accuracy: 99.18%
32	Validation loss: 0.045814	Best loss: 0.029581	Accuracy: 98.94%
33	Validation loss: 0.051354	Best loss: 0.029581	Accuracy: 98.83%
34	Validation loss: 0.050840	Best loss: 0.029581	Accuracy: 98.83%
35	Validation loss: 0.043432	Best loss: 0.029581	Accuracy: 99.10%
36	Validation loss: 0.047412	Best loss: 0.029581	Accuracy: 99.02%
37	Validation loss: 0.044335	Best loss: 0.029581	Accuracy: 99.18%
38	Validation loss: 0.040533	Best loss: 0.029581	Accuracy: 98.98%
39	Validation loss: 0.038137	Best loss: 0.029581	Accuracy: 98.87%
40	Validation loss: 0.038930	Best loss: 0.029581	Accuracy: 99.14%
41	Validation loss: 0.038749	Best loss: 0.029581	Accuracy: 99.10%
42	Validation loss: 0.041802	Best loss: 0.029581	Accuracy: 99.22%
43	Validation loss: 0.075345	Best loss: 0.029581	Accuracy: 98.63%
44	Validation loss: 0.037580	Best loss: 0.029581	Accuracy: 99.10%
45	Validation loss: 0.064270	Best loss: 0.029581	Accuracy: 98.91%
46	Validation loss: 0.035316	Best loss: 0.029581	Accuracy: 99.18%
47	Validation loss: 0.033100	Best loss: 0.029581	Accuracy: 99.18%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, batch_size=100, batch_norm_momentum=0.98, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, total=  46.5s
[CV] n_neurons=160, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.194263	Best loss: 0.194263	Accuracy: 92.18%
1	Validation loss: 0.062305	Best loss: 0.062305	Accuracy: 98.44%
2	Validation loss: 0.071555	Best loss: 0.062305	Accuracy: 98.01%
3	Validation loss: 0.081530	Best loss: 0.062305	Accuracy: 97.73%
4	Validation loss: 0.045931	Best loss: 0.045931	Accuracy: 98.48%
5	Validation loss: 0.061560	Best loss: 0.045931	Accuracy: 97.97%
6	Validation loss: 0.044690	Best loss: 0.044690	Accuracy: 98.59%
7	Validation loss: 0.038489	Best loss: 0.038489	Accuracy: 98.98%
8	Validation loss: 0.049854	Best loss: 0.038489	Accuracy: 98.32%
9	Validation loss: 0.050443	Best loss: 0.038489	Accuracy: 98.71%
10	Validation loss: 0.041573	Best loss: 0.038489	Accuracy: 98.75%
11	Validation loss: 0.041079	Best loss: 0.038489	Accuracy: 98.83%
12	Validation loss: 0.041281	Best loss: 0.038489	Accuracy: 98.94%
13	Validation loss: 0.044597	Best loss: 0.038489	Accuracy: 98.87%
14	Validation loss: 0.040933	Best loss: 0.038489	Accuracy: 98.71%
15	Validation loss: 0.035449	Best loss: 0.035449	Accuracy: 99.10%
16	Validation loss: 0.049151	Best loss: 0.035449	Accuracy: 98.51%
17	Validation loss: 0.031724	Best loss: 0.031724	Accuracy: 98.98%
18	Validation loss: 0.038157	Best loss: 0.031724	Accuracy: 98.98%
19	Validation loss: 0.034957	Best loss: 0.031724	Accuracy: 98.83%
20	Validation loss: 0.037013	Best loss: 0.031724	Accuracy: 98.94%
21	Validation loss: 0.032372	Best loss: 0.031724	Accuracy: 99.14%
22	Validation loss: 0.048745	Best loss: 0.031724	Accuracy: 98.87%
23	Validation loss: 0.044589	Best loss: 0.031724	Accuracy: 98.63%
24	Validation loss: 0.040344	Best loss: 0.031724	Accuracy: 98.87%
25	Validation loss: 0.042834	Best loss: 0.031724	Accuracy: 98.91%
26	Validation loss: 0.050206	Best loss: 0.031724	Accuracy: 98.71%
27	Validation loss: 0.036327	Best loss: 0.031724	Accuracy: 99.10%
28	Validation loss: 0.056246	Best loss: 0.031724	Accuracy: 98.91%
29	Validation loss: 0.041369	Best loss: 0.031724	Accuracy: 99.02%
30	Validation loss: 0.031294	Best loss: 0.031294	Accuracy: 99.18%
31	Validation loss: 0.043712	Best loss: 0.031294	Accuracy: 98.75%
32	Validation loss: 0.035214	Best loss: 0.031294	Accuracy: 98.91%
33	Validation loss: 0.041293	Best loss: 0.031294	Accuracy: 98.91%
34	Validation loss: 0.035680	Best loss: 0.031294	Accuracy: 99.02%
35	Validation loss: 0.035465	Best loss: 0.031294	Accuracy: 99.14%
36	Validation loss: 0.037823	Best loss: 0.031294	Accuracy: 98.94%
37	Validation loss: 0.038427	Best loss: 0.031294	Accuracy: 98.94%
38	Validation loss: 0.033131	Best loss: 0.031294	Accuracy: 99.06%
39	Validation loss: 0.028165	Best loss: 0.028165	Accuracy: 99.26%
40	Validation loss: 0.041738	Best loss: 0.028165	Accuracy: 98.87%
41	Validation loss: 0.046749	Best loss: 0.028165	Accuracy: 98.75%
42	Validation loss: 0.039929	Best loss: 0.028165	Accuracy: 99.06%
43	Validation loss: 0.041279	Best loss: 0.028165	Accuracy: 98.98%
44	Validation loss: 0.038628	Best loss: 0.028165	Accuracy: 98.91%
45	Validation loss: 0.040119	Best loss: 0.028165	Accuracy: 99.06%
46	Validation loss: 0.053333	Best loss: 0.028165	Accuracy: 98.79%
47	Validation loss: 0.043023	Best loss: 0.028165	Accuracy: 98.94%
48	Validation loss: 0.033106	Best loss: 0.028165	Accuracy: 99.06%
49	Validation loss: 0.042443	Best loss: 0.028165	Accuracy: 98.94%
50	Validation loss: 0.029007	Best loss: 0.028165	Accuracy: 99.26%
51	Validation loss: 0.041799	Best loss: 0.028165	Accuracy: 98.87%
52	Validation loss: 0.037271	Best loss: 0.028165	Accuracy: 99.22%
53	Validation loss: 0.041464	Best loss: 0.028165	Accuracy: 99.02%
54	Validation loss: 0.042819	Best loss: 0.028165	Accuracy: 99.14%
55	Validation loss: 0.029048	Best loss: 0.028165	Accuracy: 99.37%
56	Validation loss: 0.035836	Best loss: 0.028165	Accuracy: 99.06%
57	Validation loss: 0.042308	Best loss: 0.028165	Accuracy: 98.83%
58	Validation loss: 0.029882	Best loss: 0.028165	Accuracy: 99.22%
59	Validation loss: 0.038585	Best loss: 0.028165	Accuracy: 99.06%
60	Validation loss: 0.048003	Best loss: 0.028165	Accuracy: 99.06%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total= 6.4min
[CV] n_neurons=160, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.125279	Best loss: 0.125279	Accuracy: 95.93%
1	Validation loss: 0.071030	Best loss: 0.071030	Accuracy: 97.73%
2	Validation loss: 0.061144	Best loss: 0.061144	Accuracy: 98.24%
3	Validation loss: 0.047124	Best loss: 0.047124	Accuracy: 98.59%
4	Validation loss: 0.053976	Best loss: 0.047124	Accuracy: 98.44%
5	Validation loss: 0.052059	Best loss: 0.047124	Accuracy: 98.44%
6	Validation loss: 0.038567	Best loss: 0.038567	Accuracy: 98.71%
7	Validation loss: 0.039552	Best loss: 0.038567	Accuracy: 98.79%
8	Validation loss: 0.045508	Best loss: 0.038567	Accuracy: 98.63%
9	Validation loss: 0.042367	Best loss: 0.038567	Accuracy: 98.55%
10	Validation loss: 0.048430	Best loss: 0.038567	Accuracy: 98.36%
11	Validation loss: 0.030585	Best loss: 0.030585	Accuracy: 98.98%
12	Validation loss: 0.032378	Best loss: 0.030585	Accuracy: 98.94%
13	Validation loss: 0.034065	Best loss: 0.030585	Accuracy: 98.94%
14	Validation loss: 0.034666	Best loss: 0.030585	Accuracy: 98.75%
15	Validation loss: 0.035716	Best loss: 0.030585	Accuracy: 99.10%
16	Validation loss: 0.030871	Best loss: 0.030585	Accuracy: 99.14%
17	Validation loss: 0.044044	Best loss: 0.030585	Accuracy: 98.83%
18	Validation loss: 0.037901	Best loss: 0.030585	Accuracy: 98.94%
19	Validation loss: 0.033199	Best loss: 0.030585	Accuracy: 98.91%
20	Validation loss: 0.032177	Best loss: 0.030585	Accuracy: 98.98%
21	Validation loss: 0.033908	Best loss: 0.030585	Accuracy: 99.10%
22	Validation loss: 0.027974	Best loss: 0.027974	Accuracy: 98.98%
23	Validation loss: 0.030525	Best loss: 0.027974	Accuracy: 99.18%
24	Validation loss: 0.032599	Best loss: 0.027974	Accuracy: 98.94%
25	Validation loss: 0.027880	Best loss: 0.027880	Accuracy: 99.26%
26	Validation loss: 0.033229	Best loss: 0.027880	Accuracy: 99.14%
27	Validation loss: 0.031339	Best loss: 0.027880	Accuracy: 99.10%
28	Validation loss: 0.038921	Best loss: 0.027880	Accuracy: 98.91%
29	Validation loss: 0.034439	Best loss: 0.027880	Accuracy: 99.14%
30	Validation loss: 0.030426	Best loss: 0.027880	Accuracy: 99.18%
31	Validation loss: 0.030060	Best loss: 0.027880	Accuracy: 99.18%
32	Validation loss: 0.026968	Best loss: 0.026968	Accuracy: 99.10%
33	Validation loss: 0.031592	Best loss: 0.026968	Accuracy: 99.10%
34	Validation loss: 0.036234	Best loss: 0.026968	Accuracy: 99.14%
35	Validation loss: 0.026513	Best loss: 0.026513	Accuracy: 99.06%
36	Validation loss: 0.032712	Best loss: 0.026513	Accuracy: 99.02%
37	Validation loss: 0.047504	Best loss: 0.026513	Accuracy: 98.83%
38	Validation loss: 0.035101	Best loss: 0.026513	Accuracy: 98.98%
39	Validation loss: 0.034704	Best loss: 0.026513	Accuracy: 99.18%
40	Validation loss: 0.024729	Best loss: 0.024729	Accuracy: 99.26%
41	Validation loss: 0.029607	Best loss: 0.024729	Accuracy: 99.34%
42	Validation loss: 0.033165	Best loss: 0.024729	Accuracy: 99.14%
43	Validation loss: 0.031072	Best loss: 0.024729	Accuracy: 99.18%
44	Validation loss: 0.030087	Best loss: 0.024729	Accuracy: 99.30%
45	Validation loss: 0.035232	Best loss: 0.024729	Accuracy: 99.14%
46	Validation loss: 0.040086	Best loss: 0.024729	Accuracy: 99.02%
47	Validation loss: 0.036397	Best loss: 0.024729	Accuracy: 99.02%
48	Validation loss: 0.025754	Best loss: 0.024729	Accuracy: 99.30%
49	Validation loss: 0.035026	Best loss: 0.024729	Accuracy: 98.98%
50	Validation loss: 0.032219	Best loss: 0.024729	Accuracy: 99.30%
51	Validation loss: 0.028492	Best loss: 0.024729	Accuracy: 99.06%
52	Validation loss: 0.034988	Best loss: 0.024729	Accuracy: 99.26%
53	Validation loss: 0.033349	Best loss: 0.024729	Accuracy: 99.02%
54	Validation loss: 0.042076	Best loss: 0.024729	Accuracy: 98.87%
55	Validation loss: 0.031582	Best loss: 0.024729	Accuracy: 99.02%
56	Validation loss: 0.029047	Best loss: 0.024729	Accuracy: 99.14%
57	Validation loss: 0.038444	Best loss: 0.024729	Accuracy: 99.14%
58	Validation loss: 0.040142	Best loss: 0.024729	Accuracy: 99.22%
59	Validation loss: 0.040504	Best loss: 0.024729	Accuracy: 99.14%
60	Validation loss: 0.036769	Best loss: 0.024729	Accuracy: 98.91%
61	Validation loss: 0.021528	Best loss: 0.021528	Accuracy: 99.34%
62	Validation loss: 0.035025	Best loss: 0.021528	Accuracy: 99.18%
63	Validation loss: 0.029847	Best loss: 0.021528	Accuracy: 99.14%
64	Validation loss: 0.033717	Best loss: 0.021528	Accuracy: 99.02%
65	Validation loss: 0.044246	Best loss: 0.021528	Accuracy: 99.10%
66	Validation loss: 0.029384	Best loss: 0.021528	Accuracy: 99.14%
67	Validation loss: 0.028212	Best loss: 0.021528	Accuracy: 99.18%
68	Validation loss: 0.029681	Best loss: 0.021528	Accuracy: 99.41%
69	Validation loss: 0.028162	Best loss: 0.021528	Accuracy: 99.30%
70	Validation loss: 0.039692	Best loss: 0.021528	Accuracy: 98.98%
71	Validation loss: 0.031265	Best loss: 0.021528	Accuracy: 99.18%
72	Validation loss: 0.037403	Best loss: 0.021528	Accuracy: 99.10%
73	Validation loss: 0.030167	Best loss: 0.021528	Accuracy: 99.18%
74	Validation loss: 0.030428	Best loss: 0.021528	Accuracy: 99.14%
75	Validation loss: 0.027041	Best loss: 0.021528	Accuracy: 99.37%
76	Validation loss: 0.034668	Best loss: 0.021528	Accuracy: 99.02%
77	Validation loss: 0.024339	Best loss: 0.021528	Accuracy: 99.30%
78	Validation loss: 0.029523	Best loss: 0.021528	Accuracy: 98.91%
79	Validation loss: 0.027303	Best loss: 0.021528	Accuracy: 99.18%
80	Validation loss: 0.031002	Best loss: 0.021528	Accuracy: 99.22%
81	Validation loss: 0.034269	Best loss: 0.021528	Accuracy: 99.10%
82	Validation loss: 0.028585	Best loss: 0.021528	Accuracy: 99.37%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total= 8.7min
[CV] n_neurons=160, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.092178	Best loss: 0.092178	Accuracy: 97.42%
1	Validation loss: 0.055764	Best loss: 0.055764	Accuracy: 98.20%
2	Validation loss: 0.072953	Best loss: 0.055764	Accuracy: 97.97%
3	Validation loss: 0.048374	Best loss: 0.048374	Accuracy: 98.63%
4	Validation loss: 0.049489	Best loss: 0.048374	Accuracy: 98.24%
5	Validation loss: 0.051553	Best loss: 0.048374	Accuracy: 98.48%
6	Validation loss: 0.046917	Best loss: 0.046917	Accuracy: 98.32%
7	Validation loss: 0.038950	Best loss: 0.038950	Accuracy: 98.83%
8	Validation loss: 0.035565	Best loss: 0.035565	Accuracy: 98.98%
9	Validation loss: 0.048328	Best loss: 0.035565	Accuracy: 98.40%
10	Validation loss: 0.043398	Best loss: 0.035565	Accuracy: 98.67%
11	Validation loss: 0.037186	Best loss: 0.035565	Accuracy: 98.79%
12	Validation loss: 0.034787	Best loss: 0.034787	Accuracy: 98.87%
13	Validation loss: 0.038277	Best loss: 0.034787	Accuracy: 98.87%
14	Validation loss: 0.037145	Best loss: 0.034787	Accuracy: 98.83%
15	Validation loss: 0.034505	Best loss: 0.034505	Accuracy: 98.83%
16	Validation loss: 0.031613	Best loss: 0.031613	Accuracy: 99.10%
17	Validation loss: 0.030909	Best loss: 0.030909	Accuracy: 99.06%
18	Validation loss: 0.027836	Best loss: 0.027836	Accuracy: 99.06%
19	Validation loss: 0.040615	Best loss: 0.027836	Accuracy: 98.67%
20	Validation loss: 0.043191	Best loss: 0.027836	Accuracy: 98.79%
21	Validation loss: 0.029394	Best loss: 0.027836	Accuracy: 99.02%
22	Validation loss: 0.031409	Best loss: 0.027836	Accuracy: 99.14%
23	Validation loss: 0.036454	Best loss: 0.027836	Accuracy: 99.06%
24	Validation loss: 0.030455	Best loss: 0.027836	Accuracy: 99.10%
25	Validation loss: 0.040359	Best loss: 0.027836	Accuracy: 98.87%
26	Validation loss: 0.030082	Best loss: 0.027836	Accuracy: 99.30%
27	Validation loss: 0.037382	Best loss: 0.027836	Accuracy: 98.79%
28	Validation loss: 0.037749	Best loss: 0.027836	Accuracy: 98.91%
29	Validation loss: 0.042705	Best loss: 0.027836	Accuracy: 98.67%
30	Validation loss: 0.036195	Best loss: 0.027836	Accuracy: 99.22%
31	Validation loss: 0.029713	Best loss: 0.027836	Accuracy: 98.91%
32	Validation loss: 0.026628	Best loss: 0.026628	Accuracy: 99.34%
33	Validation loss: 0.033096	Best loss: 0.026628	Accuracy: 99.06%
34	Validation loss: 0.034919	Best loss: 0.026628	Accuracy: 98.94%
35	Validation loss: 0.041142	Best loss: 0.026628	Accuracy: 98.75%
36	Validation loss: 0.035386	Best loss: 0.026628	Accuracy: 99.18%
37	Validation loss: 0.034700	Best loss: 0.026628	Accuracy: 98.98%
38	Validation loss: 0.038671	Best loss: 0.026628	Accuracy: 99.02%
39	Validation loss: 0.035192	Best loss: 0.026628	Accuracy: 98.83%
40	Validation loss: 0.032017	Best loss: 0.026628	Accuracy: 99.22%
41	Validation loss: 0.040943	Best loss: 0.026628	Accuracy: 99.06%
42	Validation loss: 0.033684	Best loss: 0.026628	Accuracy: 99.26%
43	Validation loss: 0.040608	Best loss: 0.026628	Accuracy: 99.06%
44	Validation loss: 0.031976	Best loss: 0.026628	Accuracy: 99.26%
45	Validation loss: 0.029484	Best loss: 0.026628	Accuracy: 99.30%
46	Validation loss: 0.031006	Best loss: 0.026628	Accuracy: 99.18%
47	Validation loss: 0.027433	Best loss: 0.026628	Accuracy: 99.34%
48	Validation loss: 0.042889	Best loss: 0.026628	Accuracy: 98.98%
49	Validation loss: 0.055158	Best loss: 0.026628	Accuracy: 98.98%
50	Validation loss: 0.046357	Best loss: 0.026628	Accuracy: 98.94%
51	Validation loss: 0.031707	Best loss: 0.026628	Accuracy: 99.26%
52	Validation loss: 0.047847	Best loss: 0.026628	Accuracy: 99.10%
53	Validation loss: 0.033425	Best loss: 0.026628	Accuracy: 99.06%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, batch_size=10, batch_norm_momentum=0.98, activation=&lt;function relu at 0x124366d08&gt;, total= 5.7min
[CV] n_neurons=100, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.215612	Best loss: 0.215612	Accuracy: 96.83%
1	Validation loss: 0.100679	Best loss: 0.100679	Accuracy: 98.12%
2	Validation loss: 0.130656	Best loss: 0.100679	Accuracy: 96.83%
3	Validation loss: 0.096579	Best loss: 0.096579	Accuracy: 98.36%
4	Validation loss: 0.218400	Best loss: 0.096579	Accuracy: 96.17%
5	Validation loss: 0.089112	Best loss: 0.089112	Accuracy: 98.12%
6	Validation loss: 0.105809	Best loss: 0.089112	Accuracy: 98.32%
7	Validation loss: 0.126740	Best loss: 0.089112	Accuracy: 98.08%
8	Validation loss: 0.174558	Best loss: 0.089112	Accuracy: 96.52%
9	Validation loss: 0.201027	Best loss: 0.089112	Accuracy: 96.83%
10	Validation loss: 0.102146	Best loss: 0.089112	Accuracy: 97.58%
11	Validation loss: 0.069652	Best loss: 0.069652	Accuracy: 98.63%
12	Validation loss: 0.063896	Best loss: 0.063896	Accuracy: 98.63%
13	Validation loss: 0.087430	Best loss: 0.063896	Accuracy: 98.59%
14	Validation loss: 0.136114	Best loss: 0.063896	Accuracy: 97.81%
15	Validation loss: 0.147118	Best loss: 0.063896	Accuracy: 97.85%
16	Validation loss: 0.072167	Best loss: 0.063896	Accuracy: 98.79%
17	Validation loss: 0.096562	Best loss: 0.063896	Accuracy: 98.48%
18	Validation loss: 0.129649	Best loss: 0.063896	Accuracy: 98.44%
19	Validation loss: 0.160482	Best loss: 0.063896	Accuracy: 98.01%
20	Validation loss: 0.084761	Best loss: 0.063896	Accuracy: 98.44%
21	Validation loss: 0.141735	Best loss: 0.063896	Accuracy: 96.72%
22	Validation loss: 0.077262	Best loss: 0.063896	Accuracy: 98.67%
23	Validation loss: 0.062373	Best loss: 0.062373	Accuracy: 98.71%
24	Validation loss: 0.061522	Best loss: 0.061522	Accuracy: 98.94%
25	Validation loss: 0.100300	Best loss: 0.061522	Accuracy: 98.36%
26	Validation loss: 0.058802	Best loss: 0.058802	Accuracy: 98.91%
27	Validation loss: 0.068010	Best loss: 0.058802	Accuracy: 98.63%
28	Validation loss: 0.257808	Best loss: 0.058802	Accuracy: 97.46%
29	Validation loss: 0.137622	Best loss: 0.058802	Accuracy: 98.12%
30	Validation loss: 0.158576	Best loss: 0.058802	Accuracy: 98.51%
31	Validation loss: 0.125709	Best loss: 0.058802	Accuracy: 98.71%
32	Validation loss: 0.098387	Best loss: 0.058802	Accuracy: 98.63%
33	Validation loss: 0.073100	Best loss: 0.058802	Accuracy: 98.91%
34	Validation loss: 0.125319	Best loss: 0.058802	Accuracy: 98.12%
35	Validation loss: 0.075129	Best loss: 0.058802	Accuracy: 98.87%
36	Validation loss: 0.097027	Best loss: 0.058802	Accuracy: 98.55%
37	Validation loss: 0.058799	Best loss: 0.058799	Accuracy: 98.94%
38	Validation loss: 0.071077	Best loss: 0.058799	Accuracy: 98.59%
39	Validation loss: 0.074064	Best loss: 0.058799	Accuracy: 98.91%
40	Validation loss: 0.134226	Best loss: 0.058799	Accuracy: 98.55%
41	Validation loss: 0.075155	Best loss: 0.058799	Accuracy: 98.48%
42	Validation loss: 0.072121	Best loss: 0.058799	Accuracy: 98.87%
43	Validation loss: 0.087728	Best loss: 0.058799	Accuracy: 98.55%
44	Validation loss: 0.058622	Best loss: 0.058622	Accuracy: 98.94%
45	Validation loss: 0.129603	Best loss: 0.058622	Accuracy: 98.63%
46	Validation loss: 0.159270	Best loss: 0.058622	Accuracy: 98.36%
47	Validation loss: 0.160739	Best loss: 0.058622	Accuracy: 98.20%
48	Validation loss: 0.150105	Best loss: 0.058622	Accuracy: 98.20%
49	Validation loss: 0.088020	Best loss: 0.058622	Accuracy: 98.67%
50	Validation loss: 0.104522	Best loss: 0.058622	Accuracy: 98.87%
51	Validation loss: 0.073956	Best loss: 0.058622	Accuracy: 99.18%
52	Validation loss: 0.130138	Best loss: 0.058622	Accuracy: 98.59%
53	Validation loss: 0.111898	Best loss: 0.058622	Accuracy: 98.55%
54	Validation loss: 0.099204	Best loss: 0.058622	Accuracy: 98.79%
55	Validation loss: 0.059094	Best loss: 0.058622	Accuracy: 98.98%
56	Validation loss: 0.087425	Best loss: 0.058622	Accuracy: 98.48%
57	Validation loss: 0.323826	Best loss: 0.058622	Accuracy: 98.20%
58	Validation loss: 0.100809	Best loss: 0.058622	Accuracy: 98.83%
59	Validation loss: 0.094934	Best loss: 0.058622	Accuracy: 98.75%
60	Validation loss: 0.068884	Best loss: 0.058622	Accuracy: 99.02%
61	Validation loss: 0.068112	Best loss: 0.058622	Accuracy: 98.94%
62	Validation loss: 0.061194	Best loss: 0.058622	Accuracy: 98.94%
63	Validation loss: 0.129245	Best loss: 0.058622	Accuracy: 98.48%
64	Validation loss: 0.074190	Best loss: 0.058622	Accuracy: 99.02%
65	Validation loss: 0.080172	Best loss: 0.058622	Accuracy: 98.87%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total= 1.2min
[CV] n_neurons=100, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.261523	Best loss: 0.261523	Accuracy: 96.60%
1	Validation loss: 0.208166	Best loss: 0.208166	Accuracy: 97.11%
2	Validation loss: 0.129271	Best loss: 0.129271	Accuracy: 97.77%
3	Validation loss: 0.087268	Best loss: 0.087268	Accuracy: 98.32%
4	Validation loss: 0.107150	Best loss: 0.087268	Accuracy: 97.97%
5	Validation loss: 0.071630	Best loss: 0.071630	Accuracy: 98.32%
6	Validation loss: 0.108378	Best loss: 0.071630	Accuracy: 98.20%
7	Validation loss: 0.105534	Best loss: 0.071630	Accuracy: 97.54%
8	Validation loss: 0.081062	Best loss: 0.071630	Accuracy: 98.51%
9	Validation loss: 0.193077	Best loss: 0.071630	Accuracy: 96.95%
10	Validation loss: 0.062139	Best loss: 0.062139	Accuracy: 98.55%
11	Validation loss: 0.062669	Best loss: 0.062139	Accuracy: 98.67%
12	Validation loss: 0.055767	Best loss: 0.055767	Accuracy: 99.02%
13	Validation loss: 0.079090	Best loss: 0.055767	Accuracy: 98.28%
14	Validation loss: 0.080079	Best loss: 0.055767	Accuracy: 98.71%
15	Validation loss: 0.082531	Best loss: 0.055767	Accuracy: 98.83%
16	Validation loss: 0.064387	Best loss: 0.055767	Accuracy: 98.40%
17	Validation loss: 0.093203	Best loss: 0.055767	Accuracy: 98.40%
18	Validation loss: 0.122585	Best loss: 0.055767	Accuracy: 98.01%
19	Validation loss: 0.098617	Best loss: 0.055767	Accuracy: 98.12%
20	Validation loss: 0.167806	Best loss: 0.055767	Accuracy: 97.85%
21	Validation loss: 0.102969	Best loss: 0.055767	Accuracy: 98.48%
22	Validation loss: 0.103911	Best loss: 0.055767	Accuracy: 98.16%
23	Validation loss: 0.084013	Best loss: 0.055767	Accuracy: 98.79%
24	Validation loss: 0.106922	Best loss: 0.055767	Accuracy: 98.44%
25	Validation loss: 0.081526	Best loss: 0.055767	Accuracy: 98.48%
26	Validation loss: 0.093677	Best loss: 0.055767	Accuracy: 98.51%
27	Validation loss: 0.080948	Best loss: 0.055767	Accuracy: 98.59%
28	Validation loss: 0.172987	Best loss: 0.055767	Accuracy: 98.12%
29	Validation loss: 0.099060	Best loss: 0.055767	Accuracy: 98.32%
30	Validation loss: 0.068580	Best loss: 0.055767	Accuracy: 98.83%
31	Validation loss: 0.077726	Best loss: 0.055767	Accuracy: 98.48%
32	Validation loss: 0.073557	Best loss: 0.055767	Accuracy: 98.91%
33	Validation loss: 0.116289	Best loss: 0.055767	Accuracy: 98.55%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  39.7s
[CV] n_neurons=100, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt; 
0	Validation loss: 0.215648	Best loss: 0.215648	Accuracy: 95.90%
1	Validation loss: 0.128112	Best loss: 0.128112	Accuracy: 97.93%
2	Validation loss: 0.105158	Best loss: 0.105158	Accuracy: 98.08%
3	Validation loss: 0.119454	Best loss: 0.105158	Accuracy: 97.69%
4	Validation loss: 0.159320	Best loss: 0.105158	Accuracy: 97.97%
5	Validation loss: 0.087713	Best loss: 0.087713	Accuracy: 98.36%
6	Validation loss: 0.090773	Best loss: 0.087713	Accuracy: 98.24%
7	Validation loss: 0.124262	Best loss: 0.087713	Accuracy: 97.89%
8	Validation loss: 0.085766	Best loss: 0.085766	Accuracy: 98.16%
9	Validation loss: 0.101578	Best loss: 0.085766	Accuracy: 98.16%
10	Validation loss: 0.069796	Best loss: 0.069796	Accuracy: 98.75%
11	Validation loss: 0.107285	Best loss: 0.069796	Accuracy: 97.89%
12	Validation loss: 0.094734	Best loss: 0.069796	Accuracy: 98.05%
13	Validation loss: 0.074755	Best loss: 0.069796	Accuracy: 98.40%
14	Validation loss: 0.087261	Best loss: 0.069796	Accuracy: 98.36%
15	Validation loss: 0.085755	Best loss: 0.069796	Accuracy: 98.59%
16	Validation loss: 0.064096	Best loss: 0.064096	Accuracy: 98.40%
17	Validation loss: 0.075922	Best loss: 0.064096	Accuracy: 98.63%
18	Validation loss: 0.063350	Best loss: 0.063350	Accuracy: 98.87%
19	Validation loss: 0.057039	Best loss: 0.057039	Accuracy: 98.71%
20	Validation loss: 0.078337	Best loss: 0.057039	Accuracy: 98.67%
21	Validation loss: 0.204208	Best loss: 0.057039	Accuracy: 98.08%
22	Validation loss: 0.104841	Best loss: 0.057039	Accuracy: 98.79%
23	Validation loss: 0.137683	Best loss: 0.057039	Accuracy: 98.44%
24	Validation loss: 0.123482	Best loss: 0.057039	Accuracy: 98.44%
25	Validation loss: 0.060767	Best loss: 0.057039	Accuracy: 98.94%
26	Validation loss: 0.097653	Best loss: 0.057039	Accuracy: 98.40%
27	Validation loss: 0.098380	Best loss: 0.057039	Accuracy: 98.79%
28	Validation loss: 0.074537	Best loss: 0.057039	Accuracy: 98.79%
29	Validation loss: 0.078295	Best loss: 0.057039	Accuracy: 98.48%
30	Validation loss: 0.084714	Best loss: 0.057039	Accuracy: 98.83%
31	Validation loss: 0.116047	Best loss: 0.057039	Accuracy: 98.48%
32	Validation loss: 0.058219	Best loss: 0.057039	Accuracy: 99.06%
33	Validation loss: 0.081626	Best loss: 0.057039	Accuracy: 98.79%
34	Validation loss: 0.075761	Best loss: 0.057039	Accuracy: 98.98%
35	Validation loss: 0.070099	Best loss: 0.057039	Accuracy: 98.94%
36	Validation loss: 0.076656	Best loss: 0.057039	Accuracy: 98.75%
37	Validation loss: 0.101823	Best loss: 0.057039	Accuracy: 98.79%
38	Validation loss: 0.087957	Best loss: 0.057039	Accuracy: 98.94%
39	Validation loss: 0.243325	Best loss: 0.057039	Accuracy: 98.16%
40	Validation loss: 0.142500	Best loss: 0.057039	Accuracy: 98.67%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, batch_size=100, batch_norm_momentum=0.99, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;, total=  47.3s
[CV] n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.130681	Best loss: 0.130681	Accuracy: 96.48%
1	Validation loss: 0.073238	Best loss: 0.073238	Accuracy: 98.20%
2	Validation loss: 0.075592	Best loss: 0.073238	Accuracy: 98.16%
3	Validation loss: 0.074877	Best loss: 0.073238	Accuracy: 98.01%
4	Validation loss: 0.068965	Best loss: 0.068965	Accuracy: 98.32%
5	Validation loss: 0.127293	Best loss: 0.068965	Accuracy: 97.97%
6	Validation loss: 0.090429	Best loss: 0.068965	Accuracy: 98.01%
7	Validation loss: 0.133478	Best loss: 0.068965	Accuracy: 97.22%
8	Validation loss: 0.066298	Best loss: 0.066298	Accuracy: 98.48%
9	Validation loss: 0.069822	Best loss: 0.066298	Accuracy: 98.40%
10	Validation loss: 0.052736	Best loss: 0.052736	Accuracy: 98.67%
11	Validation loss: 0.085359	Best loss: 0.052736	Accuracy: 98.28%
12	Validation loss: 0.061443	Best loss: 0.052736	Accuracy: 98.59%
13	Validation loss: 0.049236	Best loss: 0.049236	Accuracy: 99.02%
14	Validation loss: 0.054593	Best loss: 0.049236	Accuracy: 98.87%
15	Validation loss: 0.070691	Best loss: 0.049236	Accuracy: 98.75%
16	Validation loss: 0.193578	Best loss: 0.049236	Accuracy: 97.46%
17	Validation loss: 0.132510	Best loss: 0.049236	Accuracy: 98.36%
18	Validation loss: 0.175113	Best loss: 0.049236	Accuracy: 97.38%
19	Validation loss: 0.059685	Best loss: 0.049236	Accuracy: 98.94%
20	Validation loss: 0.081680	Best loss: 0.049236	Accuracy: 98.87%
21	Validation loss: 0.047986	Best loss: 0.047986	Accuracy: 99.10%
22	Validation loss: 0.064450	Best loss: 0.047986	Accuracy: 98.91%
23	Validation loss: 0.241962	Best loss: 0.047986	Accuracy: 97.73%
24	Validation loss: 0.343233	Best loss: 0.047986	Accuracy: 97.26%
25	Validation loss: 0.069706	Best loss: 0.047986	Accuracy: 98.71%
26	Validation loss: 0.052558	Best loss: 0.047986	Accuracy: 98.87%
27	Validation loss: 0.044637	Best loss: 0.044637	Accuracy: 99.14%
28	Validation loss: 0.062026	Best loss: 0.044637	Accuracy: 98.98%
29	Validation loss: 0.063934	Best loss: 0.044637	Accuracy: 98.67%
30	Validation loss: 0.453302	Best loss: 0.044637	Accuracy: 98.01%
31	Validation loss: 0.078768	Best loss: 0.044637	Accuracy: 98.79%
32	Validation loss: 0.067066	Best loss: 0.044637	Accuracy: 99.02%
33	Validation loss: 0.056641	Best loss: 0.044637	Accuracy: 99.02%
34	Validation loss: 0.063653	Best loss: 0.044637	Accuracy: 98.98%
35	Validation loss: 0.061435	Best loss: 0.044637	Accuracy: 99.10%
36	Validation loss: 0.270549	Best loss: 0.044637	Accuracy: 97.11%
37	Validation loss: 0.134417	Best loss: 0.044637	Accuracy: 98.91%
38	Validation loss: 0.068843	Best loss: 0.044637	Accuracy: 99.14%
39	Validation loss: 0.050391	Best loss: 0.044637	Accuracy: 99.34%
40	Validation loss: 0.056960	Best loss: 0.044637	Accuracy: 99.14%
41	Validation loss: 0.064762	Best loss: 0.044637	Accuracy: 99.10%
42	Validation loss: 0.067800	Best loss: 0.044637	Accuracy: 98.98%
43	Validation loss: 0.070112	Best loss: 0.044637	Accuracy: 99.26%
44	Validation loss: 0.257068	Best loss: 0.044637	Accuracy: 98.67%
45	Validation loss: 0.077488	Best loss: 0.044637	Accuracy: 99.14%
46	Validation loss: 0.070664	Best loss: 0.044637	Accuracy: 99.26%
47	Validation loss: 0.056761	Best loss: 0.044637	Accuracy: 99.26%
48	Validation loss: 0.067173	Best loss: 0.044637	Accuracy: 99.37%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.6min
[CV] n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.195725	Best loss: 0.195725	Accuracy: 95.19%
1	Validation loss: 0.088697	Best loss: 0.088697	Accuracy: 97.54%
2	Validation loss: 0.102292	Best loss: 0.088697	Accuracy: 97.62%
3	Validation loss: 0.076426	Best loss: 0.076426	Accuracy: 97.77%
4	Validation loss: 0.173641	Best loss: 0.076426	Accuracy: 95.62%
5	Validation loss: 0.055132	Best loss: 0.055132	Accuracy: 98.40%
6	Validation loss: 0.105398	Best loss: 0.055132	Accuracy: 97.46%
7	Validation loss: 0.078238	Best loss: 0.055132	Accuracy: 98.05%
8	Validation loss: 0.044225	Best loss: 0.044225	Accuracy: 98.71%
9	Validation loss: 0.074608	Best loss: 0.044225	Accuracy: 98.28%
10	Validation loss: 0.170062	Best loss: 0.044225	Accuracy: 97.54%
11	Validation loss: 0.064288	Best loss: 0.044225	Accuracy: 98.59%
12	Validation loss: 0.057743	Best loss: 0.044225	Accuracy: 98.71%
13	Validation loss: 0.081350	Best loss: 0.044225	Accuracy: 98.55%
14	Validation loss: 0.174967	Best loss: 0.044225	Accuracy: 96.33%
15	Validation loss: 0.084118	Best loss: 0.044225	Accuracy: 98.16%
16	Validation loss: 0.062983	Best loss: 0.044225	Accuracy: 98.59%
17	Validation loss: 0.159873	Best loss: 0.044225	Accuracy: 97.42%
18	Validation loss: 0.162189	Best loss: 0.044225	Accuracy: 95.90%
19	Validation loss: 0.060217	Best loss: 0.044225	Accuracy: 98.83%
20	Validation loss: 0.170264	Best loss: 0.044225	Accuracy: 98.16%
21	Validation loss: 0.062275	Best loss: 0.044225	Accuracy: 98.83%
22	Validation loss: 0.055897	Best loss: 0.044225	Accuracy: 99.02%
23	Validation loss: 0.131060	Best loss: 0.044225	Accuracy: 98.08%
24	Validation loss: 0.137579	Best loss: 0.044225	Accuracy: 98.40%
25	Validation loss: 0.389327	Best loss: 0.044225	Accuracy: 96.21%
26	Validation loss: 0.062210	Best loss: 0.044225	Accuracy: 98.98%
27	Validation loss: 0.076430	Best loss: 0.044225	Accuracy: 98.94%
28	Validation loss: 0.081926	Best loss: 0.044225	Accuracy: 99.02%
29	Validation loss: 0.062088	Best loss: 0.044225	Accuracy: 99.18%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.0min
[CV] n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.131621	Best loss: 0.131621	Accuracy: 96.60%
1	Validation loss: 0.091536	Best loss: 0.091536	Accuracy: 97.22%
2	Validation loss: 0.146296	Best loss: 0.091536	Accuracy: 96.36%
3	Validation loss: 0.087725	Best loss: 0.087725	Accuracy: 97.93%
4	Validation loss: 0.069015	Best loss: 0.069015	Accuracy: 98.20%
5	Validation loss: 0.078827	Best loss: 0.069015	Accuracy: 97.93%
6	Validation loss: 0.237854	Best loss: 0.069015	Accuracy: 94.64%
7	Validation loss: 0.092917	Best loss: 0.069015	Accuracy: 98.20%
8	Validation loss: 0.058033	Best loss: 0.058033	Accuracy: 98.24%
9	Validation loss: 0.082592	Best loss: 0.058033	Accuracy: 97.97%
10	Validation loss: 0.481154	Best loss: 0.058033	Accuracy: 96.64%
11	Validation loss: 0.059237	Best loss: 0.058033	Accuracy: 98.44%
12	Validation loss: 0.040494	Best loss: 0.040494	Accuracy: 98.71%
13	Validation loss: 0.047392	Best loss: 0.040494	Accuracy: 98.87%
14	Validation loss: 0.089324	Best loss: 0.040494	Accuracy: 98.51%
15	Validation loss: 0.103993	Best loss: 0.040494	Accuracy: 98.63%
16	Validation loss: 0.046979	Best loss: 0.040494	Accuracy: 98.87%
17	Validation loss: 0.296741	Best loss: 0.040494	Accuracy: 95.86%
18	Validation loss: 0.050608	Best loss: 0.040494	Accuracy: 98.71%
19	Validation loss: 0.039963	Best loss: 0.039963	Accuracy: 99.18%
20	Validation loss: 0.040949	Best loss: 0.039963	Accuracy: 99.02%
21	Validation loss: 0.043507	Best loss: 0.039963	Accuracy: 98.75%
22	Validation loss: 0.055937	Best loss: 0.039963	Accuracy: 98.98%
23	Validation loss: 0.156106	Best loss: 0.039963	Accuracy: 98.40%
24	Validation loss: 0.061506	Best loss: 0.039963	Accuracy: 98.87%
25	Validation loss: 0.068907	Best loss: 0.039963	Accuracy: 98.67%
26	Validation loss: 0.058971	Best loss: 0.039963	Accuracy: 99.06%
27	Validation loss: 0.084871	Best loss: 0.039963	Accuracy: 98.59%
28	Validation loss: 0.575259	Best loss: 0.039963	Accuracy: 97.54%
29	Validation loss: 0.062689	Best loss: 0.039963	Accuracy: 98.67%
30	Validation loss: 0.080859	Best loss: 0.039963	Accuracy: 98.67%
31	Validation loss: 0.048816	Best loss: 0.039963	Accuracy: 99.18%
32	Validation loss: 0.054494	Best loss: 0.039963	Accuracy: 98.94%
33	Validation loss: 0.135914	Best loss: 0.039963	Accuracy: 98.28%
34	Validation loss: 0.079665	Best loss: 0.039963	Accuracy: 98.83%
35	Validation loss: 0.087140	Best loss: 0.039963	Accuracy: 98.48%
36	Validation loss: 0.132878	Best loss: 0.039963	Accuracy: 98.79%
37	Validation loss: 0.109718	Best loss: 0.039963	Accuracy: 98.32%
38	Validation loss: 0.069378	Best loss: 0.039963	Accuracy: 99.14%
39	Validation loss: 0.229705	Best loss: 0.039963	Accuracy: 98.59%
40	Validation loss: 0.094734	Best loss: 0.039963	Accuracy: 98.91%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.99, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.4min
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 239.4min finished
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.061972	Best loss: 0.061972	Accuracy: 98.20%
1	Validation loss: 0.049443	Best loss: 0.049443	Accuracy: 98.40%
2	Validation loss: 0.047846	Best loss: 0.047846	Accuracy: 98.55%
3	Validation loss: 0.049768	Best loss: 0.047846	Accuracy: 98.44%
4	Validation loss: 0.039486	Best loss: 0.039486	Accuracy: 98.75%
5	Validation loss: 0.036075	Best loss: 0.036075	Accuracy: 98.83%
6	Validation loss: 0.039363	Best loss: 0.036075	Accuracy: 98.79%
7	Validation loss: 0.035221	Best loss: 0.035221	Accuracy: 98.94%
8	Validation loss: 0.036906	Best loss: 0.035221	Accuracy: 98.71%
9	Validation loss: 0.029160	Best loss: 0.029160	Accuracy: 99.18%
10	Validation loss: 0.032186	Best loss: 0.029160	Accuracy: 98.98%
11	Validation loss: 0.034616	Best loss: 0.029160	Accuracy: 98.98%
12	Validation loss: 0.028812	Best loss: 0.028812	Accuracy: 99.06%
13	Validation loss: 0.029236	Best loss: 0.028812	Accuracy: 99.02%
14	Validation loss: 0.025666	Best loss: 0.025666	Accuracy: 99.26%
15	Validation loss: 0.041227	Best loss: 0.025666	Accuracy: 98.59%
16	Validation loss: 0.025938	Best loss: 0.025666	Accuracy: 99.22%
17	Validation loss: 0.030355	Best loss: 0.025666	Accuracy: 99.22%
18	Validation loss: 0.029403	Best loss: 0.025666	Accuracy: 99.06%
19	Validation loss: 0.035882	Best loss: 0.025666	Accuracy: 99.14%
20	Validation loss: 0.035995	Best loss: 0.025666	Accuracy: 99.18%
21	Validation loss: 0.026429	Best loss: 0.025666	Accuracy: 99.14%
22	Validation loss: 0.030839	Best loss: 0.025666	Accuracy: 99.10%
23	Validation loss: 0.029435	Best loss: 0.025666	Accuracy: 99.14%
24	Validation loss: 0.032878	Best loss: 0.025666	Accuracy: 99.06%
25	Validation loss: 0.025926	Best loss: 0.025666	Accuracy: 99.26%
26	Validation loss: 0.029947	Best loss: 0.025666	Accuracy: 99.10%
27	Validation loss: 0.033443	Best loss: 0.025666	Accuracy: 99.06%
28	Validation loss: 0.034391	Best loss: 0.025666	Accuracy: 98.98%
29	Validation loss: 0.025658	Best loss: 0.025658	Accuracy: 99.06%
30	Validation loss: 0.029674	Best loss: 0.025658	Accuracy: 99.10%
31	Validation loss: 0.032077	Best loss: 0.025658	Accuracy: 99.26%
32	Validation loss: 0.038154	Best loss: 0.025658	Accuracy: 99.18%
33	Validation loss: 0.027095	Best loss: 0.025658	Accuracy: 99.14%
34	Validation loss: 0.028143	Best loss: 0.025658	Accuracy: 99.49%
35	Validation loss: 0.021774	Best loss: 0.021774	Accuracy: 99.18%
36	Validation loss: 0.032222	Best loss: 0.021774	Accuracy: 99.14%
37	Validation loss: 0.025638	Best loss: 0.021774	Accuracy: 99.34%
38	Validation loss: 0.031702	Best loss: 0.021774	Accuracy: 99.02%
39	Validation loss: 0.027012	Best loss: 0.021774	Accuracy: 99.30%
40	Validation loss: 0.027163	Best loss: 0.021774	Accuracy: 99.30%
41	Validation loss: 0.029205	Best loss: 0.021774	Accuracy: 99.26%
42	Validation loss: 0.024973	Best loss: 0.021774	Accuracy: 99.41%
43	Validation loss: 0.036898	Best loss: 0.021774	Accuracy: 98.94%
44	Validation loss: 0.040366	Best loss: 0.021774	Accuracy: 99.14%
45	Validation loss: 0.033711	Best loss: 0.021774	Accuracy: 99.02%
46	Validation loss: 0.046615	Best loss: 0.021774	Accuracy: 98.79%
47	Validation loss: 0.032732	Best loss: 0.021774	Accuracy: 99.26%
48	Validation loss: 0.020177	Best loss: 0.020177	Accuracy: 99.45%
49	Validation loss: 0.031700	Best loss: 0.020177	Accuracy: 99.37%
50	Validation loss: 0.035962	Best loss: 0.020177	Accuracy: 99.14%
51	Validation loss: 0.031128	Best loss: 0.020177	Accuracy: 99.18%
52	Validation loss: 0.038107	Best loss: 0.020177	Accuracy: 99.14%
53	Validation loss: 0.036671	Best loss: 0.020177	Accuracy: 99.18%
54	Validation loss: 0.029867	Best loss: 0.020177	Accuracy: 99.30%
55	Validation loss: 0.039179	Best loss: 0.020177	Accuracy: 99.10%
56	Validation loss: 0.028410	Best loss: 0.020177	Accuracy: 99.10%
57	Validation loss: 0.037625	Best loss: 0.020177	Accuracy: 99.06%
58	Validation loss: 0.035516	Best loss: 0.020177	Accuracy: 99.22%
59	Validation loss: 0.030096	Best loss: 0.020177	Accuracy: 99.37%
60	Validation loss: 0.032056	Best loss: 0.020177	Accuracy: 99.22%
61	Validation loss: 0.026143	Best loss: 0.020177	Accuracy: 99.37%
62	Validation loss: 0.022387	Best loss: 0.020177	Accuracy: 99.45%
63	Validation loss: 0.026331	Best loss: 0.020177	Accuracy: 99.41%
64	Validation loss: 0.034930	Best loss: 0.020177	Accuracy: 99.10%
65	Validation loss: 0.029928	Best loss: 0.020177	Accuracy: 99.30%
66	Validation loss: 0.028943	Best loss: 0.020177	Accuracy: 99.30%
67	Validation loss: 0.034912	Best loss: 0.020177	Accuracy: 99.18%
68	Validation loss: 0.037118	Best loss: 0.020177	Accuracy: 99.18%
69	Validation loss: 0.034165	Best loss: 0.020177	Accuracy: 99.37%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[130]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>RandomizedSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;,
          estimator=DNNClassifier(activation=&lt;function elu at 0x1243639d8&gt;,
       batch_norm_momentum=None, batch_size=20, dropout_rate=None,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42),
          fit_params={&#39;X_valid&#39;: array([[0., 0., ..., 0., 0.],
       [0., 0., ..., 0., 0.],
       ...,
       [0., 0., ..., 0., 0.],
       [0., 0., ..., 0., 0.]], dtype=float32), &#39;y_valid&#39;: array([0, 4, ..., 1, 2], dtype=int32), &#39;n_epochs&#39;: 1000},
          iid=&#39;warn&#39;, n_iter=50, n_jobs=None,
          param_distributions={&#39;n_neurons&#39;: [10, 30, 50, 70, 90, 100, 120, 140, 160], &#39;batch_size&#39;: [10, 50, 100, 500], &#39;learning_rate&#39;: [0.01, 0.02, 0.05, 0.1], &#39;activation&#39;: [&lt;function relu at 0x124366d08&gt;, &lt;function elu at 0x1243639d8&gt;, &lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd2f0&gt;, &lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x1500bd378&gt;], &#39;batch_norm_momentum&#39;: [0.9, 0.95, 0.98, 0.99, 0.999]},
          pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True,
          return_train_score=&#39;warn&#39;, scoring=None, verbose=2)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[131]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnd_search_bn</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[131]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;n_neurons&#39;: 160,
 &#39;learning_rate&#39;: 0.01,
 &#39;batch_size&#39;: 10,
 &#39;batch_norm_momentum&#39;: 0.98,
 &#39;activation&#39;: &lt;function tensorflow.python.ops.gen_nn_ops.relu(features, name=None)&gt;}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[132]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">rnd_search_bn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[132]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9949406499318934</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Slightly better than earlier: 99.49% vs 99.42%. Let's see if dropout can do better.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="8.5.">8.5.<a class="anchor-link" href="#8.5.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's go back to the model we trained earlier and see how it performs on the training set:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[133]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">dnn_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[133]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9950781082816178</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The model performs significantly better on the training set than on the test set (99.51% vs 99.00%), which means it is overfitting the training set. A bit of regularization may help. Let's try adding dropout with a 50% dropout rate:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[134]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn_clf_dropout</span> <span class="o">=</span> <span class="n">DNNClassifier</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                <span class="n">n_neurons</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                                <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">dnn_clf_dropout</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid1</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.131152	Best loss: 0.131152	Accuracy: 96.91%
1	Validation loss: 0.105306	Best loss: 0.105306	Accuracy: 97.46%
2	Validation loss: 0.091219	Best loss: 0.091219	Accuracy: 97.73%
3	Validation loss: 0.089638	Best loss: 0.089638	Accuracy: 97.85%
4	Validation loss: 0.091288	Best loss: 0.089638	Accuracy: 97.69%
5	Validation loss: 0.081112	Best loss: 0.081112	Accuracy: 98.05%
6	Validation loss: 0.075575	Best loss: 0.075575	Accuracy: 98.24%
7	Validation loss: 0.084841	Best loss: 0.075575	Accuracy: 97.77%
8	Validation loss: 0.075269	Best loss: 0.075269	Accuracy: 97.65%
9	Validation loss: 0.076625	Best loss: 0.075269	Accuracy: 98.12%
10	Validation loss: 0.072509	Best loss: 0.072509	Accuracy: 97.97%
11	Validation loss: 0.071006	Best loss: 0.071006	Accuracy: 98.44%
12	Validation loss: 0.073272	Best loss: 0.071006	Accuracy: 98.08%
13	Validation loss: 0.076293	Best loss: 0.071006	Accuracy: 98.16%
14	Validation loss: 0.074955	Best loss: 0.071006	Accuracy: 98.05%
15	Validation loss: 0.066207	Best loss: 0.066207	Accuracy: 98.20%
16	Validation loss: 0.067388	Best loss: 0.066207	Accuracy: 98.08%
17	Validation loss: 0.061916	Best loss: 0.061916	Accuracy: 98.40%
18	Validation loss: 0.064908	Best loss: 0.061916	Accuracy: 98.40%
19	Validation loss: 0.064921	Best loss: 0.061916	Accuracy: 98.40%
20	Validation loss: 0.069939	Best loss: 0.061916	Accuracy: 98.40%
21	Validation loss: 0.069870	Best loss: 0.061916	Accuracy: 98.32%
22	Validation loss: 0.062807	Best loss: 0.061916	Accuracy: 98.24%
23	Validation loss: 0.065312	Best loss: 0.061916	Accuracy: 98.44%
24	Validation loss: 0.067044	Best loss: 0.061916	Accuracy: 98.44%
25	Validation loss: 0.072251	Best loss: 0.061916	Accuracy: 98.16%
26	Validation loss: 0.064444	Best loss: 0.061916	Accuracy: 98.20%
27	Validation loss: 0.069022	Best loss: 0.061916	Accuracy: 98.44%
28	Validation loss: 0.069079	Best loss: 0.061916	Accuracy: 98.28%
29	Validation loss: 0.148266	Best loss: 0.061916	Accuracy: 96.52%
30	Validation loss: 0.119943	Best loss: 0.061916	Accuracy: 96.72%
31	Validation loss: 0.167303	Best loss: 0.061916	Accuracy: 96.68%
32	Validation loss: 0.131897	Best loss: 0.061916	Accuracy: 96.52%
33	Validation loss: 0.146681	Best loss: 0.061916	Accuracy: 95.43%
34	Validation loss: 0.125731	Best loss: 0.061916	Accuracy: 96.64%
35	Validation loss: 0.099879	Best loss: 0.061916	Accuracy: 97.89%
36	Validation loss: 0.096915	Best loss: 0.061916	Accuracy: 97.73%
37	Validation loss: 0.096422	Best loss: 0.061916	Accuracy: 97.85%
38	Validation loss: 0.108040	Best loss: 0.061916	Accuracy: 97.54%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[134]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>DNNClassifier(activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e8501e0&gt;,
       batch_norm_momentum=None, batch_size=500, dropout_rate=0.5,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=5, n_neurons=90,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The best params are reached during epoch 17. Dropout somewhat slowed down convergence.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's check the accuracy:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[135]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">dnn_clf_dropout</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[135]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9861840825063242</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are out of luck, dropout does not seem to help. Let's try tuning the hyperparameters, perhaps we can squeeze a bit more performance out of this model:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[136]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">param_distribs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_neurons&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="mi">160</span><span class="p">],</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">leaky_relu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)],</span>
    <span class="c1"># you could also try exploring different numbers of hidden layers, different optimizers, etc.</span>
    <span class="c1">#&quot;n_hidden_layers&quot;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],</span>
    <span class="c1">#&quot;optimizer_class&quot;: [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],</span>
    <span class="s2">&quot;dropout_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">rnd_search_dropout</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">DNNClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">param_distribs</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                                        <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rnd_search_dropout</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid1</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:</span>
<span class="c1"># fit_params = dict(X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)</span>
<span class="c1"># rnd_search_dropout = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,</span>
<span class="c1">#                                         fit_params=fit_params, random_state=42, verbose=2)</span>
<span class="c1"># rnd_search_dropout.fit(X_train1, y_train1)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 3 folds for each of 50 candidates, totalling 150 fits
[CV] n_neurons=70, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.218595	Best loss: 0.218595	Accuracy: 93.63%
1	Validation loss: 0.210470	Best loss: 0.210470	Accuracy: 94.61%
2	Validation loss: 0.224635	Best loss: 0.210470	Accuracy: 95.50%
3	Validation loss: 0.200494	Best loss: 0.200494	Accuracy: 94.84%
4	Validation loss: 0.184056	Best loss: 0.184056	Accuracy: 95.58%
5	Validation loss: 0.187698	Best loss: 0.184056	Accuracy: 96.33%
6	Validation loss: 0.151692	Best loss: 0.151692	Accuracy: 96.17%
7	Validation loss: 0.176633	Best loss: 0.151692	Accuracy: 96.21%
8	Validation loss: 0.187090	Best loss: 0.151692	Accuracy: 96.01%
9	Validation loss: 0.204406	Best loss: 0.151692	Accuracy: 96.40%
10	Validation loss: 0.193938	Best loss: 0.151692	Accuracy: 95.74%
11	Validation loss: 0.190056	Best loss: 0.151692	Accuracy: 96.21%
12	Validation loss: 0.183601	Best loss: 0.151692	Accuracy: 96.05%
13	Validation loss: 0.179737	Best loss: 0.151692	Accuracy: 96.25%
14	Validation loss: 0.289718	Best loss: 0.151692	Accuracy: 96.29%
15	Validation loss: 0.188605	Best loss: 0.151692	Accuracy: 95.86%
16	Validation loss: 0.195911	Best loss: 0.151692	Accuracy: 96.01%
17	Validation loss: 0.158151	Best loss: 0.151692	Accuracy: 96.25%
18	Validation loss: 0.168049	Best loss: 0.151692	Accuracy: 96.25%
19	Validation loss: 0.170637	Best loss: 0.151692	Accuracy: 96.40%
20	Validation loss: 0.192890	Best loss: 0.151692	Accuracy: 96.21%
21	Validation loss: 0.178800	Best loss: 0.151692	Accuracy: 95.97%
22	Validation loss: 0.185295	Best loss: 0.151692	Accuracy: 96.44%
23	Validation loss: 0.150369	Best loss: 0.150369	Accuracy: 96.91%
24	Validation loss: 0.161164	Best loss: 0.150369	Accuracy: 96.52%
25	Validation loss: 0.180860	Best loss: 0.150369	Accuracy: 96.13%
26	Validation loss: 0.182730	Best loss: 0.150369	Accuracy: 96.52%
27	Validation loss: 0.184583	Best loss: 0.150369	Accuracy: 96.09%
28	Validation loss: 0.183952	Best loss: 0.150369	Accuracy: 95.39%
29	Validation loss: 0.211111	Best loss: 0.150369	Accuracy: 95.54%
30	Validation loss: 0.225760	Best loss: 0.150369	Accuracy: 95.97%
31	Validation loss: 0.170313	Best loss: 0.150369	Accuracy: 96.91%
32	Validation loss: 0.167672	Best loss: 0.150369	Accuracy: 96.64%
33	Validation loss: 0.130638	Best loss: 0.130638	Accuracy: 96.72%
34	Validation loss: 0.149062	Best loss: 0.130638	Accuracy: 96.60%
35	Validation loss: 0.145651	Best loss: 0.130638	Accuracy: 96.76%
36	Validation loss: 0.172332	Best loss: 0.130638	Accuracy: 96.21%
37	Validation loss: 0.176990	Best loss: 0.130638	Accuracy: 96.29%
38	Validation loss: 0.161247	Best loss: 0.130638	Accuracy: 96.56%
39	Validation loss: 0.164381	Best loss: 0.130638	Accuracy: 96.76%
40	Validation loss: 0.156797	Best loss: 0.130638	Accuracy: 96.99%
41	Validation loss: 0.171714	Best loss: 0.130638	Accuracy: 96.64%
42	Validation loss: 0.155848	Best loss: 0.130638	Accuracy: 96.83%
43	Validation loss: 0.175209	Best loss: 0.130638	Accuracy: 96.68%
44	Validation loss: 0.215410	Best loss: 0.130638	Accuracy: 96.48%
45	Validation loss: 0.208599	Best loss: 0.130638	Accuracy: 96.36%
46	Validation loss: 0.169704	Best loss: 0.130638	Accuracy: 97.22%
47	Validation loss: 0.192643	Best loss: 0.130638	Accuracy: 96.52%
48	Validation loss: 0.173377	Best loss: 0.130638	Accuracy: 96.60%
49	Validation loss: 0.174715	Best loss: 0.130638	Accuracy: 96.91%
50	Validation loss: 0.167332	Best loss: 0.130638	Accuracy: 96.40%
51	Validation loss: 0.162273	Best loss: 0.130638	Accuracy: 96.56%
52	Validation loss: 0.176838	Best loss: 0.130638	Accuracy: 96.95%
53	Validation loss: 0.139360	Best loss: 0.130638	Accuracy: 96.87%
54	Validation loss: 0.176649	Best loss: 0.130638	Accuracy: 97.03%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  23.5s
[CV] n_neurons=70, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.6s remaining:    0.0s
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.276019	Best loss: 0.276019	Accuracy: 93.04%
1	Validation loss: 0.227061	Best loss: 0.227061	Accuracy: 94.21%
2	Validation loss: 0.167071	Best loss: 0.167071	Accuracy: 96.09%
3	Validation loss: 0.196825	Best loss: 0.167071	Accuracy: 95.50%
4	Validation loss: 0.179096	Best loss: 0.167071	Accuracy: 96.05%
5	Validation loss: 0.185107	Best loss: 0.167071	Accuracy: 95.70%
6	Validation loss: 0.214200	Best loss: 0.167071	Accuracy: 96.40%
7	Validation loss: 0.183017	Best loss: 0.167071	Accuracy: 95.50%
8	Validation loss: 0.174349	Best loss: 0.167071	Accuracy: 96.91%
9	Validation loss: 0.154234	Best loss: 0.154234	Accuracy: 96.99%
10	Validation loss: 0.187668	Best loss: 0.154234	Accuracy: 96.87%
11	Validation loss: 0.162060	Best loss: 0.154234	Accuracy: 96.36%
12	Validation loss: 0.151462	Best loss: 0.151462	Accuracy: 96.40%
13	Validation loss: 0.179043	Best loss: 0.151462	Accuracy: 96.33%
14	Validation loss: 0.153862	Best loss: 0.151462	Accuracy: 96.79%
15	Validation loss: 0.151592	Best loss: 0.151462	Accuracy: 96.48%
16	Validation loss: 0.138754	Best loss: 0.138754	Accuracy: 96.68%
17	Validation loss: 0.160446	Best loss: 0.138754	Accuracy: 96.40%
18	Validation loss: 0.170256	Best loss: 0.138754	Accuracy: 95.62%
19	Validation loss: 0.144416	Best loss: 0.138754	Accuracy: 96.29%
20	Validation loss: 0.171699	Best loss: 0.138754	Accuracy: 96.83%
21	Validation loss: 0.133689	Best loss: 0.133689	Accuracy: 96.83%
22	Validation loss: 0.181461	Best loss: 0.133689	Accuracy: 96.76%
23	Validation loss: 0.157310	Best loss: 0.133689	Accuracy: 96.87%
24	Validation loss: 0.143672	Best loss: 0.133689	Accuracy: 96.68%
25	Validation loss: 0.144550	Best loss: 0.133689	Accuracy: 96.68%
26	Validation loss: 0.151214	Best loss: 0.133689	Accuracy: 97.38%
27	Validation loss: 0.124557	Best loss: 0.124557	Accuracy: 97.22%
28	Validation loss: 0.175760	Best loss: 0.124557	Accuracy: 95.90%
29	Validation loss: 0.149028	Best loss: 0.124557	Accuracy: 96.87%
30	Validation loss: 0.153891	Best loss: 0.124557	Accuracy: 97.11%
31	Validation loss: 0.140080	Best loss: 0.124557	Accuracy: 96.87%
32	Validation loss: 0.138244	Best loss: 0.124557	Accuracy: 96.95%
33	Validation loss: 0.145208	Best loss: 0.124557	Accuracy: 97.30%
34	Validation loss: 0.138933	Best loss: 0.124557	Accuracy: 97.11%
35	Validation loss: 0.143655	Best loss: 0.124557	Accuracy: 96.83%
36	Validation loss: 0.128823	Best loss: 0.124557	Accuracy: 96.91%
37	Validation loss: 0.139477	Best loss: 0.124557	Accuracy: 97.07%
38	Validation loss: 0.141320	Best loss: 0.124557	Accuracy: 96.36%
39	Validation loss: 0.184301	Best loss: 0.124557	Accuracy: 97.19%
40	Validation loss: 0.123212	Best loss: 0.123212	Accuracy: 97.38%
41	Validation loss: 0.115788	Best loss: 0.115788	Accuracy: 97.15%
42	Validation loss: 0.112611	Best loss: 0.112611	Accuracy: 97.30%
43	Validation loss: 0.145742	Best loss: 0.112611	Accuracy: 95.70%
44	Validation loss: 0.143982	Best loss: 0.112611	Accuracy: 96.48%
45	Validation loss: 0.166225	Best loss: 0.112611	Accuracy: 96.95%
46	Validation loss: 0.144000	Best loss: 0.112611	Accuracy: 96.72%
47	Validation loss: 0.147083	Best loss: 0.112611	Accuracy: 96.99%
48	Validation loss: 0.144231	Best loss: 0.112611	Accuracy: 96.95%
49	Validation loss: 0.155562	Best loss: 0.112611	Accuracy: 97.26%
50	Validation loss: 0.163666	Best loss: 0.112611	Accuracy: 97.19%
51	Validation loss: 0.173634	Best loss: 0.112611	Accuracy: 96.95%
52	Validation loss: 0.156091	Best loss: 0.112611	Accuracy: 96.56%
53	Validation loss: 0.153787	Best loss: 0.112611	Accuracy: 96.79%
54	Validation loss: 0.129556	Best loss: 0.112611	Accuracy: 96.95%
55	Validation loss: 0.123356	Best loss: 0.112611	Accuracy: 97.30%
56	Validation loss: 0.178430	Best loss: 0.112611	Accuracy: 97.03%
57	Validation loss: 0.132625	Best loss: 0.112611	Accuracy: 96.79%
58	Validation loss: 0.162502	Best loss: 0.112611	Accuracy: 96.76%
59	Validation loss: 0.141680	Best loss: 0.112611	Accuracy: 97.03%
60	Validation loss: 0.148366	Best loss: 0.112611	Accuracy: 96.87%
61	Validation loss: 0.126012	Best loss: 0.112611	Accuracy: 97.46%
62	Validation loss: 0.149869	Best loss: 0.112611	Accuracy: 97.34%
63	Validation loss: 0.148718	Best loss: 0.112611	Accuracy: 96.95%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  27.0s
[CV] n_neurons=70, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.245950	Best loss: 0.245950	Accuracy: 94.33%
1	Validation loss: 0.341022	Best loss: 0.245950	Accuracy: 94.10%
2	Validation loss: 0.269338	Best loss: 0.245950	Accuracy: 95.04%
3	Validation loss: 0.237689	Best loss: 0.237689	Accuracy: 94.76%
4	Validation loss: 0.235703	Best loss: 0.235703	Accuracy: 95.58%
5	Validation loss: 0.185173	Best loss: 0.185173	Accuracy: 96.17%
6	Validation loss: 0.174293	Best loss: 0.174293	Accuracy: 95.82%
7	Validation loss: 0.190062	Best loss: 0.174293	Accuracy: 95.90%
8	Validation loss: 0.173456	Best loss: 0.173456	Accuracy: 96.33%
9	Validation loss: 0.185978	Best loss: 0.173456	Accuracy: 96.25%
10	Validation loss: 0.191988	Best loss: 0.173456	Accuracy: 96.17%
11	Validation loss: 0.206212	Best loss: 0.173456	Accuracy: 94.72%
12	Validation loss: 0.169545	Best loss: 0.169545	Accuracy: 95.97%
13	Validation loss: 0.177225	Best loss: 0.169545	Accuracy: 95.82%
14	Validation loss: 0.176032	Best loss: 0.169545	Accuracy: 96.25%
15	Validation loss: 0.185158	Best loss: 0.169545	Accuracy: 96.29%
16	Validation loss: 0.178815	Best loss: 0.169545	Accuracy: 94.84%
17	Validation loss: 0.192015	Best loss: 0.169545	Accuracy: 96.64%
18	Validation loss: 0.162905	Best loss: 0.162905	Accuracy: 96.44%
19	Validation loss: 0.207113	Best loss: 0.162905	Accuracy: 96.13%
20	Validation loss: 0.153924	Best loss: 0.153924	Accuracy: 96.72%
21	Validation loss: 0.141469	Best loss: 0.141469	Accuracy: 96.87%
22	Validation loss: 0.189280	Best loss: 0.141469	Accuracy: 96.21%
23	Validation loss: 0.165525	Best loss: 0.141469	Accuracy: 96.09%
24	Validation loss: 0.216577	Best loss: 0.141469	Accuracy: 96.33%
25	Validation loss: 0.171487	Best loss: 0.141469	Accuracy: 96.68%
26	Validation loss: 0.173146	Best loss: 0.141469	Accuracy: 95.97%
27	Validation loss: 0.155203	Best loss: 0.141469	Accuracy: 96.33%
28	Validation loss: 0.175633	Best loss: 0.141469	Accuracy: 96.13%
29	Validation loss: 0.202532	Best loss: 0.141469	Accuracy: 96.48%
30	Validation loss: 0.244739	Best loss: 0.141469	Accuracy: 95.47%
31	Validation loss: 0.156575	Best loss: 0.141469	Accuracy: 96.52%
32	Validation loss: 0.185258	Best loss: 0.141469	Accuracy: 96.29%
33	Validation loss: 0.172475	Best loss: 0.141469	Accuracy: 96.09%
34	Validation loss: 0.159551	Best loss: 0.141469	Accuracy: 96.48%
35	Validation loss: 0.193882	Best loss: 0.141469	Accuracy: 96.44%
36	Validation loss: 0.205723	Best loss: 0.141469	Accuracy: 96.95%
37	Validation loss: 0.186940	Best loss: 0.141469	Accuracy: 96.52%
38	Validation loss: 0.173718	Best loss: 0.141469	Accuracy: 96.83%
39	Validation loss: 0.144475	Best loss: 0.141469	Accuracy: 97.03%
40	Validation loss: 0.232533	Best loss: 0.141469	Accuracy: 96.25%
41	Validation loss: 0.217063	Best loss: 0.141469	Accuracy: 95.86%
42	Validation loss: 0.168672	Best loss: 0.141469	Accuracy: 96.21%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  18.5s
[CV] n_neurons=90, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 1.065952	Best loss: 1.065952	Accuracy: 63.60%
1	Validation loss: 4.371808	Best loss: 1.065952	Accuracy: 41.01%
2	Validation loss: 1.857064	Best loss: 1.065952	Accuracy: 34.52%
3	Validation loss: 1.252414	Best loss: 1.065952	Accuracy: 51.72%
4	Validation loss: 1.069369	Best loss: 1.065952	Accuracy: 54.34%
5	Validation loss: 12.385769	Best loss: 1.065952	Accuracy: 24.63%
6	Validation loss: 39.437206	Best loss: 1.065952	Accuracy: 22.32%
7	Validation loss: 25.032534	Best loss: 1.065952	Accuracy: 26.23%
8	Validation loss: 31.873491	Best loss: 1.065952	Accuracy: 19.39%
9	Validation loss: 9.973841	Best loss: 1.065952	Accuracy: 31.08%
10	Validation loss: 7.600448	Best loss: 1.065952	Accuracy: 34.83%
11	Validation loss: 7.276349	Best loss: 1.065952	Accuracy: 37.33%
12	Validation loss: 31.970934	Best loss: 1.065952	Accuracy: 24.24%
13	Validation loss: 31.138105	Best loss: 1.065952	Accuracy: 19.55%
14	Validation loss: 18.025707	Best loss: 1.065952	Accuracy: 28.03%
15	Validation loss: 10.958380	Best loss: 1.065952	Accuracy: 38.43%
16	Validation loss: 9.105950	Best loss: 1.065952	Accuracy: 31.20%
17	Validation loss: 20.618380	Best loss: 1.065952	Accuracy: 37.18%
18	Validation loss: 6.066339	Best loss: 1.065952	Accuracy: 37.57%
19	Validation loss: 12.335027	Best loss: 1.065952	Accuracy: 33.35%
20	Validation loss: 54.909836	Best loss: 1.065952	Accuracy: 20.84%
21	Validation loss: 44.954987	Best loss: 1.065952	Accuracy: 20.05%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  56.4s
[CV] n_neurons=90, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 1.192677	Best loss: 1.192677	Accuracy: 51.37%
1	Validation loss: 1.228218	Best loss: 1.192677	Accuracy: 40.42%
2	Validation loss: 10.034978	Best loss: 1.192677	Accuracy: 30.92%
3	Validation loss: 7.621964	Best loss: 1.192677	Accuracy: 26.15%
4	Validation loss: 4.522920	Best loss: 1.192677	Accuracy: 27.80%
5	Validation loss: 3.108067	Best loss: 1.192677	Accuracy: 38.66%
6	Validation loss: 40.893085	Best loss: 1.192677	Accuracy: 18.33%
7	Validation loss: 18122.537109	Best loss: 1.192677	Accuracy: 30.84%
8	Validation loss: 30.669233	Best loss: 1.192677	Accuracy: 19.19%
9	Validation loss: 8.290009	Best loss: 1.192677	Accuracy: 42.06%
10	Validation loss: 4.865032	Best loss: 1.192677	Accuracy: 40.50%
11	Validation loss: 7.020617	Best loss: 1.192677	Accuracy: 27.52%
12	Validation loss: 316.502441	Best loss: 1.192677	Accuracy: 18.73%
13	Validation loss: 17.661232	Best loss: 1.192677	Accuracy: 28.34%
14	Validation loss: 8.495950	Best loss: 1.192677	Accuracy: 32.33%
15	Validation loss: 15.920877	Best loss: 1.192677	Accuracy: 33.15%
16	Validation loss: 17.859270	Best loss: 1.192677	Accuracy: 24.00%
17	Validation loss: 3391888.750000	Best loss: 1.192677	Accuracy: 35.69%
18	Validation loss: 449674.843750	Best loss: 1.192677	Accuracy: 23.10%
19	Validation loss: 319176.562500	Best loss: 1.192677	Accuracy: 22.01%
20	Validation loss: 105201.515625	Best loss: 1.192677	Accuracy: 20.88%
21	Validation loss: 109980.406250	Best loss: 1.192677	Accuracy: 25.96%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  56.0s
[CV] n_neurons=90, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 3.149109	Best loss: 3.149109	Accuracy: 34.60%
1	Validation loss: 3.123392	Best loss: 3.123392	Accuracy: 33.93%
2	Validation loss: 2.898104	Best loss: 2.898104	Accuracy: 27.56%
3	Validation loss: 2.927112	Best loss: 2.898104	Accuracy: 42.81%
4	Validation loss: 2.635811	Best loss: 2.635811	Accuracy: 26.31%
5	Validation loss: 7.130519	Best loss: 2.635811	Accuracy: 30.14%
6	Validation loss: 8.380322	Best loss: 2.635811	Accuracy: 31.35%
7	Validation loss: 31.768156	Best loss: 2.635811	Accuracy: 20.45%
8	Validation loss: 74.629852	Best loss: 2.635811	Accuracy: 18.92%
9	Validation loss: 257.500153	Best loss: 2.635811	Accuracy: 22.05%
10	Validation loss: 125.243965	Best loss: 2.635811	Accuracy: 27.05%
11	Validation loss: 78.628250	Best loss: 2.635811	Accuracy: 34.01%
12	Validation loss: 56.589096	Best loss: 2.635811	Accuracy: 24.86%
13	Validation loss: 44.678085	Best loss: 2.635811	Accuracy: 21.19%
14	Validation loss: 68.907845	Best loss: 2.635811	Accuracy: 33.35%
15	Validation loss: 1302.249878	Best loss: 2.635811	Accuracy: 19.08%
16	Validation loss: 95.379982	Best loss: 2.635811	Accuracy: 31.39%
17	Validation loss: 139.520844	Best loss: 2.635811	Accuracy: 36.28%
18	Validation loss: 205.550522	Best loss: 2.635811	Accuracy: 34.56%
19	Validation loss: 4210814.500000	Best loss: 2.635811	Accuracy: 19.55%
20	Validation loss: 9063020.000000	Best loss: 2.635811	Accuracy: 19.62%
21	Validation loss: 25815854.000000	Best loss: 2.635811	Accuracy: 22.01%
22	Validation loss: 2125249.000000	Best loss: 2.635811	Accuracy: 15.52%
23	Validation loss: 35288.660156	Best loss: 2.635811	Accuracy: 20.91%
24	Validation loss: 150256.531250	Best loss: 2.635811	Accuracy: 36.75%
25	Validation loss: 360149.500000	Best loss: 2.635811	Accuracy: 30.14%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.1min
[CV] n_neurons=30, learning_rate=0.01, dropout_rate=0.3, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.124547	Best loss: 0.124547	Accuracy: 96.36%
1	Validation loss: 0.126200	Best loss: 0.124547	Accuracy: 96.29%
2	Validation loss: 0.115314	Best loss: 0.115314	Accuracy: 96.91%
3	Validation loss: 0.131513	Best loss: 0.115314	Accuracy: 97.03%
4	Validation loss: 0.107771	Best loss: 0.107771	Accuracy: 97.62%
5	Validation loss: 0.110831	Best loss: 0.107771	Accuracy: 97.19%
6	Validation loss: 0.097330	Best loss: 0.097330	Accuracy: 97.54%
7	Validation loss: 0.122829	Best loss: 0.097330	Accuracy: 96.72%
8	Validation loss: 0.106893	Best loss: 0.097330	Accuracy: 97.58%
9	Validation loss: 0.108716	Best loss: 0.097330	Accuracy: 97.46%
10	Validation loss: 0.100174	Best loss: 0.097330	Accuracy: 97.26%
11	Validation loss: 0.102163	Best loss: 0.097330	Accuracy: 97.73%
12	Validation loss: 0.123273	Best loss: 0.097330	Accuracy: 97.50%
13	Validation loss: 0.105853	Best loss: 0.097330	Accuracy: 97.81%
14	Validation loss: 0.098501	Best loss: 0.097330	Accuracy: 97.50%
15	Validation loss: 0.099896	Best loss: 0.097330	Accuracy: 97.62%
16	Validation loss: 0.128206	Best loss: 0.097330	Accuracy: 97.50%
17	Validation loss: 0.099504	Best loss: 0.097330	Accuracy: 97.69%
18	Validation loss: 0.080406	Best loss: 0.080406	Accuracy: 98.01%
19	Validation loss: 0.098465	Best loss: 0.080406	Accuracy: 97.73%
20	Validation loss: 0.086868	Best loss: 0.080406	Accuracy: 97.93%
21	Validation loss: 0.089237	Best loss: 0.080406	Accuracy: 98.05%
22	Validation loss: 0.136142	Best loss: 0.080406	Accuracy: 96.83%
23	Validation loss: 0.108300	Best loss: 0.080406	Accuracy: 97.85%
24	Validation loss: 0.091058	Best loss: 0.080406	Accuracy: 97.50%
25	Validation loss: 0.106284	Best loss: 0.080406	Accuracy: 97.85%
26	Validation loss: 0.093341	Best loss: 0.080406	Accuracy: 97.89%
27	Validation loss: 0.086322	Best loss: 0.080406	Accuracy: 97.89%
28	Validation loss: 0.088272	Best loss: 0.080406	Accuracy: 97.62%
29	Validation loss: 0.091130	Best loss: 0.080406	Accuracy: 97.97%
30	Validation loss: 0.108894	Best loss: 0.080406	Accuracy: 97.50%
31	Validation loss: 0.104060	Best loss: 0.080406	Accuracy: 97.73%
32	Validation loss: 0.093893	Best loss: 0.080406	Accuracy: 97.69%
33	Validation loss: 0.080712	Best loss: 0.080406	Accuracy: 97.93%
34	Validation loss: 0.082936	Best loss: 0.080406	Accuracy: 97.93%
35	Validation loss: 0.100541	Best loss: 0.080406	Accuracy: 97.97%
36	Validation loss: 0.102966	Best loss: 0.080406	Accuracy: 97.34%
37	Validation loss: 0.117753	Best loss: 0.080406	Accuracy: 97.65%
38	Validation loss: 0.095443	Best loss: 0.080406	Accuracy: 98.01%
39	Validation loss: 0.093190	Best loss: 0.080406	Accuracy: 97.77%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, dropout_rate=0.3, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  22.7s
[CV] n_neurons=30, learning_rate=0.01, dropout_rate=0.3, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.153628	Best loss: 0.153628	Accuracy: 95.39%
1	Validation loss: 0.121111	Best loss: 0.121111	Accuracy: 97.22%
2	Validation loss: 0.127724	Best loss: 0.121111	Accuracy: 96.68%
3	Validation loss: 0.113141	Best loss: 0.113141	Accuracy: 97.15%
4	Validation loss: 0.109848	Best loss: 0.109848	Accuracy: 97.73%
5	Validation loss: 0.135818	Best loss: 0.109848	Accuracy: 96.21%
6	Validation loss: 0.091785	Best loss: 0.091785	Accuracy: 97.65%
7	Validation loss: 0.111496	Best loss: 0.091785	Accuracy: 97.15%
8	Validation loss: 0.096230	Best loss: 0.091785	Accuracy: 97.58%
9	Validation loss: 0.102441	Best loss: 0.091785	Accuracy: 97.73%
10	Validation loss: 0.107768	Best loss: 0.091785	Accuracy: 97.65%
11	Validation loss: 0.099049	Best loss: 0.091785	Accuracy: 97.54%
12	Validation loss: 0.082138	Best loss: 0.082138	Accuracy: 97.81%
13	Validation loss: 0.083023	Best loss: 0.082138	Accuracy: 97.81%
14	Validation loss: 0.094314	Best loss: 0.082138	Accuracy: 97.42%
15	Validation loss: 0.085858	Best loss: 0.082138	Accuracy: 97.62%
16	Validation loss: 0.099134	Best loss: 0.082138	Accuracy: 97.58%
17	Validation loss: 0.080198	Best loss: 0.080198	Accuracy: 97.81%
18	Validation loss: 0.099838	Best loss: 0.080198	Accuracy: 97.81%
19	Validation loss: 0.116023	Best loss: 0.080198	Accuracy: 97.54%
20	Validation loss: 0.093423	Best loss: 0.080198	Accuracy: 97.77%
21	Validation loss: 0.113890	Best loss: 0.080198	Accuracy: 97.15%
22	Validation loss: 0.091524	Best loss: 0.080198	Accuracy: 97.85%
23	Validation loss: 0.082849	Best loss: 0.080198	Accuracy: 98.16%
24	Validation loss: 0.093717	Best loss: 0.080198	Accuracy: 97.81%
25	Validation loss: 0.081416	Best loss: 0.080198	Accuracy: 97.97%
26	Validation loss: 0.081506	Best loss: 0.080198	Accuracy: 98.05%
27	Validation loss: 0.087031	Best loss: 0.080198	Accuracy: 98.08%
28	Validation loss: 0.079542	Best loss: 0.079542	Accuracy: 97.97%
29	Validation loss: 0.081227	Best loss: 0.079542	Accuracy: 97.89%
30	Validation loss: 0.096435	Best loss: 0.079542	Accuracy: 97.62%
31	Validation loss: 0.093053	Best loss: 0.079542	Accuracy: 97.97%
32	Validation loss: 0.093693	Best loss: 0.079542	Accuracy: 97.89%
33	Validation loss: 0.095223	Best loss: 0.079542	Accuracy: 98.08%
34	Validation loss: 0.080887	Best loss: 0.079542	Accuracy: 97.93%
35	Validation loss: 0.105172	Best loss: 0.079542	Accuracy: 97.34%
36	Validation loss: 0.121281	Best loss: 0.079542	Accuracy: 97.62%
37	Validation loss: 0.081244	Best loss: 0.079542	Accuracy: 97.89%
38	Validation loss: 0.107326	Best loss: 0.079542	Accuracy: 97.26%
39	Validation loss: 0.083010	Best loss: 0.079542	Accuracy: 97.42%
40	Validation loss: 0.111600	Best loss: 0.079542	Accuracy: 97.54%
41	Validation loss: 0.117297	Best loss: 0.079542	Accuracy: 97.85%
42	Validation loss: 0.094388	Best loss: 0.079542	Accuracy: 97.85%
43	Validation loss: 0.077203	Best loss: 0.077203	Accuracy: 98.20%
44	Validation loss: 0.111137	Best loss: 0.077203	Accuracy: 97.73%
45	Validation loss: 0.095757	Best loss: 0.077203	Accuracy: 97.38%
46	Validation loss: 0.091989	Best loss: 0.077203	Accuracy: 98.20%
47	Validation loss: 0.088911	Best loss: 0.077203	Accuracy: 97.85%
48	Validation loss: 0.087639	Best loss: 0.077203	Accuracy: 98.05%
49	Validation loss: 0.074145	Best loss: 0.074145	Accuracy: 98.32%
50	Validation loss: 0.105206	Best loss: 0.074145	Accuracy: 98.01%
51	Validation loss: 0.075868	Best loss: 0.074145	Accuracy: 98.28%
52	Validation loss: 0.076314	Best loss: 0.074145	Accuracy: 97.93%
53	Validation loss: 0.113472	Best loss: 0.074145	Accuracy: 97.65%
54	Validation loss: 0.102042	Best loss: 0.074145	Accuracy: 98.08%
55	Validation loss: 0.120913	Best loss: 0.074145	Accuracy: 97.69%
56	Validation loss: 0.111772	Best loss: 0.074145	Accuracy: 97.03%
57	Validation loss: 0.090768	Best loss: 0.074145	Accuracy: 98.01%
58	Validation loss: 0.076732	Best loss: 0.074145	Accuracy: 98.08%
59	Validation loss: 0.094617	Best loss: 0.074145	Accuracy: 97.97%
60	Validation loss: 0.082671	Best loss: 0.074145	Accuracy: 97.73%
61	Validation loss: 0.102796	Best loss: 0.074145	Accuracy: 97.62%
62	Validation loss: 0.096650	Best loss: 0.074145	Accuracy: 97.65%
63	Validation loss: 0.085570	Best loss: 0.074145	Accuracy: 98.01%
64	Validation loss: 0.091483	Best loss: 0.074145	Accuracy: 97.69%
65	Validation loss: 0.097417	Best loss: 0.074145	Accuracy: 97.81%
66	Validation loss: 0.083333	Best loss: 0.074145	Accuracy: 97.58%
67	Validation loss: 0.084142	Best loss: 0.074145	Accuracy: 97.89%
68	Validation loss: 0.073276	Best loss: 0.073276	Accuracy: 98.20%
69	Validation loss: 0.081643	Best loss: 0.073276	Accuracy: 98.24%
70	Validation loss: 0.081595	Best loss: 0.073276	Accuracy: 97.69%
71	Validation loss: 0.104317	Best loss: 0.073276	Accuracy: 97.65%
72	Validation loss: 0.098055	Best loss: 0.073276	Accuracy: 97.62%
73	Validation loss: 0.095641	Best loss: 0.073276	Accuracy: 97.50%
74	Validation loss: 0.107960	Best loss: 0.073276	Accuracy: 97.42%
75	Validation loss: 0.076062	Best loss: 0.073276	Accuracy: 98.16%
76	Validation loss: 0.088314	Best loss: 0.073276	Accuracy: 98.05%
77	Validation loss: 0.098763	Best loss: 0.073276	Accuracy: 97.85%
78	Validation loss: 0.076700	Best loss: 0.073276	Accuracy: 98.08%
79	Validation loss: 0.072213	Best loss: 0.072213	Accuracy: 98.36%
80	Validation loss: 0.078771	Best loss: 0.072213	Accuracy: 98.24%
81	Validation loss: 0.089848	Best loss: 0.072213	Accuracy: 97.69%
82	Validation loss: 0.133184	Best loss: 0.072213	Accuracy: 97.65%
83	Validation loss: 0.106056	Best loss: 0.072213	Accuracy: 97.46%
84	Validation loss: 0.106113	Best loss: 0.072213	Accuracy: 97.73%
85	Validation loss: 0.091995	Best loss: 0.072213	Accuracy: 97.89%
86	Validation loss: 0.078245	Best loss: 0.072213	Accuracy: 98.16%
87	Validation loss: 0.085583	Best loss: 0.072213	Accuracy: 97.73%
88	Validation loss: 0.073607	Best loss: 0.072213	Accuracy: 97.97%
89	Validation loss: 0.081464	Best loss: 0.072213	Accuracy: 98.05%
90	Validation loss: 0.129943	Best loss: 0.072213	Accuracy: 97.54%
91	Validation loss: 0.130624	Best loss: 0.072213	Accuracy: 97.77%
92	Validation loss: 0.099594	Best loss: 0.072213	Accuracy: 97.73%
93	Validation loss: 0.101586	Best loss: 0.072213	Accuracy: 97.22%
94	Validation loss: 0.094874	Best loss: 0.072213	Accuracy: 97.77%
95	Validation loss: 0.084132	Best loss: 0.072213	Accuracy: 98.01%
96	Validation loss: 0.091050	Best loss: 0.072213	Accuracy: 97.93%
97	Validation loss: 0.089599	Best loss: 0.072213	Accuracy: 97.93%
98	Validation loss: 0.079683	Best loss: 0.072213	Accuracy: 98.28%
99	Validation loss: 0.079796	Best loss: 0.072213	Accuracy: 97.97%
100	Validation loss: 0.089503	Best loss: 0.072213	Accuracy: 97.89%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, dropout_rate=0.3, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  55.7s
[CV] n_neurons=30, learning_rate=0.01, dropout_rate=0.3, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.125498	Best loss: 0.125498	Accuracy: 96.52%
1	Validation loss: 0.117504	Best loss: 0.117504	Accuracy: 96.64%
2	Validation loss: 0.112952	Best loss: 0.112952	Accuracy: 97.03%
3	Validation loss: 0.106153	Best loss: 0.106153	Accuracy: 97.26%
4	Validation loss: 0.099807	Best loss: 0.099807	Accuracy: 97.15%
5	Validation loss: 0.102514	Best loss: 0.099807	Accuracy: 97.07%
6	Validation loss: 0.099050	Best loss: 0.099050	Accuracy: 97.93%
7	Validation loss: 0.113753	Best loss: 0.099050	Accuracy: 97.50%
8	Validation loss: 0.088899	Best loss: 0.088899	Accuracy: 98.01%
9	Validation loss: 0.113870	Best loss: 0.088899	Accuracy: 97.93%
10	Validation loss: 0.126519	Best loss: 0.088899	Accuracy: 97.15%
11	Validation loss: 0.100891	Best loss: 0.088899	Accuracy: 97.77%
12	Validation loss: 0.094385	Best loss: 0.088899	Accuracy: 97.97%
13	Validation loss: 0.109120	Best loss: 0.088899	Accuracy: 97.62%
14	Validation loss: 0.097719	Best loss: 0.088899	Accuracy: 97.73%
15	Validation loss: 0.096433	Best loss: 0.088899	Accuracy: 98.05%
16	Validation loss: 0.107030	Best loss: 0.088899	Accuracy: 97.42%
17	Validation loss: 0.091396	Best loss: 0.088899	Accuracy: 98.16%
18	Validation loss: 0.098694	Best loss: 0.088899	Accuracy: 97.81%
19	Validation loss: 0.135505	Best loss: 0.088899	Accuracy: 96.25%
20	Validation loss: 0.091852	Best loss: 0.088899	Accuracy: 97.54%
21	Validation loss: 0.115990	Best loss: 0.088899	Accuracy: 97.93%
22	Validation loss: 0.096151	Best loss: 0.088899	Accuracy: 97.81%
23	Validation loss: 0.083617	Best loss: 0.083617	Accuracy: 97.81%
24	Validation loss: 0.094183	Best loss: 0.083617	Accuracy: 97.69%
25	Validation loss: 0.093302	Best loss: 0.083617	Accuracy: 97.85%
26	Validation loss: 0.096547	Best loss: 0.083617	Accuracy: 98.24%
27	Validation loss: 0.092205	Best loss: 0.083617	Accuracy: 97.81%
28	Validation loss: 0.091366	Best loss: 0.083617	Accuracy: 97.34%
29	Validation loss: 0.099451	Best loss: 0.083617	Accuracy: 97.73%
30	Validation loss: 0.099919	Best loss: 0.083617	Accuracy: 97.81%
31	Validation loss: 0.096880	Best loss: 0.083617	Accuracy: 97.85%
32	Validation loss: 0.087808	Best loss: 0.083617	Accuracy: 97.97%
33	Validation loss: 0.092480	Best loss: 0.083617	Accuracy: 97.97%
34	Validation loss: 0.094937	Best loss: 0.083617	Accuracy: 97.58%
35	Validation loss: 0.099161	Best loss: 0.083617	Accuracy: 98.08%
36	Validation loss: 0.130177	Best loss: 0.083617	Accuracy: 97.42%
37	Validation loss: 0.089174	Best loss: 0.083617	Accuracy: 97.46%
38	Validation loss: 0.096209	Best loss: 0.083617	Accuracy: 97.89%
39	Validation loss: 0.086798	Best loss: 0.083617	Accuracy: 98.20%
40	Validation loss: 0.089263	Best loss: 0.083617	Accuracy: 97.97%
41	Validation loss: 0.101665	Best loss: 0.083617	Accuracy: 97.89%
42	Validation loss: 0.085405	Best loss: 0.083617	Accuracy: 98.05%
43	Validation loss: 0.095107	Best loss: 0.083617	Accuracy: 97.73%
44	Validation loss: 0.091646	Best loss: 0.083617	Accuracy: 97.85%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, dropout_rate=0.3, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  25.6s
[CV] n_neurons=160, learning_rate=0.02, dropout_rate=0.6, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 31.755949	Best loss: 31.755949	Accuracy: 18.73%
1	Validation loss: 41.187599	Best loss: 31.755949	Accuracy: 25.88%
2	Validation loss: 458.869873	Best loss: 31.755949	Accuracy: 19.27%
3	Validation loss: 98.124886	Best loss: 31.755949	Accuracy: 18.84%
4	Validation loss: 2215.378174	Best loss: 31.755949	Accuracy: 22.01%
5	Validation loss: 514.222717	Best loss: 31.755949	Accuracy: 19.27%
6	Validation loss: 373.956940	Best loss: 31.755949	Accuracy: 18.80%
7	Validation loss: 571.827637	Best loss: 31.755949	Accuracy: 18.73%
8	Validation loss: 618.447266	Best loss: 31.755949	Accuracy: 22.01%
9	Validation loss: 2207.020020	Best loss: 31.755949	Accuracy: 19.27%
10	Validation loss: 343.717865	Best loss: 31.755949	Accuracy: 33.23%
11	Validation loss: 8842.362305	Best loss: 31.755949	Accuracy: 18.73%
12	Validation loss: 5575.280762	Best loss: 31.755949	Accuracy: 19.27%
13	Validation loss: 935.747681	Best loss: 31.755949	Accuracy: 18.73%
14	Validation loss: 836.168213	Best loss: 31.755949	Accuracy: 22.40%
15	Validation loss: 1828.602051	Best loss: 31.755949	Accuracy: 20.99%
16	Validation loss: 2176.522217	Best loss: 31.755949	Accuracy: 19.08%
17	Validation loss: 5167.926270	Best loss: 31.755949	Accuracy: 20.91%
18	Validation loss: 5901.267578	Best loss: 31.755949	Accuracy: 19.08%
19	Validation loss: 26079.894531	Best loss: 31.755949	Accuracy: 18.73%
20	Validation loss: 16150.806641	Best loss: 31.755949	Accuracy: 19.08%
21	Validation loss: 5289.549316	Best loss: 31.755949	Accuracy: 23.57%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.02, dropout_rate=0.6, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.2min
[CV] n_neurons=160, learning_rate=0.02, dropout_rate=0.6, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 62.122227	Best loss: 62.122227	Accuracy: 18.73%
1	Validation loss: 173.970886	Best loss: 62.122227	Accuracy: 22.01%
2	Validation loss: 98.909653	Best loss: 62.122227	Accuracy: 20.91%
3	Validation loss: 174.749603	Best loss: 62.122227	Accuracy: 19.27%
4	Validation loss: 2028.185547	Best loss: 62.122227	Accuracy: 19.08%
5	Validation loss: 325.622101	Best loss: 62.122227	Accuracy: 22.01%
6	Validation loss: 662.654907	Best loss: 62.122227	Accuracy: 19.27%
7	Validation loss: 296.678192	Best loss: 62.122227	Accuracy: 18.73%
8	Validation loss: 784.133301	Best loss: 62.122227	Accuracy: 22.44%
9	Validation loss: 2599.937744	Best loss: 62.122227	Accuracy: 18.57%
10	Validation loss: 3728.447266	Best loss: 62.122227	Accuracy: 22.01%
11	Validation loss: 5987.675293	Best loss: 62.122227	Accuracy: 20.91%
12	Validation loss: 6189.053223	Best loss: 62.122227	Accuracy: 20.91%
13	Validation loss: 4358.875488	Best loss: 62.122227	Accuracy: 22.01%
14	Validation loss: 3006.277344	Best loss: 62.122227	Accuracy: 19.08%
15	Validation loss: 6855.130371	Best loss: 62.122227	Accuracy: 18.73%
16	Validation loss: 3176.745605	Best loss: 62.122227	Accuracy: 20.91%
17	Validation loss: 3842.056641	Best loss: 62.122227	Accuracy: 19.39%
18	Validation loss: 7369.037598	Best loss: 62.122227	Accuracy: 19.27%
19	Validation loss: 4583.865723	Best loss: 62.122227	Accuracy: 20.91%
20	Validation loss: 1609.454712	Best loss: 62.122227	Accuracy: 18.84%
21	Validation loss: 39176.933594	Best loss: 62.122227	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.02, dropout_rate=0.6, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.2min
[CV] n_neurons=160, learning_rate=0.02, dropout_rate=0.6, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 12.420126	Best loss: 12.420126	Accuracy: 24.63%
1	Validation loss: 934.885376	Best loss: 12.420126	Accuracy: 22.01%
2	Validation loss: 205.143661	Best loss: 12.420126	Accuracy: 19.00%
3	Validation loss: 1749.303589	Best loss: 12.420126	Accuracy: 19.27%
4	Validation loss: 270.632416	Best loss: 12.420126	Accuracy: 18.45%
5	Validation loss: 1074.663696	Best loss: 12.420126	Accuracy: 20.91%
6	Validation loss: 551.823181	Best loss: 12.420126	Accuracy: 19.08%
7	Validation loss: 1889.940186	Best loss: 12.420126	Accuracy: 22.01%
8	Validation loss: 2313.185303	Best loss: 12.420126	Accuracy: 21.42%
9	Validation loss: 3216.696777	Best loss: 12.420126	Accuracy: 19.27%
10	Validation loss: 954.407166	Best loss: 12.420126	Accuracy: 21.66%
11	Validation loss: 1920.309814	Best loss: 12.420126	Accuracy: 22.01%
12	Validation loss: 4397.389160	Best loss: 12.420126	Accuracy: 18.73%
13	Validation loss: 1363.468628	Best loss: 12.420126	Accuracy: 18.73%
14	Validation loss: 1504.946289	Best loss: 12.420126	Accuracy: 18.73%
15	Validation loss: 2924.896484	Best loss: 12.420126	Accuracy: 18.73%
16	Validation loss: 5183.500977	Best loss: 12.420126	Accuracy: 22.01%
17	Validation loss: 3312.605225	Best loss: 12.420126	Accuracy: 19.08%
18	Validation loss: 5272.114258	Best loss: 12.420126	Accuracy: 19.08%
19	Validation loss: 2819.134277	Best loss: 12.420126	Accuracy: 19.27%
20	Validation loss: 2407.189941	Best loss: 12.420126	Accuracy: 22.05%
21	Validation loss: 16488.035156	Best loss: 12.420126	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.02, dropout_rate=0.6, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.2min
[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.097760	Best loss: 0.097760	Accuracy: 97.26%
1	Validation loss: 0.111294	Best loss: 0.097760	Accuracy: 97.58%
2	Validation loss: 0.362426	Best loss: 0.097760	Accuracy: 92.53%
3	Validation loss: 0.268337	Best loss: 0.097760	Accuracy: 94.96%
4	Validation loss: 0.899830	Best loss: 0.097760	Accuracy: 61.57%
5	Validation loss: 1.616819	Best loss: 0.097760	Accuracy: 20.91%
6	Validation loss: 1.675467	Best loss: 0.097760	Accuracy: 19.27%
7	Validation loss: 1.637107	Best loss: 0.097760	Accuracy: 22.01%
8	Validation loss: 1.642585	Best loss: 0.097760	Accuracy: 19.27%
9	Validation loss: 1.758621	Best loss: 0.097760	Accuracy: 22.01%
10	Validation loss: 1.650370	Best loss: 0.097760	Accuracy: 20.91%
11	Validation loss: 1.660387	Best loss: 0.097760	Accuracy: 18.73%
12	Validation loss: 1.611855	Best loss: 0.097760	Accuracy: 18.73%
13	Validation loss: 1.609943	Best loss: 0.097760	Accuracy: 22.01%
14	Validation loss: 1.650425	Best loss: 0.097760	Accuracy: 19.08%
15	Validation loss: 1.628186	Best loss: 0.097760	Accuracy: 22.01%
16	Validation loss: 1.657647	Best loss: 0.097760	Accuracy: 18.73%
17	Validation loss: 1.637847	Best loss: 0.097760	Accuracy: 20.91%
18	Validation loss: 1.692948	Best loss: 0.097760	Accuracy: 22.01%
19	Validation loss: 1.640447	Best loss: 0.097760	Accuracy: 18.73%
20	Validation loss: 1.692037	Best loss: 0.097760	Accuracy: 19.27%
21	Validation loss: 1.620940	Best loss: 0.097760	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  12.6s
[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.092926	Best loss: 0.092926	Accuracy: 97.38%
1	Validation loss: 0.091383	Best loss: 0.091383	Accuracy: 97.77%
2	Validation loss: 0.081719	Best loss: 0.081719	Accuracy: 97.97%
3	Validation loss: 0.129291	Best loss: 0.081719	Accuracy: 96.44%
4	Validation loss: 1.676664	Best loss: 0.081719	Accuracy: 22.01%
5	Validation loss: 1.639133	Best loss: 0.081719	Accuracy: 22.01%
6	Validation loss: 1.717435	Best loss: 0.081719	Accuracy: 19.08%
7	Validation loss: 1.622812	Best loss: 0.081719	Accuracy: 19.27%
8	Validation loss: 1.640758	Best loss: 0.081719	Accuracy: 19.27%
9	Validation loss: 1.671862	Best loss: 0.081719	Accuracy: 22.01%
10	Validation loss: 1.646632	Best loss: 0.081719	Accuracy: 20.91%
11	Validation loss: 1.632683	Best loss: 0.081719	Accuracy: 19.27%
12	Validation loss: 1.650191	Best loss: 0.081719	Accuracy: 22.01%
13	Validation loss: 1.632150	Best loss: 0.081719	Accuracy: 18.73%
14	Validation loss: 1.680196	Best loss: 0.081719	Accuracy: 19.08%
15	Validation loss: 1.623157	Best loss: 0.081719	Accuracy: 20.91%
16	Validation loss: 1.649056	Best loss: 0.081719	Accuracy: 19.27%
17	Validation loss: 1.722346	Best loss: 0.081719	Accuracy: 20.91%
18	Validation loss: 1.687604	Best loss: 0.081719	Accuracy: 22.01%
19	Validation loss: 1.629647	Best loss: 0.081719	Accuracy: 18.73%
20	Validation loss: 1.667140	Best loss: 0.081719	Accuracy: 20.91%
21	Validation loss: 1.633874	Best loss: 0.081719	Accuracy: 18.73%
22	Validation loss: 1.618437	Best loss: 0.081719	Accuracy: 19.08%
23	Validation loss: 1.681983	Best loss: 0.081719	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  13.6s
[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.116153	Best loss: 0.116153	Accuracy: 96.99%
1	Validation loss: 0.111415	Best loss: 0.111415	Accuracy: 97.73%
2	Validation loss: 0.121498	Best loss: 0.111415	Accuracy: 97.11%
3	Validation loss: 1.021870	Best loss: 0.111415	Accuracy: 53.64%
4	Validation loss: 1.494093	Best loss: 0.111415	Accuracy: 18.73%
5	Validation loss: 1.660310	Best loss: 0.111415	Accuracy: 22.01%
6	Validation loss: 1.764827	Best loss: 0.111415	Accuracy: 19.27%
7	Validation loss: 1.623096	Best loss: 0.111415	Accuracy: 22.01%
8	Validation loss: 1.653297	Best loss: 0.111415	Accuracy: 18.73%
9	Validation loss: 1.663819	Best loss: 0.111415	Accuracy: 19.27%
10	Validation loss: 1.640570	Best loss: 0.111415	Accuracy: 22.01%
11	Validation loss: 1.661332	Best loss: 0.111415	Accuracy: 19.27%
12	Validation loss: 1.629516	Best loss: 0.111415	Accuracy: 20.91%
13	Validation loss: 1.650596	Best loss: 0.111415	Accuracy: 19.08%
14	Validation loss: 1.658841	Best loss: 0.111415	Accuracy: 18.73%
15	Validation loss: 1.625707	Best loss: 0.111415	Accuracy: 20.91%
16	Validation loss: 1.692742	Best loss: 0.111415	Accuracy: 19.27%
17	Validation loss: 1.664775	Best loss: 0.111415	Accuracy: 19.08%
18	Validation loss: 1.685849	Best loss: 0.111415	Accuracy: 18.73%
19	Validation loss: 1.663798	Best loss: 0.111415	Accuracy: 18.73%
20	Validation loss: 1.640504	Best loss: 0.111415	Accuracy: 19.08%
21	Validation loss: 1.631610	Best loss: 0.111415	Accuracy: 18.73%
22	Validation loss: 1.651950	Best loss: 0.111415	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  14.2s
[CV] n_neurons=90, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.809552	Best loss: 1.809552	Accuracy: 19.27%
1	Validation loss: 1.667253	Best loss: 1.667253	Accuracy: 19.27%
2	Validation loss: 1.711594	Best loss: 1.667253	Accuracy: 22.01%
3	Validation loss: 1.665511	Best loss: 1.665511	Accuracy: 22.01%
4	Validation loss: 1.984284	Best loss: 1.665511	Accuracy: 19.27%
5	Validation loss: 1.729693	Best loss: 1.665511	Accuracy: 19.08%
6	Validation loss: 1.749931	Best loss: 1.665511	Accuracy: 18.73%
7	Validation loss: 1.695296	Best loss: 1.665511	Accuracy: 19.27%
8	Validation loss: 1.705371	Best loss: 1.665511	Accuracy: 19.27%
9	Validation loss: 1.702078	Best loss: 1.665511	Accuracy: 19.08%
10	Validation loss: 1.680433	Best loss: 1.665511	Accuracy: 19.08%
11	Validation loss: 1.644501	Best loss: 1.644501	Accuracy: 19.08%
12	Validation loss: 1.737914	Best loss: 1.644501	Accuracy: 18.73%
13	Validation loss: 1.720829	Best loss: 1.644501	Accuracy: 19.27%
14	Validation loss: 1.791111	Best loss: 1.644501	Accuracy: 19.08%
15	Validation loss: 1.754777	Best loss: 1.644501	Accuracy: 22.01%
16	Validation loss: 1.872805	Best loss: 1.644501	Accuracy: 22.01%
17	Validation loss: 1.651306	Best loss: 1.644501	Accuracy: 20.91%
18	Validation loss: 2.058171	Best loss: 1.644501	Accuracy: 19.27%
19	Validation loss: 1.684081	Best loss: 1.644501	Accuracy: 20.91%
20	Validation loss: 1.616664	Best loss: 1.616664	Accuracy: 22.01%
21	Validation loss: 1.767431	Best loss: 1.616664	Accuracy: 19.27%
22	Validation loss: 1.624398	Best loss: 1.616664	Accuracy: 19.08%
23	Validation loss: 2.230286	Best loss: 1.616664	Accuracy: 22.01%
24	Validation loss: 1.645891	Best loss: 1.616664	Accuracy: 20.91%
25	Validation loss: 1.865884	Best loss: 1.616664	Accuracy: 19.27%
26	Validation loss: 1.750912	Best loss: 1.616664	Accuracy: 19.27%
27	Validation loss: 1.773167	Best loss: 1.616664	Accuracy: 18.73%
28	Validation loss: 1.718678	Best loss: 1.616664	Accuracy: 19.08%
29	Validation loss: 1.726344	Best loss: 1.616664	Accuracy: 18.73%
30	Validation loss: 1.747628	Best loss: 1.616664	Accuracy: 18.73%
31	Validation loss: 1.802443	Best loss: 1.616664	Accuracy: 22.01%
32	Validation loss: 1.789850	Best loss: 1.616664	Accuracy: 19.08%
33	Validation loss: 2.006667	Best loss: 1.616664	Accuracy: 19.08%
34	Validation loss: 1.747846	Best loss: 1.616664	Accuracy: 22.01%
35	Validation loss: 1.918961	Best loss: 1.616664	Accuracy: 19.08%
36	Validation loss: 1.816369	Best loss: 1.616664	Accuracy: 18.73%
37	Validation loss: 1.957915	Best loss: 1.616664	Accuracy: 19.27%
38	Validation loss: 1.925216	Best loss: 1.616664	Accuracy: 22.01%
39	Validation loss: 1.657459	Best loss: 1.616664	Accuracy: 19.08%
40	Validation loss: 1.734327	Best loss: 1.616664	Accuracy: 19.27%
41	Validation loss: 1.693575	Best loss: 1.616664	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  22.3s
[CV] n_neurons=90, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.247414	Best loss: 2.247414	Accuracy: 22.01%
1	Validation loss: 1.786676	Best loss: 1.786676	Accuracy: 18.73%
2	Validation loss: 1.735868	Best loss: 1.735868	Accuracy: 20.91%
3	Validation loss: 1.752936	Best loss: 1.735868	Accuracy: 22.01%
4	Validation loss: 1.733070	Best loss: 1.733070	Accuracy: 19.08%
5	Validation loss: 1.631109	Best loss: 1.631109	Accuracy: 19.27%
6	Validation loss: 1.675342	Best loss: 1.631109	Accuracy: 19.08%
7	Validation loss: 1.705677	Best loss: 1.631109	Accuracy: 18.73%
8	Validation loss: 1.666357	Best loss: 1.631109	Accuracy: 22.01%
9	Validation loss: 1.973899	Best loss: 1.631109	Accuracy: 19.27%
10	Validation loss: 1.773498	Best loss: 1.631109	Accuracy: 19.08%
11	Validation loss: 1.731041	Best loss: 1.631109	Accuracy: 22.01%
12	Validation loss: 1.683045	Best loss: 1.631109	Accuracy: 19.08%
13	Validation loss: 1.711134	Best loss: 1.631109	Accuracy: 19.08%
14	Validation loss: 1.746579	Best loss: 1.631109	Accuracy: 18.73%
15	Validation loss: 1.637564	Best loss: 1.631109	Accuracy: 22.01%
16	Validation loss: 1.652699	Best loss: 1.631109	Accuracy: 22.01%
17	Validation loss: 1.771894	Best loss: 1.631109	Accuracy: 19.08%
18	Validation loss: 1.738461	Best loss: 1.631109	Accuracy: 20.91%
19	Validation loss: 1.673039	Best loss: 1.631109	Accuracy: 22.01%
20	Validation loss: 1.848861	Best loss: 1.631109	Accuracy: 19.08%
21	Validation loss: 1.727324	Best loss: 1.631109	Accuracy: 22.01%
22	Validation loss: 2.025171	Best loss: 1.631109	Accuracy: 18.73%
23	Validation loss: 1.697259	Best loss: 1.631109	Accuracy: 20.91%
24	Validation loss: 2.174164	Best loss: 1.631109	Accuracy: 18.73%
25	Validation loss: 1.804741	Best loss: 1.631109	Accuracy: 22.01%
26	Validation loss: 1.776861	Best loss: 1.631109	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  14.5s
[CV] n_neurons=90, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.699739	Best loss: 1.699739	Accuracy: 20.91%
1	Validation loss: 1.686078	Best loss: 1.686078	Accuracy: 19.27%
2	Validation loss: 1.672616	Best loss: 1.672616	Accuracy: 22.01%
3	Validation loss: 1.712841	Best loss: 1.672616	Accuracy: 19.27%
4	Validation loss: 1.722020	Best loss: 1.672616	Accuracy: 19.27%
5	Validation loss: 1.714829	Best loss: 1.672616	Accuracy: 20.91%
6	Validation loss: 1.733285	Best loss: 1.672616	Accuracy: 20.91%
7	Validation loss: 1.687666	Best loss: 1.672616	Accuracy: 18.73%
8	Validation loss: 1.690967	Best loss: 1.672616	Accuracy: 22.01%
9	Validation loss: 1.663349	Best loss: 1.663349	Accuracy: 20.91%
10	Validation loss: 1.719319	Best loss: 1.663349	Accuracy: 19.08%
11	Validation loss: 1.806249	Best loss: 1.663349	Accuracy: 22.01%
12	Validation loss: 1.779296	Best loss: 1.663349	Accuracy: 19.08%
13	Validation loss: 1.641739	Best loss: 1.641739	Accuracy: 22.01%
14	Validation loss: 2.110538	Best loss: 1.641739	Accuracy: 19.27%
15	Validation loss: 1.643810	Best loss: 1.641739	Accuracy: 22.01%
16	Validation loss: 1.768661	Best loss: 1.641739	Accuracy: 22.01%
17	Validation loss: 1.734377	Best loss: 1.641739	Accuracy: 18.73%
18	Validation loss: 1.642779	Best loss: 1.641739	Accuracy: 22.01%
19	Validation loss: 1.902249	Best loss: 1.641739	Accuracy: 22.01%
20	Validation loss: 1.750175	Best loss: 1.641739	Accuracy: 19.27%
21	Validation loss: 1.927128	Best loss: 1.641739	Accuracy: 22.01%
22	Validation loss: 1.865211	Best loss: 1.641739	Accuracy: 19.27%
23	Validation loss: 1.688991	Best loss: 1.641739	Accuracy: 20.91%
24	Validation loss: 1.715047	Best loss: 1.641739	Accuracy: 18.73%
25	Validation loss: 1.779720	Best loss: 1.641739	Accuracy: 19.27%
26	Validation loss: 1.712986	Best loss: 1.641739	Accuracy: 18.73%
27	Validation loss: 1.848784	Best loss: 1.641739	Accuracy: 19.27%
28	Validation loss: 1.774314	Best loss: 1.641739	Accuracy: 19.08%
29	Validation loss: 1.895794	Best loss: 1.641739	Accuracy: 19.08%
30	Validation loss: 1.925831	Best loss: 1.641739	Accuracy: 20.91%
31	Validation loss: 2.020853	Best loss: 1.641739	Accuracy: 22.01%
32	Validation loss: 1.719339	Best loss: 1.641739	Accuracy: 19.08%
33	Validation loss: 1.692791	Best loss: 1.641739	Accuracy: 18.73%
34	Validation loss: 1.825000	Best loss: 1.641739	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  18.9s
[CV] n_neurons=70, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.206771	Best loss: 0.206771	Accuracy: 95.82%
1	Validation loss: 0.133008	Best loss: 0.133008	Accuracy: 96.52%
2	Validation loss: 0.126154	Best loss: 0.126154	Accuracy: 96.87%
3	Validation loss: 0.141296	Best loss: 0.126154	Accuracy: 96.91%
4	Validation loss: 0.348047	Best loss: 0.126154	Accuracy: 90.30%
5	Validation loss: 1.614878	Best loss: 0.126154	Accuracy: 22.01%
6	Validation loss: 1.694780	Best loss: 0.126154	Accuracy: 19.27%
7	Validation loss: 1.623662	Best loss: 0.126154	Accuracy: 22.01%
8	Validation loss: 1.705412	Best loss: 0.126154	Accuracy: 19.27%
9	Validation loss: 1.717698	Best loss: 0.126154	Accuracy: 19.08%
10	Validation loss: 1.632342	Best loss: 0.126154	Accuracy: 20.91%
11	Validation loss: 1.627156	Best loss: 0.126154	Accuracy: 22.01%
12	Validation loss: 1.620368	Best loss: 0.126154	Accuracy: 22.01%
13	Validation loss: 1.609050	Best loss: 0.126154	Accuracy: 20.91%
14	Validation loss: 1.645960	Best loss: 0.126154	Accuracy: 19.08%
15	Validation loss: 1.626092	Best loss: 0.126154	Accuracy: 22.01%
16	Validation loss: 1.633659	Best loss: 0.126154	Accuracy: 22.01%
17	Validation loss: 1.629109	Best loss: 0.126154	Accuracy: 19.27%
18	Validation loss: 1.687921	Best loss: 0.126154	Accuracy: 22.01%
19	Validation loss: 1.624402	Best loss: 0.126154	Accuracy: 22.01%
20	Validation loss: 1.651303	Best loss: 0.126154	Accuracy: 19.27%
21	Validation loss: 1.649554	Best loss: 0.126154	Accuracy: 19.27%
22	Validation loss: 1.634667	Best loss: 0.126154	Accuracy: 22.01%
23	Validation loss: 1.614666	Best loss: 0.126154	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  11.4s
[CV] n_neurons=70, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.148001	Best loss: 0.148001	Accuracy: 96.48%
1	Validation loss: 0.133143	Best loss: 0.133143	Accuracy: 96.91%
2	Validation loss: 0.137344	Best loss: 0.133143	Accuracy: 96.52%
3	Validation loss: 0.323064	Best loss: 0.133143	Accuracy: 96.87%
4	Validation loss: 1.045518	Best loss: 0.133143	Accuracy: 51.88%
5	Validation loss: 1.665301	Best loss: 0.133143	Accuracy: 22.01%
6	Validation loss: 1.644206	Best loss: 0.133143	Accuracy: 19.35%
7	Validation loss: 1.535326	Best loss: 0.133143	Accuracy: 24.20%
8	Validation loss: 1.637996	Best loss: 0.133143	Accuracy: 19.27%
9	Validation loss: 1.700608	Best loss: 0.133143	Accuracy: 19.27%
10	Validation loss: 1.614174	Best loss: 0.133143	Accuracy: 22.01%
11	Validation loss: 1.642591	Best loss: 0.133143	Accuracy: 22.01%
12	Validation loss: 1.632229	Best loss: 0.133143	Accuracy: 22.01%
13	Validation loss: 1.656791	Best loss: 0.133143	Accuracy: 18.73%
14	Validation loss: 1.639519	Best loss: 0.133143	Accuracy: 22.01%
15	Validation loss: 1.632744	Best loss: 0.133143	Accuracy: 19.08%
16	Validation loss: 1.618833	Best loss: 0.133143	Accuracy: 19.27%
17	Validation loss: 1.653544	Best loss: 0.133143	Accuracy: 20.91%
18	Validation loss: 1.685818	Best loss: 0.133143	Accuracy: 22.01%
19	Validation loss: 1.632244	Best loss: 0.133143	Accuracy: 19.27%
20	Validation loss: 1.638079	Best loss: 0.133143	Accuracy: 18.73%
21	Validation loss: 1.696035	Best loss: 0.133143	Accuracy: 18.73%
22	Validation loss: 1.623832	Best loss: 0.133143	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  10.9s
[CV] n_neurons=70, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.133174	Best loss: 0.133174	Accuracy: 96.72%
1	Validation loss: 0.184310	Best loss: 0.133174	Accuracy: 95.43%
2	Validation loss: 0.408739	Best loss: 0.133174	Accuracy: 88.74%
3	Validation loss: 0.936387	Best loss: 0.133174	Accuracy: 57.94%
4	Validation loss: 0.728194	Best loss: 0.133174	Accuracy: 74.82%
5	Validation loss: 1.711876	Best loss: 0.133174	Accuracy: 47.89%
6	Validation loss: 1.341217	Best loss: 0.133174	Accuracy: 34.87%
7	Validation loss: 1.654058	Best loss: 0.133174	Accuracy: 22.01%
8	Validation loss: 1.542459	Best loss: 0.133174	Accuracy: 27.72%
9	Validation loss: 1.686077	Best loss: 0.133174	Accuracy: 19.27%
10	Validation loss: 1.637494	Best loss: 0.133174	Accuracy: 22.01%
11	Validation loss: 1.649722	Best loss: 0.133174	Accuracy: 19.27%
12	Validation loss: 1.660311	Best loss: 0.133174	Accuracy: 20.91%
13	Validation loss: 1.621232	Best loss: 0.133174	Accuracy: 19.08%
14	Validation loss: 1.669061	Best loss: 0.133174	Accuracy: 19.08%
15	Validation loss: 1.664208	Best loss: 0.133174	Accuracy: 20.91%
16	Validation loss: 1.684420	Best loss: 0.133174	Accuracy: 18.73%
17	Validation loss: 1.657306	Best loss: 0.133174	Accuracy: 20.91%
18	Validation loss: 1.660587	Best loss: 0.133174	Accuracy: 18.73%
19	Validation loss: 1.618982	Best loss: 0.133174	Accuracy: 19.27%
20	Validation loss: 1.610656	Best loss: 0.133174	Accuracy: 22.01%
21	Validation loss: 1.685967	Best loss: 0.133174	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  10.5s
[CV] n_neurons=50, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 112.840599	Best loss: 112.840599	Accuracy: 21.23%
1	Validation loss: 17.754686	Best loss: 17.754686	Accuracy: 26.62%
2	Validation loss: 3093.645508	Best loss: 17.754686	Accuracy: 19.43%
3	Validation loss: 1884.427490	Best loss: 17.754686	Accuracy: 19.27%
4	Validation loss: 1017.661865	Best loss: 17.754686	Accuracy: 20.91%
5	Validation loss: 482.869812	Best loss: 17.754686	Accuracy: 24.16%
6	Validation loss: 3259.035400	Best loss: 17.754686	Accuracy: 19.31%
7	Validation loss: 622.014282	Best loss: 17.754686	Accuracy: 20.91%
8	Validation loss: 71.771278	Best loss: 17.754686	Accuracy: 25.45%
9	Validation loss: 9514.324219	Best loss: 17.754686	Accuracy: 18.65%
10	Validation loss: 1326.144653	Best loss: 17.754686	Accuracy: 19.08%
11	Validation loss: 651.226746	Best loss: 17.754686	Accuracy: 22.01%
12	Validation loss: 516.290405	Best loss: 17.754686	Accuracy: 29.67%
13	Validation loss: 2088.074219	Best loss: 17.754686	Accuracy: 19.19%
14	Validation loss: 2740.358154	Best loss: 17.754686	Accuracy: 18.73%
15	Validation loss: 744.129578	Best loss: 17.754686	Accuracy: 22.01%
16	Validation loss: 30128.400391	Best loss: 17.754686	Accuracy: 20.91%
17	Validation loss: 410.898254	Best loss: 17.754686	Accuracy: 33.27%
18	Validation loss: 8389.719727	Best loss: 17.754686	Accuracy: 22.01%
19	Validation loss: 23607.337891	Best loss: 17.754686	Accuracy: 19.08%
20	Validation loss: 2818.914307	Best loss: 17.754686	Accuracy: 19.08%
21	Validation loss: 3718.082764	Best loss: 17.754686	Accuracy: 19.27%
22	Validation loss: 1241.060669	Best loss: 17.754686	Accuracy: 31.51%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  50.6s
[CV] n_neurons=50, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 1734.812500	Best loss: 1734.812500	Accuracy: 20.91%
1	Validation loss: 277.595337	Best loss: 277.595337	Accuracy: 19.78%
2	Validation loss: 55505.445312	Best loss: 277.595337	Accuracy: 22.17%
3	Validation loss: 14101.044922	Best loss: 277.595337	Accuracy: 19.23%
4	Validation loss: 11261.555664	Best loss: 277.595337	Accuracy: 19.23%
5	Validation loss: 10318.612305	Best loss: 277.595337	Accuracy: 19.23%
6	Validation loss: 2905.109375	Best loss: 277.595337	Accuracy: 18.80%
7	Validation loss: 2763.647705	Best loss: 277.595337	Accuracy: 18.73%
8	Validation loss: 15370.276367	Best loss: 277.595337	Accuracy: 18.73%
9	Validation loss: 2136.562988	Best loss: 277.595337	Accuracy: 18.73%
10	Validation loss: 981.197693	Best loss: 277.595337	Accuracy: 30.06%
11	Validation loss: 4928.207520	Best loss: 277.595337	Accuracy: 26.66%
12	Validation loss: 703.536865	Best loss: 277.595337	Accuracy: 34.28%
13	Validation loss: 3390.514404	Best loss: 277.595337	Accuracy: 19.27%
14	Validation loss: 3671.494873	Best loss: 277.595337	Accuracy: 22.01%
15	Validation loss: 2393.150146	Best loss: 277.595337	Accuracy: 18.88%
16	Validation loss: 3751.143066	Best loss: 277.595337	Accuracy: 19.12%
17	Validation loss: 11628.908203	Best loss: 277.595337	Accuracy: 20.91%
18	Validation loss: 24219.406250	Best loss: 277.595337	Accuracy: 20.91%
19	Validation loss: 1460.932373	Best loss: 277.595337	Accuracy: 20.91%
20	Validation loss: 7239.874023	Best loss: 277.595337	Accuracy: 18.73%
21	Validation loss: 3692.166260	Best loss: 277.595337	Accuracy: 22.01%
22	Validation loss: 435.223907	Best loss: 277.595337	Accuracy: 23.14%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  50.9s
[CV] n_neurons=50, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 8043.408691	Best loss: 8043.408691	Accuracy: 22.28%
1	Validation loss: 1034.181763	Best loss: 1034.181763	Accuracy: 20.95%
2	Validation loss: 193.191635	Best loss: 193.191635	Accuracy: 19.23%
3	Validation loss: 2923.393311	Best loss: 193.191635	Accuracy: 19.27%
4	Validation loss: 1366.188965	Best loss: 193.191635	Accuracy: 19.08%
5	Validation loss: 5672.761719	Best loss: 193.191635	Accuracy: 20.91%
6	Validation loss: 3778.775146	Best loss: 193.191635	Accuracy: 19.27%
7	Validation loss: 954.368958	Best loss: 193.191635	Accuracy: 18.73%
8	Validation loss: 188552.828125	Best loss: 193.191635	Accuracy: 22.01%
9	Validation loss: 4720.750000	Best loss: 193.191635	Accuracy: 18.73%
10	Validation loss: 2372.748291	Best loss: 193.191635	Accuracy: 18.73%
11	Validation loss: 762.327942	Best loss: 193.191635	Accuracy: 19.27%
12	Validation loss: 31248.888672	Best loss: 193.191635	Accuracy: 20.91%
13	Validation loss: 7025.569824	Best loss: 193.191635	Accuracy: 19.27%
14	Validation loss: 1615.626343	Best loss: 193.191635	Accuracy: 28.11%
15	Validation loss: 6086.400391	Best loss: 193.191635	Accuracy: 16.81%
16	Validation loss: 433.296265	Best loss: 193.191635	Accuracy: 24.28%
17	Validation loss: 13517.555664	Best loss: 193.191635	Accuracy: 20.91%
18	Validation loss: 566.276611	Best loss: 193.191635	Accuracy: 32.88%
19	Validation loss: 11103.629883	Best loss: 193.191635	Accuracy: 18.73%
20	Validation loss: 542.338867	Best loss: 193.191635	Accuracy: 19.27%
21	Validation loss: 408.071411	Best loss: 193.191635	Accuracy: 20.91%
22	Validation loss: 22669.162109	Best loss: 193.191635	Accuracy: 20.91%
23	Validation loss: 10827.114258	Best loss: 193.191635	Accuracy: 20.95%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  53.2s
[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.209996	Best loss: 0.209996	Accuracy: 93.71%
1	Validation loss: 0.227178	Best loss: 0.209996	Accuracy: 93.71%
2	Validation loss: 0.181597	Best loss: 0.181597	Accuracy: 95.39%
3	Validation loss: 0.180284	Best loss: 0.180284	Accuracy: 95.74%
4	Validation loss: 0.174584	Best loss: 0.174584	Accuracy: 95.50%
5	Validation loss: 0.175791	Best loss: 0.174584	Accuracy: 94.68%
6	Validation loss: 0.153330	Best loss: 0.153330	Accuracy: 95.82%
7	Validation loss: 0.193530	Best loss: 0.153330	Accuracy: 95.27%
8	Validation loss: 0.169955	Best loss: 0.153330	Accuracy: 95.54%
9	Validation loss: 0.169321	Best loss: 0.153330	Accuracy: 95.90%
10	Validation loss: 0.157794	Best loss: 0.153330	Accuracy: 96.09%
11	Validation loss: 0.161048	Best loss: 0.153330	Accuracy: 96.05%
12	Validation loss: 0.150454	Best loss: 0.150454	Accuracy: 95.39%
13	Validation loss: 0.171704	Best loss: 0.150454	Accuracy: 95.70%
14	Validation loss: 0.162856	Best loss: 0.150454	Accuracy: 96.29%
15	Validation loss: 0.156168	Best loss: 0.150454	Accuracy: 96.40%
16	Validation loss: 0.159982	Best loss: 0.150454	Accuracy: 96.01%
17	Validation loss: 0.156982	Best loss: 0.150454	Accuracy: 96.25%
18	Validation loss: 0.164442	Best loss: 0.150454	Accuracy: 96.17%
19	Validation loss: 0.155100	Best loss: 0.150454	Accuracy: 96.09%
20	Validation loss: 0.147356	Best loss: 0.147356	Accuracy: 96.25%
21	Validation loss: 0.164624	Best loss: 0.147356	Accuracy: 95.50%
22	Validation loss: 0.147363	Best loss: 0.147356	Accuracy: 96.29%
23	Validation loss: 0.168109	Best loss: 0.147356	Accuracy: 95.78%
24	Validation loss: 0.150704	Best loss: 0.147356	Accuracy: 96.29%
25	Validation loss: 0.163214	Best loss: 0.147356	Accuracy: 95.54%
26	Validation loss: 0.175230	Best loss: 0.147356	Accuracy: 95.50%
27	Validation loss: 0.172972	Best loss: 0.147356	Accuracy: 96.33%
28	Validation loss: 0.157662	Best loss: 0.147356	Accuracy: 96.09%
29	Validation loss: 0.176225	Best loss: 0.147356	Accuracy: 95.07%
30	Validation loss: 0.160818	Best loss: 0.147356	Accuracy: 95.97%
31	Validation loss: 0.149761	Best loss: 0.147356	Accuracy: 96.68%
32	Validation loss: 0.171219	Best loss: 0.147356	Accuracy: 96.01%
33	Validation loss: 0.143312	Best loss: 0.143312	Accuracy: 96.17%
34	Validation loss: 0.148127	Best loss: 0.143312	Accuracy: 96.25%
35	Validation loss: 0.155631	Best loss: 0.143312	Accuracy: 95.97%
36	Validation loss: 0.142618	Best loss: 0.142618	Accuracy: 96.21%
37	Validation loss: 0.143982	Best loss: 0.142618	Accuracy: 96.36%
38	Validation loss: 0.162531	Best loss: 0.142618	Accuracy: 96.21%
39	Validation loss: 0.147272	Best loss: 0.142618	Accuracy: 95.58%
40	Validation loss: 0.148650	Best loss: 0.142618	Accuracy: 96.25%
41	Validation loss: 0.150080	Best loss: 0.142618	Accuracy: 96.17%
42	Validation loss: 0.142196	Best loss: 0.142196	Accuracy: 96.40%
43	Validation loss: 0.183586	Best loss: 0.142196	Accuracy: 94.72%
44	Validation loss: 0.165583	Best loss: 0.142196	Accuracy: 95.86%
45	Validation loss: 0.154117	Best loss: 0.142196	Accuracy: 95.70%
46	Validation loss: 0.171207	Best loss: 0.142196	Accuracy: 96.25%
47	Validation loss: 0.155019	Best loss: 0.142196	Accuracy: 96.52%
48	Validation loss: 0.164345	Best loss: 0.142196	Accuracy: 96.17%
49	Validation loss: 0.157713	Best loss: 0.142196	Accuracy: 96.13%
50	Validation loss: 0.166760	Best loss: 0.142196	Accuracy: 95.93%
51	Validation loss: 0.142981	Best loss: 0.142196	Accuracy: 96.76%
52	Validation loss: 0.165983	Best loss: 0.142196	Accuracy: 95.82%
53	Validation loss: 0.180641	Best loss: 0.142196	Accuracy: 96.25%
54	Validation loss: 0.179991	Best loss: 0.142196	Accuracy: 95.82%
55	Validation loss: 0.177855	Best loss: 0.142196	Accuracy: 94.88%
56	Validation loss: 0.179104	Best loss: 0.142196	Accuracy: 95.66%
57	Validation loss: 0.168772	Best loss: 0.142196	Accuracy: 95.90%
58	Validation loss: 0.154332	Best loss: 0.142196	Accuracy: 96.48%
59	Validation loss: 0.170840	Best loss: 0.142196	Accuracy: 95.90%
60	Validation loss: 0.168451	Best loss: 0.142196	Accuracy: 95.74%
61	Validation loss: 0.189464	Best loss: 0.142196	Accuracy: 96.25%
62	Validation loss: 0.146018	Best loss: 0.142196	Accuracy: 96.44%
63	Validation loss: 0.149252	Best loss: 0.142196	Accuracy: 96.05%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  27.1s
[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.413532	Best loss: 0.413532	Accuracy: 78.93%
1	Validation loss: 0.385050	Best loss: 0.385050	Accuracy: 79.05%
2	Validation loss: 0.384567	Best loss: 0.384567	Accuracy: 79.05%
3	Validation loss: 0.392639	Best loss: 0.384567	Accuracy: 79.28%
4	Validation loss: 0.387301	Best loss: 0.384567	Accuracy: 78.38%
5	Validation loss: 0.426143	Best loss: 0.384567	Accuracy: 78.42%
6	Validation loss: 0.397964	Best loss: 0.384567	Accuracy: 78.69%
7	Validation loss: 0.385946	Best loss: 0.384567	Accuracy: 78.58%
8	Validation loss: 0.385579	Best loss: 0.384567	Accuracy: 79.20%
9	Validation loss: 0.392171	Best loss: 0.384567	Accuracy: 78.54%
10	Validation loss: 0.382838	Best loss: 0.382838	Accuracy: 79.48%
11	Validation loss: 0.435074	Best loss: 0.382838	Accuracy: 78.46%
12	Validation loss: 0.399240	Best loss: 0.382838	Accuracy: 79.44%
13	Validation loss: 0.396836	Best loss: 0.382838	Accuracy: 79.24%
14	Validation loss: 0.385720	Best loss: 0.382838	Accuracy: 78.15%
15	Validation loss: 0.397929	Best loss: 0.382838	Accuracy: 78.26%
16	Validation loss: 0.404445	Best loss: 0.382838	Accuracy: 79.36%
17	Validation loss: 0.400299	Best loss: 0.382838	Accuracy: 78.54%
18	Validation loss: 0.406849	Best loss: 0.382838	Accuracy: 79.75%
19	Validation loss: 0.374512	Best loss: 0.374512	Accuracy: 79.75%
20	Validation loss: 0.398755	Best loss: 0.374512	Accuracy: 79.05%
21	Validation loss: 0.438105	Best loss: 0.374512	Accuracy: 79.32%
22	Validation loss: 0.393677	Best loss: 0.374512	Accuracy: 79.20%
23	Validation loss: 0.404245	Best loss: 0.374512	Accuracy: 79.32%
24	Validation loss: 0.396426	Best loss: 0.374512	Accuracy: 79.05%
25	Validation loss: 0.408338	Best loss: 0.374512	Accuracy: 78.97%
26	Validation loss: 0.408705	Best loss: 0.374512	Accuracy: 78.93%
27	Validation loss: 0.405111	Best loss: 0.374512	Accuracy: 78.66%
28	Validation loss: 0.395781	Best loss: 0.374512	Accuracy: 78.50%
29	Validation loss: 0.409347	Best loss: 0.374512	Accuracy: 79.36%
30	Validation loss: 0.422132	Best loss: 0.374512	Accuracy: 79.48%
31	Validation loss: 0.407705	Best loss: 0.374512	Accuracy: 79.36%
32	Validation loss: 0.393173	Best loss: 0.374512	Accuracy: 78.42%
33	Validation loss: 0.381865	Best loss: 0.374512	Accuracy: 79.63%
34	Validation loss: 0.401122	Best loss: 0.374512	Accuracy: 78.66%
35	Validation loss: 0.389908	Best loss: 0.374512	Accuracy: 79.44%
36	Validation loss: 0.464427	Best loss: 0.374512	Accuracy: 79.32%
37	Validation loss: 0.381457	Best loss: 0.374512	Accuracy: 79.71%
38	Validation loss: 0.414418	Best loss: 0.374512	Accuracy: 79.05%
39	Validation loss: 0.411891	Best loss: 0.374512	Accuracy: 78.97%
40	Validation loss: 0.401615	Best loss: 0.374512	Accuracy: 79.28%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  17.6s
[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.409961	Best loss: 0.409961	Accuracy: 79.01%
1	Validation loss: 0.387816	Best loss: 0.387816	Accuracy: 79.40%
2	Validation loss: 0.384068	Best loss: 0.384068	Accuracy: 79.36%
3	Validation loss: 0.383089	Best loss: 0.383089	Accuracy: 78.34%
4	Validation loss: 0.407078	Best loss: 0.383089	Accuracy: 77.83%
5	Validation loss: 0.382259	Best loss: 0.382259	Accuracy: 79.40%
6	Validation loss: 0.391912	Best loss: 0.382259	Accuracy: 79.36%
7	Validation loss: 0.379487	Best loss: 0.379487	Accuracy: 78.26%
8	Validation loss: 0.381597	Best loss: 0.379487	Accuracy: 79.59%
9	Validation loss: 0.426586	Best loss: 0.379487	Accuracy: 78.30%
10	Validation loss: 0.414779	Best loss: 0.379487	Accuracy: 79.05%
11	Validation loss: 0.462480	Best loss: 0.379487	Accuracy: 79.20%
12	Validation loss: 0.379861	Best loss: 0.379487	Accuracy: 78.46%
13	Validation loss: 0.381398	Best loss: 0.379487	Accuracy: 78.50%
14	Validation loss: 0.388665	Best loss: 0.379487	Accuracy: 78.23%
15	Validation loss: 0.384490	Best loss: 0.379487	Accuracy: 78.46%
16	Validation loss: 0.413577	Best loss: 0.379487	Accuracy: 79.55%
17	Validation loss: 0.375271	Best loss: 0.375271	Accuracy: 78.58%
18	Validation loss: 0.380215	Best loss: 0.375271	Accuracy: 79.59%
19	Validation loss: 0.376369	Best loss: 0.375271	Accuracy: 79.24%
20	Validation loss: 0.386766	Best loss: 0.375271	Accuracy: 79.28%
21	Validation loss: 0.453061	Best loss: 0.375271	Accuracy: 79.40%
22	Validation loss: 0.407842	Best loss: 0.375271	Accuracy: 77.99%
23	Validation loss: 0.389902	Best loss: 0.375271	Accuracy: 78.42%
24	Validation loss: 0.403545	Best loss: 0.375271	Accuracy: 79.44%
25	Validation loss: 0.406225	Best loss: 0.375271	Accuracy: 79.20%
26	Validation loss: 0.398683	Best loss: 0.375271	Accuracy: 79.32%
27	Validation loss: 0.383898	Best loss: 0.375271	Accuracy: 79.24%
28	Validation loss: 0.395271	Best loss: 0.375271	Accuracy: 79.20%
29	Validation loss: 0.386411	Best loss: 0.375271	Accuracy: 79.52%
30	Validation loss: 0.373275	Best loss: 0.373275	Accuracy: 78.66%
31	Validation loss: 0.424772	Best loss: 0.373275	Accuracy: 77.83%
32	Validation loss: 0.397199	Best loss: 0.373275	Accuracy: 79.55%
33	Validation loss: 0.412365	Best loss: 0.373275	Accuracy: 77.91%
34	Validation loss: 0.400686	Best loss: 0.373275	Accuracy: 78.69%
35	Validation loss: 0.418290	Best loss: 0.373275	Accuracy: 77.83%
36	Validation loss: 0.396967	Best loss: 0.373275	Accuracy: 78.97%
37	Validation loss: 0.390092	Best loss: 0.373275	Accuracy: 79.36%
38	Validation loss: 0.400241	Best loss: 0.373275	Accuracy: 79.44%
39	Validation loss: 0.391027	Best loss: 0.373275	Accuracy: 79.40%
40	Validation loss: 0.379812	Best loss: 0.373275	Accuracy: 79.40%
41	Validation loss: 0.391546	Best loss: 0.373275	Accuracy: 78.19%
42	Validation loss: 0.390962	Best loss: 0.373275	Accuracy: 78.93%
43	Validation loss: 0.396540	Best loss: 0.373275	Accuracy: 78.73%
44	Validation loss: 0.415155	Best loss: 0.373275	Accuracy: 79.16%
45	Validation loss: 0.393795	Best loss: 0.373275	Accuracy: 78.97%
46	Validation loss: 0.404473	Best loss: 0.373275	Accuracy: 78.23%
47	Validation loss: 0.414569	Best loss: 0.373275	Accuracy: 79.40%
48	Validation loss: 0.416241	Best loss: 0.373275	Accuracy: 79.05%
49	Validation loss: 0.390862	Best loss: 0.373275	Accuracy: 78.89%
50	Validation loss: 0.398784	Best loss: 0.373275	Accuracy: 79.20%
51	Validation loss: 0.381571	Best loss: 0.373275	Accuracy: 79.16%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  22.5s
[CV] n_neurons=10, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.223560	Best loss: 0.223560	Accuracy: 94.33%
1	Validation loss: 0.176295	Best loss: 0.176295	Accuracy: 95.31%
2	Validation loss: 0.175062	Best loss: 0.175062	Accuracy: 95.47%
3	Validation loss: 0.162911	Best loss: 0.162911	Accuracy: 95.90%
4	Validation loss: 0.162536	Best loss: 0.162536	Accuracy: 95.90%
5	Validation loss: 0.164865	Best loss: 0.162536	Accuracy: 94.80%
6	Validation loss: 0.158408	Best loss: 0.158408	Accuracy: 95.97%
7	Validation loss: 0.157929	Best loss: 0.157929	Accuracy: 95.47%
8	Validation loss: 0.141264	Best loss: 0.141264	Accuracy: 95.82%
9	Validation loss: 0.122310	Best loss: 0.122310	Accuracy: 96.56%
10	Validation loss: 0.140372	Best loss: 0.122310	Accuracy: 96.56%
11	Validation loss: 0.137491	Best loss: 0.122310	Accuracy: 96.29%
12	Validation loss: 0.138250	Best loss: 0.122310	Accuracy: 96.17%
13	Validation loss: 0.141205	Best loss: 0.122310	Accuracy: 96.05%
14	Validation loss: 0.141283	Best loss: 0.122310	Accuracy: 96.05%
15	Validation loss: 0.147391	Best loss: 0.122310	Accuracy: 95.62%
16	Validation loss: 0.131060	Best loss: 0.122310	Accuracy: 96.21%
17	Validation loss: 0.137984	Best loss: 0.122310	Accuracy: 96.48%
18	Validation loss: 0.133221	Best loss: 0.122310	Accuracy: 96.64%
19	Validation loss: 0.134194	Best loss: 0.122310	Accuracy: 95.74%
20	Validation loss: 0.157467	Best loss: 0.122310	Accuracy: 96.05%
21	Validation loss: 0.156973	Best loss: 0.122310	Accuracy: 95.66%
22	Validation loss: 0.131786	Best loss: 0.122310	Accuracy: 96.25%
23	Validation loss: 0.144455	Best loss: 0.122310	Accuracy: 96.56%
24	Validation loss: 0.137335	Best loss: 0.122310	Accuracy: 95.90%
25	Validation loss: 0.136814	Best loss: 0.122310	Accuracy: 96.01%
26	Validation loss: 0.151481	Best loss: 0.122310	Accuracy: 96.29%
27	Validation loss: 0.135732	Best loss: 0.122310	Accuracy: 96.72%
28	Validation loss: 0.132082	Best loss: 0.122310	Accuracy: 96.56%
29	Validation loss: 0.127044	Best loss: 0.122310	Accuracy: 96.68%
30	Validation loss: 0.123939	Best loss: 0.122310	Accuracy: 96.72%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=   9.4s
[CV] n_neurons=10, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.220661	Best loss: 0.220661	Accuracy: 94.53%
1	Validation loss: 0.183279	Best loss: 0.183279	Accuracy: 95.74%
2	Validation loss: 0.155556	Best loss: 0.155556	Accuracy: 95.97%
3	Validation loss: 0.141742	Best loss: 0.141742	Accuracy: 96.05%
4	Validation loss: 0.139942	Best loss: 0.139942	Accuracy: 96.01%
5	Validation loss: 0.142205	Best loss: 0.139942	Accuracy: 96.05%
6	Validation loss: 0.157960	Best loss: 0.139942	Accuracy: 96.05%
7	Validation loss: 0.134274	Best loss: 0.134274	Accuracy: 96.56%
8	Validation loss: 0.135081	Best loss: 0.134274	Accuracy: 96.33%
9	Validation loss: 0.139023	Best loss: 0.134274	Accuracy: 96.09%
10	Validation loss: 0.137827	Best loss: 0.134274	Accuracy: 96.40%
11	Validation loss: 0.135776	Best loss: 0.134274	Accuracy: 96.21%
12	Validation loss: 0.120504	Best loss: 0.120504	Accuracy: 96.56%
13	Validation loss: 0.123797	Best loss: 0.120504	Accuracy: 96.25%
14	Validation loss: 0.126739	Best loss: 0.120504	Accuracy: 96.60%
15	Validation loss: 0.116333	Best loss: 0.116333	Accuracy: 96.68%
16	Validation loss: 0.138091	Best loss: 0.116333	Accuracy: 95.86%
17	Validation loss: 0.133715	Best loss: 0.116333	Accuracy: 96.33%
18	Validation loss: 0.122593	Best loss: 0.116333	Accuracy: 96.29%
19	Validation loss: 0.125929	Best loss: 0.116333	Accuracy: 96.40%
20	Validation loss: 0.123980	Best loss: 0.116333	Accuracy: 96.36%
21	Validation loss: 0.125803	Best loss: 0.116333	Accuracy: 96.40%
22	Validation loss: 0.119394	Best loss: 0.116333	Accuracy: 96.40%
23	Validation loss: 0.113852	Best loss: 0.113852	Accuracy: 96.48%
24	Validation loss: 0.112243	Best loss: 0.112243	Accuracy: 96.72%
25	Validation loss: 0.109345	Best loss: 0.109345	Accuracy: 96.68%
26	Validation loss: 0.114428	Best loss: 0.109345	Accuracy: 96.44%
27	Validation loss: 0.108152	Best loss: 0.108152	Accuracy: 96.79%
28	Validation loss: 0.122996	Best loss: 0.108152	Accuracy: 96.25%
29	Validation loss: 0.114148	Best loss: 0.108152	Accuracy: 96.56%
30	Validation loss: 0.110002	Best loss: 0.108152	Accuracy: 96.79%
31	Validation loss: 0.111838	Best loss: 0.108152	Accuracy: 96.87%
32	Validation loss: 0.111476	Best loss: 0.108152	Accuracy: 96.76%
33	Validation loss: 0.113877	Best loss: 0.108152	Accuracy: 96.60%
34	Validation loss: 0.100506	Best loss: 0.100506	Accuracy: 97.22%
35	Validation loss: 0.109417	Best loss: 0.100506	Accuracy: 96.79%
36	Validation loss: 0.116986	Best loss: 0.100506	Accuracy: 96.79%
37	Validation loss: 0.116699	Best loss: 0.100506	Accuracy: 96.79%
38	Validation loss: 0.103437	Best loss: 0.100506	Accuracy: 97.15%
39	Validation loss: 0.104591	Best loss: 0.100506	Accuracy: 97.11%
40	Validation loss: 0.115433	Best loss: 0.100506	Accuracy: 96.87%
41	Validation loss: 0.106610	Best loss: 0.100506	Accuracy: 96.91%
42	Validation loss: 0.111701	Best loss: 0.100506	Accuracy: 96.95%
43	Validation loss: 0.102311	Best loss: 0.100506	Accuracy: 97.11%
44	Validation loss: 0.117988	Best loss: 0.100506	Accuracy: 96.68%
45	Validation loss: 0.117666	Best loss: 0.100506	Accuracy: 96.79%
46	Validation loss: 0.114910	Best loss: 0.100506	Accuracy: 96.91%
47	Validation loss: 0.118043	Best loss: 0.100506	Accuracy: 96.64%
48	Validation loss: 0.109852	Best loss: 0.100506	Accuracy: 96.99%
49	Validation loss: 0.120322	Best loss: 0.100506	Accuracy: 97.03%
50	Validation loss: 0.116769	Best loss: 0.100506	Accuracy: 96.60%
51	Validation loss: 0.128013	Best loss: 0.100506	Accuracy: 96.79%
52	Validation loss: 0.113716	Best loss: 0.100506	Accuracy: 96.83%
53	Validation loss: 0.109846	Best loss: 0.100506	Accuracy: 96.83%
54	Validation loss: 0.113067	Best loss: 0.100506	Accuracy: 97.22%
55	Validation loss: 0.107540	Best loss: 0.100506	Accuracy: 96.87%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  16.3s
[CV] n_neurons=10, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.208157	Best loss: 0.208157	Accuracy: 94.61%
1	Validation loss: 0.189623	Best loss: 0.189623	Accuracy: 95.66%
2	Validation loss: 0.167042	Best loss: 0.167042	Accuracy: 95.35%
3	Validation loss: 0.160647	Best loss: 0.160647	Accuracy: 96.25%
4	Validation loss: 0.182306	Best loss: 0.160647	Accuracy: 95.66%
5	Validation loss: 0.146235	Best loss: 0.146235	Accuracy: 96.29%
6	Validation loss: 0.139235	Best loss: 0.139235	Accuracy: 95.90%
7	Validation loss: 0.145803	Best loss: 0.139235	Accuracy: 95.58%
8	Validation loss: 0.133731	Best loss: 0.133731	Accuracy: 96.17%
9	Validation loss: 0.124870	Best loss: 0.124870	Accuracy: 96.68%
10	Validation loss: 0.129829	Best loss: 0.124870	Accuracy: 96.64%
11	Validation loss: 0.123794	Best loss: 0.123794	Accuracy: 96.68%
12	Validation loss: 0.151692	Best loss: 0.123794	Accuracy: 95.78%
13	Validation loss: 0.123041	Best loss: 0.123041	Accuracy: 96.48%
14	Validation loss: 0.139635	Best loss: 0.123041	Accuracy: 96.05%
15	Validation loss: 0.133645	Best loss: 0.123041	Accuracy: 96.72%
16	Validation loss: 0.121269	Best loss: 0.121269	Accuracy: 96.76%
17	Validation loss: 0.121953	Best loss: 0.121269	Accuracy: 96.17%
18	Validation loss: 0.122428	Best loss: 0.121269	Accuracy: 96.48%
19	Validation loss: 0.127335	Best loss: 0.121269	Accuracy: 96.25%
20	Validation loss: 0.108819	Best loss: 0.108819	Accuracy: 96.87%
21	Validation loss: 0.109668	Best loss: 0.108819	Accuracy: 96.79%
22	Validation loss: 0.119590	Best loss: 0.108819	Accuracy: 96.60%
23	Validation loss: 0.116325	Best loss: 0.108819	Accuracy: 96.79%
24	Validation loss: 0.113060	Best loss: 0.108819	Accuracy: 97.11%
25	Validation loss: 0.108858	Best loss: 0.108819	Accuracy: 96.91%
26	Validation loss: 0.104013	Best loss: 0.104013	Accuracy: 97.22%
27	Validation loss: 0.109018	Best loss: 0.104013	Accuracy: 97.19%
28	Validation loss: 0.114578	Best loss: 0.104013	Accuracy: 96.76%
29	Validation loss: 0.137861	Best loss: 0.104013	Accuracy: 96.29%
30	Validation loss: 0.102469	Best loss: 0.102469	Accuracy: 97.07%
31	Validation loss: 0.115107	Best loss: 0.102469	Accuracy: 96.68%
32	Validation loss: 0.098130	Best loss: 0.098130	Accuracy: 97.34%
33	Validation loss: 0.111720	Best loss: 0.098130	Accuracy: 97.07%
34	Validation loss: 0.109654	Best loss: 0.098130	Accuracy: 97.34%
35	Validation loss: 0.104056	Best loss: 0.098130	Accuracy: 97.11%
36	Validation loss: 0.096064	Best loss: 0.096064	Accuracy: 97.42%
37	Validation loss: 0.095910	Best loss: 0.095910	Accuracy: 97.46%
38	Validation loss: 0.102486	Best loss: 0.095910	Accuracy: 97.26%
39	Validation loss: 0.104211	Best loss: 0.095910	Accuracy: 97.11%
40	Validation loss: 0.093515	Best loss: 0.093515	Accuracy: 97.50%
41	Validation loss: 0.100717	Best loss: 0.093515	Accuracy: 97.26%
42	Validation loss: 0.104095	Best loss: 0.093515	Accuracy: 97.11%
43	Validation loss: 0.092390	Best loss: 0.092390	Accuracy: 97.38%
44	Validation loss: 0.101410	Best loss: 0.092390	Accuracy: 97.22%
45	Validation loss: 0.099258	Best loss: 0.092390	Accuracy: 97.07%
46	Validation loss: 0.097728	Best loss: 0.092390	Accuracy: 97.30%
47	Validation loss: 0.104042	Best loss: 0.092390	Accuracy: 96.83%
48	Validation loss: 0.105510	Best loss: 0.092390	Accuracy: 96.99%
49	Validation loss: 0.103663	Best loss: 0.092390	Accuracy: 97.42%
50	Validation loss: 0.100743	Best loss: 0.092390	Accuracy: 97.22%
51	Validation loss: 0.110786	Best loss: 0.092390	Accuracy: 97.19%
52	Validation loss: 0.105773	Best loss: 0.092390	Accuracy: 97.50%
53	Validation loss: 0.107655	Best loss: 0.092390	Accuracy: 97.42%
54	Validation loss: 0.099762	Best loss: 0.092390	Accuracy: 97.46%
55	Validation loss: 0.100896	Best loss: 0.092390	Accuracy: 97.58%
56	Validation loss: 0.096299	Best loss: 0.092390	Accuracy: 97.42%
57	Validation loss: 0.099846	Best loss: 0.092390	Accuracy: 97.50%
58	Validation loss: 0.112837	Best loss: 0.092390	Accuracy: 96.83%
59	Validation loss: 0.106145	Best loss: 0.092390	Accuracy: 97.15%
60	Validation loss: 0.113760	Best loss: 0.092390	Accuracy: 97.15%
61	Validation loss: 0.105376	Best loss: 0.092390	Accuracy: 96.99%
62	Validation loss: 0.104641	Best loss: 0.092390	Accuracy: 97.42%
63	Validation loss: 0.105603	Best loss: 0.092390	Accuracy: 97.50%
64	Validation loss: 0.109388	Best loss: 0.092390	Accuracy: 97.11%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  18.9s
[CV] n_neurons=50, learning_rate=0.01, dropout_rate=0.4, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.138174	Best loss: 0.138174	Accuracy: 95.78%
1	Validation loss: 0.117632	Best loss: 0.117632	Accuracy: 96.76%
2	Validation loss: 0.098189	Best loss: 0.098189	Accuracy: 97.34%
3	Validation loss: 0.106874	Best loss: 0.098189	Accuracy: 96.99%
4	Validation loss: 0.083618	Best loss: 0.083618	Accuracy: 97.50%
5	Validation loss: 0.084933	Best loss: 0.083618	Accuracy: 97.81%
6	Validation loss: 0.086314	Best loss: 0.083618	Accuracy: 97.81%
7	Validation loss: 0.087699	Best loss: 0.083618	Accuracy: 98.01%
8	Validation loss: 0.079423	Best loss: 0.079423	Accuracy: 97.89%
9	Validation loss: 0.076041	Best loss: 0.076041	Accuracy: 98.08%
10	Validation loss: 0.073260	Best loss: 0.073260	Accuracy: 98.05%
11	Validation loss: 0.077434	Best loss: 0.073260	Accuracy: 98.08%
12	Validation loss: 0.064646	Best loss: 0.064646	Accuracy: 98.40%
13	Validation loss: 0.088756	Best loss: 0.064646	Accuracy: 97.81%
14	Validation loss: 0.075049	Best loss: 0.064646	Accuracy: 98.16%
15	Validation loss: 0.080646	Best loss: 0.064646	Accuracy: 98.01%
16	Validation loss: 0.066631	Best loss: 0.064646	Accuracy: 98.16%
17	Validation loss: 0.069648	Best loss: 0.064646	Accuracy: 98.36%
18	Validation loss: 0.069311	Best loss: 0.064646	Accuracy: 98.12%
19	Validation loss: 0.070120	Best loss: 0.064646	Accuracy: 98.24%
20	Validation loss: 0.065043	Best loss: 0.064646	Accuracy: 98.55%
21	Validation loss: 0.066535	Best loss: 0.064646	Accuracy: 98.24%
22	Validation loss: 0.068983	Best loss: 0.064646	Accuracy: 98.12%
23	Validation loss: 0.071299	Best loss: 0.064646	Accuracy: 98.40%
24	Validation loss: 0.073618	Best loss: 0.064646	Accuracy: 98.24%
25	Validation loss: 0.068013	Best loss: 0.064646	Accuracy: 98.28%
26	Validation loss: 0.067161	Best loss: 0.064646	Accuracy: 98.44%
27	Validation loss: 0.065611	Best loss: 0.064646	Accuracy: 98.24%
28	Validation loss: 0.073481	Best loss: 0.064646	Accuracy: 98.24%
29	Validation loss: 0.067665	Best loss: 0.064646	Accuracy: 98.24%
30	Validation loss: 0.068178	Best loss: 0.064646	Accuracy: 98.36%
31	Validation loss: 0.061191	Best loss: 0.061191	Accuracy: 98.24%
32	Validation loss: 0.069976	Best loss: 0.061191	Accuracy: 98.36%
33	Validation loss: 0.062570	Best loss: 0.061191	Accuracy: 98.59%
34	Validation loss: 0.063832	Best loss: 0.061191	Accuracy: 98.55%
35	Validation loss: 0.061063	Best loss: 0.061063	Accuracy: 98.55%
36	Validation loss: 0.058998	Best loss: 0.058998	Accuracy: 98.55%
37	Validation loss: 0.065651	Best loss: 0.058998	Accuracy: 98.51%
38	Validation loss: 0.065943	Best loss: 0.058998	Accuracy: 98.20%
39	Validation loss: 0.064944	Best loss: 0.058998	Accuracy: 98.44%
40	Validation loss: 0.061951	Best loss: 0.058998	Accuracy: 98.40%
41	Validation loss: 0.058779	Best loss: 0.058779	Accuracy: 98.28%
42	Validation loss: 0.065189	Best loss: 0.058779	Accuracy: 98.48%
43	Validation loss: 0.067050	Best loss: 0.058779	Accuracy: 98.32%
44	Validation loss: 0.056271	Best loss: 0.056271	Accuracy: 98.71%
45	Validation loss: 0.060282	Best loss: 0.056271	Accuracy: 98.63%
46	Validation loss: 0.077345	Best loss: 0.056271	Accuracy: 98.24%
47	Validation loss: 0.059483	Best loss: 0.056271	Accuracy: 98.48%
48	Validation loss: 0.059453	Best loss: 0.056271	Accuracy: 98.44%
49	Validation loss: 0.060845	Best loss: 0.056271	Accuracy: 98.36%
50	Validation loss: 0.073008	Best loss: 0.056271	Accuracy: 98.55%
51	Validation loss: 0.060295	Best loss: 0.056271	Accuracy: 98.40%
52	Validation loss: 0.059491	Best loss: 0.056271	Accuracy: 98.36%
53	Validation loss: 0.069039	Best loss: 0.056271	Accuracy: 98.55%
54	Validation loss: 0.057416	Best loss: 0.056271	Accuracy: 98.63%
55	Validation loss: 0.054719	Best loss: 0.054719	Accuracy: 98.55%
56	Validation loss: 0.059445	Best loss: 0.054719	Accuracy: 98.48%
57	Validation loss: 0.065929	Best loss: 0.054719	Accuracy: 98.36%
58	Validation loss: 0.064276	Best loss: 0.054719	Accuracy: 98.51%
59	Validation loss: 0.072216	Best loss: 0.054719	Accuracy: 98.36%
60	Validation loss: 0.063836	Best loss: 0.054719	Accuracy: 98.36%
61	Validation loss: 0.061320	Best loss: 0.054719	Accuracy: 98.36%
62	Validation loss: 0.060500	Best loss: 0.054719	Accuracy: 98.36%
63	Validation loss: 0.067177	Best loss: 0.054719	Accuracy: 98.36%
64	Validation loss: 0.056068	Best loss: 0.054719	Accuracy: 98.55%
65	Validation loss: 0.062205	Best loss: 0.054719	Accuracy: 98.59%
66	Validation loss: 0.072619	Best loss: 0.054719	Accuracy: 98.28%
67	Validation loss: 0.057567	Best loss: 0.054719	Accuracy: 98.40%
68	Validation loss: 0.058629	Best loss: 0.054719	Accuracy: 98.48%
69	Validation loss: 0.059790	Best loss: 0.054719	Accuracy: 98.40%
70	Validation loss: 0.059190	Best loss: 0.054719	Accuracy: 98.40%
71	Validation loss: 0.075120	Best loss: 0.054719	Accuracy: 98.48%
72	Validation loss: 0.073985	Best loss: 0.054719	Accuracy: 98.44%
73	Validation loss: 0.065937	Best loss: 0.054719	Accuracy: 98.40%
74	Validation loss: 0.057389	Best loss: 0.054719	Accuracy: 98.51%
75	Validation loss: 0.060947	Best loss: 0.054719	Accuracy: 98.32%
76	Validation loss: 0.054949	Best loss: 0.054719	Accuracy: 98.63%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, dropout_rate=0.4, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  16.1s
[CV] n_neurons=50, learning_rate=0.01, dropout_rate=0.4, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.142985	Best loss: 0.142985	Accuracy: 95.70%
1	Validation loss: 0.121630	Best loss: 0.121630	Accuracy: 96.64%
2	Validation loss: 0.097617	Best loss: 0.097617	Accuracy: 97.38%
3	Validation loss: 0.085762	Best loss: 0.085762	Accuracy: 97.69%
4	Validation loss: 0.081630	Best loss: 0.081630	Accuracy: 97.65%
5	Validation loss: 0.075052	Best loss: 0.075052	Accuracy: 98.01%
6	Validation loss: 0.071723	Best loss: 0.071723	Accuracy: 98.12%
7	Validation loss: 0.075092	Best loss: 0.071723	Accuracy: 97.85%
8	Validation loss: 0.075589	Best loss: 0.071723	Accuracy: 97.69%
9	Validation loss: 0.069914	Best loss: 0.069914	Accuracy: 98.08%
10	Validation loss: 0.070292	Best loss: 0.069914	Accuracy: 98.05%
11	Validation loss: 0.064642	Best loss: 0.064642	Accuracy: 98.32%
12	Validation loss: 0.070056	Best loss: 0.064642	Accuracy: 97.89%
13	Validation loss: 0.065511	Best loss: 0.064642	Accuracy: 98.16%
14	Validation loss: 0.068356	Best loss: 0.064642	Accuracy: 98.20%
15	Validation loss: 0.065724	Best loss: 0.064642	Accuracy: 98.28%
16	Validation loss: 0.069895	Best loss: 0.064642	Accuracy: 98.01%
17	Validation loss: 0.060967	Best loss: 0.060967	Accuracy: 98.32%
18	Validation loss: 0.061518	Best loss: 0.060967	Accuracy: 98.28%
19	Validation loss: 0.063351	Best loss: 0.060967	Accuracy: 98.32%
20	Validation loss: 0.072102	Best loss: 0.060967	Accuracy: 98.20%
21	Validation loss: 0.060087	Best loss: 0.060087	Accuracy: 98.24%
22	Validation loss: 0.063075	Best loss: 0.060087	Accuracy: 98.12%
23	Validation loss: 0.066440	Best loss: 0.060087	Accuracy: 98.28%
24	Validation loss: 0.063513	Best loss: 0.060087	Accuracy: 97.89%
25	Validation loss: 0.066052	Best loss: 0.060087	Accuracy: 98.16%
26	Validation loss: 0.056684	Best loss: 0.056684	Accuracy: 98.16%
27	Validation loss: 0.057640	Best loss: 0.056684	Accuracy: 98.20%
28	Validation loss: 0.066612	Best loss: 0.056684	Accuracy: 98.05%
29	Validation loss: 0.060083	Best loss: 0.056684	Accuracy: 98.40%
30	Validation loss: 0.065326	Best loss: 0.056684	Accuracy: 98.28%
31	Validation loss: 0.065014	Best loss: 0.056684	Accuracy: 98.08%
32	Validation loss: 0.061559	Best loss: 0.056684	Accuracy: 98.40%
33	Validation loss: 0.061637	Best loss: 0.056684	Accuracy: 98.36%
34	Validation loss: 0.061597	Best loss: 0.056684	Accuracy: 98.40%
35	Validation loss: 0.067328	Best loss: 0.056684	Accuracy: 98.20%
36	Validation loss: 0.058535	Best loss: 0.056684	Accuracy: 98.32%
37	Validation loss: 0.066712	Best loss: 0.056684	Accuracy: 98.36%
38	Validation loss: 0.062039	Best loss: 0.056684	Accuracy: 98.40%
39	Validation loss: 0.066788	Best loss: 0.056684	Accuracy: 98.44%
40	Validation loss: 0.066686	Best loss: 0.056684	Accuracy: 98.16%
41	Validation loss: 0.064766	Best loss: 0.056684	Accuracy: 98.32%
42	Validation loss: 0.060630	Best loss: 0.056684	Accuracy: 98.44%
43	Validation loss: 0.059657	Best loss: 0.056684	Accuracy: 98.40%
44	Validation loss: 0.059810	Best loss: 0.056684	Accuracy: 98.55%
45	Validation loss: 0.059163	Best loss: 0.056684	Accuracy: 98.44%
46	Validation loss: 0.068779	Best loss: 0.056684	Accuracy: 98.44%
47	Validation loss: 0.064256	Best loss: 0.056684	Accuracy: 98.48%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, dropout_rate=0.4, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  11.3s
[CV] n_neurons=50, learning_rate=0.01, dropout_rate=0.4, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.133765	Best loss: 0.133765	Accuracy: 96.05%
1	Validation loss: 0.106877	Best loss: 0.106877	Accuracy: 97.26%
2	Validation loss: 0.088228	Best loss: 0.088228	Accuracy: 97.54%
3	Validation loss: 0.080725	Best loss: 0.080725	Accuracy: 98.16%
4	Validation loss: 0.074479	Best loss: 0.074479	Accuracy: 98.24%
5	Validation loss: 0.084025	Best loss: 0.074479	Accuracy: 97.65%
6	Validation loss: 0.080182	Best loss: 0.074479	Accuracy: 97.97%
7	Validation loss: 0.073211	Best loss: 0.073211	Accuracy: 97.93%
8	Validation loss: 0.067193	Best loss: 0.067193	Accuracy: 98.05%
9	Validation loss: 0.071166	Best loss: 0.067193	Accuracy: 98.05%
10	Validation loss: 0.071885	Best loss: 0.067193	Accuracy: 97.93%
11	Validation loss: 0.072926	Best loss: 0.067193	Accuracy: 97.85%
12	Validation loss: 0.066863	Best loss: 0.066863	Accuracy: 98.16%
13	Validation loss: 0.063576	Best loss: 0.063576	Accuracy: 98.40%
14	Validation loss: 0.064599	Best loss: 0.063576	Accuracy: 98.36%
15	Validation loss: 0.065738	Best loss: 0.063576	Accuracy: 98.20%
16	Validation loss: 0.061253	Best loss: 0.061253	Accuracy: 98.24%
17	Validation loss: 0.065532	Best loss: 0.061253	Accuracy: 98.28%
18	Validation loss: 0.065928	Best loss: 0.061253	Accuracy: 98.08%
19	Validation loss: 0.057996	Best loss: 0.057996	Accuracy: 98.36%
20	Validation loss: 0.062300	Best loss: 0.057996	Accuracy: 98.28%
21	Validation loss: 0.060979	Best loss: 0.057996	Accuracy: 98.28%
22	Validation loss: 0.065489	Best loss: 0.057996	Accuracy: 98.16%
23	Validation loss: 0.062441	Best loss: 0.057996	Accuracy: 98.40%
24	Validation loss: 0.066981	Best loss: 0.057996	Accuracy: 98.28%
25	Validation loss: 0.062482	Best loss: 0.057996	Accuracy: 98.32%
26	Validation loss: 0.061195	Best loss: 0.057996	Accuracy: 98.20%
27	Validation loss: 0.061729	Best loss: 0.057996	Accuracy: 98.32%
28	Validation loss: 0.059962	Best loss: 0.057996	Accuracy: 98.44%
29	Validation loss: 0.064029	Best loss: 0.057996	Accuracy: 98.48%
30	Validation loss: 0.063557	Best loss: 0.057996	Accuracy: 98.16%
31	Validation loss: 0.063404	Best loss: 0.057996	Accuracy: 98.32%
32	Validation loss: 0.057912	Best loss: 0.057912	Accuracy: 98.32%
33	Validation loss: 0.064456	Best loss: 0.057912	Accuracy: 98.51%
34	Validation loss: 0.062328	Best loss: 0.057912	Accuracy: 98.40%
35	Validation loss: 0.056023	Best loss: 0.056023	Accuracy: 98.44%
36	Validation loss: 0.058291	Best loss: 0.056023	Accuracy: 98.48%
37	Validation loss: 0.062367	Best loss: 0.056023	Accuracy: 98.16%
38	Validation loss: 0.066892	Best loss: 0.056023	Accuracy: 98.44%
39	Validation loss: 0.062672	Best loss: 0.056023	Accuracy: 98.36%
40	Validation loss: 0.063140	Best loss: 0.056023	Accuracy: 98.36%
41	Validation loss: 0.056693	Best loss: 0.056023	Accuracy: 98.40%
42	Validation loss: 0.061458	Best loss: 0.056023	Accuracy: 98.48%
43	Validation loss: 0.059696	Best loss: 0.056023	Accuracy: 98.48%
44	Validation loss: 0.060701	Best loss: 0.056023	Accuracy: 98.28%
45	Validation loss: 0.060277	Best loss: 0.056023	Accuracy: 98.44%
46	Validation loss: 0.055510	Best loss: 0.055510	Accuracy: 98.40%
47	Validation loss: 0.056748	Best loss: 0.055510	Accuracy: 98.48%
48	Validation loss: 0.055166	Best loss: 0.055166	Accuracy: 98.48%
49	Validation loss: 0.057505	Best loss: 0.055166	Accuracy: 98.40%
50	Validation loss: 0.052857	Best loss: 0.052857	Accuracy: 98.44%
51	Validation loss: 0.058690	Best loss: 0.052857	Accuracy: 98.44%
52	Validation loss: 0.058328	Best loss: 0.052857	Accuracy: 98.55%
53	Validation loss: 0.056981	Best loss: 0.052857	Accuracy: 98.48%
54	Validation loss: 0.064150	Best loss: 0.052857	Accuracy: 98.40%
55	Validation loss: 0.059911	Best loss: 0.052857	Accuracy: 98.44%
56	Validation loss: 0.057344	Best loss: 0.052857	Accuracy: 98.55%
57	Validation loss: 0.053709	Best loss: 0.052857	Accuracy: 98.55%
58	Validation loss: 0.058305	Best loss: 0.052857	Accuracy: 98.51%
59	Validation loss: 0.057528	Best loss: 0.052857	Accuracy: 98.40%
60	Validation loss: 0.062193	Best loss: 0.052857	Accuracy: 98.40%
61	Validation loss: 0.059098	Best loss: 0.052857	Accuracy: 98.12%
62	Validation loss: 0.059351	Best loss: 0.052857	Accuracy: 98.48%
63	Validation loss: 0.063718	Best loss: 0.052857	Accuracy: 98.44%
64	Validation loss: 0.053818	Best loss: 0.052857	Accuracy: 98.71%
65	Validation loss: 0.060935	Best loss: 0.052857	Accuracy: 98.48%
66	Validation loss: 0.056630	Best loss: 0.052857	Accuracy: 98.63%
67	Validation loss: 0.055758	Best loss: 0.052857	Accuracy: 98.63%
68	Validation loss: 0.064188	Best loss: 0.052857	Accuracy: 98.28%
69	Validation loss: 0.065690	Best loss: 0.052857	Accuracy: 98.51%
70	Validation loss: 0.060180	Best loss: 0.052857	Accuracy: 98.32%
71	Validation loss: 0.064457	Best loss: 0.052857	Accuracy: 98.51%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, dropout_rate=0.4, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  15.5s
[CV] n_neurons=10, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.242406	Best loss: 0.242406	Accuracy: 94.21%
1	Validation loss: 0.157338	Best loss: 0.157338	Accuracy: 95.27%
2	Validation loss: 0.168325	Best loss: 0.157338	Accuracy: 96.44%
3	Validation loss: 0.155268	Best loss: 0.155268	Accuracy: 95.62%
4	Validation loss: 0.157894	Best loss: 0.155268	Accuracy: 96.33%
5	Validation loss: 0.168759	Best loss: 0.155268	Accuracy: 94.64%
6	Validation loss: 0.273733	Best loss: 0.155268	Accuracy: 89.44%
7	Validation loss: 0.299255	Best loss: 0.155268	Accuracy: 94.88%
8	Validation loss: 0.149070	Best loss: 0.149070	Accuracy: 95.90%
9	Validation loss: 0.171457	Best loss: 0.149070	Accuracy: 96.36%
10	Validation loss: 0.186647	Best loss: 0.149070	Accuracy: 95.78%
11	Validation loss: 0.156968	Best loss: 0.149070	Accuracy: 96.64%
12	Validation loss: 0.134555	Best loss: 0.134555	Accuracy: 96.17%
13	Validation loss: 0.174798	Best loss: 0.134555	Accuracy: 95.93%
14	Validation loss: 0.134188	Best loss: 0.134188	Accuracy: 96.83%
15	Validation loss: 0.125429	Best loss: 0.125429	Accuracy: 97.07%
16	Validation loss: 0.131803	Best loss: 0.125429	Accuracy: 96.72%
17	Validation loss: 0.139121	Best loss: 0.125429	Accuracy: 96.91%
18	Validation loss: 0.132440	Best loss: 0.125429	Accuracy: 96.40%
19	Validation loss: 0.138549	Best loss: 0.125429	Accuracy: 96.17%
20	Validation loss: 0.176348	Best loss: 0.125429	Accuracy: 96.72%
21	Validation loss: 0.131128	Best loss: 0.125429	Accuracy: 96.68%
22	Validation loss: 0.156843	Best loss: 0.125429	Accuracy: 96.68%
23	Validation loss: 0.143454	Best loss: 0.125429	Accuracy: 96.33%
24	Validation loss: 0.163757	Best loss: 0.125429	Accuracy: 96.64%
25	Validation loss: 0.156600	Best loss: 0.125429	Accuracy: 96.56%
26	Validation loss: 0.179945	Best loss: 0.125429	Accuracy: 96.83%
27	Validation loss: 0.195479	Best loss: 0.125429	Accuracy: 94.84%
28	Validation loss: 1.054391	Best loss: 0.125429	Accuracy: 49.57%
29	Validation loss: 1.976097	Best loss: 0.125429	Accuracy: 23.69%
30	Validation loss: 1.582842	Best loss: 0.125429	Accuracy: 18.80%
31	Validation loss: 1.535527	Best loss: 0.125429	Accuracy: 38.74%
32	Validation loss: 1.467951	Best loss: 0.125429	Accuracy: 45.27%
33	Validation loss: 1.466760	Best loss: 0.125429	Accuracy: 37.96%
34	Validation loss: 1.418249	Best loss: 0.125429	Accuracy: 41.67%
35	Validation loss: 1.338984	Best loss: 0.125429	Accuracy: 48.36%
36	Validation loss: 1.263155	Best loss: 0.125429	Accuracy: 43.71%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=   5.2s
[CV] n_neurons=10, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.848895	Best loss: 0.848895	Accuracy: 60.09%
1	Validation loss: 0.439716	Best loss: 0.439716	Accuracy: 81.74%
2	Validation loss: 0.281822	Best loss: 0.281822	Accuracy: 92.03%
3	Validation loss: 0.223170	Best loss: 0.223170	Accuracy: 93.94%
4	Validation loss: 0.270226	Best loss: 0.223170	Accuracy: 94.33%
5	Validation loss: 0.209201	Best loss: 0.209201	Accuracy: 94.37%
6	Validation loss: 0.229335	Best loss: 0.209201	Accuracy: 94.33%
7	Validation loss: 0.189218	Best loss: 0.189218	Accuracy: 95.00%
8	Validation loss: 0.165367	Best loss: 0.165367	Accuracy: 95.74%
9	Validation loss: 0.149662	Best loss: 0.149662	Accuracy: 95.58%
10	Validation loss: 0.173404	Best loss: 0.149662	Accuracy: 95.93%
11	Validation loss: 0.140964	Best loss: 0.140964	Accuracy: 96.40%
12	Validation loss: 0.152245	Best loss: 0.140964	Accuracy: 95.97%
13	Validation loss: 0.141364	Best loss: 0.140964	Accuracy: 96.79%
14	Validation loss: 0.137258	Best loss: 0.137258	Accuracy: 96.87%
15	Validation loss: 0.174034	Best loss: 0.137258	Accuracy: 96.64%
16	Validation loss: 0.156421	Best loss: 0.137258	Accuracy: 96.29%
17	Validation loss: 0.183130	Best loss: 0.137258	Accuracy: 96.83%
18	Validation loss: 0.290229	Best loss: 0.137258	Accuracy: 95.62%
19	Validation loss: 1.642537	Best loss: 0.137258	Accuracy: 21.34%
20	Validation loss: 1.538381	Best loss: 0.137258	Accuracy: 30.96%
21	Validation loss: 1.330092	Best loss: 0.137258	Accuracy: 36.71%
22	Validation loss: 1.130978	Best loss: 0.137258	Accuracy: 44.84%
23	Validation loss: 1.473164	Best loss: 0.137258	Accuracy: 37.57%
24	Validation loss: 1.299364	Best loss: 0.137258	Accuracy: 49.18%
25	Validation loss: 1.054677	Best loss: 0.137258	Accuracy: 53.79%
26	Validation loss: 0.999033	Best loss: 0.137258	Accuracy: 51.84%
27	Validation loss: 0.857278	Best loss: 0.137258	Accuracy: 58.41%
28	Validation loss: 0.782055	Best loss: 0.137258	Accuracy: 56.65%
29	Validation loss: 0.776362	Best loss: 0.137258	Accuracy: 56.80%
30	Validation loss: 0.771008	Best loss: 0.137258	Accuracy: 57.78%
31	Validation loss: 0.758437	Best loss: 0.137258	Accuracy: 58.76%
32	Validation loss: 0.771914	Best loss: 0.137258	Accuracy: 59.58%
33	Validation loss: 0.781242	Best loss: 0.137258	Accuracy: 59.97%
34	Validation loss: 0.751990	Best loss: 0.137258	Accuracy: 59.81%
35	Validation loss: 0.744507	Best loss: 0.137258	Accuracy: 59.19%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=   5.1s
[CV] n_neurons=10, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.408414	Best loss: 0.408414	Accuracy: 86.32%
1	Validation loss: 0.195538	Best loss: 0.195538	Accuracy: 95.47%
2	Validation loss: 0.151025	Best loss: 0.151025	Accuracy: 95.47%
3	Validation loss: 0.165157	Best loss: 0.151025	Accuracy: 95.97%
4	Validation loss: 0.151083	Best loss: 0.151025	Accuracy: 96.05%
5	Validation loss: 0.171685	Best loss: 0.151025	Accuracy: 95.27%
6	Validation loss: 0.187873	Best loss: 0.151025	Accuracy: 94.64%
7	Validation loss: 0.203985	Best loss: 0.151025	Accuracy: 95.39%
8	Validation loss: 0.184592	Best loss: 0.151025	Accuracy: 95.39%
9	Validation loss: 0.175416	Best loss: 0.151025	Accuracy: 95.86%
10	Validation loss: 0.157589	Best loss: 0.151025	Accuracy: 96.60%
11	Validation loss: 0.147509	Best loss: 0.147509	Accuracy: 96.44%
12	Validation loss: 0.160292	Best loss: 0.147509	Accuracy: 96.40%
13	Validation loss: 0.157430	Best loss: 0.147509	Accuracy: 96.60%
14	Validation loss: 0.141602	Best loss: 0.141602	Accuracy: 96.87%
15	Validation loss: 0.132913	Best loss: 0.132913	Accuracy: 97.19%
16	Validation loss: 0.150297	Best loss: 0.132913	Accuracy: 96.56%
17	Validation loss: 0.139870	Best loss: 0.132913	Accuracy: 96.91%
18	Validation loss: 0.568741	Best loss: 0.132913	Accuracy: 84.71%
19	Validation loss: 1.035302	Best loss: 0.132913	Accuracy: 51.72%
20	Validation loss: 0.925859	Best loss: 0.132913	Accuracy: 54.89%
21	Validation loss: 0.739591	Best loss: 0.132913	Accuracy: 68.96%
22	Validation loss: 0.646206	Best loss: 0.132913	Accuracy: 70.52%
23	Validation loss: 0.556538	Best loss: 0.132913	Accuracy: 76.11%
24	Validation loss: 0.867836	Best loss: 0.132913	Accuracy: 66.93%
25	Validation loss: 0.793819	Best loss: 0.132913	Accuracy: 68.88%
26	Validation loss: 0.709253	Best loss: 0.132913	Accuracy: 70.60%
27	Validation loss: 0.599614	Best loss: 0.132913	Accuracy: 73.49%
28	Validation loss: 0.449147	Best loss: 0.132913	Accuracy: 76.54%
29	Validation loss: 0.430435	Best loss: 0.132913	Accuracy: 80.02%
30	Validation loss: 0.437657	Best loss: 0.132913	Accuracy: 77.25%
31	Validation loss: 0.430257	Best loss: 0.132913	Accuracy: 78.97%
32	Validation loss: 0.416101	Best loss: 0.132913	Accuracy: 80.57%
33	Validation loss: 0.415919	Best loss: 0.132913	Accuracy: 81.90%
34	Validation loss: 0.413662	Best loss: 0.132913	Accuracy: 80.69%
35	Validation loss: 0.402965	Best loss: 0.132913	Accuracy: 82.29%
36	Validation loss: 0.403827	Best loss: 0.132913	Accuracy: 79.32%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=   5.5s
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.133038	Best loss: 0.133038	Accuracy: 96.64%
1	Validation loss: 0.154520	Best loss: 0.133038	Accuracy: 96.83%
2	Validation loss: 0.092499	Best loss: 0.092499	Accuracy: 97.89%
3	Validation loss: 0.078848	Best loss: 0.078848	Accuracy: 97.93%
4	Validation loss: 0.080821	Best loss: 0.078848	Accuracy: 97.89%
5	Validation loss: 0.070142	Best loss: 0.070142	Accuracy: 97.85%
6	Validation loss: 0.077337	Best loss: 0.070142	Accuracy: 98.12%
7	Validation loss: 0.066499	Best loss: 0.066499	Accuracy: 98.44%
8	Validation loss: 0.080770	Best loss: 0.066499	Accuracy: 98.36%
9	Validation loss: 0.089839	Best loss: 0.066499	Accuracy: 98.12%
10	Validation loss: 0.084697	Best loss: 0.066499	Accuracy: 98.16%
11	Validation loss: 0.087902	Best loss: 0.066499	Accuracy: 98.12%
12	Validation loss: 0.109924	Best loss: 0.066499	Accuracy: 98.24%
13	Validation loss: 0.080884	Best loss: 0.066499	Accuracy: 98.12%
14	Validation loss: 0.068049	Best loss: 0.066499	Accuracy: 98.44%
15	Validation loss: 0.063866	Best loss: 0.063866	Accuracy: 98.28%
16	Validation loss: 0.071600	Best loss: 0.063866	Accuracy: 98.48%
17	Validation loss: 0.095410	Best loss: 0.063866	Accuracy: 98.40%
18	Validation loss: 0.090726	Best loss: 0.063866	Accuracy: 97.65%
19	Validation loss: 0.062923	Best loss: 0.062923	Accuracy: 98.20%
20	Validation loss: 0.082477	Best loss: 0.062923	Accuracy: 98.01%
21	Validation loss: 0.062915	Best loss: 0.062915	Accuracy: 98.63%
22	Validation loss: 0.053135	Best loss: 0.053135	Accuracy: 98.55%
23	Validation loss: 0.104539	Best loss: 0.053135	Accuracy: 98.44%
24	Validation loss: 0.073885	Best loss: 0.053135	Accuracy: 98.24%
25	Validation loss: 0.079331	Best loss: 0.053135	Accuracy: 98.32%
26	Validation loss: 0.074869	Best loss: 0.053135	Accuracy: 97.69%
27	Validation loss: 0.060601	Best loss: 0.053135	Accuracy: 98.36%
28	Validation loss: 0.093752	Best loss: 0.053135	Accuracy: 97.97%
29	Validation loss: 0.103479	Best loss: 0.053135	Accuracy: 97.19%
30	Validation loss: 0.114838	Best loss: 0.053135	Accuracy: 98.12%
31	Validation loss: 0.072446	Best loss: 0.053135	Accuracy: 98.32%
32	Validation loss: 0.081364	Best loss: 0.053135	Accuracy: 98.44%
33	Validation loss: 0.097512	Best loss: 0.053135	Accuracy: 98.16%
34	Validation loss: 0.099006	Best loss: 0.053135	Accuracy: 98.48%
35	Validation loss: 0.149052	Best loss: 0.053135	Accuracy: 98.01%
36	Validation loss: 0.111595	Best loss: 0.053135	Accuracy: 97.93%
37	Validation loss: 0.164470	Best loss: 0.053135	Accuracy: 97.85%
38	Validation loss: 0.082429	Best loss: 0.053135	Accuracy: 98.28%
39	Validation loss: 0.118302	Best loss: 0.053135	Accuracy: 97.69%
40	Validation loss: 0.224021	Best loss: 0.053135	Accuracy: 98.28%
41	Validation loss: 0.084541	Best loss: 0.053135	Accuracy: 98.51%
42	Validation loss: 0.229446	Best loss: 0.053135	Accuracy: 98.55%
43	Validation loss: 0.105323	Best loss: 0.053135	Accuracy: 98.40%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  28.8s
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.108530	Best loss: 0.108530	Accuracy: 97.15%
1	Validation loss: 0.114079	Best loss: 0.108530	Accuracy: 97.54%
2	Validation loss: 0.075124	Best loss: 0.075124	Accuracy: 97.81%
3	Validation loss: 0.090295	Best loss: 0.075124	Accuracy: 97.97%
4	Validation loss: 0.152570	Best loss: 0.075124	Accuracy: 98.08%
5	Validation loss: 0.087451	Best loss: 0.075124	Accuracy: 97.93%
6	Validation loss: 0.096549	Best loss: 0.075124	Accuracy: 97.85%
7	Validation loss: 0.080204	Best loss: 0.075124	Accuracy: 98.40%
8	Validation loss: 0.084818	Best loss: 0.075124	Accuracy: 98.08%
9	Validation loss: 0.136959	Best loss: 0.075124	Accuracy: 96.87%
10	Validation loss: 0.087893	Best loss: 0.075124	Accuracy: 98.36%
11	Validation loss: 0.079503	Best loss: 0.075124	Accuracy: 98.28%
12	Validation loss: 0.084010	Best loss: 0.075124	Accuracy: 98.05%
13	Validation loss: 0.081476	Best loss: 0.075124	Accuracy: 98.16%
14	Validation loss: 0.105654	Best loss: 0.075124	Accuracy: 97.54%
15	Validation loss: 0.080218	Best loss: 0.075124	Accuracy: 98.24%
16	Validation loss: 0.080117	Best loss: 0.075124	Accuracy: 98.24%
17	Validation loss: 0.096720	Best loss: 0.075124	Accuracy: 97.69%
18	Validation loss: 0.085743	Best loss: 0.075124	Accuracy: 98.12%
19	Validation loss: 0.113211	Best loss: 0.075124	Accuracy: 98.40%
20	Validation loss: 0.095754	Best loss: 0.075124	Accuracy: 97.97%
21	Validation loss: 0.068272	Best loss: 0.068272	Accuracy: 98.36%
22	Validation loss: 0.088605	Best loss: 0.068272	Accuracy: 97.38%
23	Validation loss: 0.072207	Best loss: 0.068272	Accuracy: 98.36%
24	Validation loss: 0.083533	Best loss: 0.068272	Accuracy: 98.12%
25	Validation loss: 0.133703	Best loss: 0.068272	Accuracy: 97.89%
26	Validation loss: 0.092707	Best loss: 0.068272	Accuracy: 98.01%
27	Validation loss: 0.090618	Best loss: 0.068272	Accuracy: 98.20%
28	Validation loss: 0.101569	Best loss: 0.068272	Accuracy: 98.20%
29	Validation loss: 0.081621	Best loss: 0.068272	Accuracy: 98.05%
30	Validation loss: 0.134248	Best loss: 0.068272	Accuracy: 98.36%
31	Validation loss: 0.125098	Best loss: 0.068272	Accuracy: 98.12%
32	Validation loss: 0.168910	Best loss: 0.068272	Accuracy: 98.44%
33	Validation loss: 0.173567	Best loss: 0.068272	Accuracy: 98.08%
34	Validation loss: 0.113378	Best loss: 0.068272	Accuracy: 98.05%
35	Validation loss: 0.181186	Best loss: 0.068272	Accuracy: 98.28%
36	Validation loss: 0.154033	Best loss: 0.068272	Accuracy: 98.20%
37	Validation loss: 0.146095	Best loss: 0.068272	Accuracy: 97.81%
38	Validation loss: 0.154835	Best loss: 0.068272	Accuracy: 98.32%
39	Validation loss: 0.124162	Best loss: 0.068272	Accuracy: 98.05%
40	Validation loss: 0.092008	Best loss: 0.068272	Accuracy: 98.16%
41	Validation loss: 0.361242	Best loss: 0.068272	Accuracy: 97.62%
42	Validation loss: 0.192033	Best loss: 0.068272	Accuracy: 97.58%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  27.0s
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.103915	Best loss: 0.103915	Accuracy: 97.58%
1	Validation loss: 0.106842	Best loss: 0.103915	Accuracy: 97.54%
2	Validation loss: 0.111685	Best loss: 0.103915	Accuracy: 97.58%
3	Validation loss: 0.080490	Best loss: 0.080490	Accuracy: 98.36%
4	Validation loss: 0.053586	Best loss: 0.053586	Accuracy: 98.44%
5	Validation loss: 0.120786	Best loss: 0.053586	Accuracy: 97.89%
6	Validation loss: 0.098250	Best loss: 0.053586	Accuracy: 98.20%
7	Validation loss: 0.078132	Best loss: 0.053586	Accuracy: 98.40%
8	Validation loss: 0.074209	Best loss: 0.053586	Accuracy: 98.16%
9	Validation loss: 0.171193	Best loss: 0.053586	Accuracy: 98.40%
10	Validation loss: 0.095387	Best loss: 0.053586	Accuracy: 98.08%
11	Validation loss: 0.094073	Best loss: 0.053586	Accuracy: 98.08%
12	Validation loss: 0.139001	Best loss: 0.053586	Accuracy: 98.32%
13	Validation loss: 0.086170	Best loss: 0.053586	Accuracy: 98.44%
14	Validation loss: 0.090075	Best loss: 0.053586	Accuracy: 98.28%
15	Validation loss: 0.101888	Best loss: 0.053586	Accuracy: 98.59%
16	Validation loss: 0.088037	Best loss: 0.053586	Accuracy: 98.05%
17	Validation loss: 0.096450	Best loss: 0.053586	Accuracy: 98.71%
18	Validation loss: 0.085634	Best loss: 0.053586	Accuracy: 98.40%
19	Validation loss: 0.099661	Best loss: 0.053586	Accuracy: 98.48%
20	Validation loss: 0.134797	Best loss: 0.053586	Accuracy: 98.55%
21	Validation loss: 0.118265	Best loss: 0.053586	Accuracy: 97.93%
22	Validation loss: 0.107201	Best loss: 0.053586	Accuracy: 98.20%
23	Validation loss: 0.093966	Best loss: 0.053586	Accuracy: 97.81%
24	Validation loss: 0.086824	Best loss: 0.053586	Accuracy: 98.28%
25	Validation loss: 0.099026	Best loss: 0.053586	Accuracy: 98.48%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  16.9s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 3926.420166	Best loss: 3926.420166	Accuracy: 19.31%
1	Validation loss: 8460.396484	Best loss: 3926.420166	Accuracy: 19.27%
2	Validation loss: 20628.904297	Best loss: 3926.420166	Accuracy: 22.01%
3	Validation loss: 21185.761719	Best loss: 3926.420166	Accuracy: 19.08%
4	Validation loss: 72283.914062	Best loss: 3926.420166	Accuracy: 19.12%
5	Validation loss: 253592.218750	Best loss: 3926.420166	Accuracy: 19.08%
6	Validation loss: 290929.656250	Best loss: 3926.420166	Accuracy: 20.91%
7	Validation loss: 81829.882812	Best loss: 3926.420166	Accuracy: 19.27%
8	Validation loss: 94377.203125	Best loss: 3926.420166	Accuracy: 19.08%
9	Validation loss: 37683.832031	Best loss: 3926.420166	Accuracy: 19.27%
10	Validation loss: 114154.398438	Best loss: 3926.420166	Accuracy: 19.12%
11	Validation loss: 231329.031250	Best loss: 3926.420166	Accuracy: 18.73%
12	Validation loss: 74216.328125	Best loss: 3926.420166	Accuracy: 20.91%
13	Validation loss: 71989.578125	Best loss: 3926.420166	Accuracy: 18.80%
14	Validation loss: 27463.343750	Best loss: 3926.420166	Accuracy: 19.00%
15	Validation loss: 89906.000000	Best loss: 3926.420166	Accuracy: 20.91%
16	Validation loss: 50145.574219	Best loss: 3926.420166	Accuracy: 22.01%
17	Validation loss: 255925.843750	Best loss: 3926.420166	Accuracy: 21.74%
18	Validation loss: 672757.312500	Best loss: 3926.420166	Accuracy: 22.01%
19	Validation loss: 480099.968750	Best loss: 3926.420166	Accuracy: 20.91%
20	Validation loss: 299339.593750	Best loss: 3926.420166	Accuracy: 19.27%
21	Validation loss: 368433.031250	Best loss: 3926.420166	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  18.4s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 12636.470703	Best loss: 12636.470703	Accuracy: 18.73%
1	Validation loss: 4823.841797	Best loss: 4823.841797	Accuracy: 19.08%
2	Validation loss: 3819.648926	Best loss: 3819.648926	Accuracy: 20.72%
3	Validation loss: 96110.070312	Best loss: 3819.648926	Accuracy: 18.96%
4	Validation loss: 21521.863281	Best loss: 3819.648926	Accuracy: 20.88%
5	Validation loss: 9460.471680	Best loss: 3819.648926	Accuracy: 19.08%
6	Validation loss: 39091.750000	Best loss: 3819.648926	Accuracy: 19.27%
7	Validation loss: 14166.535156	Best loss: 3819.648926	Accuracy: 19.27%
8	Validation loss: 10316.300781	Best loss: 3819.648926	Accuracy: 19.27%
9	Validation loss: 2885.426025	Best loss: 2885.426025	Accuracy: 19.27%
10	Validation loss: 60518.800781	Best loss: 2885.426025	Accuracy: 19.08%
11	Validation loss: 120834.562500	Best loss: 2885.426025	Accuracy: 20.91%
12	Validation loss: 66105.335938	Best loss: 2885.426025	Accuracy: 19.08%
13	Validation loss: 62894.503906	Best loss: 2885.426025	Accuracy: 19.08%
14	Validation loss: 27315.246094	Best loss: 2885.426025	Accuracy: 19.27%
15	Validation loss: 153742.968750	Best loss: 2885.426025	Accuracy: 19.08%
16	Validation loss: 49429.210938	Best loss: 2885.426025	Accuracy: 20.05%
17	Validation loss: 679859.812500	Best loss: 2885.426025	Accuracy: 18.73%
18	Validation loss: 48661.011719	Best loss: 2885.426025	Accuracy: 22.01%
19	Validation loss: 85452.656250	Best loss: 2885.426025	Accuracy: 22.01%
20	Validation loss: 32815.746094	Best loss: 2885.426025	Accuracy: 20.48%
21	Validation loss: 82910.039062	Best loss: 2885.426025	Accuracy: 29.05%
22	Validation loss: 178023.078125	Best loss: 2885.426025	Accuracy: 19.27%
23	Validation loss: 67743.218750	Best loss: 2885.426025	Accuracy: 22.01%
24	Validation loss: 1103434.000000	Best loss: 2885.426025	Accuracy: 20.91%
25	Validation loss: 1175010.125000	Best loss: 2885.426025	Accuracy: 22.01%
26	Validation loss: 211872.921875	Best loss: 2885.426025	Accuracy: 18.73%
27	Validation loss: 188575.156250	Best loss: 2885.426025	Accuracy: 19.27%
28	Validation loss: 433422.718750	Best loss: 2885.426025	Accuracy: 21.74%
29	Validation loss: 155230.765625	Best loss: 2885.426025	Accuracy: 19.08%
30	Validation loss: 6057197.000000	Best loss: 2885.426025	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  25.4s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 3471.527344	Best loss: 3471.527344	Accuracy: 22.01%
1	Validation loss: 890.768738	Best loss: 890.768738	Accuracy: 19.27%
2	Validation loss: 8369.264648	Best loss: 890.768738	Accuracy: 20.91%
3	Validation loss: 109593.093750	Best loss: 890.768738	Accuracy: 20.91%
4	Validation loss: 84408.101562	Best loss: 890.768738	Accuracy: 18.73%
5	Validation loss: 251119.359375	Best loss: 890.768738	Accuracy: 19.08%
6	Validation loss: 107776.851562	Best loss: 890.768738	Accuracy: 18.73%
7	Validation loss: 45693.859375	Best loss: 890.768738	Accuracy: 22.01%
8	Validation loss: 129196.859375	Best loss: 890.768738	Accuracy: 22.01%
9	Validation loss: 220336.484375	Best loss: 890.768738	Accuracy: 19.08%
10	Validation loss: 295280.156250	Best loss: 890.768738	Accuracy: 19.27%
11	Validation loss: 184143.859375	Best loss: 890.768738	Accuracy: 18.73%
12	Validation loss: 74465.578125	Best loss: 890.768738	Accuracy: 18.73%
13	Validation loss: 244463.484375	Best loss: 890.768738	Accuracy: 18.73%
14	Validation loss: 100297.656250	Best loss: 890.768738	Accuracy: 18.73%
15	Validation loss: 132891.359375	Best loss: 890.768738	Accuracy: 18.73%
16	Validation loss: 239424.250000	Best loss: 890.768738	Accuracy: 22.01%
17	Validation loss: 263563.281250	Best loss: 890.768738	Accuracy: 18.73%
18	Validation loss: 171620.781250	Best loss: 890.768738	Accuracy: 18.73%
19	Validation loss: 1728000.375000	Best loss: 890.768738	Accuracy: 19.08%
20	Validation loss: 157182.390625	Best loss: 890.768738	Accuracy: 22.01%
21	Validation loss: 175728.015625	Best loss: 890.768738	Accuracy: 18.73%
22	Validation loss: 84327.468750	Best loss: 890.768738	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  19.5s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.001616	Best loss: 2.001616	Accuracy: 18.73%
1	Validation loss: 1.748362	Best loss: 1.748362	Accuracy: 18.73%
2	Validation loss: 2.205875	Best loss: 1.748362	Accuracy: 19.27%
3	Validation loss: 3.239836	Best loss: 1.748362	Accuracy: 19.08%
4	Validation loss: 1.994765	Best loss: 1.748362	Accuracy: 19.08%
5	Validation loss: 1.978238	Best loss: 1.748362	Accuracy: 20.91%
6	Validation loss: 2.072779	Best loss: 1.748362	Accuracy: 18.73%
7	Validation loss: 1.781043	Best loss: 1.748362	Accuracy: 19.08%
8	Validation loss: 2.190683	Best loss: 1.748362	Accuracy: 19.08%
9	Validation loss: 3.199287	Best loss: 1.748362	Accuracy: 19.27%
10	Validation loss: 1.648731	Best loss: 1.648731	Accuracy: 19.27%
11	Validation loss: 1.994221	Best loss: 1.648731	Accuracy: 19.08%
12	Validation loss: 2.487053	Best loss: 1.648731	Accuracy: 18.73%
13	Validation loss: 1.842769	Best loss: 1.648731	Accuracy: 22.01%
14	Validation loss: 3.235569	Best loss: 1.648731	Accuracy: 20.91%
15	Validation loss: 2.229467	Best loss: 1.648731	Accuracy: 22.01%
16	Validation loss: 2.310441	Best loss: 1.648731	Accuracy: 18.73%
17	Validation loss: 2.659466	Best loss: 1.648731	Accuracy: 20.91%
18	Validation loss: 3.459507	Best loss: 1.648731	Accuracy: 22.01%
19	Validation loss: 2.868845	Best loss: 1.648731	Accuracy: 22.01%
20	Validation loss: 3.448839	Best loss: 1.648731	Accuracy: 20.91%
21	Validation loss: 2.025246	Best loss: 1.648731	Accuracy: 20.91%
22	Validation loss: 2.174608	Best loss: 1.648731	Accuracy: 18.73%
23	Validation loss: 2.191875	Best loss: 1.648731	Accuracy: 19.08%
24	Validation loss: 2.602127	Best loss: 1.648731	Accuracy: 20.91%
25	Validation loss: 2.151465	Best loss: 1.648731	Accuracy: 20.91%
26	Validation loss: 1.858750	Best loss: 1.648731	Accuracy: 20.91%
27	Validation loss: 2.318470	Best loss: 1.648731	Accuracy: 22.01%
28	Validation loss: 2.395480	Best loss: 1.648731	Accuracy: 19.08%
29	Validation loss: 2.070764	Best loss: 1.648731	Accuracy: 19.08%
30	Validation loss: 1.897778	Best loss: 1.648731	Accuracy: 20.91%
31	Validation loss: 2.392999	Best loss: 1.648731	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.5min
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.948038	Best loss: 1.948038	Accuracy: 18.73%
1	Validation loss: 2.305699	Best loss: 1.948038	Accuracy: 19.27%
2	Validation loss: 1.771229	Best loss: 1.771229	Accuracy: 19.08%
3	Validation loss: 2.160145	Best loss: 1.771229	Accuracy: 22.01%
4	Validation loss: 1.968034	Best loss: 1.771229	Accuracy: 19.08%
5	Validation loss: 1.768641	Best loss: 1.768641	Accuracy: 18.73%
6	Validation loss: 2.299831	Best loss: 1.768641	Accuracy: 19.27%
7	Validation loss: 2.762520	Best loss: 1.768641	Accuracy: 18.73%
8	Validation loss: 2.974286	Best loss: 1.768641	Accuracy: 18.73%
9	Validation loss: 1.944613	Best loss: 1.768641	Accuracy: 22.01%
10	Validation loss: 2.251292	Best loss: 1.768641	Accuracy: 19.08%
11	Validation loss: 2.570494	Best loss: 1.768641	Accuracy: 19.08%
12	Validation loss: 1.912920	Best loss: 1.768641	Accuracy: 19.08%
13	Validation loss: 3.344528	Best loss: 1.768641	Accuracy: 19.27%
14	Validation loss: 3.026260	Best loss: 1.768641	Accuracy: 19.27%
15	Validation loss: 2.246663	Best loss: 1.768641	Accuracy: 20.91%
16	Validation loss: 1.895599	Best loss: 1.768641	Accuracy: 20.91%
17	Validation loss: 2.221243	Best loss: 1.768641	Accuracy: 18.73%
18	Validation loss: 2.348200	Best loss: 1.768641	Accuracy: 20.91%
19	Validation loss: 2.635219	Best loss: 1.768641	Accuracy: 18.73%
20	Validation loss: 1.692789	Best loss: 1.692789	Accuracy: 18.73%
21	Validation loss: 1.764788	Best loss: 1.692789	Accuracy: 20.91%
22	Validation loss: 1.742778	Best loss: 1.692789	Accuracy: 18.73%
23	Validation loss: 2.439057	Best loss: 1.692789	Accuracy: 19.27%
24	Validation loss: 2.004299	Best loss: 1.692789	Accuracy: 18.73%
25	Validation loss: 1.677778	Best loss: 1.677778	Accuracy: 18.73%
26	Validation loss: 3.584424	Best loss: 1.677778	Accuracy: 19.08%
27	Validation loss: 2.415836	Best loss: 1.677778	Accuracy: 18.73%
28	Validation loss: 2.223391	Best loss: 1.677778	Accuracy: 20.91%
29	Validation loss: 1.764659	Best loss: 1.677778	Accuracy: 19.08%
30	Validation loss: 2.291367	Best loss: 1.677778	Accuracy: 22.01%
31	Validation loss: 2.017366	Best loss: 1.677778	Accuracy: 19.08%
32	Validation loss: 2.286280	Best loss: 1.677778	Accuracy: 19.27%
33	Validation loss: 3.297982	Best loss: 1.677778	Accuracy: 19.08%
34	Validation loss: 2.718412	Best loss: 1.677778	Accuracy: 20.91%
35	Validation loss: 2.742929	Best loss: 1.677778	Accuracy: 22.01%
36	Validation loss: 4.069742	Best loss: 1.677778	Accuracy: 19.27%
37	Validation loss: 3.691679	Best loss: 1.677778	Accuracy: 20.91%
38	Validation loss: 1.797883	Best loss: 1.677778	Accuracy: 18.73%
39	Validation loss: 4.717593	Best loss: 1.677778	Accuracy: 18.73%
40	Validation loss: 2.697995	Best loss: 1.677778	Accuracy: 18.73%
41	Validation loss: 4.481767	Best loss: 1.677778	Accuracy: 19.08%
42	Validation loss: 3.123604	Best loss: 1.677778	Accuracy: 20.91%
43	Validation loss: 2.381690	Best loss: 1.677778	Accuracy: 18.73%
44	Validation loss: 2.135468	Best loss: 1.677778	Accuracy: 20.91%
45	Validation loss: 2.215276	Best loss: 1.677778	Accuracy: 19.08%
46	Validation loss: 1.741351	Best loss: 1.677778	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 2.2min
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.154868	Best loss: 2.154868	Accuracy: 18.73%
1	Validation loss: 2.199036	Best loss: 2.154868	Accuracy: 22.01%
2	Validation loss: 2.509864	Best loss: 2.154868	Accuracy: 19.08%
3	Validation loss: 2.432184	Best loss: 2.154868	Accuracy: 19.27%
4	Validation loss: 3.845874	Best loss: 2.154868	Accuracy: 20.91%
5	Validation loss: 3.079849	Best loss: 2.154868	Accuracy: 20.91%
6	Validation loss: 2.079954	Best loss: 2.079954	Accuracy: 19.27%
7	Validation loss: 2.166255	Best loss: 2.079954	Accuracy: 22.01%
8	Validation loss: 1.643958	Best loss: 1.643958	Accuracy: 22.01%
9	Validation loss: 2.753394	Best loss: 1.643958	Accuracy: 22.01%
10	Validation loss: 2.115745	Best loss: 1.643958	Accuracy: 18.73%
11	Validation loss: 2.240570	Best loss: 1.643958	Accuracy: 18.73%
12	Validation loss: 2.006001	Best loss: 1.643958	Accuracy: 22.01%
13	Validation loss: 3.826780	Best loss: 1.643958	Accuracy: 19.08%
14	Validation loss: 1.815757	Best loss: 1.643958	Accuracy: 22.01%
15	Validation loss: 2.066557	Best loss: 1.643958	Accuracy: 19.27%
16	Validation loss: 2.754961	Best loss: 1.643958	Accuracy: 18.73%
17	Validation loss: 2.060835	Best loss: 1.643958	Accuracy: 18.73%
18	Validation loss: 1.814764	Best loss: 1.643958	Accuracy: 19.27%
19	Validation loss: 3.161479	Best loss: 1.643958	Accuracy: 22.01%
20	Validation loss: 1.780306	Best loss: 1.643958	Accuracy: 20.91%
21	Validation loss: 2.535266	Best loss: 1.643958	Accuracy: 22.01%
22	Validation loss: 2.039859	Best loss: 1.643958	Accuracy: 19.27%
23	Validation loss: 3.371031	Best loss: 1.643958	Accuracy: 19.08%
24	Validation loss: 2.130876	Best loss: 1.643958	Accuracy: 18.73%
25	Validation loss: 2.323254	Best loss: 1.643958	Accuracy: 22.01%
26	Validation loss: 2.270942	Best loss: 1.643958	Accuracy: 18.73%
27	Validation loss: 2.031498	Best loss: 1.643958	Accuracy: 18.73%
28	Validation loss: 1.793972	Best loss: 1.643958	Accuracy: 19.27%
29	Validation loss: 3.396077	Best loss: 1.643958	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.4, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.5min
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.139482	Best loss: 2.139482	Accuracy: 18.73%
1	Validation loss: 2.361400	Best loss: 2.139482	Accuracy: 18.73%
2	Validation loss: 2.202041	Best loss: 2.139482	Accuracy: 19.27%
3	Validation loss: 3.499602	Best loss: 2.139482	Accuracy: 22.01%
4	Validation loss: 2.297296	Best loss: 2.139482	Accuracy: 19.27%
5	Validation loss: 2.352470	Best loss: 2.139482	Accuracy: 20.91%
6	Validation loss: 2.011614	Best loss: 2.011614	Accuracy: 18.73%
7	Validation loss: 1.989898	Best loss: 1.989898	Accuracy: 19.08%
8	Validation loss: 1.900756	Best loss: 1.900756	Accuracy: 19.08%
9	Validation loss: 2.798062	Best loss: 1.900756	Accuracy: 22.01%
10	Validation loss: 1.698989	Best loss: 1.698989	Accuracy: 20.91%
11	Validation loss: 2.597043	Best loss: 1.698989	Accuracy: 19.08%
12	Validation loss: 2.101565	Best loss: 1.698989	Accuracy: 19.08%
13	Validation loss: 2.150096	Best loss: 1.698989	Accuracy: 22.01%
14	Validation loss: 3.129025	Best loss: 1.698989	Accuracy: 18.73%
15	Validation loss: 2.106534	Best loss: 1.698989	Accuracy: 19.08%
16	Validation loss: 2.770750	Best loss: 1.698989	Accuracy: 20.91%
17	Validation loss: 2.775406	Best loss: 1.698989	Accuracy: 20.91%
18	Validation loss: 3.618689	Best loss: 1.698989	Accuracy: 22.01%
19	Validation loss: 2.789518	Best loss: 1.698989	Accuracy: 22.01%
20	Validation loss: 3.340817	Best loss: 1.698989	Accuracy: 20.91%
21	Validation loss: 2.024678	Best loss: 1.698989	Accuracy: 20.91%
22	Validation loss: 2.090389	Best loss: 1.698989	Accuracy: 18.73%
23	Validation loss: 1.884792	Best loss: 1.698989	Accuracy: 19.08%
24	Validation loss: 2.896493	Best loss: 1.698989	Accuracy: 20.91%
25	Validation loss: 1.716432	Best loss: 1.698989	Accuracy: 19.27%
26	Validation loss: 2.469761	Best loss: 1.698989	Accuracy: 20.91%
27	Validation loss: 1.692703	Best loss: 1.692703	Accuracy: 22.01%
28	Validation loss: 2.471041	Best loss: 1.692703	Accuracy: 22.01%
29	Validation loss: 2.505400	Best loss: 1.692703	Accuracy: 19.08%
30	Validation loss: 2.413335	Best loss: 1.692703	Accuracy: 19.08%
31	Validation loss: 2.474255	Best loss: 1.692703	Accuracy: 19.08%
32	Validation loss: 2.285786	Best loss: 1.692703	Accuracy: 20.91%
33	Validation loss: 1.865666	Best loss: 1.692703	Accuracy: 19.27%
34	Validation loss: 1.669401	Best loss: 1.669401	Accuracy: 20.91%
35	Validation loss: 2.920837	Best loss: 1.669401	Accuracy: 18.73%
36	Validation loss: 2.727559	Best loss: 1.669401	Accuracy: 22.01%
37	Validation loss: 2.534883	Best loss: 1.669401	Accuracy: 20.91%
38	Validation loss: 2.201831	Best loss: 1.669401	Accuracy: 22.01%
39	Validation loss: 2.898136	Best loss: 1.669401	Accuracy: 18.73%
40	Validation loss: 3.293873	Best loss: 1.669401	Accuracy: 22.01%
41	Validation loss: 2.303053	Best loss: 1.669401	Accuracy: 19.27%
42	Validation loss: 2.579028	Best loss: 1.669401	Accuracy: 19.27%
43	Validation loss: 2.110131	Best loss: 1.669401	Accuracy: 19.27%
44	Validation loss: 2.558131	Best loss: 1.669401	Accuracy: 20.91%
45	Validation loss: 2.395762	Best loss: 1.669401	Accuracy: 18.73%
46	Validation loss: 2.866009	Best loss: 1.669401	Accuracy: 18.73%
47	Validation loss: 2.598667	Best loss: 1.669401	Accuracy: 19.27%
48	Validation loss: 2.472276	Best loss: 1.669401	Accuracy: 20.91%
49	Validation loss: 2.244418	Best loss: 1.669401	Accuracy: 19.27%
50	Validation loss: 1.780017	Best loss: 1.669401	Accuracy: 20.91%
51	Validation loss: 2.152925	Best loss: 1.669401	Accuracy: 22.01%
52	Validation loss: 2.019986	Best loss: 1.669401	Accuracy: 22.01%
53	Validation loss: 2.814372	Best loss: 1.669401	Accuracy: 19.27%
54	Validation loss: 2.988756	Best loss: 1.669401	Accuracy: 19.27%
55	Validation loss: 2.236036	Best loss: 1.669401	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 2.8min
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.776266	Best loss: 1.776266	Accuracy: 18.73%
1	Validation loss: 2.286452	Best loss: 1.776266	Accuracy: 19.27%
2	Validation loss: 1.695882	Best loss: 1.695882	Accuracy: 22.01%
3	Validation loss: 2.321965	Best loss: 1.695882	Accuracy: 22.01%
4	Validation loss: 1.946106	Best loss: 1.695882	Accuracy: 19.08%
5	Validation loss: 1.802564	Best loss: 1.695882	Accuracy: 19.27%
6	Validation loss: 1.836664	Best loss: 1.695882	Accuracy: 19.27%
7	Validation loss: 2.405649	Best loss: 1.695882	Accuracy: 18.73%
8	Validation loss: 2.616276	Best loss: 1.695882	Accuracy: 18.73%
9	Validation loss: 2.004930	Best loss: 1.695882	Accuracy: 19.27%
10	Validation loss: 2.237389	Best loss: 1.695882	Accuracy: 20.91%
11	Validation loss: 2.739076	Best loss: 1.695882	Accuracy: 19.08%
12	Validation loss: 1.817056	Best loss: 1.695882	Accuracy: 19.08%
13	Validation loss: 3.006981	Best loss: 1.695882	Accuracy: 19.27%
14	Validation loss: 2.897690	Best loss: 1.695882	Accuracy: 19.27%
15	Validation loss: 2.677606	Best loss: 1.695882	Accuracy: 20.91%
16	Validation loss: 1.843229	Best loss: 1.695882	Accuracy: 22.01%
17	Validation loss: 3.036057	Best loss: 1.695882	Accuracy: 19.08%
18	Validation loss: 2.172425	Best loss: 1.695882	Accuracy: 19.08%
19	Validation loss: 3.255152	Best loss: 1.695882	Accuracy: 19.27%
20	Validation loss: 1.689038	Best loss: 1.689038	Accuracy: 18.73%
21	Validation loss: 1.941973	Best loss: 1.689038	Accuracy: 19.27%
22	Validation loss: 1.715747	Best loss: 1.689038	Accuracy: 18.73%
23	Validation loss: 2.647643	Best loss: 1.689038	Accuracy: 19.27%
24	Validation loss: 2.005494	Best loss: 1.689038	Accuracy: 19.08%
25	Validation loss: 2.103759	Best loss: 1.689038	Accuracy: 22.01%
26	Validation loss: 3.711790	Best loss: 1.689038	Accuracy: 19.08%
27	Validation loss: 2.268840	Best loss: 1.689038	Accuracy: 20.91%
28	Validation loss: 2.313325	Best loss: 1.689038	Accuracy: 22.01%
29	Validation loss: 2.151689	Best loss: 1.689038	Accuracy: 19.08%
30	Validation loss: 2.127470	Best loss: 1.689038	Accuracy: 22.01%
31	Validation loss: 2.198866	Best loss: 1.689038	Accuracy: 19.08%
32	Validation loss: 2.623166	Best loss: 1.689038	Accuracy: 18.73%
33	Validation loss: 3.572831	Best loss: 1.689038	Accuracy: 19.08%
34	Validation loss: 2.301634	Best loss: 1.689038	Accuracy: 20.91%
35	Validation loss: 3.194148	Best loss: 1.689038	Accuracy: 20.91%
36	Validation loss: 3.219940	Best loss: 1.689038	Accuracy: 19.27%
37	Validation loss: 1.866071	Best loss: 1.689038	Accuracy: 18.73%
38	Validation loss: 1.776575	Best loss: 1.689038	Accuracy: 18.73%
39	Validation loss: 4.672285	Best loss: 1.689038	Accuracy: 18.73%
40	Validation loss: 2.626274	Best loss: 1.689038	Accuracy: 18.73%
41	Validation loss: 3.195865	Best loss: 1.689038	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 2.1min
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 2.010494	Best loss: 2.010494	Accuracy: 18.73%
1	Validation loss: 1.943797	Best loss: 1.943797	Accuracy: 19.27%
2	Validation loss: 2.490443	Best loss: 1.943797	Accuracy: 19.08%
3	Validation loss: 2.013548	Best loss: 1.943797	Accuracy: 22.01%
4	Validation loss: 2.703228	Best loss: 1.943797	Accuracy: 20.91%
5	Validation loss: 2.861736	Best loss: 1.943797	Accuracy: 20.91%
6	Validation loss: 2.530509	Best loss: 1.943797	Accuracy: 19.27%
7	Validation loss: 2.083091	Best loss: 1.943797	Accuracy: 19.27%
8	Validation loss: 1.775421	Best loss: 1.775421	Accuracy: 22.01%
9	Validation loss: 2.662640	Best loss: 1.775421	Accuracy: 22.01%
10	Validation loss: 2.085462	Best loss: 1.775421	Accuracy: 19.27%
11	Validation loss: 2.298519	Best loss: 1.775421	Accuracy: 18.73%
12	Validation loss: 2.389056	Best loss: 1.775421	Accuracy: 22.01%
13	Validation loss: 4.750522	Best loss: 1.775421	Accuracy: 19.08%
14	Validation loss: 1.956875	Best loss: 1.775421	Accuracy: 19.27%
15	Validation loss: 2.018093	Best loss: 1.775421	Accuracy: 19.27%
16	Validation loss: 2.192061	Best loss: 1.775421	Accuracy: 18.73%
17	Validation loss: 2.382256	Best loss: 1.775421	Accuracy: 18.73%
18	Validation loss: 2.007759	Best loss: 1.775421	Accuracy: 19.27%
19	Validation loss: 3.220553	Best loss: 1.775421	Accuracy: 22.01%
20	Validation loss: 2.419333	Best loss: 1.775421	Accuracy: 19.27%
21	Validation loss: 2.930693	Best loss: 1.775421	Accuracy: 22.01%
22	Validation loss: 1.942248	Best loss: 1.775421	Accuracy: 19.27%
23	Validation loss: 3.364224	Best loss: 1.775421	Accuracy: 19.08%
24	Validation loss: 2.112572	Best loss: 1.775421	Accuracy: 18.73%
25	Validation loss: 1.808807	Best loss: 1.775421	Accuracy: 20.91%
26	Validation loss: 2.403497	Best loss: 1.775421	Accuracy: 18.73%
27	Validation loss: 2.512130	Best loss: 1.775421	Accuracy: 19.27%
28	Validation loss: 3.033399	Best loss: 1.775421	Accuracy: 20.91%
29	Validation loss: 3.386788	Best loss: 1.775421	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.5min
[CV] n_neurons=70, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.611197	Best loss: 1.611197	Accuracy: 19.08%
1	Validation loss: 1.612927	Best loss: 1.611197	Accuracy: 19.27%
2	Validation loss: 1.609791	Best loss: 1.609791	Accuracy: 22.01%
3	Validation loss: 1.609717	Best loss: 1.609717	Accuracy: 22.01%
4	Validation loss: 1.619321	Best loss: 1.609717	Accuracy: 22.01%
5	Validation loss: 1.612526	Best loss: 1.609717	Accuracy: 19.27%
6	Validation loss: 1.609897	Best loss: 1.609717	Accuracy: 22.01%
7	Validation loss: 1.615147	Best loss: 1.609717	Accuracy: 22.01%
8	Validation loss: 1.621828	Best loss: 1.609717	Accuracy: 22.01%
9	Validation loss: 1.610758	Best loss: 1.609717	Accuracy: 22.01%
10	Validation loss: 1.608220	Best loss: 1.608220	Accuracy: 20.91%
11	Validation loss: 1.619423	Best loss: 1.608220	Accuracy: 19.27%
12	Validation loss: 1.608462	Best loss: 1.608220	Accuracy: 20.91%
13	Validation loss: 1.618570	Best loss: 1.608220	Accuracy: 19.08%
14	Validation loss: 1.613490	Best loss: 1.608220	Accuracy: 18.73%
15	Validation loss: 1.611271	Best loss: 1.608220	Accuracy: 22.01%
16	Validation loss: 1.610684	Best loss: 1.608220	Accuracy: 19.08%
17	Validation loss: 1.611027	Best loss: 1.608220	Accuracy: 19.27%
18	Validation loss: 1.614784	Best loss: 1.608220	Accuracy: 18.73%
19	Validation loss: 1.610605	Best loss: 1.608220	Accuracy: 19.27%
20	Validation loss: 1.611324	Best loss: 1.608220	Accuracy: 20.91%
21	Validation loss: 1.614878	Best loss: 1.608220	Accuracy: 19.27%
22	Validation loss: 1.616508	Best loss: 1.608220	Accuracy: 19.27%
23	Validation loss: 1.618049	Best loss: 1.608220	Accuracy: 19.27%
24	Validation loss: 1.614491	Best loss: 1.608220	Accuracy: 22.01%
25	Validation loss: 1.609585	Best loss: 1.608220	Accuracy: 22.01%
26	Validation loss: 1.610505	Best loss: 1.608220	Accuracy: 22.01%
27	Validation loss: 1.612527	Best loss: 1.608220	Accuracy: 19.08%
28	Validation loss: 1.610706	Best loss: 1.608220	Accuracy: 19.08%
29	Validation loss: 1.614644	Best loss: 1.608220	Accuracy: 18.73%
30	Validation loss: 1.622702	Best loss: 1.608220	Accuracy: 19.27%
31	Validation loss: 1.608780	Best loss: 1.608220	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  14.0s
[CV] n_neurons=70, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.610033	Best loss: 1.610033	Accuracy: 19.27%
1	Validation loss: 1.610411	Best loss: 1.610033	Accuracy: 19.08%
2	Validation loss: 1.615144	Best loss: 1.610033	Accuracy: 19.27%
3	Validation loss: 1.624002	Best loss: 1.610033	Accuracy: 18.73%
4	Validation loss: 1.614279	Best loss: 1.610033	Accuracy: 22.01%
5	Validation loss: 1.613998	Best loss: 1.610033	Accuracy: 19.27%
6	Validation loss: 1.614724	Best loss: 1.610033	Accuracy: 19.27%
7	Validation loss: 1.611922	Best loss: 1.610033	Accuracy: 22.01%
8	Validation loss: 1.609730	Best loss: 1.609730	Accuracy: 20.91%
9	Validation loss: 1.609590	Best loss: 1.609590	Accuracy: 20.91%
10	Validation loss: 1.618782	Best loss: 1.609590	Accuracy: 22.01%
11	Validation loss: 1.612229	Best loss: 1.609590	Accuracy: 22.01%
12	Validation loss: 1.609187	Best loss: 1.609187	Accuracy: 22.01%
13	Validation loss: 1.613472	Best loss: 1.609187	Accuracy: 19.27%
14	Validation loss: 1.613037	Best loss: 1.609187	Accuracy: 19.27%
15	Validation loss: 1.612357	Best loss: 1.609187	Accuracy: 22.01%
16	Validation loss: 1.612104	Best loss: 1.609187	Accuracy: 22.01%
17	Validation loss: 1.618430	Best loss: 1.609187	Accuracy: 19.08%
18	Validation loss: 1.610900	Best loss: 1.609187	Accuracy: 22.01%
19	Validation loss: 1.610625	Best loss: 1.609187	Accuracy: 22.01%
20	Validation loss: 1.612261	Best loss: 1.609187	Accuracy: 19.08%
21	Validation loss: 1.610903	Best loss: 1.609187	Accuracy: 22.01%
22	Validation loss: 1.617964	Best loss: 1.609187	Accuracy: 22.01%
23	Validation loss: 1.616953	Best loss: 1.609187	Accuracy: 22.01%
24	Validation loss: 1.612183	Best loss: 1.609187	Accuracy: 22.01%
25	Validation loss: 1.615082	Best loss: 1.609187	Accuracy: 22.01%
26	Validation loss: 1.607679	Best loss: 1.607679	Accuracy: 22.01%
27	Validation loss: 1.611636	Best loss: 1.607679	Accuracy: 22.01%
28	Validation loss: 1.626023	Best loss: 1.607679	Accuracy: 18.73%
29	Validation loss: 1.609960	Best loss: 1.607679	Accuracy: 18.73%
30	Validation loss: 1.611003	Best loss: 1.607679	Accuracy: 19.08%
31	Validation loss: 1.611653	Best loss: 1.607679	Accuracy: 19.27%
32	Validation loss: 1.612803	Best loss: 1.607679	Accuracy: 18.73%
33	Validation loss: 1.608863	Best loss: 1.607679	Accuracy: 20.91%
34	Validation loss: 1.613799	Best loss: 1.607679	Accuracy: 22.01%
35	Validation loss: 1.610112	Best loss: 1.607679	Accuracy: 22.01%
36	Validation loss: 1.613164	Best loss: 1.607679	Accuracy: 20.91%
37	Validation loss: 1.609830	Best loss: 1.607679	Accuracy: 22.01%
38	Validation loss: 1.620756	Best loss: 1.607679	Accuracy: 22.01%
39	Validation loss: 1.623759	Best loss: 1.607679	Accuracy: 19.27%
40	Validation loss: 1.609533	Best loss: 1.607679	Accuracy: 22.01%
41	Validation loss: 1.609140	Best loss: 1.607679	Accuracy: 22.01%
42	Validation loss: 1.622813	Best loss: 1.607679	Accuracy: 19.27%
43	Validation loss: 1.617255	Best loss: 1.607679	Accuracy: 22.01%
44	Validation loss: 1.616075	Best loss: 1.607679	Accuracy: 19.08%
45	Validation loss: 1.611636	Best loss: 1.607679	Accuracy: 19.08%
46	Validation loss: 1.623663	Best loss: 1.607679	Accuracy: 18.73%
47	Validation loss: 1.611053	Best loss: 1.607679	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  21.2s
[CV] n_neurons=70, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.614218	Best loss: 1.614218	Accuracy: 22.01%
1	Validation loss: 1.610091	Best loss: 1.610091	Accuracy: 19.08%
2	Validation loss: 1.618630	Best loss: 1.610091	Accuracy: 22.01%
3	Validation loss: 1.620299	Best loss: 1.610091	Accuracy: 19.27%
4	Validation loss: 1.609074	Best loss: 1.609074	Accuracy: 22.01%
5	Validation loss: 1.610014	Best loss: 1.609074	Accuracy: 22.01%
6	Validation loss: 1.615215	Best loss: 1.609074	Accuracy: 22.01%
7	Validation loss: 1.616394	Best loss: 1.609074	Accuracy: 18.73%
8	Validation loss: 1.611633	Best loss: 1.609074	Accuracy: 19.27%
9	Validation loss: 1.614079	Best loss: 1.609074	Accuracy: 22.01%
10	Validation loss: 1.612409	Best loss: 1.609074	Accuracy: 18.73%
11	Validation loss: 1.623532	Best loss: 1.609074	Accuracy: 22.01%
12	Validation loss: 1.608298	Best loss: 1.608298	Accuracy: 22.01%
13	Validation loss: 1.611845	Best loss: 1.608298	Accuracy: 19.27%
14	Validation loss: 1.608458	Best loss: 1.608298	Accuracy: 20.91%
15	Validation loss: 1.617296	Best loss: 1.608298	Accuracy: 19.08%
16	Validation loss: 1.618834	Best loss: 1.608298	Accuracy: 18.73%
17	Validation loss: 1.615237	Best loss: 1.608298	Accuracy: 19.08%
18	Validation loss: 1.608079	Best loss: 1.608079	Accuracy: 20.91%
19	Validation loss: 1.608636	Best loss: 1.608079	Accuracy: 22.01%
20	Validation loss: 1.620149	Best loss: 1.608079	Accuracy: 19.27%
21	Validation loss: 1.611817	Best loss: 1.608079	Accuracy: 22.01%
22	Validation loss: 1.613169	Best loss: 1.608079	Accuracy: 22.01%
23	Validation loss: 1.613920	Best loss: 1.608079	Accuracy: 22.01%
24	Validation loss: 1.613877	Best loss: 1.608079	Accuracy: 20.91%
25	Validation loss: 1.617803	Best loss: 1.608079	Accuracy: 22.01%
26	Validation loss: 1.613108	Best loss: 1.608079	Accuracy: 22.01%
27	Validation loss: 1.614583	Best loss: 1.608079	Accuracy: 19.27%
28	Validation loss: 1.630601	Best loss: 1.608079	Accuracy: 18.73%
29	Validation loss: 1.611474	Best loss: 1.608079	Accuracy: 22.01%
30	Validation loss: 1.611859	Best loss: 1.608079	Accuracy: 19.08%
31	Validation loss: 1.611838	Best loss: 1.608079	Accuracy: 22.01%
32	Validation loss: 1.615408	Best loss: 1.608079	Accuracy: 18.73%
33	Validation loss: 1.608509	Best loss: 1.608079	Accuracy: 22.01%
34	Validation loss: 1.613685	Best loss: 1.608079	Accuracy: 22.01%
35	Validation loss: 1.609677	Best loss: 1.608079	Accuracy: 19.27%
36	Validation loss: 1.609911	Best loss: 1.608079	Accuracy: 18.73%
37	Validation loss: 1.608673	Best loss: 1.608079	Accuracy: 22.01%
38	Validation loss: 1.617389	Best loss: 1.608079	Accuracy: 19.27%
39	Validation loss: 1.625782	Best loss: 1.608079	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function relu at 0x124366d08&gt;, total=  18.4s
[CV] n_neurons=100, learning_rate=0.05, dropout_rate=0.3, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.515809	Best loss: 1.515809	Accuracy: 38.94%
1	Validation loss: 1.354420	Best loss: 1.354420	Accuracy: 39.17%
2	Validation loss: 1.300688	Best loss: 1.300688	Accuracy: 41.52%
3	Validation loss: 1.288900	Best loss: 1.288900	Accuracy: 41.52%
4	Validation loss: 1.310081	Best loss: 1.288900	Accuracy: 39.56%
5	Validation loss: 1.616486	Best loss: 1.288900	Accuracy: 18.73%
6	Validation loss: 1.616507	Best loss: 1.288900	Accuracy: 22.01%
7	Validation loss: 1.624240	Best loss: 1.288900	Accuracy: 22.01%
8	Validation loss: 1.615757	Best loss: 1.288900	Accuracy: 19.27%
9	Validation loss: 1.609365	Best loss: 1.288900	Accuracy: 22.01%
10	Validation loss: 1.608095	Best loss: 1.288900	Accuracy: 22.01%
11	Validation loss: 1.613582	Best loss: 1.288900	Accuracy: 19.27%
12	Validation loss: 1.608926	Best loss: 1.288900	Accuracy: 22.01%
13	Validation loss: 1.615841	Best loss: 1.288900	Accuracy: 19.08%
14	Validation loss: 1.613107	Best loss: 1.288900	Accuracy: 18.73%
15	Validation loss: 1.611221	Best loss: 1.288900	Accuracy: 22.01%
16	Validation loss: 1.609470	Best loss: 1.288900	Accuracy: 19.08%
17	Validation loss: 1.610091	Best loss: 1.288900	Accuracy: 19.27%
18	Validation loss: 1.615305	Best loss: 1.288900	Accuracy: 18.73%
19	Validation loss: 1.608995	Best loss: 1.288900	Accuracy: 22.01%
20	Validation loss: 1.609121	Best loss: 1.288900	Accuracy: 20.91%
21	Validation loss: 1.615655	Best loss: 1.288900	Accuracy: 22.01%
22	Validation loss: 1.613701	Best loss: 1.288900	Accuracy: 19.27%
23	Validation loss: 1.617186	Best loss: 1.288900	Accuracy: 19.27%
24	Validation loss: 1.614718	Best loss: 1.288900	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, dropout_rate=0.3, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  18.6s
[CV] n_neurons=100, learning_rate=0.05, dropout_rate=0.3, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.548864	Best loss: 1.548864	Accuracy: 22.28%
1	Validation loss: 1.611302	Best loss: 1.548864	Accuracy: 19.31%
2	Validation loss: 1.610552	Best loss: 1.548864	Accuracy: 22.09%
3	Validation loss: 1.619121	Best loss: 1.548864	Accuracy: 18.76%
4	Validation loss: 1.609051	Best loss: 1.548864	Accuracy: 22.05%
5	Validation loss: 1.586797	Best loss: 1.548864	Accuracy: 20.84%
6	Validation loss: 1.592746	Best loss: 1.548864	Accuracy: 20.84%
7	Validation loss: 1.614728	Best loss: 1.548864	Accuracy: 19.08%
8	Validation loss: 1.609129	Best loss: 1.548864	Accuracy: 22.01%
9	Validation loss: 1.608323	Best loss: 1.548864	Accuracy: 22.01%
10	Validation loss: 1.614523	Best loss: 1.548864	Accuracy: 22.01%
11	Validation loss: 1.610579	Best loss: 1.548864	Accuracy: 22.01%
12	Validation loss: 1.609131	Best loss: 1.548864	Accuracy: 22.01%
13	Validation loss: 1.612335	Best loss: 1.548864	Accuracy: 19.27%
14	Validation loss: 1.609190	Best loss: 1.548864	Accuracy: 22.01%
15	Validation loss: 1.610181	Best loss: 1.548864	Accuracy: 22.01%
16	Validation loss: 1.613045	Best loss: 1.548864	Accuracy: 22.01%
17	Validation loss: 1.615458	Best loss: 1.548864	Accuracy: 19.08%
18	Validation loss: 1.610245	Best loss: 1.548864	Accuracy: 22.01%
19	Validation loss: 1.609072	Best loss: 1.548864	Accuracy: 22.01%
20	Validation loss: 1.610652	Best loss: 1.548864	Accuracy: 22.01%
21	Validation loss: 1.612862	Best loss: 1.548864	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, dropout_rate=0.3, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  16.3s
[CV] n_neurons=100, learning_rate=0.05, dropout_rate=0.3, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.612644	Best loss: 1.612644	Accuracy: 22.05%
1	Validation loss: 1.610435	Best loss: 1.610435	Accuracy: 22.01%
2	Validation loss: 1.615703	Best loss: 1.610435	Accuracy: 22.01%
3	Validation loss: 1.622233	Best loss: 1.610435	Accuracy: 19.27%
4	Validation loss: 1.608048	Best loss: 1.608048	Accuracy: 22.01%
5	Validation loss: 1.610911	Best loss: 1.608048	Accuracy: 22.01%
6	Validation loss: 1.612929	Best loss: 1.608048	Accuracy: 22.01%
7	Validation loss: 1.615302	Best loss: 1.608048	Accuracy: 22.01%
8	Validation loss: 1.609174	Best loss: 1.608048	Accuracy: 22.01%
9	Validation loss: 1.611439	Best loss: 1.608048	Accuracy: 22.01%
10	Validation loss: 1.612149	Best loss: 1.608048	Accuracy: 18.73%
11	Validation loss: 1.617146	Best loss: 1.608048	Accuracy: 22.01%
12	Validation loss: 1.608019	Best loss: 1.608019	Accuracy: 22.01%
13	Validation loss: 1.614043	Best loss: 1.608019	Accuracy: 19.27%
14	Validation loss: 1.609034	Best loss: 1.608019	Accuracy: 20.91%
15	Validation loss: 1.613224	Best loss: 1.608019	Accuracy: 22.01%
16	Validation loss: 1.620475	Best loss: 1.608019	Accuracy: 18.73%
17	Validation loss: 1.614963	Best loss: 1.608019	Accuracy: 19.08%
18	Validation loss: 1.608519	Best loss: 1.608019	Accuracy: 20.91%
19	Validation loss: 1.609452	Best loss: 1.608019	Accuracy: 22.01%
20	Validation loss: 1.617043	Best loss: 1.608019	Accuracy: 19.27%
21	Validation loss: 1.612377	Best loss: 1.608019	Accuracy: 22.01%
22	Validation loss: 1.613459	Best loss: 1.608019	Accuracy: 22.01%
23	Validation loss: 1.611030	Best loss: 1.608019	Accuracy: 22.01%
24	Validation loss: 1.610688	Best loss: 1.608019	Accuracy: 20.91%
25	Validation loss: 1.614550	Best loss: 1.608019	Accuracy: 22.01%
26	Validation loss: 1.609983	Best loss: 1.608019	Accuracy: 22.01%
27	Validation loss: 1.611449	Best loss: 1.608019	Accuracy: 22.01%
28	Validation loss: 1.625299	Best loss: 1.608019	Accuracy: 18.73%
29	Validation loss: 1.611935	Best loss: 1.608019	Accuracy: 22.01%
30	Validation loss: 1.610434	Best loss: 1.608019	Accuracy: 19.08%
31	Validation loss: 1.609725	Best loss: 1.608019	Accuracy: 22.01%
32	Validation loss: 1.612844	Best loss: 1.608019	Accuracy: 18.73%
33	Validation loss: 1.609557	Best loss: 1.608019	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.05, dropout_rate=0.3, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  25.4s
[CV] n_neurons=140, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.453902	Best loss: 0.453902	Accuracy: 88.27%
1	Validation loss: 0.460290	Best loss: 0.453902	Accuracy: 86.20%
2	Validation loss: 0.413083	Best loss: 0.413083	Accuracy: 90.54%
3	Validation loss: 0.501077	Best loss: 0.413083	Accuracy: 88.98%
4	Validation loss: 0.467378	Best loss: 0.413083	Accuracy: 87.26%
5	Validation loss: 0.730643	Best loss: 0.413083	Accuracy: 70.29%
6	Validation loss: 0.759623	Best loss: 0.413083	Accuracy: 69.55%
7	Validation loss: 1.177565	Best loss: 0.413083	Accuracy: 46.17%
8	Validation loss: 0.970431	Best loss: 0.413083	Accuracy: 54.14%
9	Validation loss: 1.134098	Best loss: 0.413083	Accuracy: 47.50%
10	Validation loss: 1.049389	Best loss: 0.413083	Accuracy: 52.74%
11	Validation loss: 1.316651	Best loss: 0.413083	Accuracy: 35.97%
12	Validation loss: 1.251404	Best loss: 0.413083	Accuracy: 38.35%
13	Validation loss: 1.258664	Best loss: 0.413083	Accuracy: 38.55%
14	Validation loss: 1.248947	Best loss: 0.413083	Accuracy: 37.65%
15	Validation loss: 1.268003	Best loss: 0.413083	Accuracy: 37.96%
16	Validation loss: 1.260353	Best loss: 0.413083	Accuracy: 38.55%
17	Validation loss: 1.233938	Best loss: 0.413083	Accuracy: 39.56%
18	Validation loss: 1.239604	Best loss: 0.413083	Accuracy: 38.47%
19	Validation loss: 1.211210	Best loss: 0.413083	Accuracy: 39.80%
20	Validation loss: 1.199895	Best loss: 0.413083	Accuracy: 39.80%
21	Validation loss: 1.207813	Best loss: 0.413083	Accuracy: 39.37%
22	Validation loss: 1.199748	Best loss: 0.413083	Accuracy: 39.68%
23	Validation loss: 1.186239	Best loss: 0.413083	Accuracy: 39.99%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  21.6s
[CV] n_neurons=140, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.714250	Best loss: 0.714250	Accuracy: 69.78%
1	Validation loss: 0.579744	Best loss: 0.579744	Accuracy: 76.00%
2	Validation loss: 0.620530	Best loss: 0.579744	Accuracy: 74.35%
3	Validation loss: 0.687413	Best loss: 0.579744	Accuracy: 72.09%
4	Validation loss: 0.931509	Best loss: 0.579744	Accuracy: 57.11%
5	Validation loss: 1.015779	Best loss: 0.579744	Accuracy: 55.43%
6	Validation loss: 1.027420	Best loss: 0.579744	Accuracy: 53.44%
7	Validation loss: 1.219105	Best loss: 0.579744	Accuracy: 39.05%
8	Validation loss: 1.182564	Best loss: 0.579744	Accuracy: 39.95%
9	Validation loss: 1.351546	Best loss: 0.579744	Accuracy: 34.13%
10	Validation loss: 1.644780	Best loss: 0.579744	Accuracy: 20.76%
11	Validation loss: 1.472604	Best loss: 0.579744	Accuracy: 29.44%
12	Validation loss: 1.204002	Best loss: 0.579744	Accuracy: 40.54%
13	Validation loss: 1.227576	Best loss: 0.579744	Accuracy: 39.48%
14	Validation loss: 1.287343	Best loss: 0.579744	Accuracy: 36.83%
15	Validation loss: 1.356883	Best loss: 0.579744	Accuracy: 33.93%
16	Validation loss: 1.218909	Best loss: 0.579744	Accuracy: 38.62%
17	Validation loss: 1.202530	Best loss: 0.579744	Accuracy: 39.56%
18	Validation loss: 1.189121	Best loss: 0.579744	Accuracy: 40.11%
19	Validation loss: 1.223433	Best loss: 0.579744	Accuracy: 38.27%
20	Validation loss: 1.303066	Best loss: 0.579744	Accuracy: 35.65%
21	Validation loss: 1.281408	Best loss: 0.579744	Accuracy: 36.59%
22	Validation loss: 1.193816	Best loss: 0.579744	Accuracy: 39.68%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  20.7s
[CV] n_neurons=140, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.654498	Best loss: 0.654498	Accuracy: 81.00%
1	Validation loss: 0.562698	Best loss: 0.562698	Accuracy: 84.91%
2	Validation loss: 0.526411	Best loss: 0.526411	Accuracy: 88.12%
3	Validation loss: 0.500967	Best loss: 0.500967	Accuracy: 87.06%
4	Validation loss: 0.717519	Best loss: 0.500967	Accuracy: 73.77%
5	Validation loss: 0.986080	Best loss: 0.500967	Accuracy: 56.57%
6	Validation loss: 1.041225	Best loss: 0.500967	Accuracy: 54.85%
7	Validation loss: 1.220951	Best loss: 0.500967	Accuracy: 40.30%
8	Validation loss: 1.256008	Best loss: 0.500967	Accuracy: 41.01%
9	Validation loss: 1.220851	Best loss: 0.500967	Accuracy: 39.76%
10	Validation loss: 1.202448	Best loss: 0.500967	Accuracy: 40.15%
11	Validation loss: 1.215642	Best loss: 0.500967	Accuracy: 38.82%
12	Validation loss: 1.242469	Best loss: 0.500967	Accuracy: 38.51%
13	Validation loss: 1.219180	Best loss: 0.500967	Accuracy: 39.41%
14	Validation loss: 1.220386	Best loss: 0.500967	Accuracy: 41.01%
15	Validation loss: 1.194822	Best loss: 0.500967	Accuracy: 39.91%
16	Validation loss: 1.271325	Best loss: 0.500967	Accuracy: 36.67%
17	Validation loss: 1.184501	Best loss: 0.500967	Accuracy: 39.80%
18	Validation loss: 1.198388	Best loss: 0.500967	Accuracy: 39.48%
19	Validation loss: 1.238340	Best loss: 0.500967	Accuracy: 37.96%
20	Validation loss: 1.189677	Best loss: 0.500967	Accuracy: 39.91%
21	Validation loss: 1.180371	Best loss: 0.500967	Accuracy: 40.23%
22	Validation loss: 1.190895	Best loss: 0.500967	Accuracy: 40.19%
23	Validation loss: 1.182016	Best loss: 0.500967	Accuracy: 39.99%
24	Validation loss: 1.225041	Best loss: 0.500967	Accuracy: 38.82%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  22.7s
[CV] n_neurons=30, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.630487	Best loss: 1.630487	Accuracy: 22.01%
1	Validation loss: 1.712598	Best loss: 1.630487	Accuracy: 19.27%
2	Validation loss: 1.633370	Best loss: 1.630487	Accuracy: 19.27%
3	Validation loss: 1.686746	Best loss: 1.630487	Accuracy: 19.27%
4	Validation loss: 1.678433	Best loss: 1.630487	Accuracy: 22.01%
5	Validation loss: 1.657234	Best loss: 1.630487	Accuracy: 19.27%
6	Validation loss: 1.621683	Best loss: 1.621683	Accuracy: 22.01%
7	Validation loss: 1.696751	Best loss: 1.621683	Accuracy: 20.91%
8	Validation loss: 1.758067	Best loss: 1.621683	Accuracy: 19.27%
9	Validation loss: 1.654586	Best loss: 1.621683	Accuracy: 18.73%
10	Validation loss: 1.647033	Best loss: 1.621683	Accuracy: 22.01%
11	Validation loss: 1.671743	Best loss: 1.621683	Accuracy: 18.73%
12	Validation loss: 1.688675	Best loss: 1.621683	Accuracy: 20.91%
13	Validation loss: 2.343346	Best loss: 1.621683	Accuracy: 22.01%
14	Validation loss: 1.618675	Best loss: 1.618675	Accuracy: 22.01%
15	Validation loss: 1.616224	Best loss: 1.616224	Accuracy: 22.01%
16	Validation loss: 1.618715	Best loss: 1.616224	Accuracy: 19.08%
17	Validation loss: 6.345444	Best loss: 1.616224	Accuracy: 18.73%
18	Validation loss: 3.448291	Best loss: 1.616224	Accuracy: 19.08%
19	Validation loss: 1.637592	Best loss: 1.616224	Accuracy: 20.91%
20	Validation loss: 1.647179	Best loss: 1.616224	Accuracy: 18.73%
21	Validation loss: 1.623656	Best loss: 1.616224	Accuracy: 19.27%
22	Validation loss: 1.710334	Best loss: 1.616224	Accuracy: 18.73%
23	Validation loss: 1.648345	Best loss: 1.616224	Accuracy: 22.01%
24	Validation loss: 1.630102	Best loss: 1.616224	Accuracy: 22.01%
25	Validation loss: 1.616586	Best loss: 1.616224	Accuracy: 20.91%
26	Validation loss: 1.629213	Best loss: 1.616224	Accuracy: 19.08%
27	Validation loss: 1.638779	Best loss: 1.616224	Accuracy: 18.73%
28	Validation loss: 1.713975	Best loss: 1.616224	Accuracy: 20.91%
29	Validation loss: 1.645061	Best loss: 1.616224	Accuracy: 20.91%
30	Validation loss: 1.637765	Best loss: 1.616224	Accuracy: 19.27%
31	Validation loss: 1.618234	Best loss: 1.616224	Accuracy: 20.91%
32	Validation loss: 1.695945	Best loss: 1.616224	Accuracy: 19.27%
33	Validation loss: 1.715378	Best loss: 1.616224	Accuracy: 19.27%
34	Validation loss: 1.623907	Best loss: 1.616224	Accuracy: 19.08%
35	Validation loss: 1.721041	Best loss: 1.616224	Accuracy: 19.08%
36	Validation loss: 1.825864	Best loss: 1.616224	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.1min
[CV] n_neurons=30, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.663356	Best loss: 1.663356	Accuracy: 22.01%
1	Validation loss: 1.614118	Best loss: 1.614118	Accuracy: 19.39%
2	Validation loss: 1.682696	Best loss: 1.614118	Accuracy: 19.31%
3	Validation loss: 1.685229	Best loss: 1.614118	Accuracy: 19.08%
4	Validation loss: 1.682138	Best loss: 1.614118	Accuracy: 19.08%
5	Validation loss: 1.638210	Best loss: 1.614118	Accuracy: 20.91%
6	Validation loss: 1.640900	Best loss: 1.614118	Accuracy: 19.27%
7	Validation loss: 1.709452	Best loss: 1.614118	Accuracy: 19.27%
8	Validation loss: 1.717851	Best loss: 1.614118	Accuracy: 18.73%
9	Validation loss: 1.667701	Best loss: 1.614118	Accuracy: 22.01%
10	Validation loss: 1.655898	Best loss: 1.614118	Accuracy: 19.08%
11	Validation loss: 1.710838	Best loss: 1.614118	Accuracy: 20.91%
12	Validation loss: 1.652433	Best loss: 1.614118	Accuracy: 19.08%
13	Validation loss: 1.647032	Best loss: 1.614118	Accuracy: 22.01%
14	Validation loss: 1.862143	Best loss: 1.614118	Accuracy: 18.73%
15	Validation loss: 1.731843	Best loss: 1.614118	Accuracy: 22.01%
16	Validation loss: 1.653818	Best loss: 1.614118	Accuracy: 19.27%
17	Validation loss: 1.705435	Best loss: 1.614118	Accuracy: 19.08%
18	Validation loss: 1.723720	Best loss: 1.614118	Accuracy: 20.91%
19	Validation loss: 1.794170	Best loss: 1.614118	Accuracy: 20.91%
20	Validation loss: 1.628069	Best loss: 1.614118	Accuracy: 20.91%
21	Validation loss: 1.654825	Best loss: 1.614118	Accuracy: 22.01%
22	Validation loss: 1.615196	Best loss: 1.614118	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  42.1s
[CV] n_neurons=30, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.612772	Best loss: 1.612772	Accuracy: 19.31%
1	Validation loss: 1.653072	Best loss: 1.612772	Accuracy: 20.91%
2	Validation loss: 1.620379	Best loss: 1.612772	Accuracy: 22.01%
3	Validation loss: 1.633836	Best loss: 1.612772	Accuracy: 19.08%
4	Validation loss: 1.736362	Best loss: 1.612772	Accuracy: 19.08%
5	Validation loss: 1.683920	Best loss: 1.612772	Accuracy: 20.91%
6	Validation loss: 1.700954	Best loss: 1.612772	Accuracy: 19.27%
7	Validation loss: 1.622791	Best loss: 1.612772	Accuracy: 22.01%
8	Validation loss: 1.736459	Best loss: 1.612772	Accuracy: 18.73%
9	Validation loss: 1.671845	Best loss: 1.612772	Accuracy: 20.91%
10	Validation loss: 1.637939	Best loss: 1.612772	Accuracy: 18.73%
11	Validation loss: 1.684148	Best loss: 1.612772	Accuracy: 18.73%
12	Validation loss: 1.709923	Best loss: 1.612772	Accuracy: 20.91%
13	Validation loss: 1.681301	Best loss: 1.612772	Accuracy: 22.01%
14	Validation loss: 1.856940	Best loss: 1.612772	Accuracy: 18.73%
15	Validation loss: 1.624888	Best loss: 1.612772	Accuracy: 22.01%
16	Validation loss: 1.711975	Best loss: 1.612772	Accuracy: 19.27%
17	Validation loss: 1.661299	Best loss: 1.612772	Accuracy: 20.91%
18	Validation loss: 1.691868	Best loss: 1.612772	Accuracy: 22.01%
19	Validation loss: 1.646308	Best loss: 1.612772	Accuracy: 18.73%
20	Validation loss: 1.638595	Best loss: 1.612772	Accuracy: 20.91%
21	Validation loss: 1.628314	Best loss: 1.612772	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  40.3s
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.478130	Best loss: 0.478130	Accuracy: 89.21%
1	Validation loss: 1.644744	Best loss: 0.478130	Accuracy: 19.27%
2	Validation loss: 1.865886	Best loss: 0.478130	Accuracy: 19.27%
3	Validation loss: 1.784230	Best loss: 0.478130	Accuracy: 19.27%
4	Validation loss: 1.959939	Best loss: 0.478130	Accuracy: 18.73%
5	Validation loss: 1.638594	Best loss: 0.478130	Accuracy: 22.01%
6	Validation loss: 1.707546	Best loss: 0.478130	Accuracy: 18.73%
7	Validation loss: 1.750556	Best loss: 0.478130	Accuracy: 19.08%
8	Validation loss: 1.787924	Best loss: 0.478130	Accuracy: 19.27%
9	Validation loss: 1.882547	Best loss: 0.478130	Accuracy: 20.91%
10	Validation loss: 1.715301	Best loss: 0.478130	Accuracy: 22.01%
11	Validation loss: 1.890315	Best loss: 0.478130	Accuracy: 22.01%
12	Validation loss: 1.646065	Best loss: 0.478130	Accuracy: 19.08%
13	Validation loss: 1.860772	Best loss: 0.478130	Accuracy: 20.91%
14	Validation loss: 1.770219	Best loss: 0.478130	Accuracy: 22.01%
15	Validation loss: 1.684367	Best loss: 0.478130	Accuracy: 19.08%
16	Validation loss: 1.826237	Best loss: 0.478130	Accuracy: 18.73%
17	Validation loss: 1.642998	Best loss: 0.478130	Accuracy: 20.91%
18	Validation loss: 1.685298	Best loss: 0.478130	Accuracy: 19.27%
19	Validation loss: 1.685552	Best loss: 0.478130	Accuracy: 22.01%
20	Validation loss: 1.670048	Best loss: 0.478130	Accuracy: 19.27%
21	Validation loss: 1.815465	Best loss: 0.478130	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  21.6s
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.566814	Best loss: 0.566814	Accuracy: 73.22%
1	Validation loss: 1.647264	Best loss: 0.566814	Accuracy: 20.91%
2	Validation loss: 1.646072	Best loss: 0.566814	Accuracy: 20.91%
3	Validation loss: 1.684925	Best loss: 0.566814	Accuracy: 18.73%
4	Validation loss: 1.679468	Best loss: 0.566814	Accuracy: 20.91%
5	Validation loss: 1.760523	Best loss: 0.566814	Accuracy: 20.91%
6	Validation loss: 1.652630	Best loss: 0.566814	Accuracy: 19.27%
7	Validation loss: 1.786651	Best loss: 0.566814	Accuracy: 18.73%
8	Validation loss: 1.836835	Best loss: 0.566814	Accuracy: 18.73%
9	Validation loss: 1.699086	Best loss: 0.566814	Accuracy: 18.73%
10	Validation loss: 1.673563	Best loss: 0.566814	Accuracy: 19.08%
11	Validation loss: 1.691045	Best loss: 0.566814	Accuracy: 22.01%
12	Validation loss: 1.880471	Best loss: 0.566814	Accuracy: 20.91%
13	Validation loss: 1.670797	Best loss: 0.566814	Accuracy: 19.08%
14	Validation loss: 1.639854	Best loss: 0.566814	Accuracy: 18.73%
15	Validation loss: 1.830887	Best loss: 0.566814	Accuracy: 19.08%
16	Validation loss: 1.666029	Best loss: 0.566814	Accuracy: 18.73%
17	Validation loss: 1.674505	Best loss: 0.566814	Accuracy: 19.27%
18	Validation loss: 1.651120	Best loss: 0.566814	Accuracy: 19.08%
19	Validation loss: 1.697046	Best loss: 0.566814	Accuracy: 19.27%
20	Validation loss: 1.701644	Best loss: 0.566814	Accuracy: 22.01%
21	Validation loss: 1.745296	Best loss: 0.566814	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  21.2s
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.486060	Best loss: 0.486060	Accuracy: 74.47%
1	Validation loss: 1.682003	Best loss: 0.486060	Accuracy: 19.27%
2	Validation loss: 1.790692	Best loss: 0.486060	Accuracy: 20.91%
3	Validation loss: 1.655211	Best loss: 0.486060	Accuracy: 22.01%
4	Validation loss: 1.772332	Best loss: 0.486060	Accuracy: 22.01%
5	Validation loss: 1.795906	Best loss: 0.486060	Accuracy: 20.91%
6	Validation loss: 1.637136	Best loss: 0.486060	Accuracy: 19.27%
7	Validation loss: 1.662999	Best loss: 0.486060	Accuracy: 18.73%
8	Validation loss: 1.715490	Best loss: 0.486060	Accuracy: 19.27%
9	Validation loss: 1.929220	Best loss: 0.486060	Accuracy: 20.91%
10	Validation loss: 1.629040	Best loss: 0.486060	Accuracy: 18.73%
11	Validation loss: 1.765468	Best loss: 0.486060	Accuracy: 18.73%
12	Validation loss: 1.988653	Best loss: 0.486060	Accuracy: 20.91%
13	Validation loss: 1.637999	Best loss: 0.486060	Accuracy: 18.73%
14	Validation loss: 1.735515	Best loss: 0.486060	Accuracy: 19.08%
15	Validation loss: 1.655772	Best loss: 0.486060	Accuracy: 22.01%
16	Validation loss: 1.625487	Best loss: 0.486060	Accuracy: 22.01%
17	Validation loss: 1.677021	Best loss: 0.486060	Accuracy: 19.27%
18	Validation loss: 1.637293	Best loss: 0.486060	Accuracy: 18.73%
19	Validation loss: 1.725691	Best loss: 0.486060	Accuracy: 19.27%
20	Validation loss: 1.721558	Best loss: 0.486060	Accuracy: 22.01%
21	Validation loss: 1.822746	Best loss: 0.486060	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.3, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  21.3s
[CV] n_neurons=120, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.770528	Best loss: 1.770528	Accuracy: 19.27%
1	Validation loss: 1.697124	Best loss: 1.697124	Accuracy: 19.08%
2	Validation loss: 1.722606	Best loss: 1.697124	Accuracy: 19.27%
3	Validation loss: 1.653856	Best loss: 1.653856	Accuracy: 18.73%
4	Validation loss: 1.682928	Best loss: 1.653856	Accuracy: 20.91%
5	Validation loss: 1.672880	Best loss: 1.653856	Accuracy: 20.91%
6	Validation loss: 1.881894	Best loss: 1.653856	Accuracy: 18.73%
7	Validation loss: 1.736941	Best loss: 1.653856	Accuracy: 19.08%
8	Validation loss: 1.875395	Best loss: 1.653856	Accuracy: 19.08%
9	Validation loss: 2.049903	Best loss: 1.653856	Accuracy: 19.27%
10	Validation loss: 1.774765	Best loss: 1.653856	Accuracy: 19.27%
11	Validation loss: 1.932756	Best loss: 1.653856	Accuracy: 22.01%
12	Validation loss: 1.922739	Best loss: 1.653856	Accuracy: 19.08%
13	Validation loss: 1.858206	Best loss: 1.653856	Accuracy: 20.91%
14	Validation loss: 1.843611	Best loss: 1.653856	Accuracy: 18.73%
15	Validation loss: 1.745884	Best loss: 1.653856	Accuracy: 20.91%
16	Validation loss: 2.168352	Best loss: 1.653856	Accuracy: 19.27%
17	Validation loss: 1.719034	Best loss: 1.653856	Accuracy: 20.91%
18	Validation loss: 1.837500	Best loss: 1.653856	Accuracy: 20.91%
19	Validation loss: 1.758132	Best loss: 1.653856	Accuracy: 22.01%
20	Validation loss: 1.790906	Best loss: 1.653856	Accuracy: 19.27%
21	Validation loss: 1.753993	Best loss: 1.653856	Accuracy: 20.91%
22	Validation loss: 1.797515	Best loss: 1.653856	Accuracy: 22.01%
23	Validation loss: 2.354420	Best loss: 1.653856	Accuracy: 22.01%
24	Validation loss: 1.740294	Best loss: 1.653856	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  16.2s
[CV] n_neurons=120, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.631669	Best loss: 1.631669	Accuracy: 22.01%
1	Validation loss: 1.809664	Best loss: 1.631669	Accuracy: 19.08%
2	Validation loss: 1.672989	Best loss: 1.631669	Accuracy: 19.08%
3	Validation loss: 1.675496	Best loss: 1.631669	Accuracy: 22.01%
4	Validation loss: 1.682078	Best loss: 1.631669	Accuracy: 20.91%
5	Validation loss: 1.684994	Best loss: 1.631669	Accuracy: 19.08%
6	Validation loss: 1.695252	Best loss: 1.631669	Accuracy: 19.27%
7	Validation loss: 1.810604	Best loss: 1.631669	Accuracy: 19.08%
8	Validation loss: 1.925478	Best loss: 1.631669	Accuracy: 22.01%
9	Validation loss: 1.832950	Best loss: 1.631669	Accuracy: 18.73%
10	Validation loss: 1.704044	Best loss: 1.631669	Accuracy: 20.91%
11	Validation loss: 1.721506	Best loss: 1.631669	Accuracy: 19.08%
12	Validation loss: 1.978302	Best loss: 1.631669	Accuracy: 20.91%
13	Validation loss: 1.651609	Best loss: 1.631669	Accuracy: 22.01%
14	Validation loss: 2.016912	Best loss: 1.631669	Accuracy: 19.08%
15	Validation loss: 1.802285	Best loss: 1.631669	Accuracy: 18.73%
16	Validation loss: 1.823462	Best loss: 1.631669	Accuracy: 19.08%
17	Validation loss: 1.768519	Best loss: 1.631669	Accuracy: 19.27%
18	Validation loss: 2.018564	Best loss: 1.631669	Accuracy: 20.91%
19	Validation loss: 1.886739	Best loss: 1.631669	Accuracy: 22.01%
20	Validation loss: 1.850900	Best loss: 1.631669	Accuracy: 19.27%
21	Validation loss: 1.788279	Best loss: 1.631669	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  13.9s
[CV] n_neurons=120, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.647526	Best loss: 1.647526	Accuracy: 22.01%
1	Validation loss: 1.674509	Best loss: 1.647526	Accuracy: 19.27%
2	Validation loss: 1.622443	Best loss: 1.622443	Accuracy: 22.01%
3	Validation loss: 1.712927	Best loss: 1.622443	Accuracy: 19.27%
4	Validation loss: 2.036093	Best loss: 1.622443	Accuracy: 19.27%
5	Validation loss: 1.734639	Best loss: 1.622443	Accuracy: 20.91%
6	Validation loss: 2.165146	Best loss: 1.622443	Accuracy: 20.91%
7	Validation loss: 1.722512	Best loss: 1.622443	Accuracy: 18.73%
8	Validation loss: 1.874207	Best loss: 1.622443	Accuracy: 22.01%
9	Validation loss: 2.237444	Best loss: 1.622443	Accuracy: 19.08%
10	Validation loss: 1.942981	Best loss: 1.622443	Accuracy: 20.91%
11	Validation loss: 2.027216	Best loss: 1.622443	Accuracy: 18.73%
12	Validation loss: 1.769670	Best loss: 1.622443	Accuracy: 19.08%
13	Validation loss: 1.714112	Best loss: 1.622443	Accuracy: 22.01%
14	Validation loss: 2.212814	Best loss: 1.622443	Accuracy: 19.27%
15	Validation loss: 1.650069	Best loss: 1.622443	Accuracy: 22.01%
16	Validation loss: 1.963560	Best loss: 1.622443	Accuracy: 18.73%
17	Validation loss: 1.747859	Best loss: 1.622443	Accuracy: 22.01%
18	Validation loss: 1.687710	Best loss: 1.622443	Accuracy: 22.01%
19	Validation loss: 1.917205	Best loss: 1.622443	Accuracy: 18.73%
20	Validation loss: 1.771739	Best loss: 1.622443	Accuracy: 22.01%
21	Validation loss: 1.956576	Best loss: 1.622443	Accuracy: 22.01%
22	Validation loss: 2.293172	Best loss: 1.622443	Accuracy: 19.27%
23	Validation loss: 1.746112	Best loss: 1.622443	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.1, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  15.2s
[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.375601	Best loss: 0.375601	Accuracy: 78.30%
1	Validation loss: 0.323413	Best loss: 0.323413	Accuracy: 87.80%
2	Validation loss: 0.163240	Best loss: 0.163240	Accuracy: 96.01%
3	Validation loss: 0.134133	Best loss: 0.134133	Accuracy: 96.40%
4	Validation loss: 0.114148	Best loss: 0.114148	Accuracy: 96.83%
5	Validation loss: 0.125443	Best loss: 0.114148	Accuracy: 96.72%
6	Validation loss: 0.112145	Best loss: 0.112145	Accuracy: 97.22%
7	Validation loss: 0.114397	Best loss: 0.112145	Accuracy: 96.72%
8	Validation loss: 0.117919	Best loss: 0.112145	Accuracy: 96.95%
9	Validation loss: 0.109655	Best loss: 0.109655	Accuracy: 96.87%
10	Validation loss: 0.113111	Best loss: 0.109655	Accuracy: 96.91%
11	Validation loss: 0.114047	Best loss: 0.109655	Accuracy: 96.95%
12	Validation loss: 0.103676	Best loss: 0.103676	Accuracy: 97.15%
13	Validation loss: 0.109979	Best loss: 0.103676	Accuracy: 97.03%
14	Validation loss: 0.095206	Best loss: 0.095206	Accuracy: 97.58%
15	Validation loss: 0.106702	Best loss: 0.095206	Accuracy: 97.38%
16	Validation loss: 0.104571	Best loss: 0.095206	Accuracy: 97.34%
17	Validation loss: 0.106767	Best loss: 0.095206	Accuracy: 97.19%
18	Validation loss: 0.101568	Best loss: 0.095206	Accuracy: 97.54%
19	Validation loss: 0.104509	Best loss: 0.095206	Accuracy: 97.26%
20	Validation loss: 0.098452	Best loss: 0.095206	Accuracy: 97.50%
21	Validation loss: 0.102426	Best loss: 0.095206	Accuracy: 97.34%
22	Validation loss: 0.100660	Best loss: 0.095206	Accuracy: 97.30%
23	Validation loss: 0.107144	Best loss: 0.095206	Accuracy: 97.42%
24	Validation loss: 0.101534	Best loss: 0.095206	Accuracy: 97.50%
25	Validation loss: 0.112121	Best loss: 0.095206	Accuracy: 97.42%
26	Validation loss: 0.101901	Best loss: 0.095206	Accuracy: 97.26%
27	Validation loss: 0.103703	Best loss: 0.095206	Accuracy: 97.22%
28	Validation loss: 0.096405	Best loss: 0.095206	Accuracy: 97.42%
29	Validation loss: 0.101370	Best loss: 0.095206	Accuracy: 97.38%
30	Validation loss: 0.109265	Best loss: 0.095206	Accuracy: 97.26%
31	Validation loss: 0.097887	Best loss: 0.095206	Accuracy: 97.46%
32	Validation loss: 0.114785	Best loss: 0.095206	Accuracy: 97.19%
33	Validation loss: 0.104386	Best loss: 0.095206	Accuracy: 97.38%
34	Validation loss: 0.102402	Best loss: 0.095206	Accuracy: 97.42%
35	Validation loss: 0.111743	Best loss: 0.095206	Accuracy: 97.34%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   5.1s
[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.362937	Best loss: 0.362937	Accuracy: 79.71%
1	Validation loss: 0.332137	Best loss: 0.332137	Accuracy: 86.20%
2	Validation loss: 0.157646	Best loss: 0.157646	Accuracy: 95.74%
3	Validation loss: 0.138940	Best loss: 0.138940	Accuracy: 96.29%
4	Validation loss: 0.126688	Best loss: 0.126688	Accuracy: 96.79%
5	Validation loss: 0.116495	Best loss: 0.116495	Accuracy: 96.95%
6	Validation loss: 0.114193	Best loss: 0.114193	Accuracy: 96.79%
7	Validation loss: 0.109777	Best loss: 0.109777	Accuracy: 96.87%
8	Validation loss: 0.111446	Best loss: 0.109777	Accuracy: 96.83%
9	Validation loss: 0.105754	Best loss: 0.105754	Accuracy: 97.03%
10	Validation loss: 0.108094	Best loss: 0.105754	Accuracy: 97.03%
11	Validation loss: 0.109655	Best loss: 0.105754	Accuracy: 97.07%
12	Validation loss: 0.131084	Best loss: 0.105754	Accuracy: 96.40%
13	Validation loss: 0.105905	Best loss: 0.105754	Accuracy: 97.15%
14	Validation loss: 0.115661	Best loss: 0.105754	Accuracy: 96.79%
15	Validation loss: 0.115982	Best loss: 0.105754	Accuracy: 96.91%
16	Validation loss: 0.115027	Best loss: 0.105754	Accuracy: 96.87%
17	Validation loss: 0.116789	Best loss: 0.105754	Accuracy: 96.72%
18	Validation loss: 0.120048	Best loss: 0.105754	Accuracy: 96.91%
19	Validation loss: 0.115515	Best loss: 0.105754	Accuracy: 96.79%
20	Validation loss: 0.115355	Best loss: 0.105754	Accuracy: 96.87%
21	Validation loss: 0.100840	Best loss: 0.100840	Accuracy: 97.15%
22	Validation loss: 0.103032	Best loss: 0.100840	Accuracy: 97.11%
23	Validation loss: 0.108328	Best loss: 0.100840	Accuracy: 97.15%
24	Validation loss: 0.105553	Best loss: 0.100840	Accuracy: 96.87%
25	Validation loss: 0.108381	Best loss: 0.100840	Accuracy: 96.87%
26	Validation loss: 0.109395	Best loss: 0.100840	Accuracy: 97.07%
27	Validation loss: 0.115862	Best loss: 0.100840	Accuracy: 96.87%
28	Validation loss: 0.108951	Best loss: 0.100840	Accuracy: 97.19%
29	Validation loss: 0.112354	Best loss: 0.100840	Accuracy: 96.95%
30	Validation loss: 0.111971	Best loss: 0.100840	Accuracy: 96.95%
31	Validation loss: 0.098874	Best loss: 0.098874	Accuracy: 97.22%
32	Validation loss: 0.115651	Best loss: 0.098874	Accuracy: 96.72%
33	Validation loss: 0.112134	Best loss: 0.098874	Accuracy: 97.03%
34	Validation loss: 0.098635	Best loss: 0.098635	Accuracy: 97.30%
35	Validation loss: 0.120389	Best loss: 0.098635	Accuracy: 96.83%
36	Validation loss: 0.109474	Best loss: 0.098635	Accuracy: 96.72%
37	Validation loss: 0.095682	Best loss: 0.095682	Accuracy: 97.26%
38	Validation loss: 0.101420	Best loss: 0.095682	Accuracy: 97.03%
39	Validation loss: 0.109149	Best loss: 0.095682	Accuracy: 96.95%
40	Validation loss: 0.110531	Best loss: 0.095682	Accuracy: 96.95%
41	Validation loss: 0.099785	Best loss: 0.095682	Accuracy: 97.11%
42	Validation loss: 0.102421	Best loss: 0.095682	Accuracy: 97.15%
43	Validation loss: 0.111203	Best loss: 0.095682	Accuracy: 96.68%
44	Validation loss: 0.108417	Best loss: 0.095682	Accuracy: 97.11%
45	Validation loss: 0.106311	Best loss: 0.095682	Accuracy: 97.22%
46	Validation loss: 0.110796	Best loss: 0.095682	Accuracy: 96.76%
47	Validation loss: 0.105860	Best loss: 0.095682	Accuracy: 96.91%
48	Validation loss: 0.098866	Best loss: 0.095682	Accuracy: 96.99%
49	Validation loss: 0.107024	Best loss: 0.095682	Accuracy: 96.76%
50	Validation loss: 0.106404	Best loss: 0.095682	Accuracy: 97.38%
51	Validation loss: 0.098351	Best loss: 0.095682	Accuracy: 97.42%
52	Validation loss: 0.109375	Best loss: 0.095682	Accuracy: 97.19%
53	Validation loss: 0.106615	Best loss: 0.095682	Accuracy: 97.07%
54	Validation loss: 0.105417	Best loss: 0.095682	Accuracy: 97.26%
55	Validation loss: 0.106302	Best loss: 0.095682	Accuracy: 97.19%
56	Validation loss: 0.100619	Best loss: 0.095682	Accuracy: 97.38%
57	Validation loss: 0.121476	Best loss: 0.095682	Accuracy: 96.95%
58	Validation loss: 0.103328	Best loss: 0.095682	Accuracy: 97.19%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   7.4s
[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.359997	Best loss: 0.359997	Accuracy: 79.91%
1	Validation loss: 0.173868	Best loss: 0.173868	Accuracy: 95.23%
2	Validation loss: 0.149724	Best loss: 0.149724	Accuracy: 95.93%
3	Validation loss: 0.126375	Best loss: 0.126375	Accuracy: 96.36%
4	Validation loss: 0.128071	Best loss: 0.126375	Accuracy: 96.56%
5	Validation loss: 0.120905	Best loss: 0.120905	Accuracy: 96.76%
6	Validation loss: 0.108867	Best loss: 0.108867	Accuracy: 96.79%
7	Validation loss: 0.112184	Best loss: 0.108867	Accuracy: 96.99%
8	Validation loss: 0.108699	Best loss: 0.108699	Accuracy: 97.34%
9	Validation loss: 0.099295	Best loss: 0.099295	Accuracy: 97.19%
10	Validation loss: 0.103084	Best loss: 0.099295	Accuracy: 97.15%
11	Validation loss: 0.105503	Best loss: 0.099295	Accuracy: 97.15%
12	Validation loss: 0.111888	Best loss: 0.099295	Accuracy: 97.22%
13	Validation loss: 0.108699	Best loss: 0.099295	Accuracy: 96.95%
14	Validation loss: 0.104082	Best loss: 0.099295	Accuracy: 97.30%
15	Validation loss: 0.101963	Best loss: 0.099295	Accuracy: 97.26%
16	Validation loss: 0.107879	Best loss: 0.099295	Accuracy: 96.87%
17	Validation loss: 0.111019	Best loss: 0.099295	Accuracy: 97.03%
18	Validation loss: 0.109653	Best loss: 0.099295	Accuracy: 97.07%
19	Validation loss: 0.109506	Best loss: 0.099295	Accuracy: 97.42%
20	Validation loss: 0.099505	Best loss: 0.099295	Accuracy: 97.62%
21	Validation loss: 0.112437	Best loss: 0.099295	Accuracy: 96.83%
22	Validation loss: 0.108212	Best loss: 0.099295	Accuracy: 97.26%
23	Validation loss: 0.094104	Best loss: 0.094104	Accuracy: 97.26%
24	Validation loss: 0.109633	Best loss: 0.094104	Accuracy: 97.03%
25	Validation loss: 0.108968	Best loss: 0.094104	Accuracy: 97.11%
26	Validation loss: 0.109553	Best loss: 0.094104	Accuracy: 96.83%
27	Validation loss: 0.110157	Best loss: 0.094104	Accuracy: 97.30%
28	Validation loss: 0.095615	Best loss: 0.094104	Accuracy: 97.54%
29	Validation loss: 0.107991	Best loss: 0.094104	Accuracy: 97.19%
30	Validation loss: 0.098743	Best loss: 0.094104	Accuracy: 97.50%
31	Validation loss: 0.099235	Best loss: 0.094104	Accuracy: 97.46%
32	Validation loss: 0.110326	Best loss: 0.094104	Accuracy: 97.11%
33	Validation loss: 0.099625	Best loss: 0.094104	Accuracy: 97.34%
34	Validation loss: 0.107015	Best loss: 0.094104	Accuracy: 97.11%
35	Validation loss: 0.112964	Best loss: 0.094104	Accuracy: 97.07%
36	Validation loss: 0.110731	Best loss: 0.094104	Accuracy: 96.91%
37	Validation loss: 0.112162	Best loss: 0.094104	Accuracy: 96.95%
38	Validation loss: 0.105121	Best loss: 0.094104	Accuracy: 97.11%
39	Validation loss: 0.096973	Best loss: 0.094104	Accuracy: 97.50%
40	Validation loss: 0.100220	Best loss: 0.094104	Accuracy: 97.46%
41	Validation loss: 0.100193	Best loss: 0.094104	Accuracy: 97.30%
42	Validation loss: 0.104770	Best loss: 0.094104	Accuracy: 97.15%
43	Validation loss: 0.116870	Best loss: 0.094104	Accuracy: 96.99%
44	Validation loss: 0.098686	Best loss: 0.094104	Accuracy: 97.15%
Early stopping!
[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=   5.9s
[CV] n_neurons=160, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 5.259931	Best loss: 5.259931	Accuracy: 92.85%
1	Validation loss: 2.988282	Best loss: 2.988282	Accuracy: 94.21%
2	Validation loss: 1.003097	Best loss: 1.003097	Accuracy: 95.19%
3	Validation loss: 3568771.000000	Best loss: 1.003097	Accuracy: 43.90%
4	Validation loss: 571217.875000	Best loss: 1.003097	Accuracy: 65.36%
5	Validation loss: 468551.750000	Best loss: 1.003097	Accuracy: 55.28%
6	Validation loss: 255846.984375	Best loss: 1.003097	Accuracy: 67.47%
7	Validation loss: 214108.875000	Best loss: 1.003097	Accuracy: 58.48%
8	Validation loss: 243744.828125	Best loss: 1.003097	Accuracy: 61.26%
9	Validation loss: 118960.851562	Best loss: 1.003097	Accuracy: 71.81%
10	Validation loss: 192117.781250	Best loss: 1.003097	Accuracy: 57.39%
11	Validation loss: 90351.750000	Best loss: 1.003097	Accuracy: 81.67%
12	Validation loss: 43571.105469	Best loss: 1.003097	Accuracy: 90.66%
13	Validation loss: 36322.027344	Best loss: 1.003097	Accuracy: 93.47%
14	Validation loss: 145877.296875	Best loss: 1.003097	Accuracy: 78.66%
15	Validation loss: 41463.449219	Best loss: 1.003097	Accuracy: 94.29%
16	Validation loss: 47806.015625	Best loss: 1.003097	Accuracy: 94.61%
17	Validation loss: 36996.414062	Best loss: 1.003097	Accuracy: 95.27%
18	Validation loss: 21177.433594	Best loss: 1.003097	Accuracy: 94.76%
19	Validation loss: 1945038336.000000	Best loss: 1.003097	Accuracy: 93.32%
20	Validation loss: 11886858.000000	Best loss: 1.003097	Accuracy: 95.39%
21	Validation loss: 8249648.000000	Best loss: 1.003097	Accuracy: 94.33%
22	Validation loss: 5254323.000000	Best loss: 1.003097	Accuracy: 93.86%
23	Validation loss: 2893384.500000	Best loss: 1.003097	Accuracy: 96.48%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  25.4s
[CV] n_neurons=160, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 6.160419	Best loss: 6.160419	Accuracy: 83.23%
1	Validation loss: 10.902327	Best loss: 6.160419	Accuracy: 91.48%
2	Validation loss: 132691.796875	Best loss: 6.160419	Accuracy: 89.13%
3	Validation loss: 118867.625000	Best loss: 6.160419	Accuracy: 74.43%
4	Validation loss: 45814.691406	Best loss: 6.160419	Accuracy: 89.29%
5	Validation loss: 33703.527344	Best loss: 6.160419	Accuracy: 91.16%
6	Validation loss: 25191.644531	Best loss: 6.160419	Accuracy: 92.03%
7	Validation loss: 22420.878906	Best loss: 6.160419	Accuracy: 91.71%
8	Validation loss: 15922.872070	Best loss: 6.160419	Accuracy: 93.24%
9	Validation loss: 23509.960938	Best loss: 6.160419	Accuracy: 91.95%
10	Validation loss: 28166.875000	Best loss: 6.160419	Accuracy: 84.52%
11	Validation loss: 10877.873047	Best loss: 6.160419	Accuracy: 93.86%
12	Validation loss: 12524.200195	Best loss: 6.160419	Accuracy: 93.63%
13	Validation loss: 11062.577148	Best loss: 6.160419	Accuracy: 94.45%
14	Validation loss: 9216.127930	Best loss: 6.160419	Accuracy: 94.61%
15	Validation loss: 10952.794922	Best loss: 6.160419	Accuracy: 94.80%
16	Validation loss: 8607.067383	Best loss: 6.160419	Accuracy: 95.39%
17	Validation loss: 10180.590820	Best loss: 6.160419	Accuracy: 95.74%
18	Validation loss: 8156.129883	Best loss: 6.160419	Accuracy: 96.33%
19	Validation loss: 9018673.000000	Best loss: 6.160419	Accuracy: 95.70%
20	Validation loss: 4503273.000000	Best loss: 6.160419	Accuracy: 95.27%
21	Validation loss: 2712925.750000	Best loss: 6.160419	Accuracy: 95.82%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  23.1s
[CV] n_neurons=160, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 18.046289	Best loss: 18.046289	Accuracy: 84.95%
1	Validation loss: 4.225152	Best loss: 4.225152	Accuracy: 91.83%
2	Validation loss: 7.226954	Best loss: 4.225152	Accuracy: 88.70%
3	Validation loss: 716169.937500	Best loss: 4.225152	Accuracy: 49.96%
4	Validation loss: 324579.625000	Best loss: 4.225152	Accuracy: 30.30%
5	Validation loss: 323984.531250	Best loss: 4.225152	Accuracy: 37.57%
6	Validation loss: 333615.625000	Best loss: 4.225152	Accuracy: 29.95%
7	Validation loss: 93797.156250	Best loss: 4.225152	Accuracy: 67.28%
8	Validation loss: 115268.226562	Best loss: 4.225152	Accuracy: 49.18%
9	Validation loss: 23102.777344	Best loss: 4.225152	Accuracy: 84.48%
10	Validation loss: 14935.549805	Best loss: 4.225152	Accuracy: 79.71%
11	Validation loss: 18896.572266	Best loss: 4.225152	Accuracy: 73.30%
12	Validation loss: 23926.097656	Best loss: 4.225152	Accuracy: 84.68%
13	Validation loss: 6561.737305	Best loss: 4.225152	Accuracy: 91.40%
14	Validation loss: 4608347.500000	Best loss: 4.225152	Accuracy: 68.14%
15	Validation loss: 362535.125000	Best loss: 4.225152	Accuracy: 73.53%
16	Validation loss: 107796.367188	Best loss: 4.225152	Accuracy: 90.62%
17	Validation loss: 70569.953125	Best loss: 4.225152	Accuracy: 91.59%
18	Validation loss: 98490.335938	Best loss: 4.225152	Accuracy: 89.01%
19	Validation loss: 97446.992188	Best loss: 4.225152	Accuracy: 90.66%
20	Validation loss: 44264.167969	Best loss: 4.225152	Accuracy: 92.06%
21	Validation loss: 35086.140625	Best loss: 4.225152	Accuracy: 93.51%
22	Validation loss: 44092.679688	Best loss: 4.225152	Accuracy: 93.32%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  23.7s
[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 1.172974	Best loss: 1.172974	Accuracy: 50.70%
1	Validation loss: 1.118301	Best loss: 1.118301	Accuracy: 49.61%
2	Validation loss: 1.334205	Best loss: 1.118301	Accuracy: 47.77%
3	Validation loss: 1.508044	Best loss: 1.118301	Accuracy: 32.80%
4	Validation loss: 1.573850	Best loss: 1.118301	Accuracy: 26.62%
5	Validation loss: 1.378019	Best loss: 1.118301	Accuracy: 41.24%
6	Validation loss: 1.485990	Best loss: 1.118301	Accuracy: 30.84%
7	Validation loss: 1.380571	Best loss: 1.118301	Accuracy: 36.24%
8	Validation loss: 1.368096	Best loss: 1.118301	Accuracy: 39.13%
9	Validation loss: 1.312520	Best loss: 1.118301	Accuracy: 47.62%
10	Validation loss: 1.582551	Best loss: 1.118301	Accuracy: 23.89%
11	Validation loss: 1.491246	Best loss: 1.118301	Accuracy: 34.95%
12	Validation loss: 2.490223	Best loss: 1.118301	Accuracy: 24.98%
13	Validation loss: 1.625418	Best loss: 1.118301	Accuracy: 32.92%
14	Validation loss: 2.441607	Best loss: 1.118301	Accuracy: 21.62%
15	Validation loss: 2.825254	Best loss: 1.118301	Accuracy: 24.98%
16	Validation loss: 2.269755	Best loss: 1.118301	Accuracy: 23.53%
17	Validation loss: 2.610322	Best loss: 1.118301	Accuracy: 30.34%
18	Validation loss: 7.865357	Best loss: 1.118301	Accuracy: 27.68%
19	Validation loss: 6.346726	Best loss: 1.118301	Accuracy: 13.64%
20	Validation loss: 8.507144	Best loss: 1.118301	Accuracy: 36.79%
21	Validation loss: 16.788525	Best loss: 1.118301	Accuracy: 20.37%
22	Validation loss: 10.223419	Best loss: 1.118301	Accuracy: 24.43%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  20.3s
[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 1.099951	Best loss: 1.099951	Accuracy: 46.01%
1	Validation loss: 0.855722	Best loss: 0.855722	Accuracy: 55.32%
2	Validation loss: 1.009684	Best loss: 0.855722	Accuracy: 53.21%
3	Validation loss: 1.220662	Best loss: 0.855722	Accuracy: 41.05%
4	Validation loss: 1.383702	Best loss: 0.855722	Accuracy: 40.93%
5	Validation loss: 1.719968	Best loss: 0.855722	Accuracy: 26.27%
6	Validation loss: 9.002348	Best loss: 0.855722	Accuracy: 22.63%
7	Validation loss: 2.355790	Best loss: 0.855722	Accuracy: 37.26%
8	Validation loss: 6.992247	Best loss: 0.855722	Accuracy: 31.20%
9	Validation loss: 3.548500	Best loss: 0.855722	Accuracy: 31.94%
10	Validation loss: 5.530536	Best loss: 0.855722	Accuracy: 19.70%
11	Validation loss: 4.479380	Best loss: 0.855722	Accuracy: 23.61%
12	Validation loss: 17.611937	Best loss: 0.855722	Accuracy: 19.08%
13	Validation loss: 20.699547	Best loss: 0.855722	Accuracy: 20.91%
14	Validation loss: 8.241798	Best loss: 0.855722	Accuracy: 23.73%
15	Validation loss: 49.104179	Best loss: 0.855722	Accuracy: 20.88%
16	Validation loss: 7.687710	Best loss: 0.855722	Accuracy: 34.44%
17	Validation loss: 5.722558	Best loss: 0.855722	Accuracy: 24.67%
18	Validation loss: 19.501608	Best loss: 0.855722	Accuracy: 24.63%
19	Validation loss: 12.221816	Best loss: 0.855722	Accuracy: 23.22%
20	Validation loss: 15.023331	Best loss: 0.855722	Accuracy: 22.99%
21	Validation loss: 15.756519	Best loss: 0.855722	Accuracy: 22.79%
22	Validation loss: 23.626083	Best loss: 0.855722	Accuracy: 34.52%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  19.5s
[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 1.563530	Best loss: 1.563530	Accuracy: 29.79%
1	Validation loss: 1.185789	Best loss: 1.185789	Accuracy: 47.58%
2	Validation loss: 1.486643	Best loss: 1.185789	Accuracy: 33.89%
3	Validation loss: 1.425628	Best loss: 1.185789	Accuracy: 35.22%
4	Validation loss: 1.477763	Best loss: 1.185789	Accuracy: 29.05%
5	Validation loss: 1.605227	Best loss: 1.185789	Accuracy: 23.06%
6	Validation loss: 1.580876	Best loss: 1.185789	Accuracy: 23.30%
7	Validation loss: 1.439023	Best loss: 1.185789	Accuracy: 34.21%
8	Validation loss: 1.538341	Best loss: 1.185789	Accuracy: 39.09%
9	Validation loss: 1.553120	Best loss: 1.185789	Accuracy: 34.48%
10	Validation loss: 2.159517	Best loss: 1.185789	Accuracy: 13.41%
11	Validation loss: 4.479972	Best loss: 1.185789	Accuracy: 24.51%
12	Validation loss: 2.783789	Best loss: 1.185789	Accuracy: 19.16%
13	Validation loss: 1.793587	Best loss: 1.185789	Accuracy: 26.94%
14	Validation loss: 1.618826	Best loss: 1.185789	Accuracy: 37.10%
15	Validation loss: 1.863891	Best loss: 1.185789	Accuracy: 30.34%
16	Validation loss: 5.303472	Best loss: 1.185789	Accuracy: 18.73%
17	Validation loss: 8.884335	Best loss: 1.185789	Accuracy: 12.98%
18	Validation loss: 66.765144	Best loss: 1.185789	Accuracy: 22.01%
19	Validation loss: 11.861637	Best loss: 1.185789	Accuracy: 20.05%
20	Validation loss: 19.033516	Best loss: 1.185789	Accuracy: 20.41%
21	Validation loss: 22.861588	Best loss: 1.185789	Accuracy: 22.01%
22	Validation loss: 11.589438	Best loss: 1.185789	Accuracy: 21.31%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  19.5s
[CV] n_neurons=100, learning_rate=0.01, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.550716	Best loss: 0.550716	Accuracy: 78.42%
1	Validation loss: 0.727282	Best loss: 0.550716	Accuracy: 65.52%
2	Validation loss: 0.670525	Best loss: 0.550716	Accuracy: 69.82%
3	Validation loss: 1.110482	Best loss: 0.550716	Accuracy: 47.77%
4	Validation loss: 1.003404	Best loss: 0.550716	Accuracy: 48.20%
5	Validation loss: 1.249739	Best loss: 0.550716	Accuracy: 43.47%
6	Validation loss: 1.084169	Best loss: 0.550716	Accuracy: 54.57%
7	Validation loss: 1.163335	Best loss: 0.550716	Accuracy: 49.26%
8	Validation loss: 1.215873	Best loss: 0.550716	Accuracy: 47.11%
9	Validation loss: 1.267380	Best loss: 0.550716	Accuracy: 52.03%
10	Validation loss: 1.195934	Best loss: 0.550716	Accuracy: 45.19%
11	Validation loss: 1.269349	Best loss: 0.550716	Accuracy: 48.32%
12	Validation loss: 1.027071	Best loss: 0.550716	Accuracy: 52.81%
13	Validation loss: 1.168802	Best loss: 0.550716	Accuracy: 46.25%
14	Validation loss: 1.048636	Best loss: 0.550716	Accuracy: 51.21%
15	Validation loss: 1.055285	Best loss: 0.550716	Accuracy: 50.94%
16	Validation loss: 2.394385	Best loss: 0.550716	Accuracy: 43.20%
17	Validation loss: 2.550143	Best loss: 0.550716	Accuracy: 41.01%
18	Validation loss: 3.154441	Best loss: 0.550716	Accuracy: 22.48%
19	Validation loss: 4.956509	Best loss: 0.550716	Accuracy: 24.43%
20	Validation loss: 2.181752	Best loss: 0.550716	Accuracy: 36.32%
21	Validation loss: 2.170082	Best loss: 0.550716	Accuracy: 25.02%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  58.4s
[CV] n_neurons=100, learning_rate=0.01, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.599331	Best loss: 0.599331	Accuracy: 74.24%
1	Validation loss: 0.420247	Best loss: 0.420247	Accuracy: 92.06%
2	Validation loss: 0.440169	Best loss: 0.420247	Accuracy: 86.63%
3	Validation loss: 0.439654	Best loss: 0.420247	Accuracy: 88.31%
4	Validation loss: 0.526961	Best loss: 0.420247	Accuracy: 85.03%
5	Validation loss: 1.132521	Best loss: 0.420247	Accuracy: 47.19%
6	Validation loss: 1.278799	Best loss: 0.420247	Accuracy: 44.45%
7	Validation loss: 0.817430	Best loss: 0.420247	Accuracy: 70.56%
8	Validation loss: 0.826321	Best loss: 0.420247	Accuracy: 71.31%
9	Validation loss: 1.462781	Best loss: 0.420247	Accuracy: 26.90%
10	Validation loss: 1.146846	Best loss: 0.420247	Accuracy: 54.03%
11	Validation loss: 7.179943	Best loss: 0.420247	Accuracy: 23.42%
12	Validation loss: 1.236811	Best loss: 0.420247	Accuracy: 43.82%
13	Validation loss: 1.198098	Best loss: 0.420247	Accuracy: 48.28%
14	Validation loss: 1.174275	Best loss: 0.420247	Accuracy: 50.08%
15	Validation loss: 4.452858	Best loss: 0.420247	Accuracy: 30.88%
16	Validation loss: 1.373764	Best loss: 0.420247	Accuracy: 45.15%
17	Validation loss: 1.571638	Best loss: 0.420247	Accuracy: 37.02%
18	Validation loss: 1.501929	Best loss: 0.420247	Accuracy: 35.18%
19	Validation loss: 1.349779	Best loss: 0.420247	Accuracy: 33.89%
20	Validation loss: 1.743510	Best loss: 0.420247	Accuracy: 38.82%
21	Validation loss: 8.284121	Best loss: 0.420247	Accuracy: 32.17%
22	Validation loss: 3.869760	Best loss: 0.420247	Accuracy: 31.08%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.0min
[CV] n_neurons=100, learning_rate=0.01, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.477335	Best loss: 0.477335	Accuracy: 87.88%
1	Validation loss: 0.560740	Best loss: 0.477335	Accuracy: 77.87%
2	Validation loss: 0.691022	Best loss: 0.477335	Accuracy: 71.50%
3	Validation loss: 0.848261	Best loss: 0.477335	Accuracy: 65.21%
4	Validation loss: 0.604467	Best loss: 0.477335	Accuracy: 78.85%
5	Validation loss: 0.816751	Best loss: 0.477335	Accuracy: 70.80%
6	Validation loss: 0.639773	Best loss: 0.477335	Accuracy: 79.83%
7	Validation loss: 1.046253	Best loss: 0.477335	Accuracy: 57.97%
8	Validation loss: 0.866234	Best loss: 0.477335	Accuracy: 69.35%
9	Validation loss: 0.962440	Best loss: 0.477335	Accuracy: 63.68%
10	Validation loss: 0.992167	Best loss: 0.477335	Accuracy: 56.25%
11	Validation loss: 0.988258	Best loss: 0.477335	Accuracy: 52.54%
12	Validation loss: 1.083424	Best loss: 0.477335	Accuracy: 50.47%
13	Validation loss: 1.115683	Best loss: 0.477335	Accuracy: 50.16%
14	Validation loss: 0.984964	Best loss: 0.477335	Accuracy: 53.75%
15	Validation loss: 0.926846	Best loss: 0.477335	Accuracy: 55.82%
16	Validation loss: 1.028140	Best loss: 0.477335	Accuracy: 52.31%
17	Validation loss: 1.501449	Best loss: 0.477335	Accuracy: 53.67%
18	Validation loss: 1.136520	Best loss: 0.477335	Accuracy: 51.60%
19	Validation loss: 1.405691	Best loss: 0.477335	Accuracy: 51.49%
20	Validation loss: 2.400941	Best loss: 0.477335	Accuracy: 31.47%
21	Validation loss: 1.327338	Best loss: 0.477335	Accuracy: 40.97%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, dropout_rate=0.4, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.0min
[CV] n_neurons=90, learning_rate=0.05, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.349212	Best loss: 0.349212	Accuracy: 92.42%
1	Validation loss: 0.152024	Best loss: 0.152024	Accuracy: 95.07%
2	Validation loss: 0.152099	Best loss: 0.152024	Accuracy: 95.86%
3	Validation loss: 0.125327	Best loss: 0.125327	Accuracy: 96.17%
4	Validation loss: 0.141351	Best loss: 0.125327	Accuracy: 95.97%
5	Validation loss: 0.134390	Best loss: 0.125327	Accuracy: 97.03%
6	Validation loss: 0.127108	Best loss: 0.125327	Accuracy: 96.36%
7	Validation loss: 0.123122	Best loss: 0.123122	Accuracy: 96.48%
8	Validation loss: 0.324462	Best loss: 0.123122	Accuracy: 95.19%
9	Validation loss: 0.833671	Best loss: 0.123122	Accuracy: 68.02%
10	Validation loss: 2.944423	Best loss: 0.123122	Accuracy: 31.55%
11	Validation loss: 2.233417	Best loss: 0.123122	Accuracy: 42.53%
12	Validation loss: 1.100613	Best loss: 0.123122	Accuracy: 51.13%
13	Validation loss: 0.913818	Best loss: 0.123122	Accuracy: 59.62%
14	Validation loss: 0.853648	Best loss: 0.123122	Accuracy: 70.21%
15	Validation loss: 0.854247	Best loss: 0.123122	Accuracy: 62.28%
16	Validation loss: 0.781967	Best loss: 0.123122	Accuracy: 69.90%
17	Validation loss: 0.876521	Best loss: 0.123122	Accuracy: 67.67%
18	Validation loss: 1.075017	Best loss: 0.123122	Accuracy: 50.55%
19	Validation loss: 0.829222	Best loss: 0.123122	Accuracy: 76.47%
20	Validation loss: 0.796094	Best loss: 0.123122	Accuracy: 69.43%
21	Validation loss: 0.712221	Best loss: 0.123122	Accuracy: 79.52%
22	Validation loss: 0.820939	Best loss: 0.123122	Accuracy: 70.76%
23	Validation loss: 1.206309	Best loss: 0.123122	Accuracy: 49.96%
24	Validation loss: 1.260739	Best loss: 0.123122	Accuracy: 46.21%
25	Validation loss: 1.521918	Best loss: 0.123122	Accuracy: 33.46%
26	Validation loss: 1.116938	Best loss: 0.123122	Accuracy: 55.04%
27	Validation loss: 1.221864	Best loss: 0.123122	Accuracy: 42.69%
28	Validation loss: 1.396751	Best loss: 0.123122	Accuracy: 42.38%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  10.3s
[CV] n_neurons=90, learning_rate=0.05, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.238640	Best loss: 0.238640	Accuracy: 92.18%
1	Validation loss: 0.173422	Best loss: 0.173422	Accuracy: 94.72%
2	Validation loss: 0.130750	Best loss: 0.130750	Accuracy: 96.21%
3	Validation loss: 0.129563	Best loss: 0.129563	Accuracy: 96.60%
4	Validation loss: 0.134878	Best loss: 0.129563	Accuracy: 96.68%
5	Validation loss: 0.190662	Best loss: 0.129563	Accuracy: 94.61%
6	Validation loss: 0.165999	Best loss: 0.129563	Accuracy: 95.11%
7	Validation loss: 1.136336	Best loss: 0.129563	Accuracy: 70.60%
8	Validation loss: 1.075549	Best loss: 0.129563	Accuracy: 58.41%
9	Validation loss: 0.892550	Best loss: 0.129563	Accuracy: 61.81%
10	Validation loss: 0.504251	Best loss: 0.129563	Accuracy: 88.15%
11	Validation loss: 27.694622	Best loss: 0.129563	Accuracy: 18.84%
12	Validation loss: 23.590178	Best loss: 0.129563	Accuracy: 30.18%
13	Validation loss: 9.467207	Best loss: 0.129563	Accuracy: 30.41%
14	Validation loss: 14.252538	Best loss: 0.129563	Accuracy: 20.88%
15	Validation loss: 6.908952	Best loss: 0.129563	Accuracy: 38.62%
16	Validation loss: 14.023297	Best loss: 0.129563	Accuracy: 32.17%
17	Validation loss: 6.543952	Best loss: 0.129563	Accuracy: 36.51%
18	Validation loss: 7.561842	Best loss: 0.129563	Accuracy: 35.46%
19	Validation loss: 3.327480	Best loss: 0.129563	Accuracy: 34.05%
20	Validation loss: 552.141907	Best loss: 0.129563	Accuracy: 20.91%
21	Validation loss: 377.868622	Best loss: 0.129563	Accuracy: 30.84%
22	Validation loss: 46.252583	Best loss: 0.129563	Accuracy: 32.33%
23	Validation loss: 52.035000	Best loss: 0.129563	Accuracy: 22.01%
24	Validation loss: 19.072931	Best loss: 0.129563	Accuracy: 22.24%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=   8.7s
[CV] n_neurons=90, learning_rate=0.05, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.370467	Best loss: 0.370467	Accuracy: 89.91%
1	Validation loss: 0.210905	Best loss: 0.210905	Accuracy: 94.18%
2	Validation loss: 0.155954	Best loss: 0.155954	Accuracy: 95.62%
3	Validation loss: 0.161853	Best loss: 0.155954	Accuracy: 95.50%
4	Validation loss: 0.175803	Best loss: 0.155954	Accuracy: 96.09%
5	Validation loss: 0.139573	Best loss: 0.139573	Accuracy: 96.40%
6	Validation loss: 0.149484	Best loss: 0.139573	Accuracy: 95.58%
7	Validation loss: 0.157094	Best loss: 0.139573	Accuracy: 95.90%
8	Validation loss: 0.163955	Best loss: 0.139573	Accuracy: 95.27%
9	Validation loss: 0.145490	Best loss: 0.139573	Accuracy: 95.86%
10	Validation loss: 0.148681	Best loss: 0.139573	Accuracy: 95.86%
11	Validation loss: 0.158246	Best loss: 0.139573	Accuracy: 95.39%
12	Validation loss: 0.151396	Best loss: 0.139573	Accuracy: 95.70%
13	Validation loss: 0.149717	Best loss: 0.139573	Accuracy: 95.90%
14	Validation loss: 0.171570	Best loss: 0.139573	Accuracy: 95.35%
15	Validation loss: 0.306534	Best loss: 0.139573	Accuracy: 93.43%
16	Validation loss: 0.477794	Best loss: 0.139573	Accuracy: 83.78%
17	Validation loss: 2.620908	Best loss: 0.139573	Accuracy: 57.35%
18	Validation loss: 10.444653	Best loss: 0.139573	Accuracy: 43.16%
19	Validation loss: 150.416092	Best loss: 0.139573	Accuracy: 20.91%
20	Validation loss: 18.992344	Best loss: 0.139573	Accuracy: 29.95%
21	Validation loss: 11.948531	Best loss: 0.139573	Accuracy: 37.72%
22	Validation loss: 3.771893	Best loss: 0.139573	Accuracy: 49.53%
23	Validation loss: 2.886663	Best loss: 0.139573	Accuracy: 56.06%
24	Validation loss: 2.560138	Best loss: 0.139573	Accuracy: 52.07%
25	Validation loss: 3.645892	Best loss: 0.139573	Accuracy: 44.61%
26	Validation loss: 2.442414	Best loss: 0.139573	Accuracy: 48.55%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.05, dropout_rate=0.2, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=   9.7s
[CV] n_neurons=50, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.185211	Best loss: 0.185211	Accuracy: 95.27%
1	Validation loss: 0.150539	Best loss: 0.150539	Accuracy: 96.52%
2	Validation loss: 0.136636	Best loss: 0.136636	Accuracy: 96.56%
3	Validation loss: 0.150217	Best loss: 0.136636	Accuracy: 96.13%
4	Validation loss: 0.140417	Best loss: 0.136636	Accuracy: 96.21%
5	Validation loss: 0.136467	Best loss: 0.136467	Accuracy: 96.56%
6	Validation loss: 0.155597	Best loss: 0.136467	Accuracy: 96.48%
7	Validation loss: 0.141905	Best loss: 0.136467	Accuracy: 96.29%
8	Validation loss: 0.191187	Best loss: 0.136467	Accuracy: 95.78%
9	Validation loss: 0.161335	Best loss: 0.136467	Accuracy: 96.21%
10	Validation loss: 0.166810	Best loss: 0.136467	Accuracy: 96.09%
11	Validation loss: 0.146188	Best loss: 0.136467	Accuracy: 96.44%
12	Validation loss: 0.128417	Best loss: 0.128417	Accuracy: 96.83%
13	Validation loss: 0.144893	Best loss: 0.128417	Accuracy: 96.60%
14	Validation loss: 0.144071	Best loss: 0.128417	Accuracy: 96.48%
15	Validation loss: 0.157001	Best loss: 0.128417	Accuracy: 96.01%
16	Validation loss: 0.173235	Best loss: 0.128417	Accuracy: 96.25%
17	Validation loss: 0.158244	Best loss: 0.128417	Accuracy: 96.17%
18	Validation loss: 0.136104	Best loss: 0.128417	Accuracy: 96.17%
19	Validation loss: 0.165360	Best loss: 0.128417	Accuracy: 96.40%
20	Validation loss: 0.122222	Best loss: 0.122222	Accuracy: 97.19%
21	Validation loss: 0.126664	Best loss: 0.122222	Accuracy: 96.83%
22	Validation loss: 0.131174	Best loss: 0.122222	Accuracy: 96.60%
23	Validation loss: 0.153310	Best loss: 0.122222	Accuracy: 95.66%
24	Validation loss: 0.149560	Best loss: 0.122222	Accuracy: 95.86%
25	Validation loss: 0.127432	Best loss: 0.122222	Accuracy: 96.68%
26	Validation loss: 0.147267	Best loss: 0.122222	Accuracy: 95.31%
27	Validation loss: 0.153533	Best loss: 0.122222	Accuracy: 96.29%
28	Validation loss: 0.131231	Best loss: 0.122222	Accuracy: 96.17%
29	Validation loss: 0.114839	Best loss: 0.114839	Accuracy: 96.33%
30	Validation loss: 0.131122	Best loss: 0.114839	Accuracy: 95.86%
31	Validation loss: 0.213194	Best loss: 0.114839	Accuracy: 97.30%
32	Validation loss: 0.152232	Best loss: 0.114839	Accuracy: 96.25%
33	Validation loss: 0.129198	Best loss: 0.114839	Accuracy: 96.68%
34	Validation loss: 0.165149	Best loss: 0.114839	Accuracy: 96.40%
35	Validation loss: 0.142493	Best loss: 0.114839	Accuracy: 96.01%
36	Validation loss: 0.117668	Best loss: 0.114839	Accuracy: 96.87%
37	Validation loss: 0.161905	Best loss: 0.114839	Accuracy: 95.07%
38	Validation loss: 0.148327	Best loss: 0.114839	Accuracy: 96.40%
39	Validation loss: 0.116985	Best loss: 0.114839	Accuracy: 96.76%
40	Validation loss: 0.135419	Best loss: 0.114839	Accuracy: 96.52%
41	Validation loss: 0.135332	Best loss: 0.114839	Accuracy: 95.54%
42	Validation loss: 0.143977	Best loss: 0.114839	Accuracy: 96.25%
43	Validation loss: 0.125365	Best loss: 0.114839	Accuracy: 96.13%
44	Validation loss: 0.160560	Best loss: 0.114839	Accuracy: 96.13%
45	Validation loss: 0.128000	Best loss: 0.114839	Accuracy: 96.64%
46	Validation loss: 0.167610	Best loss: 0.114839	Accuracy: 96.09%
47	Validation loss: 0.150077	Best loss: 0.114839	Accuracy: 96.56%
48	Validation loss: 0.143926	Best loss: 0.114839	Accuracy: 95.74%
49	Validation loss: 0.127632	Best loss: 0.114839	Accuracy: 96.76%
50	Validation loss: 0.130214	Best loss: 0.114839	Accuracy: 96.09%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  31.7s
[CV] n_neurons=50, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.152832	Best loss: 0.152832	Accuracy: 95.86%
1	Validation loss: 0.168035	Best loss: 0.152832	Accuracy: 95.47%
2	Validation loss: 0.134841	Best loss: 0.134841	Accuracy: 96.09%
3	Validation loss: 0.153173	Best loss: 0.134841	Accuracy: 96.48%
4	Validation loss: 0.143916	Best loss: 0.134841	Accuracy: 96.52%
5	Validation loss: 0.149997	Best loss: 0.134841	Accuracy: 96.17%
6	Validation loss: 0.140501	Best loss: 0.134841	Accuracy: 96.56%
7	Validation loss: 0.134762	Best loss: 0.134762	Accuracy: 96.87%
8	Validation loss: 0.164099	Best loss: 0.134762	Accuracy: 97.11%
9	Validation loss: 0.150780	Best loss: 0.134762	Accuracy: 96.52%
10	Validation loss: 0.197913	Best loss: 0.134762	Accuracy: 96.36%
11	Validation loss: 0.229703	Best loss: 0.134762	Accuracy: 95.90%
12	Validation loss: 0.220565	Best loss: 0.134762	Accuracy: 96.40%
13	Validation loss: 0.139206	Best loss: 0.134762	Accuracy: 96.60%
14	Validation loss: 0.168326	Best loss: 0.134762	Accuracy: 97.15%
15	Validation loss: 0.140869	Best loss: 0.134762	Accuracy: 96.56%
16	Validation loss: 0.133134	Best loss: 0.133134	Accuracy: 97.03%
17	Validation loss: 0.142329	Best loss: 0.133134	Accuracy: 97.07%
18	Validation loss: 0.166281	Best loss: 0.133134	Accuracy: 96.72%
19	Validation loss: 0.203199	Best loss: 0.133134	Accuracy: 95.93%
20	Validation loss: 0.149091	Best loss: 0.133134	Accuracy: 96.64%
21	Validation loss: 0.180108	Best loss: 0.133134	Accuracy: 96.21%
22	Validation loss: 0.185297	Best loss: 0.133134	Accuracy: 95.86%
23	Validation loss: 0.138026	Best loss: 0.133134	Accuracy: 96.95%
24	Validation loss: 0.173476	Best loss: 0.133134	Accuracy: 96.40%
25	Validation loss: 0.148680	Best loss: 0.133134	Accuracy: 96.17%
26	Validation loss: 0.189119	Best loss: 0.133134	Accuracy: 95.74%
27	Validation loss: 0.150339	Best loss: 0.133134	Accuracy: 96.99%
28	Validation loss: 0.147142	Best loss: 0.133134	Accuracy: 96.79%
29	Validation loss: 0.163877	Best loss: 0.133134	Accuracy: 95.27%
30	Validation loss: 0.167311	Best loss: 0.133134	Accuracy: 96.29%
31	Validation loss: 0.198573	Best loss: 0.133134	Accuracy: 95.39%
32	Validation loss: 0.161634	Best loss: 0.133134	Accuracy: 96.29%
33	Validation loss: 0.159050	Best loss: 0.133134	Accuracy: 96.56%
34	Validation loss: 0.237194	Best loss: 0.133134	Accuracy: 94.96%
35	Validation loss: 0.206276	Best loss: 0.133134	Accuracy: 96.52%
36	Validation loss: 0.191560	Best loss: 0.133134	Accuracy: 95.66%
37	Validation loss: 0.172660	Best loss: 0.133134	Accuracy: 96.52%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  22.9s
[CV] n_neurons=50, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.175107	Best loss: 0.175107	Accuracy: 95.93%
1	Validation loss: 0.164580	Best loss: 0.164580	Accuracy: 96.13%
2	Validation loss: 0.156642	Best loss: 0.156642	Accuracy: 96.01%
3	Validation loss: 0.134911	Best loss: 0.134911	Accuracy: 96.29%
4	Validation loss: 0.165557	Best loss: 0.134911	Accuracy: 96.56%
5	Validation loss: 0.152631	Best loss: 0.134911	Accuracy: 96.33%
6	Validation loss: 0.169608	Best loss: 0.134911	Accuracy: 96.68%
7	Validation loss: 0.155069	Best loss: 0.134911	Accuracy: 96.21%
8	Validation loss: 0.116461	Best loss: 0.116461	Accuracy: 97.26%
9	Validation loss: 0.194095	Best loss: 0.116461	Accuracy: 96.56%
10	Validation loss: 0.149706	Best loss: 0.116461	Accuracy: 96.44%
11	Validation loss: 0.109717	Best loss: 0.109717	Accuracy: 97.54%
12	Validation loss: 0.106570	Best loss: 0.106570	Accuracy: 97.30%
13	Validation loss: 0.116387	Best loss: 0.106570	Accuracy: 96.99%
14	Validation loss: 0.114759	Best loss: 0.106570	Accuracy: 97.11%
15	Validation loss: 0.127332	Best loss: 0.106570	Accuracy: 95.97%
16	Validation loss: 0.173875	Best loss: 0.106570	Accuracy: 96.21%
17	Validation loss: 0.149104	Best loss: 0.106570	Accuracy: 96.25%
18	Validation loss: 0.144583	Best loss: 0.106570	Accuracy: 96.09%
19	Validation loss: 0.160597	Best loss: 0.106570	Accuracy: 97.19%
20	Validation loss: 0.162368	Best loss: 0.106570	Accuracy: 96.40%
21	Validation loss: 0.185934	Best loss: 0.106570	Accuracy: 95.82%
22	Validation loss: 0.137550	Best loss: 0.106570	Accuracy: 96.33%
23	Validation loss: 0.133284	Best loss: 0.106570	Accuracy: 96.87%
24	Validation loss: 0.187926	Best loss: 0.106570	Accuracy: 96.91%
25	Validation loss: 0.191571	Best loss: 0.106570	Accuracy: 96.52%
26	Validation loss: 0.145976	Best loss: 0.106570	Accuracy: 96.64%
27	Validation loss: 0.150748	Best loss: 0.106570	Accuracy: 97.07%
28	Validation loss: 0.148570	Best loss: 0.106570	Accuracy: 97.38%
29	Validation loss: 0.151756	Best loss: 0.106570	Accuracy: 97.11%
30	Validation loss: 0.223201	Best loss: 0.106570	Accuracy: 96.91%
31	Validation loss: 0.190790	Best loss: 0.106570	Accuracy: 96.64%
32	Validation loss: 0.157344	Best loss: 0.106570	Accuracy: 96.44%
33	Validation loss: 0.177268	Best loss: 0.106570	Accuracy: 96.76%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function elu at 0x1243639d8&gt;, total=  20.3s
[CV] n_neurons=120, learning_rate=0.05, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 654.435913	Best loss: 654.435913	Accuracy: 19.08%
1	Validation loss: 590.387268	Best loss: 590.387268	Accuracy: 20.91%
2	Validation loss: 866.666260	Best loss: 590.387268	Accuracy: 18.73%
3	Validation loss: 719.126587	Best loss: 590.387268	Accuracy: 20.91%
4	Validation loss: 6999.255859	Best loss: 590.387268	Accuracy: 18.73%
5	Validation loss: 4269.032715	Best loss: 590.387268	Accuracy: 18.73%
6	Validation loss: 1723.834473	Best loss: 590.387268	Accuracy: 19.27%
7	Validation loss: 1150.441528	Best loss: 590.387268	Accuracy: 18.73%
8	Validation loss: 5101.687012	Best loss: 590.387268	Accuracy: 19.27%
9	Validation loss: 2897.951416	Best loss: 590.387268	Accuracy: 18.73%
10	Validation loss: 3663.022217	Best loss: 590.387268	Accuracy: 19.08%
11	Validation loss: 2999.729736	Best loss: 590.387268	Accuracy: 18.73%
12	Validation loss: 922.643188	Best loss: 590.387268	Accuracy: 26.19%
13	Validation loss: 1550.450439	Best loss: 590.387268	Accuracy: 18.73%
14	Validation loss: 2593.550537	Best loss: 590.387268	Accuracy: 21.34%
15	Validation loss: 2786.885742	Best loss: 590.387268	Accuracy: 21.54%
16	Validation loss: 1563.026733	Best loss: 590.387268	Accuracy: 19.08%
17	Validation loss: 1093.480103	Best loss: 590.387268	Accuracy: 18.73%
18	Validation loss: 1010.893188	Best loss: 590.387268	Accuracy: 20.91%
19	Validation loss: 1608.311646	Best loss: 590.387268	Accuracy: 20.80%
20	Validation loss: 532.855164	Best loss: 532.855164	Accuracy: 20.45%
21	Validation loss: 1999.630371	Best loss: 532.855164	Accuracy: 19.08%
22	Validation loss: 10439.244141	Best loss: 532.855164	Accuracy: 22.01%
23	Validation loss: 11522.134766	Best loss: 532.855164	Accuracy: 19.23%
24	Validation loss: 5871.166504	Best loss: 532.855164	Accuracy: 18.73%
25	Validation loss: 40743.960938	Best loss: 532.855164	Accuracy: 19.27%
26	Validation loss: 14935.350586	Best loss: 532.855164	Accuracy: 22.01%
27	Validation loss: 7925.989258	Best loss: 532.855164	Accuracy: 18.73%
28	Validation loss: 26859.863281	Best loss: 532.855164	Accuracy: 22.20%
29	Validation loss: 7831.863281	Best loss: 532.855164	Accuracy: 20.91%
30	Validation loss: 45723.812500	Best loss: 532.855164	Accuracy: 20.21%
31	Validation loss: 7392.178223	Best loss: 532.855164	Accuracy: 18.73%
32	Validation loss: 30660.082031	Best loss: 532.855164	Accuracy: 20.91%
33	Validation loss: 8568.606445	Best loss: 532.855164	Accuracy: 11.02%
34	Validation loss: 8242.031250	Best loss: 532.855164	Accuracy: 27.40%
35	Validation loss: 15364.800781	Best loss: 532.855164	Accuracy: 18.73%
36	Validation loss: 9144.468750	Best loss: 532.855164	Accuracy: 24.08%
37	Validation loss: 19923.164062	Best loss: 532.855164	Accuracy: 19.08%
38	Validation loss: 24887.814453	Best loss: 532.855164	Accuracy: 22.01%
39	Validation loss: 9396.484375	Best loss: 532.855164	Accuracy: 19.08%
40	Validation loss: 11827.745117	Best loss: 532.855164	Accuracy: 18.73%
41	Validation loss: 7979.329102	Best loss: 532.855164	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.05, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  37.4s
[CV] n_neurons=120, learning_rate=0.05, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 268.379089	Best loss: 268.379089	Accuracy: 20.91%
1	Validation loss: 536.810730	Best loss: 268.379089	Accuracy: 19.12%
2	Validation loss: 4314.549805	Best loss: 268.379089	Accuracy: 19.23%
3	Validation loss: 42379.792969	Best loss: 268.379089	Accuracy: 22.01%
4	Validation loss: 2408.001953	Best loss: 268.379089	Accuracy: 20.91%
5	Validation loss: 1504.080811	Best loss: 268.379089	Accuracy: 19.59%
6	Validation loss: 2360.349609	Best loss: 268.379089	Accuracy: 18.73%
7	Validation loss: 1308.131104	Best loss: 268.379089	Accuracy: 22.01%
8	Validation loss: 2607.085205	Best loss: 268.379089	Accuracy: 22.01%
9	Validation loss: 1007.311157	Best loss: 268.379089	Accuracy: 20.91%
10	Validation loss: 9296.088867	Best loss: 268.379089	Accuracy: 19.08%
11	Validation loss: 3646.492920	Best loss: 268.379089	Accuracy: 19.27%
12	Validation loss: 2180.028809	Best loss: 268.379089	Accuracy: 20.91%
13	Validation loss: 21026.410156	Best loss: 268.379089	Accuracy: 19.08%
14	Validation loss: 3296.566406	Best loss: 268.379089	Accuracy: 18.73%
15	Validation loss: 2842.819824	Best loss: 268.379089	Accuracy: 19.12%
16	Validation loss: 1057.059570	Best loss: 268.379089	Accuracy: 19.00%
17	Validation loss: 2401.141113	Best loss: 268.379089	Accuracy: 19.08%
18	Validation loss: 8137.996094	Best loss: 268.379089	Accuracy: 19.27%
19	Validation loss: 3438.013184	Best loss: 268.379089	Accuracy: 20.91%
20	Validation loss: 1531.650513	Best loss: 268.379089	Accuracy: 20.91%
21	Validation loss: 1129.576904	Best loss: 268.379089	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.05, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  19.9s
[CV] n_neurons=120, learning_rate=0.05, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 256.507141	Best loss: 256.507141	Accuracy: 20.91%
1	Validation loss: 182.526031	Best loss: 182.526031	Accuracy: 20.91%
2	Validation loss: 510.210846	Best loss: 182.526031	Accuracy: 20.91%
3	Validation loss: 124.574669	Best loss: 124.574669	Accuracy: 19.27%
4	Validation loss: 73.302292	Best loss: 73.302292	Accuracy: 19.27%
5	Validation loss: 866.022766	Best loss: 73.302292	Accuracy: 19.08%
6	Validation loss: 141.320938	Best loss: 73.302292	Accuracy: 20.95%
7	Validation loss: 532.788574	Best loss: 73.302292	Accuracy: 18.73%
8	Validation loss: 460.979523	Best loss: 73.302292	Accuracy: 19.08%
9	Validation loss: 2680.091797	Best loss: 73.302292	Accuracy: 19.23%
10	Validation loss: 2959.045654	Best loss: 73.302292	Accuracy: 20.91%
11	Validation loss: 8952.258789	Best loss: 73.302292	Accuracy: 18.76%
12	Validation loss: 1137.145264	Best loss: 73.302292	Accuracy: 19.70%
13	Validation loss: 7802.950684	Best loss: 73.302292	Accuracy: 22.01%
14	Validation loss: 1371.415039	Best loss: 73.302292	Accuracy: 19.08%
15	Validation loss: 322.745514	Best loss: 73.302292	Accuracy: 32.99%
16	Validation loss: 2749.840088	Best loss: 73.302292	Accuracy: 19.08%
17	Validation loss: 1369.924927	Best loss: 73.302292	Accuracy: 22.01%
18	Validation loss: 1341.376465	Best loss: 73.302292	Accuracy: 20.91%
19	Validation loss: 9654.867188	Best loss: 73.302292	Accuracy: 18.73%
20	Validation loss: 21815.894531	Best loss: 73.302292	Accuracy: 20.91%
21	Validation loss: 13340.894531	Best loss: 73.302292	Accuracy: 22.01%
22	Validation loss: 11081.172852	Best loss: 73.302292	Accuracy: 20.91%
23	Validation loss: 8251.600586	Best loss: 73.302292	Accuracy: 20.91%
24	Validation loss: 5256.965332	Best loss: 73.302292	Accuracy: 19.23%
25	Validation loss: 54081.890625	Best loss: 73.302292	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.05, dropout_rate=0.5, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  23.7s
[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.152315	Best loss: 0.152315	Accuracy: 95.93%
1	Validation loss: 0.140152	Best loss: 0.140152	Accuracy: 96.52%
2	Validation loss: 0.124450	Best loss: 0.124450	Accuracy: 96.60%
3	Validation loss: 0.108917	Best loss: 0.108917	Accuracy: 96.87%
4	Validation loss: 0.139579	Best loss: 0.108917	Accuracy: 96.13%
5	Validation loss: 0.123628	Best loss: 0.108917	Accuracy: 96.79%
6	Validation loss: 0.144839	Best loss: 0.108917	Accuracy: 96.79%
7	Validation loss: 0.119549	Best loss: 0.108917	Accuracy: 97.03%
8	Validation loss: 0.137615	Best loss: 0.108917	Accuracy: 96.17%
9	Validation loss: 0.099587	Best loss: 0.099587	Accuracy: 97.11%
10	Validation loss: 0.132565	Best loss: 0.099587	Accuracy: 96.64%
11	Validation loss: 0.123467	Best loss: 0.099587	Accuracy: 96.72%
12	Validation loss: 0.125858	Best loss: 0.099587	Accuracy: 97.46%
13	Validation loss: 0.136429	Best loss: 0.099587	Accuracy: 95.62%
14	Validation loss: 0.179346	Best loss: 0.099587	Accuracy: 95.15%
15	Validation loss: 0.135806	Best loss: 0.099587	Accuracy: 95.97%
16	Validation loss: 0.162815	Best loss: 0.099587	Accuracy: 94.29%
17	Validation loss: 0.126690	Best loss: 0.099587	Accuracy: 96.09%
18	Validation loss: 0.140803	Best loss: 0.099587	Accuracy: 95.97%
19	Validation loss: 0.143319	Best loss: 0.099587	Accuracy: 96.48%
20	Validation loss: 0.127977	Best loss: 0.099587	Accuracy: 96.40%
21	Validation loss: 0.161739	Best loss: 0.099587	Accuracy: 95.07%
22	Validation loss: 0.187279	Best loss: 0.099587	Accuracy: 95.93%
23	Validation loss: 0.159886	Best loss: 0.099587	Accuracy: 95.39%
24	Validation loss: 0.136373	Best loss: 0.099587	Accuracy: 96.13%
25	Validation loss: 0.171913	Best loss: 0.099587	Accuracy: 95.47%
26	Validation loss: 0.223502	Best loss: 0.099587	Accuracy: 93.20%
27	Validation loss: 0.187589	Best loss: 0.099587	Accuracy: 95.07%
28	Validation loss: 0.176141	Best loss: 0.099587	Accuracy: 96.56%
29	Validation loss: 0.163695	Best loss: 0.099587	Accuracy: 95.47%
30	Validation loss: 0.186446	Best loss: 0.099587	Accuracy: 95.90%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  20.1s
[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.203726	Best loss: 0.203726	Accuracy: 93.86%
1	Validation loss: 0.182682	Best loss: 0.182682	Accuracy: 95.07%
2	Validation loss: 0.132674	Best loss: 0.132674	Accuracy: 97.15%
3	Validation loss: 0.130826	Best loss: 0.130826	Accuracy: 96.76%
4	Validation loss: 0.124798	Best loss: 0.124798	Accuracy: 96.87%
5	Validation loss: 0.128799	Best loss: 0.124798	Accuracy: 97.15%
6	Validation loss: 0.124091	Best loss: 0.124091	Accuracy: 96.64%
7	Validation loss: 0.122118	Best loss: 0.122118	Accuracy: 96.48%
8	Validation loss: 0.140506	Best loss: 0.122118	Accuracy: 96.95%
9	Validation loss: 0.141001	Best loss: 0.122118	Accuracy: 96.52%
10	Validation loss: 0.137211	Best loss: 0.122118	Accuracy: 96.29%
11	Validation loss: 0.126943	Best loss: 0.122118	Accuracy: 95.82%
12	Validation loss: 0.143946	Best loss: 0.122118	Accuracy: 95.35%
13	Validation loss: 0.127345	Best loss: 0.122118	Accuracy: 96.44%
14	Validation loss: 0.117610	Best loss: 0.117610	Accuracy: 96.40%
15	Validation loss: 0.114431	Best loss: 0.114431	Accuracy: 96.17%
16	Validation loss: 0.174274	Best loss: 0.114431	Accuracy: 95.31%
17	Validation loss: 0.160934	Best loss: 0.114431	Accuracy: 95.11%
18	Validation loss: 0.140678	Best loss: 0.114431	Accuracy: 95.31%
19	Validation loss: 0.150091	Best loss: 0.114431	Accuracy: 96.36%
20	Validation loss: 0.161774	Best loss: 0.114431	Accuracy: 96.29%
21	Validation loss: 0.126407	Best loss: 0.114431	Accuracy: 96.29%
22	Validation loss: 0.129350	Best loss: 0.114431	Accuracy: 96.29%
23	Validation loss: 0.114336	Best loss: 0.114336	Accuracy: 96.68%
24	Validation loss: 0.129758	Best loss: 0.114336	Accuracy: 96.33%
25	Validation loss: 0.158795	Best loss: 0.114336	Accuracy: 95.62%
26	Validation loss: 0.141528	Best loss: 0.114336	Accuracy: 95.58%
27	Validation loss: 0.137318	Best loss: 0.114336	Accuracy: 95.50%
28	Validation loss: 0.128662	Best loss: 0.114336	Accuracy: 96.40%
29	Validation loss: 0.148899	Best loss: 0.114336	Accuracy: 95.43%
30	Validation loss: 0.159988	Best loss: 0.114336	Accuracy: 95.27%
31	Validation loss: 0.145932	Best loss: 0.114336	Accuracy: 95.82%
32	Validation loss: 0.123523	Best loss: 0.114336	Accuracy: 97.11%
33	Validation loss: 0.193355	Best loss: 0.114336	Accuracy: 93.16%
34	Validation loss: 0.197967	Best loss: 0.114336	Accuracy: 93.51%
35	Validation loss: 0.185962	Best loss: 0.114336	Accuracy: 94.21%
36	Validation loss: 0.175189	Best loss: 0.114336	Accuracy: 94.45%
37	Validation loss: 0.154323	Best loss: 0.114336	Accuracy: 96.05%
38	Validation loss: 0.154470	Best loss: 0.114336	Accuracy: 95.86%
39	Validation loss: 0.152714	Best loss: 0.114336	Accuracy: 95.97%
40	Validation loss: 0.138381	Best loss: 0.114336	Accuracy: 96.99%
41	Validation loss: 0.130588	Best loss: 0.114336	Accuracy: 96.33%
42	Validation loss: 0.236787	Best loss: 0.114336	Accuracy: 92.53%
43	Validation loss: 0.192003	Best loss: 0.114336	Accuracy: 94.53%
44	Validation loss: 0.190984	Best loss: 0.114336	Accuracy: 94.02%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  28.7s
[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.247738	Best loss: 0.247738	Accuracy: 94.14%
1	Validation loss: 0.186136	Best loss: 0.186136	Accuracy: 95.47%
2	Validation loss: 0.150655	Best loss: 0.150655	Accuracy: 96.40%
3	Validation loss: 0.129610	Best loss: 0.129610	Accuracy: 96.56%
4	Validation loss: 0.154395	Best loss: 0.129610	Accuracy: 95.74%
5	Validation loss: 0.132819	Best loss: 0.129610	Accuracy: 96.72%
6	Validation loss: 0.183336	Best loss: 0.129610	Accuracy: 96.72%
7	Validation loss: 0.134195	Best loss: 0.129610	Accuracy: 96.05%
8	Validation loss: 0.106766	Best loss: 0.106766	Accuracy: 96.95%
9	Validation loss: 0.125587	Best loss: 0.106766	Accuracy: 96.76%
10	Validation loss: 0.138139	Best loss: 0.106766	Accuracy: 95.90%
11	Validation loss: 0.158819	Best loss: 0.106766	Accuracy: 94.96%
12	Validation loss: 0.121311	Best loss: 0.106766	Accuracy: 96.40%
13	Validation loss: 0.119125	Best loss: 0.106766	Accuracy: 96.48%
14	Validation loss: 0.131696	Best loss: 0.106766	Accuracy: 96.33%
15	Validation loss: 0.128957	Best loss: 0.106766	Accuracy: 96.48%
16	Validation loss: 0.111226	Best loss: 0.106766	Accuracy: 97.42%
17	Validation loss: 0.126026	Best loss: 0.106766	Accuracy: 96.36%
18	Validation loss: 0.120661	Best loss: 0.106766	Accuracy: 96.60%
19	Validation loss: 0.122487	Best loss: 0.106766	Accuracy: 96.64%
20	Validation loss: 0.126085	Best loss: 0.106766	Accuracy: 96.72%
21	Validation loss: 0.119610	Best loss: 0.106766	Accuracy: 96.33%
22	Validation loss: 0.122374	Best loss: 0.106766	Accuracy: 96.17%
23	Validation loss: 0.140819	Best loss: 0.106766	Accuracy: 96.25%
24	Validation loss: 0.137617	Best loss: 0.106766	Accuracy: 96.25%
25	Validation loss: 0.132997	Best loss: 0.106766	Accuracy: 96.83%
26	Validation loss: 0.141238	Best loss: 0.106766	Accuracy: 96.52%
27	Validation loss: 0.166871	Best loss: 0.106766	Accuracy: 94.57%
28	Validation loss: 0.156089	Best loss: 0.106766	Accuracy: 94.88%
29	Validation loss: 0.123598	Best loss: 0.106766	Accuracy: 96.13%
Early stopping!
[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.5, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  19.5s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.648990	Best loss: 1.648990	Accuracy: 19.08%
1	Validation loss: 1.683538	Best loss: 1.648990	Accuracy: 19.27%
2	Validation loss: 1.628196	Best loss: 1.628196	Accuracy: 22.01%
3	Validation loss: 1.669778	Best loss: 1.628196	Accuracy: 22.01%
4	Validation loss: 1.648365	Best loss: 1.628196	Accuracy: 22.01%
5	Validation loss: 1.642848	Best loss: 1.628196	Accuracy: 18.73%
6	Validation loss: 1.816653	Best loss: 1.628196	Accuracy: 19.27%
7	Validation loss: 1.632814	Best loss: 1.628196	Accuracy: 22.01%
8	Validation loss: 1.638161	Best loss: 1.628196	Accuracy: 20.91%
9	Validation loss: 1.726818	Best loss: 1.628196	Accuracy: 22.01%
10	Validation loss: 1.652091	Best loss: 1.628196	Accuracy: 19.08%
11	Validation loss: 1.671127	Best loss: 1.628196	Accuracy: 19.08%
12	Validation loss: 1.774113	Best loss: 1.628196	Accuracy: 22.01%
13	Validation loss: 1.667168	Best loss: 1.628196	Accuracy: 19.08%
14	Validation loss: 1.654165	Best loss: 1.628196	Accuracy: 22.01%
15	Validation loss: 1.626204	Best loss: 1.626204	Accuracy: 20.91%
16	Validation loss: 1.692105	Best loss: 1.626204	Accuracy: 19.27%
17	Validation loss: 1.634154	Best loss: 1.626204	Accuracy: 19.08%
18	Validation loss: 1.792602	Best loss: 1.626204	Accuracy: 18.73%
19	Validation loss: 1.700593	Best loss: 1.626204	Accuracy: 22.01%
20	Validation loss: 1.673207	Best loss: 1.626204	Accuracy: 19.08%
21	Validation loss: 1.633193	Best loss: 1.626204	Accuracy: 19.27%
22	Validation loss: 1.705877	Best loss: 1.626204	Accuracy: 18.73%
23	Validation loss: 1.629821	Best loss: 1.626204	Accuracy: 19.27%
24	Validation loss: 1.681154	Best loss: 1.626204	Accuracy: 22.01%
25	Validation loss: 1.630255	Best loss: 1.626204	Accuracy: 22.01%
26	Validation loss: 1.644132	Best loss: 1.626204	Accuracy: 18.73%
27	Validation loss: 1.634422	Best loss: 1.626204	Accuracy: 18.73%
28	Validation loss: 1.639392	Best loss: 1.626204	Accuracy: 19.08%
29	Validation loss: 1.628388	Best loss: 1.626204	Accuracy: 19.27%
30	Validation loss: 1.688952	Best loss: 1.626204	Accuracy: 19.27%
31	Validation loss: 1.708352	Best loss: 1.626204	Accuracy: 22.01%
32	Validation loss: 1.699985	Best loss: 1.626204	Accuracy: 22.01%
33	Validation loss: 1.676188	Best loss: 1.626204	Accuracy: 19.08%
34	Validation loss: 1.640786	Best loss: 1.626204	Accuracy: 18.73%
35	Validation loss: 1.727050	Best loss: 1.626204	Accuracy: 19.08%
36	Validation loss: 1.620631	Best loss: 1.620631	Accuracy: 20.91%
37	Validation loss: 1.633918	Best loss: 1.620631	Accuracy: 22.01%
38	Validation loss: 1.751994	Best loss: 1.620631	Accuracy: 19.27%
39	Validation loss: 1.611812	Best loss: 1.611812	Accuracy: 22.01%
40	Validation loss: 1.749695	Best loss: 1.611812	Accuracy: 22.01%
41	Validation loss: 1.692756	Best loss: 1.611812	Accuracy: 19.27%
42	Validation loss: 1.626327	Best loss: 1.611812	Accuracy: 19.27%
43	Validation loss: 1.670418	Best loss: 1.611812	Accuracy: 19.08%
44	Validation loss: 1.652975	Best loss: 1.611812	Accuracy: 19.08%
45	Validation loss: 1.629841	Best loss: 1.611812	Accuracy: 22.01%
46	Validation loss: 1.643271	Best loss: 1.611812	Accuracy: 22.01%
47	Validation loss: 1.615796	Best loss: 1.611812	Accuracy: 18.73%
48	Validation loss: 1.696510	Best loss: 1.611812	Accuracy: 22.01%
49	Validation loss: 1.693622	Best loss: 1.611812	Accuracy: 19.27%
50	Validation loss: 1.699433	Best loss: 1.611812	Accuracy: 22.01%
51	Validation loss: 1.801876	Best loss: 1.611812	Accuracy: 19.08%
52	Validation loss: 1.608844	Best loss: 1.608844	Accuracy: 20.91%
53	Validation loss: 1.734050	Best loss: 1.608844	Accuracy: 18.73%
54	Validation loss: 1.682174	Best loss: 1.608844	Accuracy: 18.73%
55	Validation loss: 1.706130	Best loss: 1.608844	Accuracy: 22.01%
56	Validation loss: 1.641558	Best loss: 1.608844	Accuracy: 22.01%
57	Validation loss: 1.673667	Best loss: 1.608844	Accuracy: 22.01%
58	Validation loss: 1.650705	Best loss: 1.608844	Accuracy: 20.91%
59	Validation loss: 1.779767	Best loss: 1.608844	Accuracy: 19.08%
60	Validation loss: 1.649523	Best loss: 1.608844	Accuracy: 20.91%
61	Validation loss: 1.729794	Best loss: 1.608844	Accuracy: 20.91%
62	Validation loss: 1.708441	Best loss: 1.608844	Accuracy: 19.08%
63	Validation loss: 1.611665	Best loss: 1.608844	Accuracy: 22.01%
64	Validation loss: 1.638851	Best loss: 1.608844	Accuracy: 22.01%
65	Validation loss: 1.732983	Best loss: 1.608844	Accuracy: 19.08%
66	Validation loss: 1.627757	Best loss: 1.608844	Accuracy: 19.27%
67	Validation loss: 1.629768	Best loss: 1.608844	Accuracy: 20.91%
68	Validation loss: 1.620057	Best loss: 1.608844	Accuracy: 19.08%
69	Validation loss: 1.729194	Best loss: 1.608844	Accuracy: 19.27%
70	Validation loss: 1.677848	Best loss: 1.608844	Accuracy: 19.08%
71	Validation loss: 1.630350	Best loss: 1.608844	Accuracy: 22.01%
72	Validation loss: 1.642838	Best loss: 1.608844	Accuracy: 19.08%
73	Validation loss: 1.621255	Best loss: 1.608844	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  30.0s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.696441	Best loss: 1.696441	Accuracy: 22.01%
1	Validation loss: 1.615128	Best loss: 1.615128	Accuracy: 20.91%
2	Validation loss: 1.620731	Best loss: 1.615128	Accuracy: 20.91%
3	Validation loss: 1.628628	Best loss: 1.615128	Accuracy: 22.01%
4	Validation loss: 1.638009	Best loss: 1.615128	Accuracy: 19.27%
5	Validation loss: 1.647281	Best loss: 1.615128	Accuracy: 20.91%
6	Validation loss: 1.706015	Best loss: 1.615128	Accuracy: 19.08%
7	Validation loss: 1.676472	Best loss: 1.615128	Accuracy: 18.73%
8	Validation loss: 1.751495	Best loss: 1.615128	Accuracy: 22.01%
9	Validation loss: 1.798470	Best loss: 1.615128	Accuracy: 22.01%
10	Validation loss: 1.661133	Best loss: 1.615128	Accuracy: 20.91%
11	Validation loss: 1.657784	Best loss: 1.615128	Accuracy: 20.91%
12	Validation loss: 1.647235	Best loss: 1.615128	Accuracy: 18.73%
13	Validation loss: 1.699468	Best loss: 1.615128	Accuracy: 19.08%
14	Validation loss: 1.772041	Best loss: 1.615128	Accuracy: 22.01%
15	Validation loss: 1.767524	Best loss: 1.615128	Accuracy: 22.01%
16	Validation loss: 1.656913	Best loss: 1.615128	Accuracy: 18.73%
17	Validation loss: 1.688671	Best loss: 1.615128	Accuracy: 19.08%
18	Validation loss: 1.636451	Best loss: 1.615128	Accuracy: 20.91%
19	Validation loss: 1.634983	Best loss: 1.615128	Accuracy: 22.01%
20	Validation loss: 1.638402	Best loss: 1.615128	Accuracy: 22.01%
21	Validation loss: 1.927962	Best loss: 1.615128	Accuracy: 22.01%
22	Validation loss: 1.666712	Best loss: 1.615128	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  10.1s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.630003	Best loss: 1.630003	Accuracy: 22.01%
1	Validation loss: 1.632153	Best loss: 1.630003	Accuracy: 19.27%
2	Validation loss: 1.718012	Best loss: 1.630003	Accuracy: 19.27%
3	Validation loss: 1.640518	Best loss: 1.630003	Accuracy: 18.73%
4	Validation loss: 1.622949	Best loss: 1.622949	Accuracy: 20.91%
5	Validation loss: 1.717082	Best loss: 1.622949	Accuracy: 18.73%
6	Validation loss: 1.666959	Best loss: 1.622949	Accuracy: 20.91%
7	Validation loss: 1.717487	Best loss: 1.622949	Accuracy: 22.01%
8	Validation loss: 1.668911	Best loss: 1.622949	Accuracy: 22.01%
9	Validation loss: 1.699464	Best loss: 1.622949	Accuracy: 19.27%
10	Validation loss: 1.633097	Best loss: 1.622949	Accuracy: 20.91%
11	Validation loss: 1.617253	Best loss: 1.617253	Accuracy: 22.01%
12	Validation loss: 1.660915	Best loss: 1.617253	Accuracy: 19.08%
13	Validation loss: 1.867097	Best loss: 1.617253	Accuracy: 19.27%
14	Validation loss: 1.671974	Best loss: 1.617253	Accuracy: 19.08%
15	Validation loss: 1.763948	Best loss: 1.617253	Accuracy: 22.01%
16	Validation loss: 1.618252	Best loss: 1.617253	Accuracy: 19.08%
17	Validation loss: 1.651562	Best loss: 1.617253	Accuracy: 19.08%
18	Validation loss: 1.656868	Best loss: 1.617253	Accuracy: 18.73%
19	Validation loss: 1.653502	Best loss: 1.617253	Accuracy: 19.08%
20	Validation loss: 1.661449	Best loss: 1.617253	Accuracy: 22.01%
21	Validation loss: 1.693546	Best loss: 1.617253	Accuracy: 22.01%
22	Validation loss: 1.640417	Best loss: 1.617253	Accuracy: 18.73%
23	Validation loss: 1.669016	Best loss: 1.617253	Accuracy: 18.73%
24	Validation loss: 1.648549	Best loss: 1.617253	Accuracy: 20.91%
25	Validation loss: 1.787219	Best loss: 1.617253	Accuracy: 18.73%
26	Validation loss: 1.649768	Best loss: 1.617253	Accuracy: 19.08%
27	Validation loss: 1.633251	Best loss: 1.617253	Accuracy: 20.91%
28	Validation loss: 1.650108	Best loss: 1.617253	Accuracy: 20.91%
29	Validation loss: 1.985681	Best loss: 1.617253	Accuracy: 19.27%
30	Validation loss: 1.675364	Best loss: 1.617253	Accuracy: 19.08%
31	Validation loss: 1.644502	Best loss: 1.617253	Accuracy: 19.27%
32	Validation loss: 2.119026	Best loss: 1.617253	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  14.3s
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 1.444507	Best loss: 1.444507	Accuracy: 39.99%
1	Validation loss: 5.266934	Best loss: 1.444507	Accuracy: 24.98%
2	Validation loss: 108.644691	Best loss: 1.444507	Accuracy: 19.27%
3	Validation loss: 52.050159	Best loss: 1.444507	Accuracy: 18.73%
4	Validation loss: 130.296112	Best loss: 1.444507	Accuracy: 19.08%
5	Validation loss: 159.300400	Best loss: 1.444507	Accuracy: 19.27%
6	Validation loss: 89.888374	Best loss: 1.444507	Accuracy: 22.01%
7	Validation loss: 368.569794	Best loss: 1.444507	Accuracy: 19.27%
8	Validation loss: 323.467590	Best loss: 1.444507	Accuracy: 22.01%
9	Validation loss: 68.848785	Best loss: 1.444507	Accuracy: 25.22%
10	Validation loss: 90.127312	Best loss: 1.444507	Accuracy: 20.91%
11	Validation loss: 34.753452	Best loss: 1.444507	Accuracy: 36.47%
12	Validation loss: 1410.754639	Best loss: 1.444507	Accuracy: 19.27%
13	Validation loss: 802.113525	Best loss: 1.444507	Accuracy: 20.91%
14	Validation loss: 997.697937	Best loss: 1.444507	Accuracy: 19.08%
15	Validation loss: 425.093933	Best loss: 1.444507	Accuracy: 20.91%
16	Validation loss: 859.058167	Best loss: 1.444507	Accuracy: 21.23%
17	Validation loss: 462.822510	Best loss: 1.444507	Accuracy: 22.01%
18	Validation loss: 641.074219	Best loss: 1.444507	Accuracy: 19.08%
19	Validation loss: 302.616180	Best loss: 1.444507	Accuracy: 22.01%
20	Validation loss: 285.632751	Best loss: 1.444507	Accuracy: 18.73%
21	Validation loss: 340.844818	Best loss: 1.444507	Accuracy: 21.03%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.1min
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 22.549009	Best loss: 22.549009	Accuracy: 20.91%
1	Validation loss: 839.486206	Best loss: 22.549009	Accuracy: 19.08%
2	Validation loss: 24.328485	Best loss: 22.549009	Accuracy: 21.11%
3	Validation loss: 85.050751	Best loss: 22.549009	Accuracy: 22.01%
4	Validation loss: 510.503326	Best loss: 22.549009	Accuracy: 19.08%
5	Validation loss: 114.719299	Best loss: 22.549009	Accuracy: 20.76%
6	Validation loss: 331.188599	Best loss: 22.549009	Accuracy: 19.16%
7	Validation loss: 700.730530	Best loss: 22.549009	Accuracy: 18.73%
8	Validation loss: 1086.966187	Best loss: 22.549009	Accuracy: 18.73%
9	Validation loss: 927.225769	Best loss: 22.549009	Accuracy: 22.01%
10	Validation loss: 765.830444	Best loss: 22.549009	Accuracy: 28.69%
11	Validation loss: 298.926422	Best loss: 22.549009	Accuracy: 28.34%
12	Validation loss: 553.791382	Best loss: 22.549009	Accuracy: 19.27%
13	Validation loss: 638.383240	Best loss: 22.549009	Accuracy: 19.27%
14	Validation loss: 2588.656250	Best loss: 22.549009	Accuracy: 19.39%
15	Validation loss: 2106.701172	Best loss: 22.549009	Accuracy: 19.08%
16	Validation loss: 1289.251587	Best loss: 22.549009	Accuracy: 18.96%
17	Validation loss: 3297.506348	Best loss: 22.549009	Accuracy: 19.08%
18	Validation loss: 2226.046631	Best loss: 22.549009	Accuracy: 23.92%
19	Validation loss: 2606.848633	Best loss: 22.549009	Accuracy: 20.91%
20	Validation loss: 3086.629639	Best loss: 22.549009	Accuracy: 18.73%
21	Validation loss: 23418.171875	Best loss: 22.549009	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.1min
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 1.905937	Best loss: 1.905937	Accuracy: 38.08%
1	Validation loss: 82.568886	Best loss: 1.905937	Accuracy: 19.08%
2	Validation loss: 251.611740	Best loss: 1.905937	Accuracy: 18.73%
3	Validation loss: 190.953339	Best loss: 1.905937	Accuracy: 19.12%
4	Validation loss: 199.270935	Best loss: 1.905937	Accuracy: 19.08%
5	Validation loss: 225.110611	Best loss: 1.905937	Accuracy: 27.21%
6	Validation loss: 136.534653	Best loss: 1.905937	Accuracy: 20.80%
7	Validation loss: 142.152634	Best loss: 1.905937	Accuracy: 32.60%
8	Validation loss: 91.191193	Best loss: 1.905937	Accuracy: 20.21%
9	Validation loss: 292.292816	Best loss: 1.905937	Accuracy: 22.01%
10	Validation loss: 122.930916	Best loss: 1.905937	Accuracy: 22.75%
11	Validation loss: 270.022827	Best loss: 1.905937	Accuracy: 18.92%
12	Validation loss: 133.968842	Best loss: 1.905937	Accuracy: 20.91%
13	Validation loss: 72.500473	Best loss: 1.905937	Accuracy: 31.55%
14	Validation loss: 1233.619873	Best loss: 1.905937	Accuracy: 18.65%
15	Validation loss: 532.067688	Best loss: 1.905937	Accuracy: 19.23%
16	Validation loss: 1150.248779	Best loss: 1.905937	Accuracy: 35.26%
17	Validation loss: 1146.630615	Best loss: 1.905937	Accuracy: 18.73%
18	Validation loss: 985.335205	Best loss: 1.905937	Accuracy: 22.01%
19	Validation loss: 934.329834	Best loss: 1.905937	Accuracy: 19.74%
20	Validation loss: 773.254150	Best loss: 1.905937	Accuracy: 19.27%
21	Validation loss: 1555.041382	Best loss: 1.905937	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 1.2min
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 5682.308594	Best loss: 5682.308594	Accuracy: 28.07%
1	Validation loss: 350.124573	Best loss: 350.124573	Accuracy: 43.82%
2	Validation loss: 357.162903	Best loss: 350.124573	Accuracy: 56.06%
3	Validation loss: 865.789001	Best loss: 350.124573	Accuracy: 46.48%
4	Validation loss: 86.925484	Best loss: 86.925484	Accuracy: 67.98%
5	Validation loss: 69.054276	Best loss: 69.054276	Accuracy: 56.41%
6	Validation loss: 30.576368	Best loss: 30.576368	Accuracy: 72.60%
7	Validation loss: 116.561607	Best loss: 30.576368	Accuracy: 55.90%
8	Validation loss: 232.449173	Best loss: 30.576368	Accuracy: 57.35%
9	Validation loss: 160.692764	Best loss: 30.576368	Accuracy: 56.96%
10	Validation loss: 17326572.000000	Best loss: 30.576368	Accuracy: 37.33%
11	Validation loss: 303549.750000	Best loss: 30.576368	Accuracy: 73.34%
12	Validation loss: 156047.984375	Best loss: 30.576368	Accuracy: 68.18%
13	Validation loss: 16584672.000000	Best loss: 30.576368	Accuracy: 40.93%
14	Validation loss: 228560.531250	Best loss: 30.576368	Accuracy: 75.06%
15	Validation loss: 141909.890625	Best loss: 30.576368	Accuracy: 81.98%
16	Validation loss: 156118.109375	Best loss: 30.576368	Accuracy: 75.80%
17	Validation loss: 49380.371094	Best loss: 30.576368	Accuracy: 92.26%
18	Validation loss: 200597.906250	Best loss: 30.576368	Accuracy: 70.41%
19	Validation loss: 48996.312500	Best loss: 30.576368	Accuracy: 91.48%
20	Validation loss: 37730.582031	Best loss: 30.576368	Accuracy: 90.11%
21	Validation loss: 258767.859375	Best loss: 30.576368	Accuracy: 72.01%
22	Validation loss: 38556.503906	Best loss: 30.576368	Accuracy: 88.90%
23	Validation loss: 65383.906250	Best loss: 30.576368	Accuracy: 77.44%
24	Validation loss: 53444.410156	Best loss: 30.576368	Accuracy: 90.97%
25	Validation loss: 67376.609375	Best loss: 30.576368	Accuracy: 83.19%
26	Validation loss: 27981.808594	Best loss: 30.576368	Accuracy: 91.71%
27	Validation loss: 19647.007812	Best loss: 30.576368	Accuracy: 93.82%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  23.5s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.753458	Best loss: 0.753458	Accuracy: 66.38%
1	Validation loss: 72862.148438	Best loss: 0.753458	Accuracy: 53.56%
2	Validation loss: 5702.186035	Best loss: 0.753458	Accuracy: 67.08%
3	Validation loss: 10513.737305	Best loss: 0.753458	Accuracy: 57.97%
4	Validation loss: 2211.380859	Best loss: 0.753458	Accuracy: 78.54%
5	Validation loss: 1493.478149	Best loss: 0.753458	Accuracy: 78.54%
6	Validation loss: 922.249817	Best loss: 0.753458	Accuracy: 85.89%
7	Validation loss: 1136.975464	Best loss: 0.753458	Accuracy: 85.54%
8	Validation loss: 3844.538330	Best loss: 0.753458	Accuracy: 63.02%
9	Validation loss: 714.323303	Best loss: 0.753458	Accuracy: 88.58%
10	Validation loss: 1312.011353	Best loss: 0.753458	Accuracy: 87.29%
11	Validation loss: 1173.512939	Best loss: 0.753458	Accuracy: 86.28%
12	Validation loss: 820.743042	Best loss: 0.753458	Accuracy: 88.90%
13	Validation loss: 417.480347	Best loss: 0.753458	Accuracy: 93.75%
14	Validation loss: 502.897858	Best loss: 0.753458	Accuracy: 90.58%
15	Validation loss: 258.329407	Best loss: 0.753458	Accuracy: 94.06%
16	Validation loss: 151.490356	Best loss: 0.753458	Accuracy: 94.45%
17	Validation loss: 1081.951660	Best loss: 0.753458	Accuracy: 92.38%
18	Validation loss: 117239.234375	Best loss: 0.753458	Accuracy: 92.77%
19	Validation loss: 46004.070312	Best loss: 0.753458	Accuracy: 93.12%
20	Validation loss: 1062147.625000	Best loss: 0.753458	Accuracy: 88.86%
21	Validation loss: 105006.750000	Best loss: 0.753458	Accuracy: 95.66%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  19.2s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 38467.781250	Best loss: 38467.781250	Accuracy: 50.82%
1	Validation loss: 469.890045	Best loss: 469.890045	Accuracy: 87.61%
2	Validation loss: 5412.985352	Best loss: 469.890045	Accuracy: 79.87%
3	Validation loss: 685.330139	Best loss: 469.890045	Accuracy: 78.11%
4	Validation loss: 3666.310303	Best loss: 469.890045	Accuracy: 78.15%
5	Validation loss: 281132.750000	Best loss: 469.890045	Accuracy: 38.12%
6	Validation loss: 55155.671875	Best loss: 469.890045	Accuracy: 53.87%
7	Validation loss: 59948.179688	Best loss: 469.890045	Accuracy: 57.35%
8	Validation loss: 12700.858398	Best loss: 469.890045	Accuracy: 65.13%
9	Validation loss: 17005.480469	Best loss: 469.890045	Accuracy: 67.44%
10	Validation loss: 12620.770508	Best loss: 469.890045	Accuracy: 67.36%
11	Validation loss: 7763.215820	Best loss: 469.890045	Accuracy: 72.17%
12	Validation loss: 488556.281250	Best loss: 469.890045	Accuracy: 88.31%
13	Validation loss: 68947.578125	Best loss: 469.890045	Accuracy: 89.33%
14	Validation loss: 1338573.875000	Best loss: 469.890045	Accuracy: 86.75%
15	Validation loss: 316271.656250	Best loss: 469.890045	Accuracy: 92.26%
16	Validation loss: 338486.000000	Best loss: 469.890045	Accuracy: 90.58%
17	Validation loss: 895817.875000	Best loss: 469.890045	Accuracy: 75.49%
18	Validation loss: 322159.125000	Best loss: 469.890045	Accuracy: 91.16%
19	Validation loss: 90847.585938	Best loss: 469.890045	Accuracy: 95.23%
20	Validation loss: 135722.328125	Best loss: 469.890045	Accuracy: 79.01%
21	Validation loss: 115141.906250	Best loss: 469.890045	Accuracy: 92.81%
22	Validation loss: 46133.125000	Best loss: 469.890045	Accuracy: 96.17%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  20.4s
[CV] n_neurons=50, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.140090	Best loss: 0.140090	Accuracy: 95.97%
1	Validation loss: 0.129329	Best loss: 0.129329	Accuracy: 96.29%
2	Validation loss: 0.505355	Best loss: 0.129329	Accuracy: 94.18%
3	Validation loss: 0.409931	Best loss: 0.129329	Accuracy: 89.87%
4	Validation loss: 1.584423	Best loss: 0.129329	Accuracy: 64.74%
5	Validation loss: 0.540177	Best loss: 0.129329	Accuracy: 83.89%
6	Validation loss: 0.437350	Best loss: 0.129329	Accuracy: 82.99%
7	Validation loss: 0.643407	Best loss: 0.129329	Accuracy: 82.49%
8	Validation loss: 0.462609	Best loss: 0.129329	Accuracy: 75.49%
9	Validation loss: 0.351235	Best loss: 0.129329	Accuracy: 91.75%
10	Validation loss: 0.293982	Best loss: 0.129329	Accuracy: 90.30%
11	Validation loss: 0.261853	Best loss: 0.129329	Accuracy: 91.56%
12	Validation loss: 0.241764	Best loss: 0.129329	Accuracy: 92.22%
13	Validation loss: 0.379838	Best loss: 0.129329	Accuracy: 83.07%
14	Validation loss: 0.220497	Best loss: 0.129329	Accuracy: 93.08%
15	Validation loss: 0.242767	Best loss: 0.129329	Accuracy: 91.67%
16	Validation loss: 0.202218	Best loss: 0.129329	Accuracy: 94.72%
17	Validation loss: 0.227477	Best loss: 0.129329	Accuracy: 93.51%
18	Validation loss: 0.225866	Best loss: 0.129329	Accuracy: 93.43%
19	Validation loss: 0.197754	Best loss: 0.129329	Accuracy: 93.78%
20	Validation loss: 0.200300	Best loss: 0.129329	Accuracy: 94.88%
21	Validation loss: 2.391336	Best loss: 0.129329	Accuracy: 78.23%
22	Validation loss: 0.471298	Best loss: 0.129329	Accuracy: 90.30%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  10.5s
[CV] n_neurons=50, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.159829	Best loss: 0.159829	Accuracy: 96.25%
1	Validation loss: 0.182832	Best loss: 0.159829	Accuracy: 96.36%
2	Validation loss: 0.442838	Best loss: 0.159829	Accuracy: 90.15%
3	Validation loss: 0.329595	Best loss: 0.159829	Accuracy: 91.59%
4	Validation loss: 0.553686	Best loss: 0.159829	Accuracy: 93.55%
5	Validation loss: 0.418327	Best loss: 0.159829	Accuracy: 87.65%
6	Validation loss: 0.374429	Best loss: 0.159829	Accuracy: 85.34%
7	Validation loss: 0.201159	Best loss: 0.159829	Accuracy: 92.69%
8	Validation loss: 0.247331	Best loss: 0.159829	Accuracy: 91.44%
9	Validation loss: 0.161708	Best loss: 0.159829	Accuracy: 95.07%
10	Validation loss: 0.216509	Best loss: 0.159829	Accuracy: 93.12%
11	Validation loss: 0.171081	Best loss: 0.159829	Accuracy: 94.45%
12	Validation loss: 0.269966	Best loss: 0.159829	Accuracy: 92.30%
13	Validation loss: 4.650885	Best loss: 0.159829	Accuracy: 64.70%
14	Validation loss: 8.311436	Best loss: 0.159829	Accuracy: 37.53%
15	Validation loss: 2.478296	Best loss: 0.159829	Accuracy: 53.67%
16	Validation loss: 2.096072	Best loss: 0.159829	Accuracy: 48.83%
17	Validation loss: 1.294042	Best loss: 0.159829	Accuracy: 51.25%
18	Validation loss: 1.629523	Best loss: 0.159829	Accuracy: 49.02%
19	Validation loss: 0.906891	Best loss: 0.159829	Accuracy: 66.93%
20	Validation loss: 0.734783	Best loss: 0.159829	Accuracy: 76.19%
21	Validation loss: 0.744318	Best loss: 0.159829	Accuracy: 66.58%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  10.0s
[CV] n_neurons=50, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.157134	Best loss: 0.157134	Accuracy: 96.25%
1	Validation loss: 0.228977	Best loss: 0.157134	Accuracy: 96.01%
2	Validation loss: 1.063774	Best loss: 0.157134	Accuracy: 64.54%
3	Validation loss: 0.489039	Best loss: 0.157134	Accuracy: 78.11%
4	Validation loss: 0.308649	Best loss: 0.157134	Accuracy: 90.66%
5	Validation loss: 0.244881	Best loss: 0.157134	Accuracy: 93.43%
6	Validation loss: 0.263123	Best loss: 0.157134	Accuracy: 92.46%
7	Validation loss: 0.250495	Best loss: 0.157134	Accuracy: 93.63%
8	Validation loss: 0.206603	Best loss: 0.157134	Accuracy: 94.64%
9	Validation loss: 0.225408	Best loss: 0.157134	Accuracy: 93.43%
10	Validation loss: 0.202760	Best loss: 0.157134	Accuracy: 94.53%
11	Validation loss: 0.427174	Best loss: 0.157134	Accuracy: 90.15%
12	Validation loss: 0.705976	Best loss: 0.157134	Accuracy: 85.46%
13	Validation loss: 0.246008	Best loss: 0.157134	Accuracy: 93.71%
14	Validation loss: 0.384900	Best loss: 0.157134	Accuracy: 88.98%
15	Validation loss: 0.811127	Best loss: 0.157134	Accuracy: 64.11%
16	Validation loss: 0.471928	Best loss: 0.157134	Accuracy: 87.10%
17	Validation loss: 0.404100	Best loss: 0.157134	Accuracy: 89.64%
18	Validation loss: 6.713084	Best loss: 0.157134	Accuracy: 60.52%
19	Validation loss: 4.513937	Best loss: 0.157134	Accuracy: 55.12%
20	Validation loss: 1.125884	Best loss: 0.157134	Accuracy: 61.49%
21	Validation loss: 0.619336	Best loss: 0.157134	Accuracy: 75.45%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=   9.8s
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.284725	Best loss: 0.284725	Accuracy: 91.67%
1	Validation loss: 0.211024	Best loss: 0.211024	Accuracy: 93.98%
2	Validation loss: 0.269843	Best loss: 0.211024	Accuracy: 93.71%
3	Validation loss: 0.247379	Best loss: 0.211024	Accuracy: 93.47%
4	Validation loss: 0.210783	Best loss: 0.210783	Accuracy: 94.53%
5	Validation loss: 0.215547	Best loss: 0.210783	Accuracy: 94.76%
6	Validation loss: 0.267279	Best loss: 0.210783	Accuracy: 94.29%
7	Validation loss: 0.329877	Best loss: 0.210783	Accuracy: 94.02%
8	Validation loss: 0.257224	Best loss: 0.210783	Accuracy: 94.72%
9	Validation loss: 0.203782	Best loss: 0.203782	Accuracy: 93.90%
10	Validation loss: 0.261874	Best loss: 0.203782	Accuracy: 91.99%
11	Validation loss: 0.205179	Best loss: 0.203782	Accuracy: 95.07%
12	Validation loss: 0.277960	Best loss: 0.203782	Accuracy: 91.28%
13	Validation loss: 0.276198	Best loss: 0.203782	Accuracy: 92.81%
14	Validation loss: 0.263114	Best loss: 0.203782	Accuracy: 93.78%
15	Validation loss: 0.218358	Best loss: 0.203782	Accuracy: 93.98%
16	Validation loss: 0.251325	Best loss: 0.203782	Accuracy: 93.43%
17	Validation loss: 0.258680	Best loss: 0.203782	Accuracy: 92.22%
18	Validation loss: 0.298243	Best loss: 0.203782	Accuracy: 91.59%
19	Validation loss: 0.505203	Best loss: 0.203782	Accuracy: 87.88%
20	Validation loss: 0.538399	Best loss: 0.203782	Accuracy: 88.00%
21	Validation loss: 0.419986	Best loss: 0.203782	Accuracy: 90.73%
22	Validation loss: 0.426196	Best loss: 0.203782	Accuracy: 91.56%
23	Validation loss: 0.422561	Best loss: 0.203782	Accuracy: 91.20%
24	Validation loss: 0.479832	Best loss: 0.203782	Accuracy: 89.68%
25	Validation loss: 0.386552	Best loss: 0.203782	Accuracy: 89.41%
26	Validation loss: 0.534701	Best loss: 0.203782	Accuracy: 82.92%
27	Validation loss: 0.903240	Best loss: 0.203782	Accuracy: 66.18%
28	Validation loss: 0.782365	Best loss: 0.203782	Accuracy: 77.40%
29	Validation loss: 0.640992	Best loss: 0.203782	Accuracy: 80.02%
30	Validation loss: 0.616284	Best loss: 0.203782	Accuracy: 86.63%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  22.4s
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.324531	Best loss: 0.324531	Accuracy: 93.63%
1	Validation loss: 0.209885	Best loss: 0.209885	Accuracy: 94.92%
2	Validation loss: 0.194788	Best loss: 0.194788	Accuracy: 94.06%
3	Validation loss: 0.196020	Best loss: 0.194788	Accuracy: 93.35%
4	Validation loss: 0.186805	Best loss: 0.186805	Accuracy: 94.10%
5	Validation loss: 0.214956	Best loss: 0.186805	Accuracy: 94.41%
6	Validation loss: 0.221283	Best loss: 0.186805	Accuracy: 94.10%
7	Validation loss: 0.225702	Best loss: 0.186805	Accuracy: 92.73%
8	Validation loss: 0.208256	Best loss: 0.186805	Accuracy: 93.24%
9	Validation loss: 0.204788	Best loss: 0.186805	Accuracy: 93.98%
10	Validation loss: 0.207777	Best loss: 0.186805	Accuracy: 93.35%
11	Validation loss: 0.271108	Best loss: 0.186805	Accuracy: 89.99%
12	Validation loss: 0.288113	Best loss: 0.186805	Accuracy: 90.03%
13	Validation loss: 0.289240	Best loss: 0.186805	Accuracy: 89.13%
14	Validation loss: 0.256782	Best loss: 0.186805	Accuracy: 90.38%
15	Validation loss: 0.264516	Best loss: 0.186805	Accuracy: 92.22%
16	Validation loss: 0.313295	Best loss: 0.186805	Accuracy: 89.17%
17	Validation loss: 0.463192	Best loss: 0.186805	Accuracy: 79.20%
18	Validation loss: 0.322267	Best loss: 0.186805	Accuracy: 87.65%
19	Validation loss: 0.414646	Best loss: 0.186805	Accuracy: 81.86%
20	Validation loss: 0.328160	Best loss: 0.186805	Accuracy: 89.95%
21	Validation loss: 0.326181	Best loss: 0.186805	Accuracy: 92.38%
22	Validation loss: 0.253489	Best loss: 0.186805	Accuracy: 92.65%
23	Validation loss: 0.444482	Best loss: 0.186805	Accuracy: 84.95%
24	Validation loss: 0.465203	Best loss: 0.186805	Accuracy: 79.98%
25	Validation loss: 0.474384	Best loss: 0.186805	Accuracy: 84.56%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  19.9s
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.244293	Best loss: 0.244293	Accuracy: 93.08%
1	Validation loss: 0.234332	Best loss: 0.234332	Accuracy: 93.28%
2	Validation loss: 0.196872	Best loss: 0.196872	Accuracy: 94.10%
3	Validation loss: 0.192541	Best loss: 0.192541	Accuracy: 94.53%
4	Validation loss: 0.239123	Best loss: 0.192541	Accuracy: 93.51%
5	Validation loss: 0.192972	Best loss: 0.192541	Accuracy: 94.64%
6	Validation loss: 0.174657	Best loss: 0.174657	Accuracy: 94.76%
7	Validation loss: 0.183477	Best loss: 0.174657	Accuracy: 95.39%
8	Validation loss: 0.199773	Best loss: 0.174657	Accuracy: 94.14%
9	Validation loss: 0.176983	Best loss: 0.174657	Accuracy: 95.54%
10	Validation loss: 0.203889	Best loss: 0.174657	Accuracy: 94.21%
11	Validation loss: 0.263194	Best loss: 0.174657	Accuracy: 93.86%
12	Validation loss: 0.272398	Best loss: 0.174657	Accuracy: 91.40%
13	Validation loss: 0.274590	Best loss: 0.174657	Accuracy: 92.42%
14	Validation loss: 0.310251	Best loss: 0.174657	Accuracy: 90.07%
15	Validation loss: 0.274308	Best loss: 0.174657	Accuracy: 92.77%
16	Validation loss: 0.232506	Best loss: 0.174657	Accuracy: 93.94%
17	Validation loss: 0.233722	Best loss: 0.174657	Accuracy: 93.94%
18	Validation loss: 0.289859	Best loss: 0.174657	Accuracy: 92.22%
19	Validation loss: 0.249019	Best loss: 0.174657	Accuracy: 94.29%
20	Validation loss: 0.308822	Best loss: 0.174657	Accuracy: 92.61%
21	Validation loss: 0.232590	Best loss: 0.174657	Accuracy: 93.94%
22	Validation loss: 0.400933	Best loss: 0.174657	Accuracy: 90.54%
23	Validation loss: 0.283238	Best loss: 0.174657	Accuracy: 91.99%
24	Validation loss: 0.416557	Best loss: 0.174657	Accuracy: 85.50%
25	Validation loss: 0.595794	Best loss: 0.174657	Accuracy: 71.85%
26	Validation loss: 0.607579	Best loss: 0.174657	Accuracy: 77.17%
27	Validation loss: 0.819190	Best loss: 0.174657	Accuracy: 73.65%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  21.0s
[CV] n_neurons=50, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.743772	Best loss: 1.743772	Accuracy: 20.91%
1	Validation loss: 1.638517	Best loss: 1.638517	Accuracy: 20.91%
2	Validation loss: 1.830034	Best loss: 1.638517	Accuracy: 18.73%
3	Validation loss: 1.647192	Best loss: 1.638517	Accuracy: 19.27%
4	Validation loss: 1.789264	Best loss: 1.638517	Accuracy: 19.27%
5	Validation loss: 2.003238	Best loss: 1.638517	Accuracy: 19.08%
6	Validation loss: 1.860392	Best loss: 1.638517	Accuracy: 22.01%
7	Validation loss: 1.950568	Best loss: 1.638517	Accuracy: 19.08%
8	Validation loss: 1.712141	Best loss: 1.638517	Accuracy: 20.91%
9	Validation loss: 1.768289	Best loss: 1.638517	Accuracy: 22.01%
10	Validation loss: 1.761620	Best loss: 1.638517	Accuracy: 19.08%
11	Validation loss: 1.842432	Best loss: 1.638517	Accuracy: 22.01%
12	Validation loss: 4.984125	Best loss: 1.638517	Accuracy: 20.91%
13	Validation loss: 1.706743	Best loss: 1.638517	Accuracy: 19.27%
14	Validation loss: 1.815742	Best loss: 1.638517	Accuracy: 19.08%
15	Validation loss: 2.044712	Best loss: 1.638517	Accuracy: 22.01%
16	Validation loss: 1.929189	Best loss: 1.638517	Accuracy: 18.73%
17	Validation loss: 1.676588	Best loss: 1.638517	Accuracy: 20.91%
18	Validation loss: 2.045578	Best loss: 1.638517	Accuracy: 19.08%
19	Validation loss: 1.967372	Best loss: 1.638517	Accuracy: 22.01%
20	Validation loss: 1.706726	Best loss: 1.638517	Accuracy: 22.01%
21	Validation loss: 2.047941	Best loss: 1.638517	Accuracy: 20.91%
22	Validation loss: 1.898497	Best loss: 1.638517	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total=  48.5s
[CV] n_neurons=50, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.689421	Best loss: 1.689421	Accuracy: 20.91%
1	Validation loss: 1.710632	Best loss: 1.689421	Accuracy: 19.27%
2	Validation loss: 1.890311	Best loss: 1.689421	Accuracy: 19.08%
3	Validation loss: 1.746993	Best loss: 1.689421	Accuracy: 22.01%
4	Validation loss: 2.181673	Best loss: 1.689421	Accuracy: 19.08%
5	Validation loss: 1.673092	Best loss: 1.673092	Accuracy: 19.08%
6	Validation loss: 1.890988	Best loss: 1.673092	Accuracy: 20.91%
7	Validation loss: 2.104563	Best loss: 1.673092	Accuracy: 18.73%
8	Validation loss: 1.834593	Best loss: 1.673092	Accuracy: 19.08%
9	Validation loss: 1.689976	Best loss: 1.673092	Accuracy: 20.91%
10	Validation loss: 1.977930	Best loss: 1.673092	Accuracy: 19.08%
11	Validation loss: 1.965271	Best loss: 1.673092	Accuracy: 19.08%
12	Validation loss: 1.930842	Best loss: 1.673092	Accuracy: 20.91%
13	Validation loss: 2.045287	Best loss: 1.673092	Accuracy: 22.01%
14	Validation loss: 1.769907	Best loss: 1.673092	Accuracy: 19.27%
15	Validation loss: 1.926087	Best loss: 1.673092	Accuracy: 20.91%
16	Validation loss: 1.928153	Best loss: 1.673092	Accuracy: 18.73%
17	Validation loss: 1.841899	Best loss: 1.673092	Accuracy: 20.91%
18	Validation loss: 1.962566	Best loss: 1.673092	Accuracy: 19.08%
19	Validation loss: 1.782029	Best loss: 1.673092	Accuracy: 19.27%
20	Validation loss: 2.106779	Best loss: 1.673092	Accuracy: 19.27%
21	Validation loss: 1.807729	Best loss: 1.673092	Accuracy: 19.27%
22	Validation loss: 2.142819	Best loss: 1.673092	Accuracy: 19.27%
23	Validation loss: 2.349216	Best loss: 1.673092	Accuracy: 22.01%
24	Validation loss: 1.758513	Best loss: 1.673092	Accuracy: 19.27%
25	Validation loss: 2.075436	Best loss: 1.673092	Accuracy: 19.08%
26	Validation loss: 2.522594	Best loss: 1.673092	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.1min
[CV] n_neurons=50, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.635975	Best loss: 1.635975	Accuracy: 19.27%
1	Validation loss: 1.742908	Best loss: 1.635975	Accuracy: 19.08%
2	Validation loss: 1.925470	Best loss: 1.635975	Accuracy: 19.08%
3	Validation loss: 1.862457	Best loss: 1.635975	Accuracy: 19.27%
4	Validation loss: 2.467992	Best loss: 1.635975	Accuracy: 19.08%
5	Validation loss: 1.888687	Best loss: 1.635975	Accuracy: 20.91%
6	Validation loss: 2.019772	Best loss: 1.635975	Accuracy: 19.27%
7	Validation loss: 1.683326	Best loss: 1.635975	Accuracy: 19.08%
8	Validation loss: 1.755415	Best loss: 1.635975	Accuracy: 19.27%
9	Validation loss: 2.175207	Best loss: 1.635975	Accuracy: 22.01%
10	Validation loss: 1.954771	Best loss: 1.635975	Accuracy: 18.73%
11	Validation loss: 1.955110	Best loss: 1.635975	Accuracy: 19.27%
12	Validation loss: 2.019582	Best loss: 1.635975	Accuracy: 20.91%
13	Validation loss: 3.821905	Best loss: 1.635975	Accuracy: 19.08%
14	Validation loss: 2.097609	Best loss: 1.635975	Accuracy: 19.08%
15	Validation loss: 1.805575	Best loss: 1.635975	Accuracy: 19.27%
16	Validation loss: 1.781908	Best loss: 1.635975	Accuracy: 18.73%
17	Validation loss: 2.049681	Best loss: 1.635975	Accuracy: 18.73%
18	Validation loss: 1.894295	Best loss: 1.635975	Accuracy: 19.27%
19	Validation loss: 2.161113	Best loss: 1.635975	Accuracy: 22.01%
20	Validation loss: 1.989593	Best loss: 1.635975	Accuracy: 19.27%
21	Validation loss: 2.116996	Best loss: 1.635975	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=50, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function elu at 0x1243639d8&gt;, total= 1.3min
[CV] n_neurons=90, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 488102.031250	Best loss: 488102.031250	Accuracy: 20.80%
1	Validation loss: 156990112.000000	Best loss: 488102.031250	Accuracy: 19.27%
2	Validation loss: 11010183.000000	Best loss: 488102.031250	Accuracy: 18.73%
3	Validation loss: 3508394.500000	Best loss: 488102.031250	Accuracy: 19.27%
4	Validation loss: 1338661.250000	Best loss: 488102.031250	Accuracy: 19.27%
5	Validation loss: 45943056.000000	Best loss: 488102.031250	Accuracy: 22.01%
6	Validation loss: 23843680.000000	Best loss: 488102.031250	Accuracy: 22.01%
7	Validation loss: 3745599.500000	Best loss: 488102.031250	Accuracy: 19.27%
8	Validation loss: 1286168.875000	Best loss: 488102.031250	Accuracy: 19.27%
9	Validation loss: 2929706.750000	Best loss: 488102.031250	Accuracy: 18.73%
10	Validation loss: 27972180.000000	Best loss: 488102.031250	Accuracy: 20.91%
11	Validation loss: 2417124.250000	Best loss: 488102.031250	Accuracy: 19.08%
12	Validation loss: 5616681.000000	Best loss: 488102.031250	Accuracy: 19.08%
13	Validation loss: 3207325.750000	Best loss: 488102.031250	Accuracy: 19.27%
14	Validation loss: 9231683.000000	Best loss: 488102.031250	Accuracy: 19.00%
15	Validation loss: 2491051.750000	Best loss: 488102.031250	Accuracy: 18.73%
16	Validation loss: 36035524.000000	Best loss: 488102.031250	Accuracy: 19.08%
17	Validation loss: 3783081.500000	Best loss: 488102.031250	Accuracy: 19.27%
18	Validation loss: 3986430.750000	Best loss: 488102.031250	Accuracy: 22.01%
19	Validation loss: 3196385.500000	Best loss: 488102.031250	Accuracy: 22.01%
20	Validation loss: 9786606.000000	Best loss: 488102.031250	Accuracy: 18.73%
21	Validation loss: 13621503.000000	Best loss: 488102.031250	Accuracy: 18.73%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 2.2min
[CV] n_neurons=90, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 5066916.500000	Best loss: 5066916.500000	Accuracy: 20.91%
1	Validation loss: 13983915.000000	Best loss: 5066916.500000	Accuracy: 20.91%
2	Validation loss: 963630.812500	Best loss: 963630.812500	Accuracy: 18.73%
3	Validation loss: 913946.750000	Best loss: 913946.750000	Accuracy: 22.01%
4	Validation loss: 697640.187500	Best loss: 697640.187500	Accuracy: 20.91%
5	Validation loss: 4886581.500000	Best loss: 697640.187500	Accuracy: 19.27%
6	Validation loss: 20235784.000000	Best loss: 697640.187500	Accuracy: 19.27%
7	Validation loss: 2669084.500000	Best loss: 697640.187500	Accuracy: 19.08%
8	Validation loss: 35018548.000000	Best loss: 697640.187500	Accuracy: 22.01%
9	Validation loss: 1344798.000000	Best loss: 697640.187500	Accuracy: 20.91%
10	Validation loss: 89761888.000000	Best loss: 697640.187500	Accuracy: 19.27%
11	Validation loss: 7566340.000000	Best loss: 697640.187500	Accuracy: 19.31%
12	Validation loss: 2397513.000000	Best loss: 697640.187500	Accuracy: 22.01%
13	Validation loss: 1954556.625000	Best loss: 697640.187500	Accuracy: 19.08%
14	Validation loss: 19872228.000000	Best loss: 697640.187500	Accuracy: 20.91%
15	Validation loss: 23090310.000000	Best loss: 697640.187500	Accuracy: 19.08%
16	Validation loss: 5655333.000000	Best loss: 697640.187500	Accuracy: 22.01%
17	Validation loss: 10031884.000000	Best loss: 697640.187500	Accuracy: 18.73%
18	Validation loss: 3942670.500000	Best loss: 697640.187500	Accuracy: 19.08%
19	Validation loss: 27617416.000000	Best loss: 697640.187500	Accuracy: 19.27%
20	Validation loss: 7526598.500000	Best loss: 697640.187500	Accuracy: 19.27%
21	Validation loss: 3372480.000000	Best loss: 697640.187500	Accuracy: 20.91%
22	Validation loss: 2140867.750000	Best loss: 697640.187500	Accuracy: 18.73%
23	Validation loss: 22021900.000000	Best loss: 697640.187500	Accuracy: 19.27%
24	Validation loss: 2621114.500000	Best loss: 697640.187500	Accuracy: 20.91%
25	Validation loss: 3022095.500000	Best loss: 697640.187500	Accuracy: 30.06%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 2.6min
[CV] n_neurons=90, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 503877.593750	Best loss: 503877.593750	Accuracy: 18.76%
1	Validation loss: 493534.468750	Best loss: 493534.468750	Accuracy: 19.08%
2	Validation loss: 4382058.000000	Best loss: 493534.468750	Accuracy: 18.73%
3	Validation loss: 1171332.250000	Best loss: 493534.468750	Accuracy: 19.27%
4	Validation loss: 5603257.000000	Best loss: 493534.468750	Accuracy: 19.08%
5	Validation loss: 6331427.000000	Best loss: 493534.468750	Accuracy: 19.27%
6	Validation loss: 41342412.000000	Best loss: 493534.468750	Accuracy: 14.93%
7	Validation loss: 3896957.250000	Best loss: 493534.468750	Accuracy: 20.91%
8	Validation loss: 15115004.000000	Best loss: 493534.468750	Accuracy: 22.01%
9	Validation loss: 5230519.000000	Best loss: 493534.468750	Accuracy: 22.01%
10	Validation loss: 19324680.000000	Best loss: 493534.468750	Accuracy: 18.73%
11	Validation loss: 1387269.875000	Best loss: 493534.468750	Accuracy: 20.91%
12	Validation loss: 2686752.250000	Best loss: 493534.468750	Accuracy: 18.73%
13	Validation loss: 2755508.750000	Best loss: 493534.468750	Accuracy: 20.91%
14	Validation loss: 26462768.000000	Best loss: 493534.468750	Accuracy: 22.01%
15	Validation loss: 12142265.000000	Best loss: 493534.468750	Accuracy: 17.94%
16	Validation loss: 16718970.000000	Best loss: 493534.468750	Accuracy: 19.08%
17	Validation loss: 61330764.000000	Best loss: 493534.468750	Accuracy: 20.91%
18	Validation loss: 6999365.000000	Best loss: 493534.468750	Accuracy: 22.01%
19	Validation loss: 3847706.500000	Best loss: 493534.468750	Accuracy: 20.91%
20	Validation loss: 23865140.000000	Best loss: 493534.468750	Accuracy: 18.73%
21	Validation loss: 7178264.000000	Best loss: 493534.468750	Accuracy: 22.01%
22	Validation loss: 6286273.500000	Best loss: 493534.468750	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 2.4min
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.613155	Best loss: 1.613155	Accuracy: 18.73%
1	Validation loss: 1.614058	Best loss: 1.613155	Accuracy: 19.27%
2	Validation loss: 1.609805	Best loss: 1.609805	Accuracy: 22.01%
3	Validation loss: 1.608039	Best loss: 1.608039	Accuracy: 22.01%
4	Validation loss: 1.619205	Best loss: 1.608039	Accuracy: 22.01%
5	Validation loss: 1.616663	Best loss: 1.608039	Accuracy: 22.01%
6	Validation loss: 1.612917	Best loss: 1.608039	Accuracy: 19.27%
7	Validation loss: 1.615297	Best loss: 1.608039	Accuracy: 22.01%
8	Validation loss: 1.621617	Best loss: 1.608039	Accuracy: 19.27%
9	Validation loss: 1.611601	Best loss: 1.608039	Accuracy: 22.01%
10	Validation loss: 1.608234	Best loss: 1.608039	Accuracy: 22.01%
11	Validation loss: 1.609519	Best loss: 1.608039	Accuracy: 22.01%
12	Validation loss: 1.608590	Best loss: 1.608039	Accuracy: 22.01%
13	Validation loss: 1.612133	Best loss: 1.608039	Accuracy: 18.73%
14	Validation loss: 1.610413	Best loss: 1.608039	Accuracy: 22.01%
15	Validation loss: 1.609214	Best loss: 1.608039	Accuracy: 22.01%
16	Validation loss: 1.608218	Best loss: 1.608039	Accuracy: 22.01%
17	Validation loss: 1.611043	Best loss: 1.608039	Accuracy: 19.27%
18	Validation loss: 1.614793	Best loss: 1.608039	Accuracy: 22.01%
19	Validation loss: 1.610234	Best loss: 1.608039	Accuracy: 19.08%
20	Validation loss: 1.610605	Best loss: 1.608039	Accuracy: 20.91%
21	Validation loss: 1.617262	Best loss: 1.608039	Accuracy: 19.27%
22	Validation loss: 1.612227	Best loss: 1.608039	Accuracy: 19.27%
23	Validation loss: 1.611049	Best loss: 1.608039	Accuracy: 19.27%
24	Validation loss: 1.614537	Best loss: 1.608039	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total= 2.7min
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.613290	Best loss: 1.613290	Accuracy: 22.01%
1	Validation loss: 1.615555	Best loss: 1.613290	Accuracy: 19.27%
2	Validation loss: 1.609274	Best loss: 1.609274	Accuracy: 22.01%
3	Validation loss: 1.613204	Best loss: 1.609274	Accuracy: 18.73%
4	Validation loss: 1.612056	Best loss: 1.609274	Accuracy: 22.01%
5	Validation loss: 1.610147	Best loss: 1.609274	Accuracy: 22.01%
6	Validation loss: 1.621744	Best loss: 1.609274	Accuracy: 19.08%
7	Validation loss: 1.612059	Best loss: 1.609274	Accuracy: 18.73%
8	Validation loss: 1.611333	Best loss: 1.609274	Accuracy: 22.01%
9	Validation loss: 1.608817	Best loss: 1.608817	Accuracy: 22.01%
10	Validation loss: 1.610080	Best loss: 1.608817	Accuracy: 22.01%
11	Validation loss: 1.610639	Best loss: 1.608817	Accuracy: 22.01%
12	Validation loss: 1.610745	Best loss: 1.608817	Accuracy: 19.27%
13	Validation loss: 1.611639	Best loss: 1.608817	Accuracy: 19.08%
14	Validation loss: 1.608832	Best loss: 1.608817	Accuracy: 22.01%
15	Validation loss: 1.608107	Best loss: 1.608107	Accuracy: 22.01%
16	Validation loss: 1.613538	Best loss: 1.608107	Accuracy: 22.01%
17	Validation loss: 1.611919	Best loss: 1.608107	Accuracy: 18.73%
18	Validation loss: 1.614612	Best loss: 1.608107	Accuracy: 22.01%
19	Validation loss: 1.608660	Best loss: 1.608107	Accuracy: 22.01%
20	Validation loss: 1.609363	Best loss: 1.608107	Accuracy: 22.01%
21	Validation loss: 1.619997	Best loss: 1.608107	Accuracy: 22.01%
22	Validation loss: 1.620809	Best loss: 1.608107	Accuracy: 19.27%
23	Validation loss: 1.618629	Best loss: 1.608107	Accuracy: 22.01%
24	Validation loss: 1.613189	Best loss: 1.608107	Accuracy: 22.01%
25	Validation loss: 1.608848	Best loss: 1.608107	Accuracy: 22.01%
26	Validation loss: 1.608850	Best loss: 1.608107	Accuracy: 22.01%
27	Validation loss: 1.612869	Best loss: 1.608107	Accuracy: 22.01%
28	Validation loss: 1.622143	Best loss: 1.608107	Accuracy: 19.08%
29	Validation loss: 1.609205	Best loss: 1.608107	Accuracy: 22.01%
30	Validation loss: 1.607766	Best loss: 1.607766	Accuracy: 20.91%
31	Validation loss: 1.613591	Best loss: 1.607766	Accuracy: 18.73%
32	Validation loss: 1.609265	Best loss: 1.607766	Accuracy: 22.01%
33	Validation loss: 1.609561	Best loss: 1.607766	Accuracy: 22.01%
34	Validation loss: 1.610186	Best loss: 1.607766	Accuracy: 22.01%
35	Validation loss: 1.609367	Best loss: 1.607766	Accuracy: 22.01%
36	Validation loss: 1.610132	Best loss: 1.607766	Accuracy: 19.27%
37	Validation loss: 1.612200	Best loss: 1.607766	Accuracy: 22.01%
38	Validation loss: 1.612036	Best loss: 1.607766	Accuracy: 22.01%
39	Validation loss: 1.614104	Best loss: 1.607766	Accuracy: 19.27%
40	Validation loss: 1.608475	Best loss: 1.607766	Accuracy: 22.01%
41	Validation loss: 1.609538	Best loss: 1.607766	Accuracy: 22.01%
42	Validation loss: 1.610706	Best loss: 1.607766	Accuracy: 19.27%
43	Validation loss: 1.614066	Best loss: 1.607766	Accuracy: 22.01%
44	Validation loss: 1.616753	Best loss: 1.607766	Accuracy: 22.01%
45	Validation loss: 1.610697	Best loss: 1.607766	Accuracy: 22.01%
46	Validation loss: 1.612054	Best loss: 1.607766	Accuracy: 18.73%
47	Validation loss: 1.608926	Best loss: 1.607766	Accuracy: 22.01%
48	Validation loss: 1.608290	Best loss: 1.607766	Accuracy: 22.01%
49	Validation loss: 1.617472	Best loss: 1.607766	Accuracy: 19.27%
50	Validation loss: 1.612358	Best loss: 1.607766	Accuracy: 19.27%
51	Validation loss: 1.610515	Best loss: 1.607766	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total= 5.7min
[CV] n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 1.615152	Best loss: 1.615152	Accuracy: 22.01%
1	Validation loss: 1.613291	Best loss: 1.613291	Accuracy: 19.27%
2	Validation loss: 1.613153	Best loss: 1.613153	Accuracy: 19.27%
3	Validation loss: 1.618712	Best loss: 1.613153	Accuracy: 22.01%
4	Validation loss: 1.608355	Best loss: 1.608355	Accuracy: 22.01%
5	Validation loss: 1.612531	Best loss: 1.608355	Accuracy: 22.01%
6	Validation loss: 1.613943	Best loss: 1.608355	Accuracy: 19.27%
7	Validation loss: 1.612980	Best loss: 1.608355	Accuracy: 22.01%
8	Validation loss: 1.611474	Best loss: 1.608355	Accuracy: 22.01%
9	Validation loss: 1.610276	Best loss: 1.608355	Accuracy: 22.01%
10	Validation loss: 1.610115	Best loss: 1.608355	Accuracy: 22.01%
11	Validation loss: 1.615095	Best loss: 1.608355	Accuracy: 22.01%
12	Validation loss: 1.609140	Best loss: 1.608355	Accuracy: 20.91%
13	Validation loss: 1.612666	Best loss: 1.608355	Accuracy: 19.27%
14	Validation loss: 1.610129	Best loss: 1.608355	Accuracy: 20.91%
15	Validation loss: 1.611325	Best loss: 1.608355	Accuracy: 22.01%
16	Validation loss: 1.620832	Best loss: 1.608355	Accuracy: 22.01%
17	Validation loss: 1.614156	Best loss: 1.608355	Accuracy: 19.08%
18	Validation loss: 1.607981	Best loss: 1.607981	Accuracy: 22.01%
19	Validation loss: 1.610185	Best loss: 1.607981	Accuracy: 19.08%
20	Validation loss: 1.612588	Best loss: 1.607981	Accuracy: 19.27%
21	Validation loss: 1.620057	Best loss: 1.607981	Accuracy: 22.01%
22	Validation loss: 1.611421	Best loss: 1.607981	Accuracy: 22.01%
23	Validation loss: 1.614215	Best loss: 1.607981	Accuracy: 22.01%
24	Validation loss: 1.612511	Best loss: 1.607981	Accuracy: 20.91%
25	Validation loss: 1.609961	Best loss: 1.607981	Accuracy: 22.01%
26	Validation loss: 1.607881	Best loss: 1.607881	Accuracy: 22.01%
27	Validation loss: 1.610301	Best loss: 1.607881	Accuracy: 22.01%
28	Validation loss: 1.617164	Best loss: 1.607881	Accuracy: 19.08%
29	Validation loss: 1.610929	Best loss: 1.607881	Accuracy: 22.01%
30	Validation loss: 1.609473	Best loss: 1.607881	Accuracy: 19.08%
31	Validation loss: 1.609233	Best loss: 1.607881	Accuracy: 22.01%
32	Validation loss: 1.609479	Best loss: 1.607881	Accuracy: 22.01%
33	Validation loss: 1.609838	Best loss: 1.607881	Accuracy: 22.01%
34	Validation loss: 1.609303	Best loss: 1.607881	Accuracy: 22.01%
35	Validation loss: 1.610944	Best loss: 1.607881	Accuracy: 22.01%
36	Validation loss: 1.610505	Best loss: 1.607881	Accuracy: 19.27%
37	Validation loss: 1.608579	Best loss: 1.607881	Accuracy: 22.01%
38	Validation loss: 1.612431	Best loss: 1.607881	Accuracy: 22.01%
39	Validation loss: 1.616539	Best loss: 1.607881	Accuracy: 19.27%
40	Validation loss: 1.610357	Best loss: 1.607881	Accuracy: 19.08%
41	Validation loss: 1.613420	Best loss: 1.607881	Accuracy: 19.08%
42	Validation loss: 1.610971	Best loss: 1.607881	Accuracy: 22.01%
43	Validation loss: 1.609562	Best loss: 1.607881	Accuracy: 20.91%
44	Validation loss: 1.612211	Best loss: 1.607881	Accuracy: 22.01%
45	Validation loss: 1.609118	Best loss: 1.607881	Accuracy: 22.01%
46	Validation loss: 1.608849	Best loss: 1.607881	Accuracy: 22.01%
47	Validation loss: 1.625010	Best loss: 1.607881	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.02, dropout_rate=0.5, batch_size=10, activation=&lt;function relu at 0x124366d08&gt;, total= 5.2min
[CV] n_neurons=140, learning_rate=0.01, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 70.235039	Best loss: 70.235039	Accuracy: 20.88%
1	Validation loss: 34.701412	Best loss: 34.701412	Accuracy: 54.18%
2	Validation loss: 32.568542	Best loss: 32.568542	Accuracy: 38.23%
3	Validation loss: 159.865921	Best loss: 32.568542	Accuracy: 38.82%
4	Validation loss: 56.997650	Best loss: 32.568542	Accuracy: 40.58%
5	Validation loss: 28.031391	Best loss: 28.031391	Accuracy: 42.69%
6	Validation loss: 105.481880	Best loss: 28.031391	Accuracy: 38.74%
7	Validation loss: 217.837784	Best loss: 28.031391	Accuracy: 40.03%
8	Validation loss: 190.927109	Best loss: 28.031391	Accuracy: 37.61%
9	Validation loss: 205.049454	Best loss: 28.031391	Accuracy: 46.79%
10	Validation loss: 33.361813	Best loss: 28.031391	Accuracy: 53.91%
11	Validation loss: 809.312439	Best loss: 28.031391	Accuracy: 51.45%
12	Validation loss: 164.849686	Best loss: 28.031391	Accuracy: 48.94%
13	Validation loss: 81.589592	Best loss: 28.031391	Accuracy: 55.59%
14	Validation loss: 149.853806	Best loss: 28.031391	Accuracy: 54.93%
15	Validation loss: 255.298157	Best loss: 28.031391	Accuracy: 53.01%
16	Validation loss: 307.256317	Best loss: 28.031391	Accuracy: 50.00%
17	Validation loss: 113.893166	Best loss: 28.031391	Accuracy: 49.41%
18	Validation loss: 279.426056	Best loss: 28.031391	Accuracy: 48.24%
19	Validation loss: 49.165943	Best loss: 28.031391	Accuracy: 56.37%
20	Validation loss: 109.890564	Best loss: 28.031391	Accuracy: 50.94%
21	Validation loss: 193.222198	Best loss: 28.031391	Accuracy: 54.85%
22	Validation loss: 329.097931	Best loss: 28.031391	Accuracy: 50.00%
23	Validation loss: 509.650116	Best loss: 28.031391	Accuracy: 47.34%
24	Validation loss: 4475.048340	Best loss: 28.031391	Accuracy: 55.43%
25	Validation loss: 315.571381	Best loss: 28.031391	Accuracy: 49.57%
26	Validation loss: 250.644867	Best loss: 28.031391	Accuracy: 50.55%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 3.2min
[CV] n_neurons=140, learning_rate=0.01, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 264.634491	Best loss: 264.634491	Accuracy: 36.79%
1	Validation loss: 31.269567	Best loss: 31.269567	Accuracy: 38.47%
2	Validation loss: 5.032660	Best loss: 5.032660	Accuracy: 46.76%
3	Validation loss: 24.762312	Best loss: 5.032660	Accuracy: 29.55%
4	Validation loss: 59.893024	Best loss: 5.032660	Accuracy: 37.26%
5	Validation loss: 14.433213	Best loss: 5.032660	Accuracy: 38.51%
6	Validation loss: 16.628532	Best loss: 5.032660	Accuracy: 39.48%
7	Validation loss: 402.433136	Best loss: 5.032660	Accuracy: 32.53%
8	Validation loss: 49.696728	Best loss: 5.032660	Accuracy: 48.48%
9	Validation loss: 35.744431	Best loss: 5.032660	Accuracy: 39.84%
10	Validation loss: 51.352245	Best loss: 5.032660	Accuracy: 41.52%
11	Validation loss: 106.050507	Best loss: 5.032660	Accuracy: 48.28%
12	Validation loss: 104.812477	Best loss: 5.032660	Accuracy: 50.78%
13	Validation loss: 204.523132	Best loss: 5.032660	Accuracy: 45.82%
14	Validation loss: 66.180931	Best loss: 5.032660	Accuracy: 55.98%
15	Validation loss: 262.672974	Best loss: 5.032660	Accuracy: 53.40%
16	Validation loss: 1125.192871	Best loss: 5.032660	Accuracy: 53.05%
17	Validation loss: 389.939514	Best loss: 5.032660	Accuracy: 44.29%
18	Validation loss: 132.097382	Best loss: 5.032660	Accuracy: 59.66%
19	Validation loss: 77.651207	Best loss: 5.032660	Accuracy: 52.81%
20	Validation loss: 341.028748	Best loss: 5.032660	Accuracy: 48.79%
21	Validation loss: 966.118652	Best loss: 5.032660	Accuracy: 50.98%
22	Validation loss: 435.432648	Best loss: 5.032660	Accuracy: 43.86%
23	Validation loss: 432.655640	Best loss: 5.032660	Accuracy: 57.86%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 2.8min
[CV] n_neurons=140, learning_rate=0.01, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 18.320158	Best loss: 18.320158	Accuracy: 34.64%
1	Validation loss: 5.529373	Best loss: 5.529373	Accuracy: 50.47%
2	Validation loss: 164.425690	Best loss: 5.529373	Accuracy: 51.92%
3	Validation loss: 40.778145	Best loss: 5.529373	Accuracy: 49.37%
4	Validation loss: 56.675503	Best loss: 5.529373	Accuracy: 38.94%
5	Validation loss: 29.435873	Best loss: 5.529373	Accuracy: 42.77%
6	Validation loss: 23.186413	Best loss: 5.529373	Accuracy: 52.58%
7	Validation loss: 65.966354	Best loss: 5.529373	Accuracy: 53.21%
8	Validation loss: 15.271042	Best loss: 5.529373	Accuracy: 53.24%
9	Validation loss: 2535.231689	Best loss: 5.529373	Accuracy: 41.63%
10	Validation loss: 1227.589478	Best loss: 5.529373	Accuracy: 35.07%
11	Validation loss: 127.928604	Best loss: 5.529373	Accuracy: 47.03%
12	Validation loss: 70.767494	Best loss: 5.529373	Accuracy: 53.83%
13	Validation loss: 84.746979	Best loss: 5.529373	Accuracy: 53.36%
14	Validation loss: 108.576637	Best loss: 5.529373	Accuracy: 50.27%
15	Validation loss: 80.176514	Best loss: 5.529373	Accuracy: 51.21%
16	Validation loss: 571.165222	Best loss: 5.529373	Accuracy: 55.98%
17	Validation loss: 223.449295	Best loss: 5.529373	Accuracy: 50.39%
18	Validation loss: 2331.108887	Best loss: 5.529373	Accuracy: 38.47%
19	Validation loss: 335.513977	Best loss: 5.529373	Accuracy: 45.97%
20	Validation loss: 147.170853	Best loss: 5.529373	Accuracy: 41.52%
21	Validation loss: 204.575256	Best loss: 5.529373	Accuracy: 47.15%
22	Validation loss: 222.944412	Best loss: 5.529373	Accuracy: 55.16%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.01, dropout_rate=0.5, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 2.8min
[CV] n_neurons=160, learning_rate=0.1, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 98.657791	Best loss: 98.657791	Accuracy: 70.41%
1	Validation loss: 28.665407	Best loss: 28.665407	Accuracy: 67.24%
2	Validation loss: 6.669576	Best loss: 6.669576	Accuracy: 84.21%
3	Validation loss: 4.452104	Best loss: 4.452104	Accuracy: 82.33%
4	Validation loss: 6.558619	Best loss: 4.452104	Accuracy: 76.86%
5	Validation loss: 3.030284	Best loss: 3.030284	Accuracy: 84.32%
6	Validation loss: 3.844363	Best loss: 3.030284	Accuracy: 78.11%
7	Validation loss: 1.395688	Best loss: 1.395688	Accuracy: 88.82%
8	Validation loss: 1.153480	Best loss: 1.153480	Accuracy: 92.69%
9	Validation loss: 1.433986	Best loss: 1.153480	Accuracy: 92.65%
10	Validation loss: 1.972766	Best loss: 1.153480	Accuracy: 92.81%
11	Validation loss: 6.182434	Best loss: 1.153480	Accuracy: 76.94%
12	Validation loss: 2.090171	Best loss: 1.153480	Accuracy: 84.48%
13	Validation loss: 0.947649	Best loss: 0.947649	Accuracy: 94.64%
14	Validation loss: 0.923766	Best loss: 0.923766	Accuracy: 89.60%
15	Validation loss: 0.686206	Best loss: 0.686206	Accuracy: 93.20%
16	Validation loss: 0.714990	Best loss: 0.686206	Accuracy: 95.04%
17	Validation loss: 0.745775	Best loss: 0.686206	Accuracy: 93.78%
18	Validation loss: 0.588278	Best loss: 0.588278	Accuracy: 94.02%
19	Validation loss: 0.827824	Best loss: 0.588278	Accuracy: 94.33%
20	Validation loss: 0.803478	Best loss: 0.588278	Accuracy: 92.03%
21	Validation loss: 0.649197	Best loss: 0.588278	Accuracy: 95.66%
22	Validation loss: 1.302945	Best loss: 0.588278	Accuracy: 81.94%
23	Validation loss: 0.434803	Best loss: 0.434803	Accuracy: 95.19%
24	Validation loss: 0.660031	Best loss: 0.434803	Accuracy: 94.80%
25	Validation loss: 1.186935	Best loss: 0.434803	Accuracy: 91.79%
26	Validation loss: 0.737538	Best loss: 0.434803	Accuracy: 93.94%
27	Validation loss: 0.555993	Best loss: 0.434803	Accuracy: 94.72%
28	Validation loss: 0.601967	Best loss: 0.434803	Accuracy: 95.19%
29	Validation loss: 0.593964	Best loss: 0.434803	Accuracy: 93.63%
30	Validation loss: 0.356613	Best loss: 0.356613	Accuracy: 96.05%
31	Validation loss: 1.831482	Best loss: 0.356613	Accuracy: 93.94%
32	Validation loss: 0.890014	Best loss: 0.356613	Accuracy: 95.54%
33	Validation loss: 0.888130	Best loss: 0.356613	Accuracy: 95.43%
34	Validation loss: 0.445143	Best loss: 0.356613	Accuracy: 95.66%
35	Validation loss: 0.541953	Best loss: 0.356613	Accuracy: 94.80%
36	Validation loss: 0.376429	Best loss: 0.356613	Accuracy: 95.54%
37	Validation loss: 0.325948	Best loss: 0.325948	Accuracy: 95.86%
38	Validation loss: 0.323365	Best loss: 0.323365	Accuracy: 95.58%
39	Validation loss: 0.488066	Best loss: 0.323365	Accuracy: 93.67%
40	Validation loss: 0.442327	Best loss: 0.323365	Accuracy: 95.62%
41	Validation loss: 0.335375	Best loss: 0.323365	Accuracy: 95.66%
42	Validation loss: 0.258489	Best loss: 0.258489	Accuracy: 96.05%
43	Validation loss: 0.294255	Best loss: 0.258489	Accuracy: 95.58%
44	Validation loss: 0.258181	Best loss: 0.258181	Accuracy: 95.35%
45	Validation loss: 0.303850	Best loss: 0.258181	Accuracy: 94.64%
46	Validation loss: 0.933808	Best loss: 0.258181	Accuracy: 93.90%
47	Validation loss: 0.851909	Best loss: 0.258181	Accuracy: 93.12%
48	Validation loss: 0.685188	Best loss: 0.258181	Accuracy: 94.02%
49	Validation loss: 0.337756	Best loss: 0.258181	Accuracy: 95.35%
50	Validation loss: 0.361409	Best loss: 0.258181	Accuracy: 94.84%
51	Validation loss: 0.352136	Best loss: 0.258181	Accuracy: 94.53%
52	Validation loss: 0.323220	Best loss: 0.258181	Accuracy: 94.61%
53	Validation loss: 0.208100	Best loss: 0.208100	Accuracy: 95.82%
54	Validation loss: 0.185712	Best loss: 0.185712	Accuracy: 95.82%
55	Validation loss: 0.235771	Best loss: 0.185712	Accuracy: 96.21%
56	Validation loss: 0.227975	Best loss: 0.185712	Accuracy: 95.62%
57	Validation loss: 0.179293	Best loss: 0.179293	Accuracy: 96.13%
58	Validation loss: 0.232773	Best loss: 0.179293	Accuracy: 96.09%
59	Validation loss: 0.309207	Best loss: 0.179293	Accuracy: 94.18%
60	Validation loss: 0.234428	Best loss: 0.179293	Accuracy: 96.36%
61	Validation loss: 875266304.000000	Best loss: 0.179293	Accuracy: 18.57%
62	Validation loss: 4827108.500000	Best loss: 0.179293	Accuracy: 42.14%
63	Validation loss: 780546.375000	Best loss: 0.179293	Accuracy: 76.08%
64	Validation loss: 252531.640625	Best loss: 0.179293	Accuracy: 81.24%
65	Validation loss: 414212.093750	Best loss: 0.179293	Accuracy: 70.88%
66	Validation loss: 98597.625000	Best loss: 0.179293	Accuracy: 84.99%
67	Validation loss: 333355.125000	Best loss: 0.179293	Accuracy: 76.90%
68	Validation loss: 326677.968750	Best loss: 0.179293	Accuracy: 66.22%
69	Validation loss: 246720.078125	Best loss: 0.179293	Accuracy: 76.08%
70	Validation loss: 686794.250000	Best loss: 0.179293	Accuracy: 55.08%
71	Validation loss: 288627.500000	Best loss: 0.179293	Accuracy: 67.87%
72	Validation loss: 331922.625000	Best loss: 0.179293	Accuracy: 68.45%
73	Validation loss: 157857.953125	Best loss: 0.179293	Accuracy: 72.63%
74	Validation loss: 108006.851562	Best loss: 0.179293	Accuracy: 81.78%
75	Validation loss: 170232.078125	Best loss: 0.179293	Accuracy: 74.59%
76	Validation loss: 90200.703125	Best loss: 0.179293	Accuracy: 80.34%
77	Validation loss: 104425.429688	Best loss: 0.179293	Accuracy: 82.76%
78	Validation loss: 729871.625000	Best loss: 0.179293	Accuracy: 65.83%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 1.3min
[CV] n_neurons=160, learning_rate=0.1, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 2445.868408	Best loss: 2445.868408	Accuracy: 19.04%
1	Validation loss: 69.774094	Best loss: 69.774094	Accuracy: 57.43%
2	Validation loss: 56.959320	Best loss: 56.959320	Accuracy: 50.35%
3	Validation loss: 17.620844	Best loss: 17.620844	Accuracy: 61.38%
4	Validation loss: 14.463291	Best loss: 14.463291	Accuracy: 54.18%
5	Validation loss: 16.113331	Best loss: 14.463291	Accuracy: 71.77%
6	Validation loss: 10.280030	Best loss: 10.280030	Accuracy: 79.63%
7	Validation loss: 39.063072	Best loss: 10.280030	Accuracy: 41.16%
8	Validation loss: 3.837636	Best loss: 3.837636	Accuracy: 77.40%
9	Validation loss: 2.901117	Best loss: 2.901117	Accuracy: 76.70%
10	Validation loss: 3.788648	Best loss: 2.901117	Accuracy: 86.75%
11	Validation loss: 2.262234	Best loss: 2.262234	Accuracy: 88.31%
12	Validation loss: 4.220784	Best loss: 2.262234	Accuracy: 89.25%
13	Validation loss: 1.011759	Best loss: 1.011759	Accuracy: 92.77%
14	Validation loss: 2.504884	Best loss: 1.011759	Accuracy: 84.75%
15	Validation loss: 3.435513	Best loss: 1.011759	Accuracy: 87.10%
16	Validation loss: 3.846106	Best loss: 1.011759	Accuracy: 77.44%
17	Validation loss: 1.445716	Best loss: 1.011759	Accuracy: 87.18%
18	Validation loss: 3.600492	Best loss: 1.011759	Accuracy: 77.25%
19	Validation loss: 3.241665	Best loss: 1.011759	Accuracy: 90.58%
20	Validation loss: 1.077159	Best loss: 1.011759	Accuracy: 90.89%
21	Validation loss: 0.754426	Best loss: 0.754426	Accuracy: 93.51%
22	Validation loss: 13.907772	Best loss: 0.754426	Accuracy: 59.27%
23	Validation loss: 1.480122	Best loss: 0.754426	Accuracy: 89.25%
24	Validation loss: 4.146547	Best loss: 0.754426	Accuracy: 83.82%
25	Validation loss: 0.780115	Best loss: 0.754426	Accuracy: 92.42%
26	Validation loss: 1.182524	Best loss: 0.754426	Accuracy: 87.69%
27	Validation loss: 3.173308	Best loss: 0.754426	Accuracy: 89.41%
28	Validation loss: 3.668905	Best loss: 0.754426	Accuracy: 72.75%
29	Validation loss: 1.117177	Best loss: 0.754426	Accuracy: 91.56%
30	Validation loss: 0.727281	Best loss: 0.727281	Accuracy: 92.73%
31	Validation loss: 1.624917	Best loss: 0.727281	Accuracy: 85.14%
32	Validation loss: 0.613533	Best loss: 0.613533	Accuracy: 92.92%
33	Validation loss: 0.867575	Best loss: 0.613533	Accuracy: 95.04%
34	Validation loss: 0.798107	Best loss: 0.613533	Accuracy: 93.59%
35	Validation loss: 2.013471	Best loss: 0.613533	Accuracy: 82.06%
36	Validation loss: 0.633457	Best loss: 0.613533	Accuracy: 95.47%
37	Validation loss: 1.178018	Best loss: 0.613533	Accuracy: 89.64%
38	Validation loss: 3.690436	Best loss: 0.613533	Accuracy: 89.37%
39	Validation loss: 13.110040	Best loss: 0.613533	Accuracy: 67.94%
40	Validation loss: 1.075258	Best loss: 0.613533	Accuracy: 94.02%
41	Validation loss: 1.358868	Best loss: 0.613533	Accuracy: 91.56%
42	Validation loss: 1.099546	Best loss: 0.613533	Accuracy: 93.28%
43	Validation loss: 0.694458	Best loss: 0.613533	Accuracy: 93.16%
44	Validation loss: 1.530102	Best loss: 0.613533	Accuracy: 87.65%
45	Validation loss: 0.632090	Best loss: 0.613533	Accuracy: 93.71%
46	Validation loss: 1.562611	Best loss: 0.613533	Accuracy: 91.71%
47	Validation loss: 0.666087	Best loss: 0.613533	Accuracy: 95.15%
48	Validation loss: 1.218484	Best loss: 0.613533	Accuracy: 95.27%
49	Validation loss: 1.655834	Best loss: 0.613533	Accuracy: 87.41%
50	Validation loss: 0.586653	Best loss: 0.586653	Accuracy: 95.31%
51	Validation loss: 0.689676	Best loss: 0.586653	Accuracy: 94.14%
52	Validation loss: 0.612210	Best loss: 0.586653	Accuracy: 94.49%
53	Validation loss: 0.957973	Best loss: 0.586653	Accuracy: 91.40%
54	Validation loss: 1.538441	Best loss: 0.586653	Accuracy: 95.54%
55	Validation loss: 1.018581	Best loss: 0.586653	Accuracy: 94.92%
56	Validation loss: 1.162414	Best loss: 0.586653	Accuracy: 92.49%
57	Validation loss: 0.632687	Best loss: 0.586653	Accuracy: 95.58%
58	Validation loss: 0.630643	Best loss: 0.586653	Accuracy: 95.04%
59	Validation loss: 1.095246	Best loss: 0.586653	Accuracy: 92.06%
60	Validation loss: 0.585973	Best loss: 0.585973	Accuracy: 95.23%
61	Validation loss: 0.779048	Best loss: 0.585973	Accuracy: 93.28%
62	Validation loss: 0.602523	Best loss: 0.585973	Accuracy: 96.05%
63	Validation loss: 0.514069	Best loss: 0.514069	Accuracy: 95.82%
64	Validation loss: 0.670417	Best loss: 0.514069	Accuracy: 94.02%
65	Validation loss: 0.610629	Best loss: 0.514069	Accuracy: 94.33%
66	Validation loss: 2.698731	Best loss: 0.514069	Accuracy: 79.32%
67	Validation loss: 0.596224	Best loss: 0.514069	Accuracy: 94.25%
68	Validation loss: 0.707700	Best loss: 0.514069	Accuracy: 94.25%
69	Validation loss: 6.770362	Best loss: 0.514069	Accuracy: 94.92%
70	Validation loss: 2.102279	Best loss: 0.514069	Accuracy: 95.39%
71	Validation loss: 1.447266	Best loss: 0.514069	Accuracy: 93.00%
72	Validation loss: 0.922017	Best loss: 0.514069	Accuracy: 95.86%
73	Validation loss: 1.088914	Best loss: 0.514069	Accuracy: 96.05%
74	Validation loss: 0.864328	Best loss: 0.514069	Accuracy: 94.80%
75	Validation loss: 0.612751	Best loss: 0.514069	Accuracy: 96.25%
76	Validation loss: 0.625478	Best loss: 0.514069	Accuracy: 94.92%
77	Validation loss: 0.996983	Best loss: 0.514069	Accuracy: 96.05%
78	Validation loss: 1.204687	Best loss: 0.514069	Accuracy: 95.47%
79	Validation loss: 0.645672	Best loss: 0.514069	Accuracy: 96.36%
80	Validation loss: 0.671268	Best loss: 0.514069	Accuracy: 96.44%
81	Validation loss: 0.707595	Best loss: 0.514069	Accuracy: 94.96%
82	Validation loss: 3.719970	Best loss: 0.514069	Accuracy: 92.69%
83	Validation loss: 0.671385	Best loss: 0.514069	Accuracy: 96.36%
84	Validation loss: 0.413887	Best loss: 0.413887	Accuracy: 96.87%
85	Validation loss: 0.439554	Best loss: 0.413887	Accuracy: 96.40%
86	Validation loss: 0.416862	Best loss: 0.413887	Accuracy: 96.68%
87	Validation loss: 8.430580	Best loss: 0.413887	Accuracy: 95.43%
88	Validation loss: 12.618447	Best loss: 0.413887	Accuracy: 81.04%
89	Validation loss: 4.611078	Best loss: 0.413887	Accuracy: 95.82%
90	Validation loss: 1.767469	Best loss: 0.413887	Accuracy: 95.04%
91	Validation loss: 1.369981	Best loss: 0.413887	Accuracy: 96.01%
92	Validation loss: 0.838287	Best loss: 0.413887	Accuracy: 96.25%
93	Validation loss: 0.984843	Best loss: 0.413887	Accuracy: 94.18%
94	Validation loss: 1.658246	Best loss: 0.413887	Accuracy: 92.77%
95	Validation loss: 0.879484	Best loss: 0.413887	Accuracy: 96.44%
96	Validation loss: 0.662804	Best loss: 0.413887	Accuracy: 96.13%
97	Validation loss: 0.468125	Best loss: 0.413887	Accuracy: 96.72%
98	Validation loss: 0.552932	Best loss: 0.413887	Accuracy: 95.70%
99	Validation loss: 0.625313	Best loss: 0.413887	Accuracy: 96.48%
100	Validation loss: 0.500940	Best loss: 0.413887	Accuracy: 95.90%
101	Validation loss: 0.523899	Best loss: 0.413887	Accuracy: 95.97%
102	Validation loss: 0.671616	Best loss: 0.413887	Accuracy: 95.27%
103	Validation loss: 0.742057	Best loss: 0.413887	Accuracy: 94.76%
104	Validation loss: 0.477563	Best loss: 0.413887	Accuracy: 96.17%
105	Validation loss: 0.621682	Best loss: 0.413887	Accuracy: 96.44%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 1.7min
[CV] n_neurons=160, learning_rate=0.1, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 98.594322	Best loss: 98.594322	Accuracy: 60.99%
1	Validation loss: 5.333787	Best loss: 5.333787	Accuracy: 91.56%
2	Validation loss: 4.825334	Best loss: 4.825334	Accuracy: 85.57%
3	Validation loss: 5.253241	Best loss: 4.825334	Accuracy: 90.23%
4	Validation loss: 3.327972	Best loss: 3.327972	Accuracy: 86.08%
5	Validation loss: 1.450422	Best loss: 1.450422	Accuracy: 95.43%
6	Validation loss: 1.355831	Best loss: 1.355831	Accuracy: 93.28%
7	Validation loss: 1.241495	Best loss: 1.241495	Accuracy: 95.47%
8	Validation loss: 1.439221	Best loss: 1.241495	Accuracy: 91.75%
9	Validation loss: 0.689051	Best loss: 0.689051	Accuracy: 96.17%
10	Validation loss: 1.490796	Best loss: 0.689051	Accuracy: 95.35%
11	Validation loss: 0.875946	Best loss: 0.689051	Accuracy: 95.82%
12	Validation loss: 0.401237	Best loss: 0.401237	Accuracy: 96.52%
13	Validation loss: 1.076335	Best loss: 0.401237	Accuracy: 94.57%
14	Validation loss: 1.172363	Best loss: 0.401237	Accuracy: 95.86%
15	Validation loss: 0.662688	Best loss: 0.401237	Accuracy: 96.09%
16	Validation loss: 3.227728	Best loss: 0.401237	Accuracy: 95.11%
17	Validation loss: 1.406355	Best loss: 0.401237	Accuracy: 95.97%
18	Validation loss: 0.680300	Best loss: 0.401237	Accuracy: 96.56%
19	Validation loss: 0.679459	Best loss: 0.401237	Accuracy: 95.39%
20	Validation loss: 0.826985	Best loss: 0.401237	Accuracy: 96.52%
21	Validation loss: 0.476885	Best loss: 0.401237	Accuracy: 95.23%
22	Validation loss: 0.492589	Best loss: 0.401237	Accuracy: 94.02%
23	Validation loss: 0.263216	Best loss: 0.263216	Accuracy: 96.68%
24	Validation loss: 0.497942	Best loss: 0.263216	Accuracy: 93.82%
25	Validation loss: 0.285571	Best loss: 0.263216	Accuracy: 96.36%
26	Validation loss: 0.398026	Best loss: 0.263216	Accuracy: 96.21%
27	Validation loss: 0.560906	Best loss: 0.263216	Accuracy: 91.36%
28	Validation loss: 0.327999	Best loss: 0.263216	Accuracy: 96.21%
29	Validation loss: 0.231582	Best loss: 0.231582	Accuracy: 95.54%
30	Validation loss: 0.285296	Best loss: 0.231582	Accuracy: 96.60%
31	Validation loss: 0.200065	Best loss: 0.200065	Accuracy: 97.19%
32	Validation loss: 0.228263	Best loss: 0.200065	Accuracy: 96.01%
33	Validation loss: 0.306950	Best loss: 0.200065	Accuracy: 95.15%
34	Validation loss: 0.196853	Best loss: 0.196853	Accuracy: 96.05%
35	Validation loss: 0.255317	Best loss: 0.196853	Accuracy: 96.25%
36	Validation loss: 0.356325	Best loss: 0.196853	Accuracy: 94.88%
37	Validation loss: 1.095532	Best loss: 0.196853	Accuracy: 95.04%
38	Validation loss: 0.747755	Best loss: 0.196853	Accuracy: 95.70%
39	Validation loss: 0.488045	Best loss: 0.196853	Accuracy: 96.36%
40	Validation loss: 0.391811	Best loss: 0.196853	Accuracy: 96.21%
41	Validation loss: 0.480150	Best loss: 0.196853	Accuracy: 95.58%
42	Validation loss: 1.180087	Best loss: 0.196853	Accuracy: 97.15%
43	Validation loss: 0.476529	Best loss: 0.196853	Accuracy: 97.65%
44	Validation loss: 0.425140	Best loss: 0.196853	Accuracy: 95.90%
45	Validation loss: 0.331582	Best loss: 0.196853	Accuracy: 96.33%
46	Validation loss: 0.239800	Best loss: 0.196853	Accuracy: 97.19%
47	Validation loss: 0.309898	Best loss: 0.196853	Accuracy: 96.44%
48	Validation loss: 0.286991	Best loss: 0.196853	Accuracy: 95.58%
49	Validation loss: 0.208723	Best loss: 0.196853	Accuracy: 97.38%
50	Validation loss: 0.314024	Best loss: 0.196853	Accuracy: 97.22%
51	Validation loss: 0.210715	Best loss: 0.196853	Accuracy: 96.99%
52	Validation loss: 0.244754	Best loss: 0.196853	Accuracy: 96.52%
53	Validation loss: 0.244665	Best loss: 0.196853	Accuracy: 95.90%
54	Validation loss: 0.195859	Best loss: 0.195859	Accuracy: 96.09%
55	Validation loss: 0.478647	Best loss: 0.195859	Accuracy: 94.02%
56	Validation loss: 0.220420	Best loss: 0.195859	Accuracy: 96.76%
57	Validation loss: 0.169925	Best loss: 0.169925	Accuracy: 96.91%
58	Validation loss: 0.171040	Best loss: 0.169925	Accuracy: 96.83%
59	Validation loss: 0.140314	Best loss: 0.140314	Accuracy: 97.58%
60	Validation loss: 0.170254	Best loss: 0.140314	Accuracy: 97.26%
61	Validation loss: 0.226688	Best loss: 0.140314	Accuracy: 95.35%
62	Validation loss: 0.200446	Best loss: 0.140314	Accuracy: 95.15%
63	Validation loss: 0.185028	Best loss: 0.140314	Accuracy: 97.26%
64	Validation loss: 0.154198	Best loss: 0.140314	Accuracy: 96.91%
65	Validation loss: 1.116819	Best loss: 0.140314	Accuracy: 78.46%
66	Validation loss: 0.782238	Best loss: 0.140314	Accuracy: 95.15%
67	Validation loss: 0.275902	Best loss: 0.140314	Accuracy: 96.21%
68	Validation loss: 0.200842	Best loss: 0.140314	Accuracy: 97.19%
69	Validation loss: 0.253413	Best loss: 0.140314	Accuracy: 96.05%
70	Validation loss: 0.195724	Best loss: 0.140314	Accuracy: 96.44%
71	Validation loss: 0.182399	Best loss: 0.140314	Accuracy: 96.79%
72	Validation loss: 0.137274	Best loss: 0.137274	Accuracy: 97.15%
73	Validation loss: 0.168686	Best loss: 0.137274	Accuracy: 96.76%
74	Validation loss: 0.137224	Best loss: 0.137224	Accuracy: 97.19%
75	Validation loss: 0.178637	Best loss: 0.137224	Accuracy: 95.86%
76	Validation loss: 0.137228	Best loss: 0.137224	Accuracy: 96.83%
77	Validation loss: 0.133124	Best loss: 0.133124	Accuracy: 97.54%
78	Validation loss: 0.127097	Best loss: 0.127097	Accuracy: 97.77%
79	Validation loss: 0.121602	Best loss: 0.121602	Accuracy: 97.81%
80	Validation loss: 0.115295	Best loss: 0.115295	Accuracy: 97.22%
81	Validation loss: 0.106010	Best loss: 0.106010	Accuracy: 97.42%
82	Validation loss: 0.136263	Best loss: 0.106010	Accuracy: 96.52%
83	Validation loss: 0.119979	Best loss: 0.106010	Accuracy: 97.54%
84	Validation loss: 0.125288	Best loss: 0.106010	Accuracy: 97.11%
85	Validation loss: 0.103399	Best loss: 0.103399	Accuracy: 97.65%
86	Validation loss: 0.139760	Best loss: 0.103399	Accuracy: 97.77%
87	Validation loss: 15929530.000000	Best loss: 0.103399	Accuracy: 52.54%
88	Validation loss: 1251887.125000	Best loss: 0.103399	Accuracy: 67.90%
89	Validation loss: 216396.828125	Best loss: 0.103399	Accuracy: 75.22%
90	Validation loss: 230884.031250	Best loss: 0.103399	Accuracy: 76.08%
91	Validation loss: 168168.453125	Best loss: 0.103399	Accuracy: 86.63%
92	Validation loss: 143156.265625	Best loss: 0.103399	Accuracy: 80.84%
93	Validation loss: 117626.171875	Best loss: 0.103399	Accuracy: 74.63%
94	Validation loss: 56224.039062	Best loss: 0.103399	Accuracy: 94.61%
95	Validation loss: 87935.023438	Best loss: 0.103399	Accuracy: 81.24%
96	Validation loss: 68055.875000	Best loss: 0.103399	Accuracy: 90.34%
97	Validation loss: 76365.578125	Best loss: 0.103399	Accuracy: 82.29%
98	Validation loss: 73721.882812	Best loss: 0.103399	Accuracy: 77.99%
99	Validation loss: 74413.187500	Best loss: 0.103399	Accuracy: 88.70%
100	Validation loss: 40450.832031	Best loss: 0.103399	Accuracy: 93.86%
101	Validation loss: 83560.546875	Best loss: 0.103399	Accuracy: 74.63%
102	Validation loss: 41254.593750	Best loss: 0.103399	Accuracy: 93.55%
103	Validation loss: 65064.980469	Best loss: 0.103399	Accuracy: 85.42%
104	Validation loss: 23498.302734	Best loss: 0.103399	Accuracy: 95.00%
105	Validation loss: 64221.328125	Best loss: 0.103399	Accuracy: 91.01%
106	Validation loss: 27665.582031	Best loss: 0.103399	Accuracy: 93.39%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.1, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total= 1.9min
[CV] n_neurons=100, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.145312	Best loss: 0.145312	Accuracy: 95.82%
1	Validation loss: 0.151515	Best loss: 0.145312	Accuracy: 95.82%
2	Validation loss: 0.162109	Best loss: 0.145312	Accuracy: 95.93%
3	Validation loss: 0.217848	Best loss: 0.145312	Accuracy: 94.68%
4	Validation loss: 0.248274	Best loss: 0.145312	Accuracy: 93.24%
5	Validation loss: 0.322382	Best loss: 0.145312	Accuracy: 90.15%
6	Validation loss: 0.365997	Best loss: 0.145312	Accuracy: 87.61%
7	Validation loss: 0.361694	Best loss: 0.145312	Accuracy: 84.71%
8	Validation loss: 0.870072	Best loss: 0.145312	Accuracy: 61.10%
9	Validation loss: 0.949508	Best loss: 0.145312	Accuracy: 63.96%
10	Validation loss: 0.682638	Best loss: 0.145312	Accuracy: 72.60%
11	Validation loss: 0.794351	Best loss: 0.145312	Accuracy: 65.09%
12	Validation loss: 0.870190	Best loss: 0.145312	Accuracy: 60.75%
13	Validation loss: 1.067379	Best loss: 0.145312	Accuracy: 58.05%
14	Validation loss: 0.752820	Best loss: 0.145312	Accuracy: 70.91%
15	Validation loss: 0.609786	Best loss: 0.145312	Accuracy: 75.18%
16	Validation loss: 1.256607	Best loss: 0.145312	Accuracy: 53.99%
17	Validation loss: 0.742707	Best loss: 0.145312	Accuracy: 66.26%
18	Validation loss: 0.918087	Best loss: 0.145312	Accuracy: 59.97%
19	Validation loss: 1.150117	Best loss: 0.145312	Accuracy: 53.87%
20	Validation loss: 1.355289	Best loss: 0.145312	Accuracy: 42.18%
21	Validation loss: 1.412488	Best loss: 0.145312	Accuracy: 34.56%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  28.2s
[CV] n_neurons=100, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.180343	Best loss: 0.180343	Accuracy: 95.27%
1	Validation loss: 0.176839	Best loss: 0.176839	Accuracy: 95.07%
2	Validation loss: 0.197946	Best loss: 0.176839	Accuracy: 94.57%
3	Validation loss: 0.208860	Best loss: 0.176839	Accuracy: 94.37%
4	Validation loss: 0.538858	Best loss: 0.176839	Accuracy: 81.94%
5	Validation loss: 0.372038	Best loss: 0.176839	Accuracy: 86.20%
6	Validation loss: 0.824392	Best loss: 0.176839	Accuracy: 65.64%
7	Validation loss: 1.117716	Best loss: 0.176839	Accuracy: 52.19%
8	Validation loss: 0.975300	Best loss: 0.176839	Accuracy: 54.10%
9	Validation loss: 1.002123	Best loss: 0.176839	Accuracy: 54.30%
10	Validation loss: 0.792374	Best loss: 0.176839	Accuracy: 64.31%
11	Validation loss: 0.809999	Best loss: 0.176839	Accuracy: 65.64%
12	Validation loss: 1.583429	Best loss: 0.176839	Accuracy: 35.65%
13	Validation loss: 1.213163	Best loss: 0.176839	Accuracy: 41.87%
14	Validation loss: 1.760047	Best loss: 0.176839	Accuracy: 36.16%
15	Validation loss: 1.679946	Best loss: 0.176839	Accuracy: 36.98%
16	Validation loss: 1.729733	Best loss: 0.176839	Accuracy: 24.75%
17	Validation loss: 1.418283	Best loss: 0.176839	Accuracy: 38.43%
18	Validation loss: 1.608266	Best loss: 0.176839	Accuracy: 33.89%
19	Validation loss: 1.281234	Best loss: 0.176839	Accuracy: 36.28%
20	Validation loss: 1.342027	Best loss: 0.176839	Accuracy: 36.20%
21	Validation loss: 2.013192	Best loss: 0.176839	Accuracy: 45.90%
22	Validation loss: 12.701291	Best loss: 0.176839	Accuracy: 20.95%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  31.5s
[CV] n_neurons=100, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 0.218463	Best loss: 0.218463	Accuracy: 94.49%
1	Validation loss: 0.157098	Best loss: 0.157098	Accuracy: 95.70%
2	Validation loss: 0.160344	Best loss: 0.157098	Accuracy: 96.09%
3	Validation loss: 0.270144	Best loss: 0.157098	Accuracy: 91.75%
4	Validation loss: 0.716478	Best loss: 0.157098	Accuracy: 72.60%
5	Validation loss: 0.737460	Best loss: 0.157098	Accuracy: 69.04%
6	Validation loss: 0.601460	Best loss: 0.157098	Accuracy: 78.73%
7	Validation loss: 0.552001	Best loss: 0.157098	Accuracy: 70.88%
8	Validation loss: 0.430070	Best loss: 0.157098	Accuracy: 82.92%
9	Validation loss: 0.501131	Best loss: 0.157098	Accuracy: 76.54%
10	Validation loss: 0.641504	Best loss: 0.157098	Accuracy: 78.50%
11	Validation loss: 0.707100	Best loss: 0.157098	Accuracy: 67.55%
12	Validation loss: 0.496379	Best loss: 0.157098	Accuracy: 79.95%
13	Validation loss: 0.427983	Best loss: 0.157098	Accuracy: 83.70%
14	Validation loss: 0.475288	Best loss: 0.157098	Accuracy: 81.82%
15	Validation loss: 1.303109	Best loss: 0.157098	Accuracy: 55.71%
16	Validation loss: 1.166016	Best loss: 0.157098	Accuracy: 56.53%
17	Validation loss: 0.877134	Best loss: 0.157098	Accuracy: 58.56%
18	Validation loss: 1.100825	Best loss: 0.157098	Accuracy: 52.27%
19	Validation loss: 0.835336	Best loss: 0.157098	Accuracy: 61.30%
20	Validation loss: 0.891947	Best loss: 0.157098	Accuracy: 64.39%
21	Validation loss: 0.622086	Best loss: 0.157098	Accuracy: 70.88%
22	Validation loss: 0.803677	Best loss: 0.157098	Accuracy: 63.96%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.01, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  30.4s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 208647.937500	Best loss: 208647.937500	Accuracy: 10.52%
1	Validation loss: 72071.171875	Best loss: 72071.171875	Accuracy: 20.95%
2	Validation loss: 24613.400391	Best loss: 24613.400391	Accuracy: 19.59%
3	Validation loss: 44282.078125	Best loss: 24613.400391	Accuracy: 22.01%
4	Validation loss: 22460.169922	Best loss: 22460.169922	Accuracy: 20.91%
5	Validation loss: 10859.149414	Best loss: 10859.149414	Accuracy: 18.73%
6	Validation loss: 28752.671875	Best loss: 10859.149414	Accuracy: 22.01%
7	Validation loss: 103282.148438	Best loss: 10859.149414	Accuracy: 18.73%
8	Validation loss: 194830.843750	Best loss: 10859.149414	Accuracy: 18.73%
9	Validation loss: 1029875.312500	Best loss: 10859.149414	Accuracy: 19.27%
10	Validation loss: 938395.312500	Best loss: 10859.149414	Accuracy: 19.27%
11	Validation loss: 984670.000000	Best loss: 10859.149414	Accuracy: 18.73%
12	Validation loss: 1713419.625000	Best loss: 10859.149414	Accuracy: 21.07%
13	Validation loss: 1957290.750000	Best loss: 10859.149414	Accuracy: 48.63%
14	Validation loss: 9858377.000000	Best loss: 10859.149414	Accuracy: 20.95%
15	Validation loss: 4796128.000000	Best loss: 10859.149414	Accuracy: 22.01%
16	Validation loss: 724137.437500	Best loss: 10859.149414	Accuracy: 19.19%
17	Validation loss: 902048.937500	Best loss: 10859.149414	Accuracy: 27.87%
18	Validation loss: 639514.437500	Best loss: 10859.149414	Accuracy: 24.32%
19	Validation loss: 890475.812500	Best loss: 10859.149414	Accuracy: 19.23%
20	Validation loss: 997807.437500	Best loss: 10859.149414	Accuracy: 22.01%
21	Validation loss: 224634.453125	Best loss: 10859.149414	Accuracy: 22.13%
22	Validation loss: 894054.375000	Best loss: 10859.149414	Accuracy: 19.27%
23	Validation loss: 528009.250000	Best loss: 10859.149414	Accuracy: 25.18%
24	Validation loss: 791791.187500	Best loss: 10859.149414	Accuracy: 26.51%
25	Validation loss: 885941.437500	Best loss: 10859.149414	Accuracy: 22.01%
26	Validation loss: 576775.312500	Best loss: 10859.149414	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 2.8min
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 234905.296875	Best loss: 234905.296875	Accuracy: 18.76%
1	Validation loss: 41722.015625	Best loss: 41722.015625	Accuracy: 19.43%
2	Validation loss: 17344.169922	Best loss: 17344.169922	Accuracy: 19.31%
3	Validation loss: 16567.103516	Best loss: 16567.103516	Accuracy: 22.01%
4	Validation loss: 8393.848633	Best loss: 8393.848633	Accuracy: 19.31%
5	Validation loss: 2795.641846	Best loss: 2795.641846	Accuracy: 18.73%
6	Validation loss: 2395276.500000	Best loss: 2795.641846	Accuracy: 19.12%
7	Validation loss: 179158.812500	Best loss: 2795.641846	Accuracy: 18.73%
8	Validation loss: 4065181.000000	Best loss: 2795.641846	Accuracy: 22.83%
9	Validation loss: 369497.093750	Best loss: 2795.641846	Accuracy: 22.01%
10	Validation loss: 588948800.000000	Best loss: 2795.641846	Accuracy: 23.73%
11	Validation loss: 491698.375000	Best loss: 2795.641846	Accuracy: 19.23%
12	Validation loss: 333756.250000	Best loss: 2795.641846	Accuracy: 36.90%
13	Validation loss: 247907.546875	Best loss: 2795.641846	Accuracy: 36.40%
14	Validation loss: 159092.718750	Best loss: 2795.641846	Accuracy: 18.73%
15	Validation loss: 457713.937500	Best loss: 2795.641846	Accuracy: 23.81%
16	Validation loss: 833350.250000	Best loss: 2795.641846	Accuracy: 18.73%
17	Validation loss: 365032.781250	Best loss: 2795.641846	Accuracy: 20.91%
18	Validation loss: 748179.625000	Best loss: 2795.641846	Accuracy: 18.73%
19	Validation loss: 49885696.000000	Best loss: 2795.641846	Accuracy: 28.46%
20	Validation loss: 605253.062500	Best loss: 2795.641846	Accuracy: 18.73%
21	Validation loss: 733623.000000	Best loss: 2795.641846	Accuracy: 18.73%
22	Validation loss: 290838.031250	Best loss: 2795.641846	Accuracy: 36.36%
23	Validation loss: 2918515.500000	Best loss: 2795.641846	Accuracy: 20.91%
24	Validation loss: 901250.875000	Best loss: 2795.641846	Accuracy: 20.91%
25	Validation loss: 903044.687500	Best loss: 2795.641846	Accuracy: 20.72%
26	Validation loss: 915403.375000	Best loss: 2795.641846	Accuracy: 18.10%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 2.8min
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 284360.562500	Best loss: 284360.562500	Accuracy: 18.73%
1	Validation loss: 240910.718750	Best loss: 240910.718750	Accuracy: 18.92%
2	Validation loss: 39711.707031	Best loss: 39711.707031	Accuracy: 22.36%
3	Validation loss: 37935.820312	Best loss: 37935.820312	Accuracy: 19.27%
4	Validation loss: 304681696.000000	Best loss: 37935.820312	Accuracy: 11.34%
5	Validation loss: 4197979.500000	Best loss: 37935.820312	Accuracy: 19.08%
6	Validation loss: 1329309.750000	Best loss: 37935.820312	Accuracy: 19.12%
7	Validation loss: 1042072.437500	Best loss: 37935.820312	Accuracy: 8.72%
8	Validation loss: 1040215.062500	Best loss: 37935.820312	Accuracy: 21.89%
9	Validation loss: 781633.500000	Best loss: 37935.820312	Accuracy: 19.23%
10	Validation loss: 1441020.625000	Best loss: 37935.820312	Accuracy: 19.23%
11	Validation loss: 641870.000000	Best loss: 37935.820312	Accuracy: 19.23%
12	Validation loss: 1268399.625000	Best loss: 37935.820312	Accuracy: 19.23%
13	Validation loss: 5846864.500000	Best loss: 37935.820312	Accuracy: 19.27%
14	Validation loss: 671003.125000	Best loss: 37935.820312	Accuracy: 21.31%
15	Validation loss: 451136.187500	Best loss: 37935.820312	Accuracy: 19.39%
16	Validation loss: 2953903.750000	Best loss: 37935.820312	Accuracy: 18.73%
17	Validation loss: 3093772.500000	Best loss: 37935.820312	Accuracy: 20.91%
18	Validation loss: 2167342.000000	Best loss: 37935.820312	Accuracy: 22.13%
19	Validation loss: 778083.250000	Best loss: 37935.820312	Accuracy: 22.01%
20	Validation loss: 551357.000000	Best loss: 37935.820312	Accuracy: 22.52%
21	Validation loss: 388114.250000	Best loss: 37935.820312	Accuracy: 19.12%
22	Validation loss: 357854.187500	Best loss: 37935.820312	Accuracy: 21.38%
23	Validation loss: 859222.687500	Best loss: 37935.820312	Accuracy: 19.55%
24	Validation loss: 343141.718750	Best loss: 37935.820312	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.2, batch_size=10, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total= 2.7min
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.522045	Best loss: 1.522045	Accuracy: 34.99%
1	Validation loss: 1.399617	Best loss: 1.399617	Accuracy: 31.55%
2	Validation loss: 1.306973	Best loss: 1.306973	Accuracy: 35.18%
3	Validation loss: 1.312889	Best loss: 1.306973	Accuracy: 37.80%
4	Validation loss: 1.631153	Best loss: 1.306973	Accuracy: 24.51%
5	Validation loss: 1.273535	Best loss: 1.273535	Accuracy: 37.72%
6	Validation loss: 1.290894	Best loss: 1.273535	Accuracy: 35.03%
7	Validation loss: 1.236297	Best loss: 1.236297	Accuracy: 39.68%
8	Validation loss: 1.257782	Best loss: 1.236297	Accuracy: 39.84%
9	Validation loss: 1.290611	Best loss: 1.236297	Accuracy: 35.38%
10	Validation loss: 1.220216	Best loss: 1.220216	Accuracy: 40.23%
11	Validation loss: 1.245789	Best loss: 1.220216	Accuracy: 41.01%
12	Validation loss: 1.282325	Best loss: 1.220216	Accuracy: 38.47%
13	Validation loss: 1.282861	Best loss: 1.220216	Accuracy: 36.04%
14	Validation loss: 1.220999	Best loss: 1.220216	Accuracy: 38.04%
15	Validation loss: 1.289126	Best loss: 1.220216	Accuracy: 40.73%
16	Validation loss: 1.227771	Best loss: 1.220216	Accuracy: 38.27%
17	Validation loss: 1.240172	Best loss: 1.220216	Accuracy: 38.66%
18	Validation loss: 1.271382	Best loss: 1.220216	Accuracy: 37.76%
19	Validation loss: 1.260998	Best loss: 1.220216	Accuracy: 38.86%
20	Validation loss: 1.224670	Best loss: 1.220216	Accuracy: 36.67%
21	Validation loss: 1.207968	Best loss: 1.207968	Accuracy: 38.58%
22	Validation loss: 1.203817	Best loss: 1.203817	Accuracy: 37.80%
23	Validation loss: 1.245248	Best loss: 1.203817	Accuracy: 39.33%
24	Validation loss: 1.892729	Best loss: 1.203817	Accuracy: 18.73%
25	Validation loss: 1.690982	Best loss: 1.203817	Accuracy: 22.01%
26	Validation loss: 1.653665	Best loss: 1.203817	Accuracy: 22.01%
27	Validation loss: 1.629849	Best loss: 1.203817	Accuracy: 19.27%
28	Validation loss: 1.647568	Best loss: 1.203817	Accuracy: 22.01%
29	Validation loss: 1.612970	Best loss: 1.203817	Accuracy: 22.01%
30	Validation loss: 1.684465	Best loss: 1.203817	Accuracy: 19.27%
31	Validation loss: 1.696190	Best loss: 1.203817	Accuracy: 22.01%
32	Validation loss: 1.670998	Best loss: 1.203817	Accuracy: 22.01%
33	Validation loss: 1.628233	Best loss: 1.203817	Accuracy: 22.01%
34	Validation loss: 1.635421	Best loss: 1.203817	Accuracy: 18.73%
35	Validation loss: 1.635309	Best loss: 1.203817	Accuracy: 18.73%
36	Validation loss: 1.633503	Best loss: 1.203817	Accuracy: 20.91%
37	Validation loss: 1.629382	Best loss: 1.203817	Accuracy: 22.01%
38	Validation loss: 1.770925	Best loss: 1.203817	Accuracy: 19.27%
39	Validation loss: 1.620903	Best loss: 1.203817	Accuracy: 19.08%
40	Validation loss: 1.739407	Best loss: 1.203817	Accuracy: 22.01%
41	Validation loss: 1.677798	Best loss: 1.203817	Accuracy: 19.27%
42	Validation loss: 1.642384	Best loss: 1.203817	Accuracy: 19.27%
43	Validation loss: 1.629859	Best loss: 1.203817	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  43.2s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.236344	Best loss: 1.236344	Accuracy: 48.12%
1	Validation loss: 0.822292	Best loss: 0.822292	Accuracy: 63.72%
2	Validation loss: 0.722207	Best loss: 0.722207	Accuracy: 80.49%
3	Validation loss: 0.527302	Best loss: 0.527302	Accuracy: 84.71%
4	Validation loss: 0.404661	Best loss: 0.404661	Accuracy: 85.50%
5	Validation loss: 0.380044	Best loss: 0.380044	Accuracy: 88.00%
6	Validation loss: 0.355648	Best loss: 0.355648	Accuracy: 90.27%
7	Validation loss: 0.382243	Best loss: 0.355648	Accuracy: 78.66%
8	Validation loss: 0.301059	Best loss: 0.301059	Accuracy: 93.35%
9	Validation loss: 0.273278	Best loss: 0.273278	Accuracy: 94.41%
10	Validation loss: 0.265368	Best loss: 0.265368	Accuracy: 91.67%
11	Validation loss: 0.215514	Best loss: 0.215514	Accuracy: 95.07%
12	Validation loss: 0.188859	Best loss: 0.188859	Accuracy: 95.27%
13	Validation loss: 0.184403	Best loss: 0.184403	Accuracy: 94.96%
14	Validation loss: 0.183327	Best loss: 0.183327	Accuracy: 95.43%
15	Validation loss: 0.185092	Best loss: 0.183327	Accuracy: 95.31%
16	Validation loss: 0.186368	Best loss: 0.183327	Accuracy: 95.66%
17	Validation loss: 0.173545	Best loss: 0.173545	Accuracy: 95.54%
18	Validation loss: 0.172221	Best loss: 0.172221	Accuracy: 95.93%
19	Validation loss: 0.182199	Best loss: 0.172221	Accuracy: 95.78%
20	Validation loss: 0.168059	Best loss: 0.168059	Accuracy: 96.29%
21	Validation loss: 0.163039	Best loss: 0.163039	Accuracy: 95.82%
22	Validation loss: 1.876119	Best loss: 0.163039	Accuracy: 32.49%
23	Validation loss: 1.624435	Best loss: 0.163039	Accuracy: 22.01%
24	Validation loss: 1.647812	Best loss: 0.163039	Accuracy: 20.91%
25	Validation loss: 1.613537	Best loss: 0.163039	Accuracy: 19.08%
26	Validation loss: 1.609349	Best loss: 0.163039	Accuracy: 20.91%
27	Validation loss: 1.613166	Best loss: 0.163039	Accuracy: 20.91%
28	Validation loss: 1.645940	Best loss: 0.163039	Accuracy: 18.73%
29	Validation loss: 1.612240	Best loss: 0.163039	Accuracy: 18.73%
30	Validation loss: 1.621144	Best loss: 0.163039	Accuracy: 19.08%
31	Validation loss: 1.632145	Best loss: 0.163039	Accuracy: 19.27%
32	Validation loss: 1.630204	Best loss: 0.163039	Accuracy: 18.73%
33	Validation loss: 1.636758	Best loss: 0.163039	Accuracy: 20.91%
34	Validation loss: 1.611316	Best loss: 0.163039	Accuracy: 18.73%
35	Validation loss: 1.628109	Best loss: 0.163039	Accuracy: 22.01%
36	Validation loss: 1.659897	Best loss: 0.163039	Accuracy: 20.91%
37	Validation loss: 1.627039	Best loss: 0.163039	Accuracy: 22.01%
38	Validation loss: 1.648242	Best loss: 0.163039	Accuracy: 22.01%
39	Validation loss: 1.642668	Best loss: 0.163039	Accuracy: 19.27%
40	Validation loss: 1.614101	Best loss: 0.163039	Accuracy: 19.27%
41	Validation loss: 1.652987	Best loss: 0.163039	Accuracy: 22.01%
42	Validation loss: 1.635304	Best loss: 0.163039	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  41.2s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.890007	Best loss: 0.890007	Accuracy: 63.37%
1	Validation loss: 0.506290	Best loss: 0.506290	Accuracy: 79.48%
2	Validation loss: 0.426239	Best loss: 0.426239	Accuracy: 78.62%
3	Validation loss: 0.402688	Best loss: 0.402688	Accuracy: 79.32%
4	Validation loss: 0.412066	Best loss: 0.402688	Accuracy: 78.34%
5	Validation loss: 0.378483	Best loss: 0.378483	Accuracy: 91.36%
6	Validation loss: 0.306570	Best loss: 0.306570	Accuracy: 91.36%
7	Validation loss: 0.405310	Best loss: 0.306570	Accuracy: 78.50%
8	Validation loss: 0.382346	Best loss: 0.306570	Accuracy: 79.71%
9	Validation loss: 0.291989	Best loss: 0.291989	Accuracy: 92.81%
10	Validation loss: 0.251454	Best loss: 0.251454	Accuracy: 95.90%
11	Validation loss: 0.232413	Best loss: 0.232413	Accuracy: 95.90%
12	Validation loss: 0.228470	Best loss: 0.228470	Accuracy: 96.17%
13	Validation loss: 0.207505	Best loss: 0.207505	Accuracy: 96.83%
14	Validation loss: 0.182329	Best loss: 0.182329	Accuracy: 96.87%
15	Validation loss: 0.173576	Best loss: 0.173576	Accuracy: 97.03%
16	Validation loss: 0.160554	Best loss: 0.160554	Accuracy: 96.95%
17	Validation loss: 0.175997	Best loss: 0.160554	Accuracy: 97.62%
18	Validation loss: 0.163860	Best loss: 0.160554	Accuracy: 97.54%
19	Validation loss: 0.189249	Best loss: 0.160554	Accuracy: 97.81%
20	Validation loss: 1.658077	Best loss: 0.160554	Accuracy: 19.08%
21	Validation loss: 1.641679	Best loss: 0.160554	Accuracy: 22.01%
22	Validation loss: 1.669038	Best loss: 0.160554	Accuracy: 22.01%
23	Validation loss: 1.626154	Best loss: 0.160554	Accuracy: 22.01%
24	Validation loss: 1.656879	Best loss: 0.160554	Accuracy: 20.91%
25	Validation loss: 1.683994	Best loss: 0.160554	Accuracy: 18.73%
26	Validation loss: 1.657002	Best loss: 0.160554	Accuracy: 19.08%
27	Validation loss: 1.679171	Best loss: 0.160554	Accuracy: 20.91%
28	Validation loss: 1.658661	Best loss: 0.160554	Accuracy: 19.08%
29	Validation loss: 1.661314	Best loss: 0.160554	Accuracy: 19.27%
30	Validation loss: 1.614457	Best loss: 0.160554	Accuracy: 19.08%
31	Validation loss: 1.616343	Best loss: 0.160554	Accuracy: 19.08%
32	Validation loss: 1.628409	Best loss: 0.160554	Accuracy: 18.73%
33	Validation loss: 1.662202	Best loss: 0.160554	Accuracy: 18.73%
34	Validation loss: 1.626792	Best loss: 0.160554	Accuracy: 18.73%
35	Validation loss: 1.620163	Best loss: 0.160554	Accuracy: 22.01%
36	Validation loss: 1.670674	Best loss: 0.160554	Accuracy: 22.01%
37	Validation loss: 1.639080	Best loss: 0.160554	Accuracy: 22.01%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.3, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  34.6s
[CV] n_neurons=70, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 525.573730	Best loss: 525.573730	Accuracy: 55.86%
1	Validation loss: 438707.031250	Best loss: 525.573730	Accuracy: 55.08%
2	Validation loss: 172549.859375	Best loss: 525.573730	Accuracy: 53.67%
3	Validation loss: 13233.594727	Best loss: 525.573730	Accuracy: 58.56%
4	Validation loss: 7593.248047	Best loss: 525.573730	Accuracy: 55.75%
5	Validation loss: 5832.312012	Best loss: 525.573730	Accuracy: 63.02%
6	Validation loss: 3307.990234	Best loss: 525.573730	Accuracy: 69.04%
7	Validation loss: 3092.170166	Best loss: 525.573730	Accuracy: 62.55%
8	Validation loss: 2421.539307	Best loss: 525.573730	Accuracy: 66.58%
9	Validation loss: 1722.414429	Best loss: 525.573730	Accuracy: 64.89%
10	Validation loss: 3166.211182	Best loss: 525.573730	Accuracy: 71.89%
11	Validation loss: 1490.361572	Best loss: 525.573730	Accuracy: 77.99%
12	Validation loss: 1766.191406	Best loss: 525.573730	Accuracy: 73.89%
13	Validation loss: 7987.383301	Best loss: 525.573730	Accuracy: 75.25%
14	Validation loss: 1029175.562500	Best loss: 525.573730	Accuracy: 67.94%
15	Validation loss: 235430.859375	Best loss: 525.573730	Accuracy: 66.03%
16	Validation loss: 77954.570312	Best loss: 525.573730	Accuracy: 73.14%
17	Validation loss: 35208.800781	Best loss: 525.573730	Accuracy: 65.60%
18	Validation loss: 58287.820312	Best loss: 525.573730	Accuracy: 75.10%
19	Validation loss: 24965.107422	Best loss: 525.573730	Accuracy: 76.58%
20	Validation loss: 43803.289062	Best loss: 525.573730	Accuracy: 65.87%
21	Validation loss: 23334.890625	Best loss: 525.573730	Accuracy: 75.65%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  35.1s
[CV] n_neurons=70, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 442.209869	Best loss: 442.209869	Accuracy: 57.74%
1	Validation loss: 3712.425049	Best loss: 442.209869	Accuracy: 52.11%
2	Validation loss: 1274.108521	Best loss: 442.209869	Accuracy: 63.25%
3	Validation loss: 456.921478	Best loss: 442.209869	Accuracy: 69.94%
4	Validation loss: 358.754761	Best loss: 358.754761	Accuracy: 67.36%
5	Validation loss: 439.790466	Best loss: 358.754761	Accuracy: 56.37%
6	Validation loss: 347.175537	Best loss: 347.175537	Accuracy: 66.73%
7	Validation loss: 334.634827	Best loss: 334.634827	Accuracy: 68.41%
8	Validation loss: 313.698334	Best loss: 313.698334	Accuracy: 73.22%
9	Validation loss: 3306.023926	Best loss: 313.698334	Accuracy: 57.31%
10	Validation loss: 394408.687500	Best loss: 313.698334	Accuracy: 53.28%
11	Validation loss: 309665.281250	Best loss: 313.698334	Accuracy: 67.90%
12	Validation loss: 28718.974609	Best loss: 313.698334	Accuracy: 52.66%
13	Validation loss: 14877.589844	Best loss: 313.698334	Accuracy: 67.47%
14	Validation loss: 11129.398438	Best loss: 313.698334	Accuracy: 64.97%
15	Validation loss: 30909.878906	Best loss: 313.698334	Accuracy: 59.11%
16	Validation loss: 315737.375000	Best loss: 313.698334	Accuracy: 49.61%
17	Validation loss: 10379.825195	Best loss: 313.698334	Accuracy: 55.28%
18	Validation loss: 20694.568359	Best loss: 313.698334	Accuracy: 51.95%
19	Validation loss: 5628.385254	Best loss: 313.698334	Accuracy: 58.64%
20	Validation loss: 14994.076172	Best loss: 313.698334	Accuracy: 43.28%
21	Validation loss: 4851.785156	Best loss: 313.698334	Accuracy: 76.08%
22	Validation loss: 17414.195312	Best loss: 313.698334	Accuracy: 55.00%
23	Validation loss: 4032.569092	Best loss: 313.698334	Accuracy: 65.95%
24	Validation loss: 1966644.250000	Best loss: 313.698334	Accuracy: 72.63%
25	Validation loss: 84241.171875	Best loss: 313.698334	Accuracy: 69.66%
26	Validation loss: 29616.437500	Best loss: 313.698334	Accuracy: 73.57%
27	Validation loss: 40769.898438	Best loss: 313.698334	Accuracy: 75.61%
28	Validation loss: 19415.447266	Best loss: 313.698334	Accuracy: 71.62%
29	Validation loss: 21677.525391	Best loss: 313.698334	Accuracy: 72.13%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  48.3s
[CV] n_neurons=70, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 1313.233032	Best loss: 1313.233032	Accuracy: 53.21%
1	Validation loss: 2355.694580	Best loss: 1313.233032	Accuracy: 81.59%
2	Validation loss: 15686.356445	Best loss: 1313.233032	Accuracy: 54.96%
3	Validation loss: 539.577698	Best loss: 539.577698	Accuracy: 83.19%
4	Validation loss: 2328.637207	Best loss: 539.577698	Accuracy: 75.22%
5	Validation loss: 993.805298	Best loss: 539.577698	Accuracy: 72.24%
6	Validation loss: 307.124695	Best loss: 307.124695	Accuracy: 82.25%
7	Validation loss: 523.085693	Best loss: 307.124695	Accuracy: 78.07%
8	Validation loss: 5948.971680	Best loss: 307.124695	Accuracy: 78.81%
9	Validation loss: 354882.062500	Best loss: 307.124695	Accuracy: 69.66%
10	Validation loss: 55527.148438	Best loss: 307.124695	Accuracy: 73.26%
11	Validation loss: 135228.187500	Best loss: 307.124695	Accuracy: 68.18%
12	Validation loss: 31657.078125	Best loss: 307.124695	Accuracy: 71.97%
13	Validation loss: 71531.312500	Best loss: 307.124695	Accuracy: 72.28%
14	Validation loss: 67058.617188	Best loss: 307.124695	Accuracy: 72.63%
15	Validation loss: 33831.898438	Best loss: 307.124695	Accuracy: 73.10%
16	Validation loss: 16312.641602	Best loss: 307.124695	Accuracy: 76.78%
17	Validation loss: 18155.107422	Best loss: 307.124695	Accuracy: 74.75%
18	Validation loss: 9498.923828	Best loss: 307.124695	Accuracy: 77.29%
19	Validation loss: 7805.110840	Best loss: 307.124695	Accuracy: 76.82%
20	Validation loss: 6149.246582	Best loss: 307.124695	Accuracy: 73.77%
21	Validation loss: 15827.110352	Best loss: 307.124695	Accuracy: 72.36%
22	Validation loss: 4680.749023	Best loss: 307.124695	Accuracy: 78.15%
23	Validation loss: 12239.550781	Best loss: 307.124695	Accuracy: 75.02%
24	Validation loss: 14137.562500	Best loss: 307.124695	Accuracy: 75.61%
25	Validation loss: 549345.812500	Best loss: 307.124695	Accuracy: 77.64%
26	Validation loss: 52342.218750	Best loss: 307.124695	Accuracy: 86.75%
27	Validation loss: 143485.906250	Best loss: 307.124695	Accuracy: 76.31%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.1, dropout_rate=0.2, batch_size=50, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  47.7s
[CV] n_neurons=30, learning_rate=0.01, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.847942	Best loss: 0.847942	Accuracy: 59.11%
1	Validation loss: 0.617616	Best loss: 0.617616	Accuracy: 72.40%
2	Validation loss: 0.462846	Best loss: 0.462846	Accuracy: 82.29%
3	Validation loss: 0.398188	Best loss: 0.398188	Accuracy: 77.91%
4	Validation loss: 0.369592	Best loss: 0.369592	Accuracy: 79.48%
5	Validation loss: 0.374410	Best loss: 0.369592	Accuracy: 78.23%
6	Validation loss: 0.367052	Best loss: 0.367052	Accuracy: 79.44%
7	Validation loss: 0.380846	Best loss: 0.367052	Accuracy: 79.67%
8	Validation loss: 0.389198	Best loss: 0.367052	Accuracy: 79.75%
9	Validation loss: 0.381294	Best loss: 0.367052	Accuracy: 79.63%
10	Validation loss: 0.363237	Best loss: 0.363237	Accuracy: 78.26%
11	Validation loss: 0.366708	Best loss: 0.363237	Accuracy: 79.63%
12	Validation loss: 0.368574	Best loss: 0.363237	Accuracy: 78.58%
13	Validation loss: 0.351640	Best loss: 0.351640	Accuracy: 80.06%
14	Validation loss: 0.359178	Best loss: 0.351640	Accuracy: 79.75%
15	Validation loss: 0.356835	Best loss: 0.351640	Accuracy: 79.55%
16	Validation loss: 0.364268	Best loss: 0.351640	Accuracy: 79.59%
17	Validation loss: 0.365065	Best loss: 0.351640	Accuracy: 79.79%
18	Validation loss: 0.355033	Best loss: 0.351640	Accuracy: 79.55%
19	Validation loss: 0.353125	Best loss: 0.351640	Accuracy: 79.71%
20	Validation loss: 0.350518	Best loss: 0.350518	Accuracy: 79.75%
21	Validation loss: 0.346175	Best loss: 0.346175	Accuracy: 79.87%
22	Validation loss: 0.357677	Best loss: 0.346175	Accuracy: 79.87%
23	Validation loss: 0.351967	Best loss: 0.346175	Accuracy: 78.89%
24	Validation loss: 0.348860	Best loss: 0.346175	Accuracy: 79.98%
25	Validation loss: 0.361044	Best loss: 0.346175	Accuracy: 78.81%
26	Validation loss: 0.363025	Best loss: 0.346175	Accuracy: 79.87%
27	Validation loss: 0.348095	Best loss: 0.346175	Accuracy: 78.85%
28	Validation loss: 0.343988	Best loss: 0.343988	Accuracy: 78.89%
29	Validation loss: 0.354096	Best loss: 0.343988	Accuracy: 79.83%
30	Validation loss: 0.356296	Best loss: 0.343988	Accuracy: 79.71%
31	Validation loss: 0.349198	Best loss: 0.343988	Accuracy: 79.71%
32	Validation loss: 0.342209	Best loss: 0.342209	Accuracy: 76.04%
33	Validation loss: 0.346310	Best loss: 0.342209	Accuracy: 79.59%
34	Validation loss: 0.345271	Best loss: 0.342209	Accuracy: 78.89%
35	Validation loss: 0.353039	Best loss: 0.342209	Accuracy: 79.59%
36	Validation loss: 0.346498	Best loss: 0.342209	Accuracy: 79.71%
37	Validation loss: 0.342824	Best loss: 0.342209	Accuracy: 79.83%
38	Validation loss: 0.343755	Best loss: 0.342209	Accuracy: 78.97%
39	Validation loss: 0.355206	Best loss: 0.342209	Accuracy: 79.01%
40	Validation loss: 0.339175	Best loss: 0.339175	Accuracy: 79.12%
41	Validation loss: 0.349347	Best loss: 0.339175	Accuracy: 79.71%
42	Validation loss: 0.349220	Best loss: 0.339175	Accuracy: 76.51%
43	Validation loss: 0.348540	Best loss: 0.339175	Accuracy: 79.79%
44	Validation loss: 0.344108	Best loss: 0.339175	Accuracy: 79.91%
45	Validation loss: 0.348775	Best loss: 0.339175	Accuracy: 78.85%
46	Validation loss: 0.352526	Best loss: 0.339175	Accuracy: 78.97%
47	Validation loss: 0.345242	Best loss: 0.339175	Accuracy: 77.25%
48	Validation loss: 0.346695	Best loss: 0.339175	Accuracy: 79.87%
49	Validation loss: 0.346542	Best loss: 0.339175	Accuracy: 79.01%
50	Validation loss: 0.342828	Best loss: 0.339175	Accuracy: 80.10%
51	Validation loss: 0.349621	Best loss: 0.339175	Accuracy: 79.83%
52	Validation loss: 0.355414	Best loss: 0.339175	Accuracy: 79.55%
53	Validation loss: 0.344462	Best loss: 0.339175	Accuracy: 79.98%
54	Validation loss: 0.353612	Best loss: 0.339175	Accuracy: 79.09%
55	Validation loss: 0.350994	Best loss: 0.339175	Accuracy: 79.52%
56	Validation loss: 0.349004	Best loss: 0.339175	Accuracy: 78.93%
57	Validation loss: 0.344033	Best loss: 0.339175	Accuracy: 79.05%
58	Validation loss: 0.350715	Best loss: 0.339175	Accuracy: 79.71%
59	Validation loss: 0.351857	Best loss: 0.339175	Accuracy: 79.67%
60	Validation loss: 0.345841	Best loss: 0.339175	Accuracy: 79.01%
61	Validation loss: 0.347440	Best loss: 0.339175	Accuracy: 78.93%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  29.9s
[CV] n_neurons=30, learning_rate=0.01, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.582940	Best loss: 0.582940	Accuracy: 79.55%
1	Validation loss: 0.381304	Best loss: 0.381304	Accuracy: 78.81%
2	Validation loss: 0.370679	Best loss: 0.370679	Accuracy: 79.16%
3	Validation loss: 0.363979	Best loss: 0.363979	Accuracy: 79.32%
4	Validation loss: 0.383331	Best loss: 0.363979	Accuracy: 78.26%
5	Validation loss: 0.372376	Best loss: 0.363979	Accuracy: 79.48%
6	Validation loss: 0.368187	Best loss: 0.363979	Accuracy: 79.40%
7	Validation loss: 0.355793	Best loss: 0.355793	Accuracy: 85.65%
8	Validation loss: 0.326864	Best loss: 0.326864	Accuracy: 83.46%
9	Validation loss: 0.302584	Best loss: 0.302584	Accuracy: 92.34%
10	Validation loss: 0.249720	Best loss: 0.249720	Accuracy: 93.86%
11	Validation loss: 0.216447	Best loss: 0.216447	Accuracy: 95.07%
12	Validation loss: 0.181849	Best loss: 0.181849	Accuracy: 95.43%
13	Validation loss: 0.177724	Best loss: 0.177724	Accuracy: 95.93%
14	Validation loss: 0.163276	Best loss: 0.163276	Accuracy: 95.74%
15	Validation loss: 0.155927	Best loss: 0.155927	Accuracy: 96.17%
16	Validation loss: 0.163826	Best loss: 0.155927	Accuracy: 95.93%
17	Validation loss: 0.152370	Best loss: 0.152370	Accuracy: 95.97%
18	Validation loss: 0.161056	Best loss: 0.152370	Accuracy: 95.70%
19	Validation loss: 0.166814	Best loss: 0.152370	Accuracy: 95.93%
20	Validation loss: 0.159169	Best loss: 0.152370	Accuracy: 96.17%
21	Validation loss: 0.162084	Best loss: 0.152370	Accuracy: 96.09%
22	Validation loss: 0.156020	Best loss: 0.152370	Accuracy: 95.90%
23	Validation loss: 0.169216	Best loss: 0.152370	Accuracy: 95.86%
24	Validation loss: 0.152296	Best loss: 0.152296	Accuracy: 96.29%
25	Validation loss: 0.162815	Best loss: 0.152296	Accuracy: 96.13%
26	Validation loss: 0.177424	Best loss: 0.152296	Accuracy: 95.07%
27	Validation loss: 0.152402	Best loss: 0.152296	Accuracy: 96.13%
28	Validation loss: 0.155214	Best loss: 0.152296	Accuracy: 96.25%
29	Validation loss: 0.179105	Best loss: 0.152296	Accuracy: 95.93%
30	Validation loss: 0.150995	Best loss: 0.150995	Accuracy: 96.17%
31	Validation loss: 0.158433	Best loss: 0.150995	Accuracy: 96.25%
32	Validation loss: 0.145228	Best loss: 0.145228	Accuracy: 96.60%
33	Validation loss: 0.157904	Best loss: 0.145228	Accuracy: 96.25%
34	Validation loss: 0.152464	Best loss: 0.145228	Accuracy: 96.13%
35	Validation loss: 0.139109	Best loss: 0.139109	Accuracy: 96.60%
36	Validation loss: 0.150911	Best loss: 0.139109	Accuracy: 96.21%
37	Validation loss: 0.154098	Best loss: 0.139109	Accuracy: 96.17%
38	Validation loss: 0.153228	Best loss: 0.139109	Accuracy: 96.40%
39	Validation loss: 0.138354	Best loss: 0.138354	Accuracy: 96.64%
40	Validation loss: 0.158072	Best loss: 0.138354	Accuracy: 96.29%
41	Validation loss: 0.145092	Best loss: 0.138354	Accuracy: 96.44%
42	Validation loss: 0.152994	Best loss: 0.138354	Accuracy: 96.36%
43	Validation loss: 0.158161	Best loss: 0.138354	Accuracy: 96.25%
44	Validation loss: 0.142079	Best loss: 0.138354	Accuracy: 96.60%
45	Validation loss: 0.151296	Best loss: 0.138354	Accuracy: 96.21%
46	Validation loss: 0.141642	Best loss: 0.138354	Accuracy: 96.72%
47	Validation loss: 0.137585	Best loss: 0.137585	Accuracy: 96.44%
48	Validation loss: 0.127121	Best loss: 0.127121	Accuracy: 96.76%
49	Validation loss: 0.149973	Best loss: 0.127121	Accuracy: 96.29%
50	Validation loss: 0.142492	Best loss: 0.127121	Accuracy: 96.33%
51	Validation loss: 0.143312	Best loss: 0.127121	Accuracy: 96.48%
52	Validation loss: 0.141625	Best loss: 0.127121	Accuracy: 96.44%
53	Validation loss: 0.147338	Best loss: 0.127121	Accuracy: 96.36%
54	Validation loss: 0.137948	Best loss: 0.127121	Accuracy: 96.60%
55	Validation loss: 0.142784	Best loss: 0.127121	Accuracy: 96.48%
56	Validation loss: 0.139787	Best loss: 0.127121	Accuracy: 96.60%
57	Validation loss: 0.144475	Best loss: 0.127121	Accuracy: 96.33%
58	Validation loss: 0.143721	Best loss: 0.127121	Accuracy: 96.56%
59	Validation loss: 0.148385	Best loss: 0.127121	Accuracy: 96.33%
60	Validation loss: 0.138819	Best loss: 0.127121	Accuracy: 96.60%
61	Validation loss: 0.148129	Best loss: 0.127121	Accuracy: 96.40%
62	Validation loss: 0.140237	Best loss: 0.127121	Accuracy: 96.64%
63	Validation loss: 0.141606	Best loss: 0.127121	Accuracy: 96.52%
64	Validation loss: 0.143720	Best loss: 0.127121	Accuracy: 96.48%
65	Validation loss: 0.162327	Best loss: 0.127121	Accuracy: 95.90%
66	Validation loss: 0.145935	Best loss: 0.127121	Accuracy: 96.48%
67	Validation loss: 0.147462	Best loss: 0.127121	Accuracy: 96.21%
68	Validation loss: 0.147127	Best loss: 0.127121	Accuracy: 96.25%
69	Validation loss: 0.138818	Best loss: 0.127121	Accuracy: 96.44%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  26.2s
[CV] n_neurons=30, learning_rate=0.01, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 0.801938	Best loss: 0.801938	Accuracy: 64.39%
1	Validation loss: 0.579761	Best loss: 0.579761	Accuracy: 73.57%
2	Validation loss: 0.417414	Best loss: 0.417414	Accuracy: 78.38%
3	Validation loss: 0.385496	Best loss: 0.385496	Accuracy: 79.20%
4	Validation loss: 0.370852	Best loss: 0.370852	Accuracy: 77.80%
5	Validation loss: 0.385477	Best loss: 0.370852	Accuracy: 79.20%
6	Validation loss: 0.371852	Best loss: 0.370852	Accuracy: 78.34%
7	Validation loss: 0.375159	Best loss: 0.370852	Accuracy: 81.70%
8	Validation loss: 0.372170	Best loss: 0.370852	Accuracy: 79.32%
9	Validation loss: 0.369181	Best loss: 0.369181	Accuracy: 78.50%
10	Validation loss: 0.372455	Best loss: 0.369181	Accuracy: 79.87%
11	Validation loss: 0.372511	Best loss: 0.369181	Accuracy: 78.46%
12	Validation loss: 0.364016	Best loss: 0.364016	Accuracy: 78.62%
13	Validation loss: 0.358267	Best loss: 0.358267	Accuracy: 79.55%
14	Validation loss: 0.365724	Best loss: 0.358267	Accuracy: 79.67%
15	Validation loss: 0.363595	Best loss: 0.358267	Accuracy: 79.75%
16	Validation loss: 0.358540	Best loss: 0.358267	Accuracy: 80.06%
17	Validation loss: 0.360701	Best loss: 0.358267	Accuracy: 79.48%
18	Validation loss: 0.362126	Best loss: 0.358267	Accuracy: 79.52%
19	Validation loss: 0.354102	Best loss: 0.354102	Accuracy: 80.38%
20	Validation loss: 0.351712	Best loss: 0.351712	Accuracy: 81.94%
21	Validation loss: 0.347010	Best loss: 0.347010	Accuracy: 83.27%
22	Validation loss: 0.359609	Best loss: 0.347010	Accuracy: 83.78%
23	Validation loss: 0.357557	Best loss: 0.347010	Accuracy: 79.40%
24	Validation loss: 0.347020	Best loss: 0.347010	Accuracy: 84.52%
25	Validation loss: 0.313402	Best loss: 0.313402	Accuracy: 89.37%
26	Validation loss: 0.307519	Best loss: 0.307519	Accuracy: 92.26%
27	Validation loss: 0.318237	Best loss: 0.307519	Accuracy: 79.48%
28	Validation loss: 0.291224	Best loss: 0.291224	Accuracy: 93.47%
29	Validation loss: 0.239026	Best loss: 0.239026	Accuracy: 94.14%
30	Validation loss: 0.227331	Best loss: 0.227331	Accuracy: 93.43%
31	Validation loss: 0.240824	Best loss: 0.227331	Accuracy: 94.33%
32	Validation loss: 0.202740	Best loss: 0.202740	Accuracy: 94.72%
33	Validation loss: 0.205337	Best loss: 0.202740	Accuracy: 95.58%
34	Validation loss: 0.194603	Best loss: 0.194603	Accuracy: 95.54%
35	Validation loss: 0.182721	Best loss: 0.182721	Accuracy: 95.82%
36	Validation loss: 0.208556	Best loss: 0.182721	Accuracy: 95.58%
37	Validation loss: 0.159405	Best loss: 0.159405	Accuracy: 95.78%
38	Validation loss: 0.171268	Best loss: 0.159405	Accuracy: 95.74%
39	Validation loss: 0.174888	Best loss: 0.159405	Accuracy: 95.62%
40	Validation loss: 0.171751	Best loss: 0.159405	Accuracy: 96.01%
41	Validation loss: 0.157472	Best loss: 0.157472	Accuracy: 96.05%
42	Validation loss: 0.157493	Best loss: 0.157472	Accuracy: 95.97%
43	Validation loss: 0.164669	Best loss: 0.157472	Accuracy: 95.62%
44	Validation loss: 0.154828	Best loss: 0.154828	Accuracy: 95.86%
45	Validation loss: 0.153140	Best loss: 0.153140	Accuracy: 96.01%
46	Validation loss: 0.153456	Best loss: 0.153140	Accuracy: 96.17%
47	Validation loss: 0.145952	Best loss: 0.145952	Accuracy: 96.09%
48	Validation loss: 0.158801	Best loss: 0.145952	Accuracy: 95.62%
49	Validation loss: 0.161921	Best loss: 0.145952	Accuracy: 95.74%
50	Validation loss: 0.158838	Best loss: 0.145952	Accuracy: 95.82%
51	Validation loss: 0.151859	Best loss: 0.145952	Accuracy: 96.09%
52	Validation loss: 0.164114	Best loss: 0.145952	Accuracy: 95.97%
53	Validation loss: 0.137809	Best loss: 0.137809	Accuracy: 96.21%
54	Validation loss: 0.140031	Best loss: 0.137809	Accuracy: 96.60%
55	Validation loss: 0.154612	Best loss: 0.137809	Accuracy: 96.21%
56	Validation loss: 0.148411	Best loss: 0.137809	Accuracy: 96.25%
57	Validation loss: 0.143065	Best loss: 0.137809	Accuracy: 96.21%
58	Validation loss: 0.151356	Best loss: 0.137809	Accuracy: 96.21%
59	Validation loss: 0.135380	Best loss: 0.135380	Accuracy: 96.48%
60	Validation loss: 0.141253	Best loss: 0.135380	Accuracy: 96.09%
61	Validation loss: 0.146236	Best loss: 0.135380	Accuracy: 96.29%
62	Validation loss: 0.148370	Best loss: 0.135380	Accuracy: 96.36%
63	Validation loss: 0.134819	Best loss: 0.134819	Accuracy: 96.48%
64	Validation loss: 0.146287	Best loss: 0.134819	Accuracy: 96.21%
65	Validation loss: 0.135518	Best loss: 0.134819	Accuracy: 96.52%
66	Validation loss: 0.137285	Best loss: 0.134819	Accuracy: 96.44%
67	Validation loss: 0.140757	Best loss: 0.134819	Accuracy: 96.52%
68	Validation loss: 0.142694	Best loss: 0.134819	Accuracy: 96.17%
69	Validation loss: 0.141713	Best loss: 0.134819	Accuracy: 96.44%
70	Validation loss: 0.138273	Best loss: 0.134819	Accuracy: 96.40%
71	Validation loss: 0.149100	Best loss: 0.134819	Accuracy: 96.56%
72	Validation loss: 0.140949	Best loss: 0.134819	Accuracy: 96.40%
73	Validation loss: 0.148105	Best loss: 0.134819	Accuracy: 96.44%
74	Validation loss: 0.124772	Best loss: 0.124772	Accuracy: 96.95%
75	Validation loss: 0.134622	Best loss: 0.124772	Accuracy: 96.60%
76	Validation loss: 0.143128	Best loss: 0.124772	Accuracy: 96.48%
77	Validation loss: 0.142221	Best loss: 0.124772	Accuracy: 96.36%
78	Validation loss: 0.143379	Best loss: 0.124772	Accuracy: 96.36%
79	Validation loss: 0.149545	Best loss: 0.124772	Accuracy: 96.76%
80	Validation loss: 0.138871	Best loss: 0.124772	Accuracy: 96.48%
81	Validation loss: 0.144424	Best loss: 0.124772	Accuracy: 96.44%
82	Validation loss: 0.150858	Best loss: 0.124772	Accuracy: 96.29%
83	Validation loss: 0.141073	Best loss: 0.124772	Accuracy: 96.52%
84	Validation loss: 0.151730	Best loss: 0.124772	Accuracy: 96.17%
85	Validation loss: 0.149383	Best loss: 0.124772	Accuracy: 96.44%
86	Validation loss: 0.146467	Best loss: 0.124772	Accuracy: 96.44%
87	Validation loss: 0.137415	Best loss: 0.124772	Accuracy: 96.52%
88	Validation loss: 0.129791	Best loss: 0.124772	Accuracy: 96.76%
89	Validation loss: 0.155530	Best loss: 0.124772	Accuracy: 96.01%
90	Validation loss: 0.139837	Best loss: 0.124772	Accuracy: 96.21%
91	Validation loss: 0.136862	Best loss: 0.124772	Accuracy: 97.03%
92	Validation loss: 0.133178	Best loss: 0.124772	Accuracy: 96.60%
93	Validation loss: 0.149370	Best loss: 0.124772	Accuracy: 96.56%
94	Validation loss: 0.138070	Best loss: 0.124772	Accuracy: 96.68%
95	Validation loss: 0.133759	Best loss: 0.124772	Accuracy: 96.60%
Early stopping!
[CV]  n_neurons=30, learning_rate=0.01, dropout_rate=0.6, batch_size=500, activation=&lt;function elu at 0x1243639d8&gt;, total=  35.7s
[CV] n_neurons=90, learning_rate=0.02, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.142226	Best loss: 0.142226	Accuracy: 95.82%
1	Validation loss: 0.101100	Best loss: 0.101100	Accuracy: 97.22%
2	Validation loss: 0.082679	Best loss: 0.082679	Accuracy: 97.65%
3	Validation loss: 0.088614	Best loss: 0.082679	Accuracy: 97.50%
4	Validation loss: 0.077992	Best loss: 0.077992	Accuracy: 97.73%
5	Validation loss: 0.082797	Best loss: 0.077992	Accuracy: 97.73%
6	Validation loss: 0.082745	Best loss: 0.077992	Accuracy: 97.62%
7	Validation loss: 0.082483	Best loss: 0.077992	Accuracy: 97.85%
8	Validation loss: 0.091473	Best loss: 0.077992	Accuracy: 98.24%
9	Validation loss: 0.068511	Best loss: 0.068511	Accuracy: 98.40%
10	Validation loss: 0.079130	Best loss: 0.068511	Accuracy: 97.93%
11	Validation loss: 0.087732	Best loss: 0.068511	Accuracy: 98.05%
12	Validation loss: 0.077804	Best loss: 0.068511	Accuracy: 97.89%
13	Validation loss: 0.093567	Best loss: 0.068511	Accuracy: 98.05%
14	Validation loss: 0.085800	Best loss: 0.068511	Accuracy: 97.62%
15	Validation loss: 0.102883	Best loss: 0.068511	Accuracy: 97.46%
16	Validation loss: 0.082945	Best loss: 0.068511	Accuracy: 98.01%
17	Validation loss: 0.096350	Best loss: 0.068511	Accuracy: 97.81%
18	Validation loss: 0.095282	Best loss: 0.068511	Accuracy: 97.93%
19	Validation loss: 0.092364	Best loss: 0.068511	Accuracy: 97.81%
20	Validation loss: 0.136048	Best loss: 0.068511	Accuracy: 97.42%
21	Validation loss: 0.146936	Best loss: 0.068511	Accuracy: 97.22%
22	Validation loss: 0.176223	Best loss: 0.068511	Accuracy: 95.07%
23	Validation loss: 0.168018	Best loss: 0.068511	Accuracy: 96.09%
24	Validation loss: 0.158425	Best loss: 0.068511	Accuracy: 95.35%
25	Validation loss: 0.122084	Best loss: 0.068511	Accuracy: 96.52%
26	Validation loss: 0.125107	Best loss: 0.068511	Accuracy: 97.26%
27	Validation loss: 0.144574	Best loss: 0.068511	Accuracy: 96.21%
28	Validation loss: 0.176887	Best loss: 0.068511	Accuracy: 96.33%
29	Validation loss: 0.122932	Best loss: 0.068511	Accuracy: 97.46%
30	Validation loss: 0.149929	Best loss: 0.068511	Accuracy: 97.65%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  22.1s
[CV] n_neurons=90, learning_rate=0.02, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.141302	Best loss: 0.141302	Accuracy: 96.52%
1	Validation loss: 0.105229	Best loss: 0.105229	Accuracy: 96.91%
2	Validation loss: 0.097837	Best loss: 0.097837	Accuracy: 97.85%
3	Validation loss: 0.094514	Best loss: 0.094514	Accuracy: 97.11%
4	Validation loss: 0.104073	Best loss: 0.094514	Accuracy: 97.69%
5	Validation loss: 0.076207	Best loss: 0.076207	Accuracy: 98.01%
6	Validation loss: 0.077966	Best loss: 0.076207	Accuracy: 97.85%
7	Validation loss: 0.074222	Best loss: 0.074222	Accuracy: 97.73%
8	Validation loss: 0.064649	Best loss: 0.064649	Accuracy: 98.08%
9	Validation loss: 0.079591	Best loss: 0.064649	Accuracy: 98.36%
10	Validation loss: 0.076935	Best loss: 0.064649	Accuracy: 98.20%
11	Validation loss: 0.067643	Best loss: 0.064649	Accuracy: 98.28%
12	Validation loss: 0.114738	Best loss: 0.064649	Accuracy: 97.97%
13	Validation loss: 0.097255	Best loss: 0.064649	Accuracy: 97.26%
14	Validation loss: 0.092114	Best loss: 0.064649	Accuracy: 97.62%
15	Validation loss: 0.091174	Best loss: 0.064649	Accuracy: 97.69%
16	Validation loss: 0.135843	Best loss: 0.064649	Accuracy: 97.85%
17	Validation loss: 0.125083	Best loss: 0.064649	Accuracy: 96.64%
18	Validation loss: 0.123685	Best loss: 0.064649	Accuracy: 97.46%
19	Validation loss: 0.102269	Best loss: 0.064649	Accuracy: 97.85%
20	Validation loss: 0.113220	Best loss: 0.064649	Accuracy: 97.89%
21	Validation loss: 0.124326	Best loss: 0.064649	Accuracy: 97.58%
22	Validation loss: 0.094635	Best loss: 0.064649	Accuracy: 97.73%
23	Validation loss: 0.089853	Best loss: 0.064649	Accuracy: 97.77%
24	Validation loss: 0.118233	Best loss: 0.064649	Accuracy: 98.01%
25	Validation loss: 0.099025	Best loss: 0.064649	Accuracy: 97.26%
26	Validation loss: 0.090234	Best loss: 0.064649	Accuracy: 97.69%
27	Validation loss: 0.080362	Best loss: 0.064649	Accuracy: 98.05%
28	Validation loss: 0.110570	Best loss: 0.064649	Accuracy: 97.07%
29	Validation loss: 0.112093	Best loss: 0.064649	Accuracy: 97.65%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  23.6s
[CV] n_neurons=90, learning_rate=0.02, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.135210	Best loss: 0.135210	Accuracy: 95.93%
1	Validation loss: 0.105114	Best loss: 0.105114	Accuracy: 97.34%
2	Validation loss: 0.087101	Best loss: 0.087101	Accuracy: 97.73%
3	Validation loss: 0.082860	Best loss: 0.082860	Accuracy: 97.65%
4	Validation loss: 0.089885	Best loss: 0.082860	Accuracy: 97.73%
5	Validation loss: 0.074400	Best loss: 0.074400	Accuracy: 97.89%
6	Validation loss: 0.074146	Best loss: 0.074146	Accuracy: 97.97%
7	Validation loss: 0.073309	Best loss: 0.073309	Accuracy: 98.44%
8	Validation loss: 0.067061	Best loss: 0.067061	Accuracy: 98.05%
9	Validation loss: 0.069381	Best loss: 0.067061	Accuracy: 98.08%
10	Validation loss: 0.079983	Best loss: 0.067061	Accuracy: 98.01%
11	Validation loss: 0.063063	Best loss: 0.063063	Accuracy: 98.12%
12	Validation loss: 0.064108	Best loss: 0.063063	Accuracy: 98.48%
13	Validation loss: 0.072180	Best loss: 0.063063	Accuracy: 98.12%
14	Validation loss: 0.069900	Best loss: 0.063063	Accuracy: 98.32%
15	Validation loss: 0.061912	Best loss: 0.061912	Accuracy: 98.44%
16	Validation loss: 0.073774	Best loss: 0.061912	Accuracy: 98.32%
17	Validation loss: 0.080184	Best loss: 0.061912	Accuracy: 98.28%
18	Validation loss: 0.082910	Best loss: 0.061912	Accuracy: 98.12%
19	Validation loss: 0.088829	Best loss: 0.061912	Accuracy: 98.16%
20	Validation loss: 0.084244	Best loss: 0.061912	Accuracy: 98.36%
21	Validation loss: 0.218212	Best loss: 0.061912	Accuracy: 97.22%
22	Validation loss: 0.194598	Best loss: 0.061912	Accuracy: 95.54%
23	Validation loss: 0.190149	Best loss: 0.061912	Accuracy: 96.68%
24	Validation loss: 0.228126	Best loss: 0.061912	Accuracy: 94.57%
25	Validation loss: 0.179392	Best loss: 0.061912	Accuracy: 95.31%
26	Validation loss: 0.137730	Best loss: 0.061912	Accuracy: 96.44%
27	Validation loss: 0.123079	Best loss: 0.061912	Accuracy: 96.76%
28	Validation loss: 0.100145	Best loss: 0.061912	Accuracy: 97.69%
29	Validation loss: 0.125392	Best loss: 0.061912	Accuracy: 97.42%
30	Validation loss: 0.129827	Best loss: 0.061912	Accuracy: 97.69%
31	Validation loss: 0.254565	Best loss: 0.061912	Accuracy: 91.44%
32	Validation loss: 0.178574	Best loss: 0.061912	Accuracy: 97.15%
33	Validation loss: 0.125917	Best loss: 0.061912	Accuracy: 97.50%
34	Validation loss: 0.153418	Best loss: 0.061912	Accuracy: 97.30%
35	Validation loss: 0.172296	Best loss: 0.061912	Accuracy: 96.56%
36	Validation loss: 0.176455	Best loss: 0.061912	Accuracy: 96.68%
Early stopping!
[CV]  n_neurons=90, learning_rate=0.02, dropout_rate=0.3, batch_size=500, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  28.6s
[CV] n_neurons=70, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.089502	Best loss: 0.089502	Accuracy: 97.81%
1	Validation loss: 0.080642	Best loss: 0.080642	Accuracy: 98.32%
2	Validation loss: 0.076685	Best loss: 0.076685	Accuracy: 98.08%
3	Validation loss: 0.070340	Best loss: 0.070340	Accuracy: 98.01%
4	Validation loss: 0.068492	Best loss: 0.068492	Accuracy: 98.01%
5	Validation loss: 0.080245	Best loss: 0.068492	Accuracy: 98.20%
6	Validation loss: 0.067895	Best loss: 0.067895	Accuracy: 98.67%
7	Validation loss: 0.071206	Best loss: 0.067895	Accuracy: 98.36%
8	Validation loss: 0.052538	Best loss: 0.052538	Accuracy: 98.63%
9	Validation loss: 0.058451	Best loss: 0.052538	Accuracy: 98.28%
10	Validation loss: 0.160285	Best loss: 0.052538	Accuracy: 98.05%
11	Validation loss: 0.063312	Best loss: 0.052538	Accuracy: 98.40%
12	Validation loss: 0.073196	Best loss: 0.052538	Accuracy: 98.24%
13	Validation loss: 0.082572	Best loss: 0.052538	Accuracy: 97.93%
14	Validation loss: 0.073538	Best loss: 0.052538	Accuracy: 98.36%
15	Validation loss: 0.100742	Best loss: 0.052538	Accuracy: 98.12%
16	Validation loss: 0.059679	Best loss: 0.052538	Accuracy: 98.32%
17	Validation loss: 0.071871	Best loss: 0.052538	Accuracy: 98.44%
18	Validation loss: 0.090862	Best loss: 0.052538	Accuracy: 98.20%
19	Validation loss: 0.118001	Best loss: 0.052538	Accuracy: 98.40%
20	Validation loss: 0.077244	Best loss: 0.052538	Accuracy: 98.28%
21	Validation loss: 0.077012	Best loss: 0.052538	Accuracy: 98.12%
22	Validation loss: 0.106992	Best loss: 0.052538	Accuracy: 98.20%
23	Validation loss: 0.099294	Best loss: 0.052538	Accuracy: 98.28%
24	Validation loss: 0.086155	Best loss: 0.052538	Accuracy: 98.24%
25	Validation loss: 0.088460	Best loss: 0.052538	Accuracy: 98.24%
26	Validation loss: 0.103893	Best loss: 0.052538	Accuracy: 98.48%
27	Validation loss: 0.111438	Best loss: 0.052538	Accuracy: 98.01%
28	Validation loss: 0.087605	Best loss: 0.052538	Accuracy: 98.40%
29	Validation loss: 0.083497	Best loss: 0.052538	Accuracy: 98.55%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  34.9s
[CV] n_neurons=70, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.103669	Best loss: 0.103669	Accuracy: 97.30%
1	Validation loss: 0.086475	Best loss: 0.086475	Accuracy: 97.81%
2	Validation loss: 0.070231	Best loss: 0.070231	Accuracy: 98.16%
3	Validation loss: 0.080969	Best loss: 0.070231	Accuracy: 98.32%
4	Validation loss: 0.078788	Best loss: 0.070231	Accuracy: 97.97%
5	Validation loss: 0.072191	Best loss: 0.070231	Accuracy: 98.20%
6	Validation loss: 0.088583	Best loss: 0.070231	Accuracy: 97.73%
7	Validation loss: 0.054985	Best loss: 0.054985	Accuracy: 98.55%
8	Validation loss: 0.069412	Best loss: 0.054985	Accuracy: 98.01%
9	Validation loss: 0.148621	Best loss: 0.054985	Accuracy: 97.58%
10	Validation loss: 0.073506	Best loss: 0.054985	Accuracy: 98.44%
11	Validation loss: 0.073169	Best loss: 0.054985	Accuracy: 98.05%
12	Validation loss: 0.096657	Best loss: 0.054985	Accuracy: 97.85%
13	Validation loss: 0.076331	Best loss: 0.054985	Accuracy: 98.36%
14	Validation loss: 0.067867	Best loss: 0.054985	Accuracy: 98.63%
15	Validation loss: 0.080017	Best loss: 0.054985	Accuracy: 98.05%
16	Validation loss: 0.113524	Best loss: 0.054985	Accuracy: 98.32%
17	Validation loss: 0.125222	Best loss: 0.054985	Accuracy: 98.01%
18	Validation loss: 0.101271	Best loss: 0.054985	Accuracy: 97.77%
19	Validation loss: 0.078673	Best loss: 0.054985	Accuracy: 98.40%
20	Validation loss: 0.102504	Best loss: 0.054985	Accuracy: 98.20%
21	Validation loss: 0.088842	Best loss: 0.054985	Accuracy: 98.28%
22	Validation loss: 0.075443	Best loss: 0.054985	Accuracy: 98.28%
23	Validation loss: 0.098337	Best loss: 0.054985	Accuracy: 98.24%
24	Validation loss: 0.064101	Best loss: 0.054985	Accuracy: 98.48%
25	Validation loss: 0.090091	Best loss: 0.054985	Accuracy: 98.20%
26	Validation loss: 0.077226	Best loss: 0.054985	Accuracy: 98.32%
27	Validation loss: 0.069569	Best loss: 0.054985	Accuracy: 98.24%
28	Validation loss: 0.068074	Best loss: 0.054985	Accuracy: 98.51%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  31.5s
[CV] n_neurons=70, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt; 
0	Validation loss: 0.103871	Best loss: 0.103871	Accuracy: 97.30%
1	Validation loss: 0.101840	Best loss: 0.101840	Accuracy: 98.01%
2	Validation loss: 0.077310	Best loss: 0.077310	Accuracy: 97.97%
3	Validation loss: 0.079186	Best loss: 0.077310	Accuracy: 97.69%
4	Validation loss: 0.075546	Best loss: 0.075546	Accuracy: 98.08%
5	Validation loss: 0.068899	Best loss: 0.068899	Accuracy: 98.16%
6	Validation loss: 0.069209	Best loss: 0.068899	Accuracy: 98.32%
7	Validation loss: 0.071266	Best loss: 0.068899	Accuracy: 98.05%
8	Validation loss: 0.069659	Best loss: 0.068899	Accuracy: 98.28%
9	Validation loss: 0.082052	Best loss: 0.068899	Accuracy: 98.48%
10	Validation loss: 0.094806	Best loss: 0.068899	Accuracy: 98.01%
11	Validation loss: 0.086660	Best loss: 0.068899	Accuracy: 98.36%
12	Validation loss: 0.088965	Best loss: 0.068899	Accuracy: 98.36%
13	Validation loss: 0.091671	Best loss: 0.068899	Accuracy: 98.44%
14	Validation loss: 0.066474	Best loss: 0.066474	Accuracy: 98.55%
15	Validation loss: 0.093433	Best loss: 0.066474	Accuracy: 98.28%
16	Validation loss: 0.071577	Best loss: 0.066474	Accuracy: 98.32%
17	Validation loss: 0.071866	Best loss: 0.066474	Accuracy: 98.40%
18	Validation loss: 0.112288	Best loss: 0.066474	Accuracy: 98.24%
19	Validation loss: 0.103696	Best loss: 0.066474	Accuracy: 98.16%
20	Validation loss: 0.097640	Best loss: 0.066474	Accuracy: 97.69%
21	Validation loss: 0.081549	Best loss: 0.066474	Accuracy: 98.32%
22	Validation loss: 0.059958	Best loss: 0.059958	Accuracy: 98.59%
23	Validation loss: 0.080632	Best loss: 0.059958	Accuracy: 98.44%
24	Validation loss: 0.076102	Best loss: 0.059958	Accuracy: 98.59%
25	Validation loss: 0.075526	Best loss: 0.059958	Accuracy: 98.40%
26	Validation loss: 0.069182	Best loss: 0.059958	Accuracy: 98.59%
27	Validation loss: 0.123582	Best loss: 0.059958	Accuracy: 98.08%
28	Validation loss: 0.111960	Best loss: 0.059958	Accuracy: 97.73%
29	Validation loss: 0.081614	Best loss: 0.059958	Accuracy: 98.12%
30	Validation loss: 0.110866	Best loss: 0.059958	Accuracy: 97.54%
31	Validation loss: 0.090705	Best loss: 0.059958	Accuracy: 98.32%
32	Validation loss: 0.082729	Best loss: 0.059958	Accuracy: 98.40%
33	Validation loss: 0.083671	Best loss: 0.059958	Accuracy: 98.67%
34	Validation loss: 0.061735	Best loss: 0.059958	Accuracy: 98.75%
35	Validation loss: 0.084922	Best loss: 0.059958	Accuracy: 98.51%
36	Validation loss: 0.072241	Best loss: 0.059958	Accuracy: 98.59%
37	Validation loss: 0.075227	Best loss: 0.059958	Accuracy: 98.71%
38	Validation loss: 0.071554	Best loss: 0.059958	Accuracy: 98.91%
39	Validation loss: 0.062178	Best loss: 0.059958	Accuracy: 98.67%
40	Validation loss: 0.090292	Best loss: 0.059958	Accuracy: 98.32%
41	Validation loss: 0.089478	Best loss: 0.059958	Accuracy: 98.44%
42	Validation loss: 0.090849	Best loss: 0.059958	Accuracy: 98.48%
43	Validation loss: 0.081332	Best loss: 0.059958	Accuracy: 98.20%
Early stopping!
[CV]  n_neurons=70, learning_rate=0.01, dropout_rate=0.2, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, total=  49.5s
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.205197	Best loss: 0.205197	Accuracy: 95.43%
1	Validation loss: 0.212285	Best loss: 0.205197	Accuracy: 94.64%
2	Validation loss: 0.186704	Best loss: 0.186704	Accuracy: 95.47%
3	Validation loss: 0.182361	Best loss: 0.182361	Accuracy: 95.04%
4	Validation loss: 0.181802	Best loss: 0.181802	Accuracy: 95.43%
5	Validation loss: 0.196816	Best loss: 0.181802	Accuracy: 95.31%
6	Validation loss: 0.207466	Best loss: 0.181802	Accuracy: 94.06%
7	Validation loss: 0.213538	Best loss: 0.181802	Accuracy: 93.94%
8	Validation loss: 0.209385	Best loss: 0.181802	Accuracy: 93.08%
9	Validation loss: 0.177911	Best loss: 0.177911	Accuracy: 95.50%
10	Validation loss: 0.326325	Best loss: 0.177911	Accuracy: 94.96%
11	Validation loss: 0.350943	Best loss: 0.177911	Accuracy: 92.77%
12	Validation loss: 0.250023	Best loss: 0.177911	Accuracy: 91.67%
13	Validation loss: 0.239372	Best loss: 0.177911	Accuracy: 92.10%
14	Validation loss: 0.281630	Best loss: 0.177911	Accuracy: 91.75%
15	Validation loss: 0.247750	Best loss: 0.177911	Accuracy: 91.63%
16	Validation loss: 0.287115	Best loss: 0.177911	Accuracy: 90.77%
17	Validation loss: 0.274021	Best loss: 0.177911	Accuracy: 92.26%
18	Validation loss: 0.344156	Best loss: 0.177911	Accuracy: 90.15%
19	Validation loss: 0.304682	Best loss: 0.177911	Accuracy: 91.05%
20	Validation loss: 0.321064	Best loss: 0.177911	Accuracy: 90.30%
21	Validation loss: 0.328108	Best loss: 0.177911	Accuracy: 89.17%
22	Validation loss: 0.554205	Best loss: 0.177911	Accuracy: 75.80%
23	Validation loss: 0.354252	Best loss: 0.177911	Accuracy: 89.56%
24	Validation loss: 0.358069	Best loss: 0.177911	Accuracy: 91.32%
25	Validation loss: 0.721719	Best loss: 0.177911	Accuracy: 77.29%
26	Validation loss: 0.367457	Best loss: 0.177911	Accuracy: 89.72%
27	Validation loss: 0.382634	Best loss: 0.177911	Accuracy: 88.00%
28	Validation loss: 0.433791	Best loss: 0.177911	Accuracy: 83.27%
29	Validation loss: 0.355365	Best loss: 0.177911	Accuracy: 88.78%
30	Validation loss: 0.323610	Best loss: 0.177911	Accuracy: 90.11%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total= 1.0min
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.240015	Best loss: 0.240015	Accuracy: 94.10%
1	Validation loss: 0.182596	Best loss: 0.182596	Accuracy: 95.15%
2	Validation loss: 0.207639	Best loss: 0.182596	Accuracy: 94.57%
3	Validation loss: 0.154565	Best loss: 0.154565	Accuracy: 96.13%
4	Validation loss: 0.159822	Best loss: 0.154565	Accuracy: 95.70%
5	Validation loss: 0.167224	Best loss: 0.154565	Accuracy: 96.29%
6	Validation loss: 0.194569	Best loss: 0.154565	Accuracy: 95.00%
7	Validation loss: 0.151625	Best loss: 0.151625	Accuracy: 95.97%
8	Validation loss: 0.200663	Best loss: 0.151625	Accuracy: 95.27%
9	Validation loss: 0.226074	Best loss: 0.151625	Accuracy: 93.67%
10	Validation loss: 0.267373	Best loss: 0.151625	Accuracy: 90.77%
11	Validation loss: 0.299768	Best loss: 0.151625	Accuracy: 92.49%
12	Validation loss: 0.297960	Best loss: 0.151625	Accuracy: 93.86%
13	Validation loss: 0.306451	Best loss: 0.151625	Accuracy: 92.81%
14	Validation loss: 0.269365	Best loss: 0.151625	Accuracy: 92.34%
15	Validation loss: 0.228697	Best loss: 0.151625	Accuracy: 94.14%
16	Validation loss: 0.314228	Best loss: 0.151625	Accuracy: 93.75%
17	Validation loss: 0.323125	Best loss: 0.151625	Accuracy: 91.40%
18	Validation loss: 0.273822	Best loss: 0.151625	Accuracy: 93.47%
19	Validation loss: 0.258546	Best loss: 0.151625	Accuracy: 92.49%
20	Validation loss: 0.300026	Best loss: 0.151625	Accuracy: 91.52%
21	Validation loss: 0.420036	Best loss: 0.151625	Accuracy: 80.96%
22	Validation loss: 0.269290	Best loss: 0.151625	Accuracy: 92.03%
23	Validation loss: 0.323932	Best loss: 0.151625	Accuracy: 91.95%
24	Validation loss: 0.317243	Best loss: 0.151625	Accuracy: 91.91%
25	Validation loss: 0.311827	Best loss: 0.151625	Accuracy: 90.89%
26	Validation loss: 0.280558	Best loss: 0.151625	Accuracy: 91.56%
27	Validation loss: 0.318231	Best loss: 0.151625	Accuracy: 92.06%
28	Validation loss: 0.321776	Best loss: 0.151625	Accuracy: 91.79%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total= 1.0min
[CV] n_neurons=160, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function relu at 0x124366d08&gt; 
0	Validation loss: 0.180646	Best loss: 0.180646	Accuracy: 95.70%
1	Validation loss: 0.208841	Best loss: 0.180646	Accuracy: 95.43%
2	Validation loss: 0.238286	Best loss: 0.180646	Accuracy: 94.14%
3	Validation loss: 0.230860	Best loss: 0.180646	Accuracy: 94.76%
4	Validation loss: 0.587307	Best loss: 0.180646	Accuracy: 79.36%
5	Validation loss: 0.527441	Best loss: 0.180646	Accuracy: 83.03%
6	Validation loss: 0.403657	Best loss: 0.180646	Accuracy: 90.50%
7	Validation loss: 0.429289	Best loss: 0.180646	Accuracy: 88.86%
8	Validation loss: 0.462165	Best loss: 0.180646	Accuracy: 82.25%
9	Validation loss: 0.422163	Best loss: 0.180646	Accuracy: 91.67%
10	Validation loss: 0.378041	Best loss: 0.180646	Accuracy: 89.60%
11	Validation loss: 0.344297	Best loss: 0.180646	Accuracy: 89.99%
12	Validation loss: 0.519798	Best loss: 0.180646	Accuracy: 87.26%
13	Validation loss: 0.439166	Best loss: 0.180646	Accuracy: 88.27%
14	Validation loss: 0.550871	Best loss: 0.180646	Accuracy: 81.47%
15	Validation loss: 0.424334	Best loss: 0.180646	Accuracy: 88.47%
16	Validation loss: 0.439553	Best loss: 0.180646	Accuracy: 90.73%
17	Validation loss: 0.723759	Best loss: 0.180646	Accuracy: 77.09%
18	Validation loss: 0.494715	Best loss: 0.180646	Accuracy: 85.11%
19	Validation loss: 0.366064	Best loss: 0.180646	Accuracy: 93.00%
20	Validation loss: 0.530490	Best loss: 0.180646	Accuracy: 84.09%
21	Validation loss: 0.575134	Best loss: 0.180646	Accuracy: 81.82%
Early stopping!
[CV]  n_neurons=160, learning_rate=0.01, dropout_rate=0.5, batch_size=50, activation=&lt;function relu at 0x124366d08&gt;, total=  50.2s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 74486.531250	Best loss: 74486.531250	Accuracy: 19.27%
1	Validation loss: 8220.798828	Best loss: 8220.798828	Accuracy: 19.08%
2	Validation loss: 131130.765625	Best loss: 8220.798828	Accuracy: 19.08%
3	Validation loss: 35765.230469	Best loss: 8220.798828	Accuracy: 19.08%
4	Validation loss: 118475.656250	Best loss: 8220.798828	Accuracy: 19.27%
5	Validation loss: 22406.175781	Best loss: 8220.798828	Accuracy: 19.08%
6	Validation loss: 9028.597656	Best loss: 8220.798828	Accuracy: 19.27%
7	Validation loss: 4903.458008	Best loss: 4903.458008	Accuracy: 20.91%
8	Validation loss: 4145.672852	Best loss: 4145.672852	Accuracy: 20.91%
9	Validation loss: 2370.601318	Best loss: 2370.601318	Accuracy: 22.01%
10	Validation loss: 3479.339355	Best loss: 2370.601318	Accuracy: 19.08%
11	Validation loss: 8075.404297	Best loss: 2370.601318	Accuracy: 18.73%
12	Validation loss: 9564.714844	Best loss: 2370.601318	Accuracy: 18.73%
13	Validation loss: 1126.287598	Best loss: 1126.287598	Accuracy: 20.91%
14	Validation loss: 4848.956055	Best loss: 1126.287598	Accuracy: 19.27%
15	Validation loss: 2922.507324	Best loss: 1126.287598	Accuracy: 20.91%
16	Validation loss: 1108.252319	Best loss: 1108.252319	Accuracy: 22.01%
17	Validation loss: 1780.974731	Best loss: 1108.252319	Accuracy: 18.73%
18	Validation loss: 1826.987061	Best loss: 1108.252319	Accuracy: 18.73%
19	Validation loss: 33999.679688	Best loss: 1108.252319	Accuracy: 18.73%
20	Validation loss: 11064615.000000	Best loss: 1108.252319	Accuracy: 19.27%
21	Validation loss: 6147041.000000	Best loss: 1108.252319	Accuracy: 18.73%
22	Validation loss: 672835.312500	Best loss: 1108.252319	Accuracy: 18.69%
23	Validation loss: 1097829.250000	Best loss: 1108.252319	Accuracy: 20.91%
24	Validation loss: 188413.625000	Best loss: 1108.252319	Accuracy: 21.85%
25	Validation loss: 389151.343750	Best loss: 1108.252319	Accuracy: 19.08%
26	Validation loss: 436933.718750	Best loss: 1108.252319	Accuracy: 18.73%
27	Validation loss: 331415.312500	Best loss: 1108.252319	Accuracy: 19.08%
28	Validation loss: 614450.875000	Best loss: 1108.252319	Accuracy: 18.73%
29	Validation loss: 85423.171875	Best loss: 1108.252319	Accuracy: 20.91%
30	Validation loss: 293587.375000	Best loss: 1108.252319	Accuracy: 19.27%
31	Validation loss: 42646.742188	Best loss: 1108.252319	Accuracy: 20.64%
32	Validation loss: 129641.195312	Best loss: 1108.252319	Accuracy: 19.08%
33	Validation loss: 57819.972656	Best loss: 1108.252319	Accuracy: 22.01%
34	Validation loss: 54554.578125	Best loss: 1108.252319	Accuracy: 19.27%
35	Validation loss: 1209596.500000	Best loss: 1108.252319	Accuracy: 19.27%
36	Validation loss: 106896.312500	Best loss: 1108.252319	Accuracy: 18.73%
37	Validation loss: 37468.457031	Best loss: 1108.252319	Accuracy: 20.91%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  51.1s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 95053.835938	Best loss: 95053.835938	Accuracy: 18.73%
1	Validation loss: 6952.389160	Best loss: 6952.389160	Accuracy: 20.91%
2	Validation loss: 104507.843750	Best loss: 6952.389160	Accuracy: 20.91%
3	Validation loss: 17182.160156	Best loss: 6952.389160	Accuracy: 19.27%
4	Validation loss: 6964.855469	Best loss: 6952.389160	Accuracy: 19.27%
5	Validation loss: 20581.054688	Best loss: 6952.389160	Accuracy: 19.27%
6	Validation loss: 13470.912109	Best loss: 6952.389160	Accuracy: 18.73%
7	Validation loss: 5958722.000000	Best loss: 6952.389160	Accuracy: 20.91%
8	Validation loss: 252956.593750	Best loss: 6952.389160	Accuracy: 19.27%
9	Validation loss: 34694.468750	Best loss: 6952.389160	Accuracy: 18.73%
10	Validation loss: 62688.601562	Best loss: 6952.389160	Accuracy: 19.27%
11	Validation loss: 54013.386719	Best loss: 6952.389160	Accuracy: 20.91%
12	Validation loss: 24482.533203	Best loss: 6952.389160	Accuracy: 22.01%
13	Validation loss: 124714.054688	Best loss: 6952.389160	Accuracy: 18.73%
14	Validation loss: 18750.750000	Best loss: 6952.389160	Accuracy: 18.73%
15	Validation loss: 41109.472656	Best loss: 6952.389160	Accuracy: 18.73%
16	Validation loss: 79004.445312	Best loss: 6952.389160	Accuracy: 22.01%
17	Validation loss: 17676.902344	Best loss: 6952.389160	Accuracy: 22.01%
18	Validation loss: 39358.792969	Best loss: 6952.389160	Accuracy: 20.91%
19	Validation loss: 18498.779297	Best loss: 6952.389160	Accuracy: 19.27%
20	Validation loss: 20108.068359	Best loss: 6952.389160	Accuracy: 19.08%
21	Validation loss: 49746.359375	Best loss: 6952.389160	Accuracy: 22.01%
22	Validation loss: 21683.742188	Best loss: 6952.389160	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  28.2s
[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt; 
0	Validation loss: 39198.656250	Best loss: 39198.656250	Accuracy: 18.73%
1	Validation loss: 14188.304688	Best loss: 14188.304688	Accuracy: 20.91%
2	Validation loss: 25198.099609	Best loss: 14188.304688	Accuracy: 19.27%
3	Validation loss: 30873.792969	Best loss: 14188.304688	Accuracy: 19.27%
4	Validation loss: 8060.634277	Best loss: 8060.634277	Accuracy: 19.08%
5	Validation loss: 8148.826660	Best loss: 8060.634277	Accuracy: 18.73%
6	Validation loss: 18647.761719	Best loss: 8060.634277	Accuracy: 18.73%
7	Validation loss: 5625.421387	Best loss: 5625.421387	Accuracy: 22.01%
8	Validation loss: 7113.268066	Best loss: 5625.421387	Accuracy: 19.08%
9	Validation loss: 913.725098	Best loss: 913.725098	Accuracy: 19.00%
10	Validation loss: 608.700073	Best loss: 608.700073	Accuracy: 19.08%
11	Validation loss: 5278.508789	Best loss: 608.700073	Accuracy: 22.01%
12	Validation loss: 1135.064575	Best loss: 608.700073	Accuracy: 22.01%
13	Validation loss: 708.484558	Best loss: 608.700073	Accuracy: 19.27%
14	Validation loss: 4194.821289	Best loss: 608.700073	Accuracy: 34.05%
15	Validation loss: 515.842834	Best loss: 515.842834	Accuracy: 22.01%
16	Validation loss: 278.518311	Best loss: 278.518311	Accuracy: 19.27%
17	Validation loss: 876.153931	Best loss: 278.518311	Accuracy: 18.73%
18	Validation loss: 69764.046875	Best loss: 278.518311	Accuracy: 18.73%
19	Validation loss: 6196066.500000	Best loss: 278.518311	Accuracy: 19.08%
20	Validation loss: 404589.656250	Best loss: 278.518311	Accuracy: 18.26%
21	Validation loss: 367235.437500	Best loss: 278.518311	Accuracy: 22.01%
22	Validation loss: 389218.687500	Best loss: 278.518311	Accuracy: 20.91%
23	Validation loss: 279403.031250	Best loss: 278.518311	Accuracy: 22.01%
24	Validation loss: 151617.109375	Best loss: 278.518311	Accuracy: 20.91%
25	Validation loss: 265072.312500	Best loss: 278.518311	Accuracy: 18.73%
26	Validation loss: 352316.812500	Best loss: 278.518311	Accuracy: 19.27%
27	Validation loss: 707286.312500	Best loss: 278.518311	Accuracy: 22.01%
28	Validation loss: 75569.203125	Best loss: 278.518311	Accuracy: 19.08%
29	Validation loss: 22486.144531	Best loss: 278.518311	Accuracy: 19.23%
30	Validation loss: 48571.425781	Best loss: 278.518311	Accuracy: 18.73%
31	Validation loss: 48974.402344	Best loss: 278.518311	Accuracy: 18.73%
32	Validation loss: 44406.531250	Best loss: 278.518311	Accuracy: 18.73%
33	Validation loss: 48602.382812	Best loss: 278.518311	Accuracy: 22.01%
34	Validation loss: 445424.250000	Best loss: 278.518311	Accuracy: 20.91%
35	Validation loss: 126802.460938	Best loss: 278.518311	Accuracy: 19.08%
36	Validation loss: 54796.511719	Best loss: 278.518311	Accuracy: 22.01%
37	Validation loss: 66875.406250	Best loss: 278.518311	Accuracy: 19.27%
Early stopping!
[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.6, batch_size=100, activation=&lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;, total=  28.1s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.727322	Best loss: 1.727322	Accuracy: 18.73%
1	Validation loss: 1.921826	Best loss: 1.727322	Accuracy: 19.08%
2	Validation loss: 1.952590	Best loss: 1.727322	Accuracy: 19.08%
3	Validation loss: 1.670816	Best loss: 1.670816	Accuracy: 22.01%
4	Validation loss: 1.634287	Best loss: 1.634287	Accuracy: 22.01%
5	Validation loss: 1.722077	Best loss: 1.634287	Accuracy: 19.08%
6	Validation loss: 1.900120	Best loss: 1.634287	Accuracy: 18.73%
7	Validation loss: 1.729834	Best loss: 1.634287	Accuracy: 19.27%
8	Validation loss: 1.759161	Best loss: 1.634287	Accuracy: 19.27%
9	Validation loss: 1.724002	Best loss: 1.634287	Accuracy: 19.08%
10	Validation loss: 1.680192	Best loss: 1.634287	Accuracy: 20.91%
11	Validation loss: 1.687592	Best loss: 1.634287	Accuracy: 19.08%
12	Validation loss: 1.695610	Best loss: 1.634287	Accuracy: 19.08%
13	Validation loss: 1.797983	Best loss: 1.634287	Accuracy: 19.27%
14	Validation loss: 1.820147	Best loss: 1.634287	Accuracy: 18.73%
15	Validation loss: 1.807248	Best loss: 1.634287	Accuracy: 22.01%
16	Validation loss: 1.892516	Best loss: 1.634287	Accuracy: 22.01%
17	Validation loss: 1.624728	Best loss: 1.624728	Accuracy: 22.01%
18	Validation loss: 2.019351	Best loss: 1.624728	Accuracy: 19.27%
19	Validation loss: 1.756233	Best loss: 1.624728	Accuracy: 20.91%
20	Validation loss: 1.633166	Best loss: 1.624728	Accuracy: 18.73%
21	Validation loss: 1.743856	Best loss: 1.624728	Accuracy: 19.27%
22	Validation loss: 1.634912	Best loss: 1.624728	Accuracy: 19.08%
23	Validation loss: 1.908723	Best loss: 1.624728	Accuracy: 22.01%
24	Validation loss: 1.696159	Best loss: 1.624728	Accuracy: 20.91%
25	Validation loss: 1.862473	Best loss: 1.624728	Accuracy: 19.27%
26	Validation loss: 1.714333	Best loss: 1.624728	Accuracy: 19.08%
27	Validation loss: 1.846734	Best loss: 1.624728	Accuracy: 22.01%
28	Validation loss: 1.841777	Best loss: 1.624728	Accuracy: 19.08%
29	Validation loss: 1.693217	Best loss: 1.624728	Accuracy: 18.73%
30	Validation loss: 1.724154	Best loss: 1.624728	Accuracy: 18.73%
31	Validation loss: 1.690138	Best loss: 1.624728	Accuracy: 22.01%
32	Validation loss: 1.816592	Best loss: 1.624728	Accuracy: 18.73%
33	Validation loss: 1.673485	Best loss: 1.624728	Accuracy: 22.01%
34	Validation loss: 1.680495	Best loss: 1.624728	Accuracy: 22.01%
35	Validation loss: 1.824637	Best loss: 1.624728	Accuracy: 19.08%
36	Validation loss: 1.857476	Best loss: 1.624728	Accuracy: 22.01%
37	Validation loss: 1.879474	Best loss: 1.624728	Accuracy: 19.27%
38	Validation loss: 1.835058	Best loss: 1.624728	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  27.9s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.646154	Best loss: 1.646154	Accuracy: 22.01%
1	Validation loss: 1.885808	Best loss: 1.646154	Accuracy: 19.08%
2	Validation loss: 1.715868	Best loss: 1.646154	Accuracy: 19.27%
3	Validation loss: 1.754722	Best loss: 1.646154	Accuracy: 22.01%
4	Validation loss: 1.651670	Best loss: 1.646154	Accuracy: 20.91%
5	Validation loss: 1.635624	Best loss: 1.635624	Accuracy: 19.08%
6	Validation loss: 1.744434	Best loss: 1.635624	Accuracy: 19.27%
7	Validation loss: 1.792549	Best loss: 1.635624	Accuracy: 18.73%
8	Validation loss: 1.847787	Best loss: 1.635624	Accuracy: 18.73%
9	Validation loss: 1.933327	Best loss: 1.635624	Accuracy: 19.08%
10	Validation loss: 1.727529	Best loss: 1.635624	Accuracy: 22.01%
11	Validation loss: 1.693805	Best loss: 1.635624	Accuracy: 22.01%
12	Validation loss: 1.650498	Best loss: 1.635624	Accuracy: 20.91%
13	Validation loss: 1.666630	Best loss: 1.635624	Accuracy: 22.01%
14	Validation loss: 1.805065	Best loss: 1.635624	Accuracy: 19.08%
15	Validation loss: 1.669979	Best loss: 1.635624	Accuracy: 19.08%
16	Validation loss: 1.767287	Best loss: 1.635624	Accuracy: 22.01%
17	Validation loss: 1.758371	Best loss: 1.635624	Accuracy: 19.08%
18	Validation loss: 1.824572	Best loss: 1.635624	Accuracy: 22.01%
19	Validation loss: 1.710135	Best loss: 1.635624	Accuracy: 19.08%
20	Validation loss: 1.724206	Best loss: 1.635624	Accuracy: 19.27%
21	Validation loss: 1.756768	Best loss: 1.635624	Accuracy: 22.01%
22	Validation loss: 1.875607	Best loss: 1.635624	Accuracy: 18.73%
23	Validation loss: 1.690160	Best loss: 1.635624	Accuracy: 22.01%
24	Validation loss: 2.131226	Best loss: 1.635624	Accuracy: 19.27%
25	Validation loss: 1.842019	Best loss: 1.635624	Accuracy: 22.01%
26	Validation loss: 1.730691	Best loss: 1.635624	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  18.4s
[CV] n_neurons=140, learning_rate=0.05, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt; 
0	Validation loss: 1.877729	Best loss: 1.877729	Accuracy: 20.91%
1	Validation loss: 1.670699	Best loss: 1.670699	Accuracy: 19.08%
2	Validation loss: 1.623784	Best loss: 1.623784	Accuracy: 20.91%
3	Validation loss: 1.727185	Best loss: 1.623784	Accuracy: 19.27%
4	Validation loss: 1.657408	Best loss: 1.623784	Accuracy: 20.91%
5	Validation loss: 1.819604	Best loss: 1.623784	Accuracy: 20.91%
6	Validation loss: 1.972966	Best loss: 1.623784	Accuracy: 19.27%
7	Validation loss: 1.810678	Best loss: 1.623784	Accuracy: 18.73%
8	Validation loss: 1.690596	Best loss: 1.623784	Accuracy: 19.27%
9	Validation loss: 1.662511	Best loss: 1.623784	Accuracy: 19.08%
10	Validation loss: 1.682422	Best loss: 1.623784	Accuracy: 19.08%
11	Validation loss: 1.810233	Best loss: 1.623784	Accuracy: 22.01%
12	Validation loss: 1.733624	Best loss: 1.623784	Accuracy: 19.08%
13	Validation loss: 1.659522	Best loss: 1.623784	Accuracy: 19.27%
14	Validation loss: 2.044307	Best loss: 1.623784	Accuracy: 22.01%
15	Validation loss: 1.669194	Best loss: 1.623784	Accuracy: 22.01%
16	Validation loss: 1.797078	Best loss: 1.623784	Accuracy: 22.01%
17	Validation loss: 1.677456	Best loss: 1.623784	Accuracy: 18.73%
18	Validation loss: 1.614105	Best loss: 1.614105	Accuracy: 22.01%
19	Validation loss: 1.882869	Best loss: 1.614105	Accuracy: 22.01%
20	Validation loss: 1.824145	Best loss: 1.614105	Accuracy: 19.27%
21	Validation loss: 1.743321	Best loss: 1.614105	Accuracy: 22.01%
22	Validation loss: 1.834937	Best loss: 1.614105	Accuracy: 20.91%
23	Validation loss: 1.658196	Best loss: 1.614105	Accuracy: 20.91%
24	Validation loss: 1.816590	Best loss: 1.614105	Accuracy: 18.73%
25	Validation loss: 1.709815	Best loss: 1.614105	Accuracy: 22.01%
26	Validation loss: 1.679542	Best loss: 1.614105	Accuracy: 18.73%
27	Validation loss: 1.714516	Best loss: 1.614105	Accuracy: 19.27%
28	Validation loss: 2.006110	Best loss: 1.614105	Accuracy: 19.08%
29	Validation loss: 1.870206	Best loss: 1.614105	Accuracy: 19.08%
30	Validation loss: 1.886626	Best loss: 1.614105	Accuracy: 20.91%
31	Validation loss: 1.918363	Best loss: 1.614105	Accuracy: 22.01%
32	Validation loss: 1.714990	Best loss: 1.614105	Accuracy: 19.08%
33	Validation loss: 1.686965	Best loss: 1.614105	Accuracy: 18.73%
34	Validation loss: 1.754006	Best loss: 1.614105	Accuracy: 20.91%
35	Validation loss: 1.663690	Best loss: 1.614105	Accuracy: 22.01%
36	Validation loss: 1.641361	Best loss: 1.614105	Accuracy: 20.91%
37	Validation loss: 1.700030	Best loss: 1.614105	Accuracy: 19.08%
38	Validation loss: 1.814707	Best loss: 1.614105	Accuracy: 18.73%
39	Validation loss: 1.659750	Best loss: 1.614105	Accuracy: 19.08%
Early stopping!
[CV]  n_neurons=140, learning_rate=0.05, dropout_rate=0.5, batch_size=100, activation=&lt;function elu at 0x1243639d8&gt;, total=  26.9s
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 117.3min finished
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.118020	Best loss: 0.118020	Accuracy: 97.30%
1	Validation loss: 0.093319	Best loss: 0.093319	Accuracy: 97.81%
2	Validation loss: 0.074783	Best loss: 0.074783	Accuracy: 98.05%
3	Validation loss: 0.065724	Best loss: 0.065724	Accuracy: 98.32%
4	Validation loss: 0.080757	Best loss: 0.065724	Accuracy: 98.01%
5	Validation loss: 0.075535	Best loss: 0.065724	Accuracy: 98.40%
6	Validation loss: 0.069407	Best loss: 0.065724	Accuracy: 98.16%
7	Validation loss: 0.072034	Best loss: 0.065724	Accuracy: 98.55%
8	Validation loss: 0.086624	Best loss: 0.065724	Accuracy: 98.01%
9	Validation loss: 0.069571	Best loss: 0.065724	Accuracy: 98.44%
10	Validation loss: 0.094720	Best loss: 0.065724	Accuracy: 98.20%
11	Validation loss: 0.070504	Best loss: 0.065724	Accuracy: 98.51%
12	Validation loss: 0.090169	Best loss: 0.065724	Accuracy: 98.24%
13	Validation loss: 0.080667	Best loss: 0.065724	Accuracy: 98.20%
14	Validation loss: 0.120917	Best loss: 0.065724	Accuracy: 96.60%
15	Validation loss: 0.105030	Best loss: 0.065724	Accuracy: 97.62%
16	Validation loss: 0.138571	Best loss: 0.065724	Accuracy: 97.85%
17	Validation loss: 0.078942	Best loss: 0.065724	Accuracy: 97.97%
18	Validation loss: 0.081645	Best loss: 0.065724	Accuracy: 97.89%
19	Validation loss: 0.054128	Best loss: 0.054128	Accuracy: 98.44%
20	Validation loss: 0.051510	Best loss: 0.051510	Accuracy: 98.44%
21	Validation loss: 0.071159	Best loss: 0.051510	Accuracy: 98.67%
22	Validation loss: 0.084647	Best loss: 0.051510	Accuracy: 98.28%
23	Validation loss: 0.081601	Best loss: 0.051510	Accuracy: 98.36%
24	Validation loss: 0.152964	Best loss: 0.051510	Accuracy: 97.93%
25	Validation loss: 0.173249	Best loss: 0.051510	Accuracy: 97.03%
26	Validation loss: 0.128901	Best loss: 0.051510	Accuracy: 96.13%
27	Validation loss: 0.110458	Best loss: 0.051510	Accuracy: 97.93%
28	Validation loss: 0.108197	Best loss: 0.051510	Accuracy: 97.30%
29	Validation loss: 0.104204	Best loss: 0.051510	Accuracy: 97.85%
30	Validation loss: 0.126637	Best loss: 0.051510	Accuracy: 98.32%
31	Validation loss: 0.142045	Best loss: 0.051510	Accuracy: 97.62%
32	Validation loss: 0.103701	Best loss: 0.051510	Accuracy: 97.69%
33	Validation loss: 0.120295	Best loss: 0.051510	Accuracy: 97.42%
34	Validation loss: 0.151388	Best loss: 0.051510	Accuracy: 97.85%
35	Validation loss: 0.096931	Best loss: 0.051510	Accuracy: 97.58%
36	Validation loss: 0.153569	Best loss: 0.051510	Accuracy: 97.11%
37	Validation loss: 0.120552	Best loss: 0.051510	Accuracy: 98.05%
38	Validation loss: 0.076677	Best loss: 0.051510	Accuracy: 98.55%
39	Validation loss: 0.071904	Best loss: 0.051510	Accuracy: 98.55%
40	Validation loss: 0.072618	Best loss: 0.051510	Accuracy: 98.12%
41	Validation loss: 0.086680	Best loss: 0.051510	Accuracy: 98.08%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[136]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>RandomizedSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;,
          estimator=DNNClassifier(activation=&lt;function elu at 0x1243639d8&gt;,
       batch_norm_momentum=None, batch_size=20, dropout_rate=None,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42),
          fit_params={&#39;X_valid&#39;: array([[0., 0., ..., 0., 0.],
       [0., 0., ..., 0., 0.],
       ...,
       [0., 0., ..., 0., 0.],
       [0., 0., ..., 0., 0.]], dtype=float32), &#39;y_valid&#39;: array([0, 4, ..., 1, 2], dtype=int32), &#39;n_epochs&#39;: 1000},
          iid=&#39;warn&#39;, n_iter=50, n_jobs=None,
          param_distributions={&#39;n_neurons&#39;: [10, 30, 50, 70, 90, 100, 120, 140, 160], &#39;batch_size&#39;: [10, 50, 100, 500], &#39;learning_rate&#39;: [0.01, 0.02, 0.05, 0.1], &#39;activation&#39;: [&lt;function relu at 0x124366d08&gt;, &lt;function elu at 0x1243639d8&gt;, &lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850620&gt;, &lt;function leaky_relu.&lt;locals&gt;.parametrized_leaky_relu at 0x14e850d08&gt;], &#39;dropout_rate&#39;: [0.2, 0.3, 0.4, 0.5, 0.6]},
          pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True,
          return_train_score=&#39;warn&#39;, scoring=None, verbose=2)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[137]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rnd_search_dropout</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[137]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;n_neurons&#39;: 160,
 &#39;learning_rate&#39;: 0.01,
 &#39;dropout_rate&#39;: 0.2,
 &#39;batch_size&#39;: 100,
 &#39;activation&#39;: &lt;function tensorflow.python.ops.gen_nn_ops.relu(features, name=None)&gt;}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[138]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">rnd_search_dropout</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test1</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[138]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.9889083479276124</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Oh well, dropout did not improve the model. Better luck next time! :)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But that's okay, we have ourselves a nice DNN that achieves 99.49% accuracy on the test set using Batch Normalization, or 98.91% without BN. Let's see if some of this expertise on digits 0 to 4 can be transferred to the task of classifying digits 5 to 9. For the sake of simplicity we will reuse the DNN without BN.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="9.-Transfer-learning">9. Transfer learning<a class="anchor-link" href="#9.-Transfer-learning">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="9.1.">9.1.<a class="anchor-link" href="#9.1.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's load the best model's graph and get a handle on all the important operations we will need. Note that instead of creating a new softmax output layer, we will just reuse the existing one (since it has the same number of outputs as the existing one). We will reinitialize its parameters before training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[139]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="s2">&quot;./my_best_mnist_model_0_to_4.meta&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;X:0&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;y:0&quot;</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;loss:0&quot;</span><span class="p">)</span>
<span class="n">Y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;Y_proba:0&quot;</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">Y_proba</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;accuracy:0&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To freeze the lower layers, we will exclude their variables from the optimizer's list of trainable variables, keeping only the output layer's trainable variables:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[140]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">output_layer_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;logits&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Adam2&quot;</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">output_layer_vars</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[141]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">five_frozen_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="9.2.">9.2.<a class="anchor-link" href="#9.2.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's create the training, validation and test sets. We need to subtract 5 from the labels because TensorFlow expects integers from 0 to <code>n_classes-1</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[142]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train2_full</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y_train2_full</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">]</span> <span class="o">-</span> <span class="mi">5</span>
<span class="n">X_valid2_full</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">[</span><span class="n">y_valid</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y_valid2_full</span> <span class="o">=</span> <span class="n">y_valid</span><span class="p">[</span><span class="n">y_valid</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">]</span> <span class="o">-</span> <span class="mi">5</span>
<span class="n">X_test2</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">y_test</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y_test2</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">y_test</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">]</span> <span class="o">-</span> <span class="mi">5</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also, for the purpose of this exercise, we want to keep only 100 instances per class in the training set (and let's keep only 30 instances per class in the validation set). Let's create a small function to do that:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[143]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sample_n_instances_per_class</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">Xc</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span><span class="n">n</span><span class="p">]</span>
        <span class="n">yc</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span><span class="n">n</span><span class="p">]</span>
        <span class="n">Xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Xc</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">yc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Xs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[144]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train2</span><span class="p">,</span> <span class="n">y_train2</span> <span class="o">=</span> <span class="n">sample_n_instances_per_class</span><span class="p">(</span><span class="n">X_train2_full</span><span class="p">,</span> <span class="n">y_train2_full</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">X_valid2</span><span class="p">,</span> <span class="n">y_valid2</span> <span class="o">=</span> <span class="n">sample_n_instances_per_class</span><span class="p">(</span><span class="n">X_valid2_full</span><span class="p">,</span> <span class="n">y_valid2_full</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's train the model. This is the same training code as earlier, using early stopping, except for the initialization: we first initialize all the variables, then we restore the best model trained earlier (on digits 0 to 4), and finally we reinitialize the output layer variables.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[145]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">max_checks_without_progress</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_best_mnist_model_0_to_4&quot;</span><span class="p">)</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid2</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">save_path</span> <span class="o">=</span> <span class="n">five_frozen_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_five_frozen&quot;</span><span class="p">)</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
            <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">checks_without_progress</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">checks_without_progress</span> <span class="o">&gt;</span> <span class="n">max_checks_without_progress</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping!&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Validation loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Best loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total training time: </span><span class="si">{:.1f}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">five_frozen_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_five_frozen&quot;</span><span class="p">)</span>
    <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test2</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final test accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc_test</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4
0	Validation loss: 1.361167	Best loss: 1.361167	Accuracy: 43.33%
1	Validation loss: 1.154602	Best loss: 1.154602	Accuracy: 57.33%
2	Validation loss: 1.054218	Best loss: 1.054218	Accuracy: 53.33%
3	Validation loss: 0.981128	Best loss: 0.981128	Accuracy: 62.67%
4	Validation loss: 0.995353	Best loss: 0.981128	Accuracy: 59.33%
5	Validation loss: 0.967000	Best loss: 0.967000	Accuracy: 65.33%
6	Validation loss: 0.955700	Best loss: 0.955700	Accuracy: 61.33%
7	Validation loss: 1.015331	Best loss: 0.955700	Accuracy: 58.67%
8	Validation loss: 0.978280	Best loss: 0.955700	Accuracy: 62.00%
9	Validation loss: 0.923389	Best loss: 0.923389	Accuracy: 69.33%
10	Validation loss: 0.996236	Best loss: 0.923389	Accuracy: 63.33%
11	Validation loss: 0.976757	Best loss: 0.923389	Accuracy: 62.67%
12	Validation loss: 0.969096	Best loss: 0.923389	Accuracy: 63.33%
13	Validation loss: 1.023069	Best loss: 0.923389	Accuracy: 63.33%
14	Validation loss: 1.104664	Best loss: 0.923389	Accuracy: 55.33%
15	Validation loss: 0.950175	Best loss: 0.923389	Accuracy: 65.33%
16	Validation loss: 1.002944	Best loss: 0.923389	Accuracy: 63.33%
17	Validation loss: 0.895543	Best loss: 0.895543	Accuracy: 70.67%
18	Validation loss: 0.961151	Best loss: 0.895543	Accuracy: 66.67%
19	Validation loss: 0.896372	Best loss: 0.895543	Accuracy: 67.33%
20	Validation loss: 0.911938	Best loss: 0.895543	Accuracy: 69.33%
21	Validation loss: 0.929007	Best loss: 0.895543	Accuracy: 68.00%
22	Validation loss: 0.939231	Best loss: 0.895543	Accuracy: 65.33%
23	Validation loss: 0.919057	Best loss: 0.895543	Accuracy: 68.67%
24	Validation loss: 0.994529	Best loss: 0.895543	Accuracy: 65.33%
25	Validation loss: 0.901279	Best loss: 0.895543	Accuracy: 68.67%
26	Validation loss: 0.916238	Best loss: 0.895543	Accuracy: 68.67%
27	Validation loss: 1.007434	Best loss: 0.895543	Accuracy: 65.33%
28	Validation loss: 0.924729	Best loss: 0.895543	Accuracy: 70.00%
29	Validation loss: 0.974399	Best loss: 0.895543	Accuracy: 66.00%
30	Validation loss: 0.899418	Best loss: 0.895543	Accuracy: 68.00%
31	Validation loss: 0.940563	Best loss: 0.895543	Accuracy: 66.00%
32	Validation loss: 0.920235	Best loss: 0.895543	Accuracy: 68.00%
33	Validation loss: 0.929848	Best loss: 0.895543	Accuracy: 68.67%
34	Validation loss: 0.930288	Best loss: 0.895543	Accuracy: 66.67%
35	Validation loss: 0.943884	Best loss: 0.895543	Accuracy: 64.67%
36	Validation loss: 0.939372	Best loss: 0.895543	Accuracy: 68.00%
37	Validation loss: 0.894239	Best loss: 0.894239	Accuracy: 67.33%
38	Validation loss: 0.888806	Best loss: 0.888806	Accuracy: 69.33%
39	Validation loss: 0.933829	Best loss: 0.888806	Accuracy: 66.00%
40	Validation loss: 0.911836	Best loss: 0.888806	Accuracy: 72.67%
41	Validation loss: 0.896729	Best loss: 0.888806	Accuracy: 70.00%
42	Validation loss: 0.929394	Best loss: 0.888806	Accuracy: 68.00%
43	Validation loss: 0.919418	Best loss: 0.888806	Accuracy: 69.33%
44	Validation loss: 0.907830	Best loss: 0.888806	Accuracy: 65.33%
45	Validation loss: 1.004304	Best loss: 0.888806	Accuracy: 71.33%
46	Validation loss: 0.871899	Best loss: 0.871899	Accuracy: 74.00%
47	Validation loss: 0.904889	Best loss: 0.871899	Accuracy: 67.33%
48	Validation loss: 0.914138	Best loss: 0.871899	Accuracy: 66.00%
49	Validation loss: 0.930001	Best loss: 0.871899	Accuracy: 69.33%
50	Validation loss: 0.962153	Best loss: 0.871899	Accuracy: 68.67%
51	Validation loss: 0.925021	Best loss: 0.871899	Accuracy: 65.33%
52	Validation loss: 0.974412	Best loss: 0.871899	Accuracy: 67.33%
53	Validation loss: 0.897499	Best loss: 0.871899	Accuracy: 68.67%
54	Validation loss: 0.933581	Best loss: 0.871899	Accuracy: 60.67%
55	Validation loss: 0.988574	Best loss: 0.871899	Accuracy: 68.67%
56	Validation loss: 0.927290	Best loss: 0.871899	Accuracy: 66.67%
57	Validation loss: 1.018713	Best loss: 0.871899	Accuracy: 64.00%
58	Validation loss: 0.964709	Best loss: 0.871899	Accuracy: 66.00%
59	Validation loss: 1.004696	Best loss: 0.871899	Accuracy: 59.33%
60	Validation loss: 1.008746	Best loss: 0.871899	Accuracy: 58.67%
61	Validation loss: 0.948558	Best loss: 0.871899	Accuracy: 68.00%
62	Validation loss: 0.966037	Best loss: 0.871899	Accuracy: 64.00%
63	Validation loss: 0.922541	Best loss: 0.871899	Accuracy: 68.00%
64	Validation loss: 0.892541	Best loss: 0.871899	Accuracy: 72.00%
65	Validation loss: 0.890340	Best loss: 0.871899	Accuracy: 70.67%
66	Validation loss: 0.957904	Best loss: 0.871899	Accuracy: 66.00%
Early stopping!
Total training time: 1.9s
INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen
Final test accuracy: 64.02%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well that's not a great accuracy, is it? Of course with such a tiny training set, and with only one layer to tweak, we should not expect miracles.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="9.3.">9.3.<a class="anchor-link" href="#9.3.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: try caching the frozen layers, and train the model again: how much faster is it now?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's start by getting a handle on the output of the last frozen layer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[146]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden5_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;hidden5_out:0&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's train the model using roughly the same code as earlier. The difference is that we compute the output of the top frozen layer at the beginning (both for the training set and the validation set), and we cache it. This makes training roughly 1.5 to 3 times faster in this example (this may vary greatly, depending on your system):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[147]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">max_checks_without_progress</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_best_mnist_model_0_to_4&quot;</span><span class="p">)</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    
    <span class="n">hidden5_train</span> <span class="o">=</span> <span class="n">hidden5_out</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_train2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_train2</span><span class="p">})</span>
    <span class="n">hidden5_valid</span> <span class="o">=</span> <span class="n">hidden5_out</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid2</span><span class="p">})</span>
        
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">h5_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">hidden5_train</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">hidden5_out</span><span class="p">:</span> <span class="n">h5_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">hidden5_out</span><span class="p">:</span> <span class="n">hidden5_valid</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid2</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">save_path</span> <span class="o">=</span> <span class="n">five_frozen_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_five_frozen&quot;</span><span class="p">)</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
            <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">checks_without_progress</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">checks_without_progress</span> <span class="o">&gt;</span> <span class="n">max_checks_without_progress</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping!&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Validation loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Best loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total training time: </span><span class="si">{:.1f}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">five_frozen_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_five_frozen&quot;</span><span class="p">)</span>
    <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test2</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final test accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc_test</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4
0	Validation loss: 1.416103	Best loss: 1.416103	Accuracy: 44.00%
1	Validation loss: 1.099216	Best loss: 1.099216	Accuracy: 53.33%
2	Validation loss: 1.024954	Best loss: 1.024954	Accuracy: 59.33%
3	Validation loss: 0.969193	Best loss: 0.969193	Accuracy: 60.00%
4	Validation loss: 0.973461	Best loss: 0.969193	Accuracy: 64.67%
5	Validation loss: 0.949333	Best loss: 0.949333	Accuracy: 64.67%
6	Validation loss: 0.922953	Best loss: 0.922953	Accuracy: 66.67%
7	Validation loss: 0.957186	Best loss: 0.922953	Accuracy: 62.67%
8	Validation loss: 0.950264	Best loss: 0.922953	Accuracy: 68.00%
9	Validation loss: 1.053465	Best loss: 0.922953	Accuracy: 59.33%
10	Validation loss: 1.069949	Best loss: 0.922953	Accuracy: 54.00%
11	Validation loss: 0.965197	Best loss: 0.922953	Accuracy: 62.67%
12	Validation loss: 0.949233	Best loss: 0.922953	Accuracy: 63.33%
13	Validation loss: 0.926229	Best loss: 0.922953	Accuracy: 63.33%
14	Validation loss: 0.922854	Best loss: 0.922854	Accuracy: 67.33%
15	Validation loss: 0.965205	Best loss: 0.922854	Accuracy: 66.67%
16	Validation loss: 1.050026	Best loss: 0.922854	Accuracy: 59.33%
17	Validation loss: 0.946699	Best loss: 0.922854	Accuracy: 64.67%
18	Validation loss: 0.973966	Best loss: 0.922854	Accuracy: 64.00%
19	Validation loss: 0.902573	Best loss: 0.902573	Accuracy: 66.67%
20	Validation loss: 0.933625	Best loss: 0.902573	Accuracy: 65.33%
21	Validation loss: 0.938296	Best loss: 0.902573	Accuracy: 64.00%
22	Validation loss: 0.938790	Best loss: 0.902573	Accuracy: 66.67%
23	Validation loss: 0.936572	Best loss: 0.902573	Accuracy: 68.00%
24	Validation loss: 1.039109	Best loss: 0.902573	Accuracy: 65.33%
25	Validation loss: 1.146837	Best loss: 0.902573	Accuracy: 59.33%
26	Validation loss: 0.958702	Best loss: 0.902573	Accuracy: 68.67%
27	Validation loss: 0.915434	Best loss: 0.902573	Accuracy: 70.67%
28	Validation loss: 0.915402	Best loss: 0.902573	Accuracy: 66.00%
29	Validation loss: 0.920591	Best loss: 0.902573	Accuracy: 70.67%
30	Validation loss: 1.029216	Best loss: 0.902573	Accuracy: 64.67%
31	Validation loss: 1.039922	Best loss: 0.902573	Accuracy: 55.33%
32	Validation loss: 0.925041	Best loss: 0.902573	Accuracy: 64.00%
33	Validation loss: 0.944033	Best loss: 0.902573	Accuracy: 67.33%
34	Validation loss: 0.941914	Best loss: 0.902573	Accuracy: 66.67%
35	Validation loss: 0.866297	Best loss: 0.866297	Accuracy: 69.33%
36	Validation loss: 0.900787	Best loss: 0.866297	Accuracy: 70.67%
37	Validation loss: 0.889670	Best loss: 0.866297	Accuracy: 66.67%
38	Validation loss: 0.968139	Best loss: 0.866297	Accuracy: 62.00%
39	Validation loss: 0.929764	Best loss: 0.866297	Accuracy: 66.00%
40	Validation loss: 0.889130	Best loss: 0.866297	Accuracy: 68.00%
41	Validation loss: 0.940024	Best loss: 0.866297	Accuracy: 70.00%
42	Validation loss: 0.896472	Best loss: 0.866297	Accuracy: 69.33%
43	Validation loss: 0.893887	Best loss: 0.866297	Accuracy: 67.33%
44	Validation loss: 0.925727	Best loss: 0.866297	Accuracy: 68.67%
45	Validation loss: 0.945748	Best loss: 0.866297	Accuracy: 66.00%
46	Validation loss: 0.897087	Best loss: 0.866297	Accuracy: 70.00%
47	Validation loss: 0.923855	Best loss: 0.866297	Accuracy: 68.67%
48	Validation loss: 0.944244	Best loss: 0.866297	Accuracy: 66.67%
49	Validation loss: 0.975582	Best loss: 0.866297	Accuracy: 66.67%
50	Validation loss: 0.889869	Best loss: 0.866297	Accuracy: 68.67%
51	Validation loss: 0.895552	Best loss: 0.866297	Accuracy: 69.33%
52	Validation loss: 0.943707	Best loss: 0.866297	Accuracy: 66.00%
53	Validation loss: 0.902883	Best loss: 0.866297	Accuracy: 70.67%
54	Validation loss: 0.958292	Best loss: 0.866297	Accuracy: 68.67%
55	Validation loss: 0.917368	Best loss: 0.866297	Accuracy: 67.33%
Early stopping!
Total training time: 1.1s
INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen
Final test accuracy: 61.16%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="9.4.">9.4.<a class="anchor-link" href="#9.4.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: try again reusing just four hidden layers instead of five. Can you achieve a higher precision?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's load the best model again, but this time we will create a new softmax output layer on top of the 4th hidden layer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[148]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="s2">&quot;./my_best_mnist_model_0_to_4.meta&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;X:0&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;y:0&quot;</span><span class="p">)</span>

<span class="n">hidden4_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s2">&quot;hidden4_out:0&quot;</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden4_out</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;new_logits&quot;</span><span class="p">)</span>
<span class="n">Y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now let's create the training operation. We want to freeze all the layers except for the new output layer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[149]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">output_layer_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;new_logits&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Adam2&quot;</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">output_layer_vars</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">four_frozen_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And once again we train the model with the same code as earlier. Note: we could of course write a function once and use it multiple times, rather than copying almost the same training code over and over again, but as we keep tweaking the code slightly, the function would need multiple arguments and <code>if</code> statements, and it would have to be at the beginning of the notebook, where it would not make much sense to readers. In short it would be very confusing, so we're better off with copy &amp; paste.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[150]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">max_checks_without_progress</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_best_mnist_model_0_to_4&quot;</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid2</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">save_path</span> <span class="o">=</span> <span class="n">four_frozen_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_four_frozen&quot;</span><span class="p">)</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
            <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">checks_without_progress</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">checks_without_progress</span> <span class="o">&gt;</span> <span class="n">max_checks_without_progress</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping!&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Validation loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Best loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">four_frozen_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_four_frozen&quot;</span><span class="p">)</span>
    <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test2</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final test accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc_test</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_best_mnist_model_0_to_4
0	Validation loss: 1.073254	Best loss: 1.073254	Accuracy: 51.33%
1	Validation loss: 1.039487	Best loss: 1.039487	Accuracy: 64.00%
2	Validation loss: 0.991418	Best loss: 0.991418	Accuracy: 59.33%
3	Validation loss: 0.902691	Best loss: 0.902691	Accuracy: 64.67%
4	Validation loss: 0.919874	Best loss: 0.902691	Accuracy: 63.33%
5	Validation loss: 0.879734	Best loss: 0.879734	Accuracy: 72.00%
6	Validation loss: 0.877940	Best loss: 0.877940	Accuracy: 70.67%
7	Validation loss: 0.899513	Best loss: 0.877940	Accuracy: 71.33%
8	Validation loss: 0.879717	Best loss: 0.877940	Accuracy: 67.33%
9	Validation loss: 0.826527	Best loss: 0.826527	Accuracy: 75.33%
10	Validation loss: 0.890165	Best loss: 0.826527	Accuracy: 67.33%
11	Validation loss: 0.876235	Best loss: 0.826527	Accuracy: 68.67%
12	Validation loss: 0.877598	Best loss: 0.826527	Accuracy: 71.33%
13	Validation loss: 0.898070	Best loss: 0.826527	Accuracy: 74.67%
14	Validation loss: 0.923526	Best loss: 0.826527	Accuracy: 68.00%
15	Validation loss: 0.859624	Best loss: 0.826527	Accuracy: 70.00%
16	Validation loss: 0.896264	Best loss: 0.826527	Accuracy: 67.33%
17	Validation loss: 0.800813	Best loss: 0.800813	Accuracy: 73.33%
18	Validation loss: 0.811318	Best loss: 0.800813	Accuracy: 74.00%
19	Validation loss: 0.809687	Best loss: 0.800813	Accuracy: 75.33%
20	Validation loss: 0.807125	Best loss: 0.800813	Accuracy: 72.67%
21	Validation loss: 0.819150	Best loss: 0.800813	Accuracy: 71.33%
22	Validation loss: 0.849812	Best loss: 0.800813	Accuracy: 76.67%
23	Validation loss: 0.801709	Best loss: 0.800813	Accuracy: 74.67%
24	Validation loss: 0.832877	Best loss: 0.800813	Accuracy: 74.00%
25	Validation loss: 0.792853	Best loss: 0.792853	Accuracy: 72.67%
26	Validation loss: 0.842031	Best loss: 0.792853	Accuracy: 76.00%
27	Validation loss: 0.872236	Best loss: 0.792853	Accuracy: 71.33%
28	Validation loss: 0.782557	Best loss: 0.782557	Accuracy: 78.00%
29	Validation loss: 0.802515	Best loss: 0.782557	Accuracy: 73.33%
30	Validation loss: 0.812652	Best loss: 0.782557	Accuracy: 72.67%
31	Validation loss: 0.825467	Best loss: 0.782557	Accuracy: 76.00%
32	Validation loss: 0.791320	Best loss: 0.782557	Accuracy: 76.67%
33	Validation loss: 0.785207	Best loss: 0.782557	Accuracy: 77.33%
34	Validation loss: 0.815450	Best loss: 0.782557	Accuracy: 76.67%
35	Validation loss: 0.865081	Best loss: 0.782557	Accuracy: 71.33%
36	Validation loss: 0.852323	Best loss: 0.782557	Accuracy: 74.67%
37	Validation loss: 0.836967	Best loss: 0.782557	Accuracy: 72.00%
38	Validation loss: 0.807404	Best loss: 0.782557	Accuracy: 77.33%
39	Validation loss: 0.821566	Best loss: 0.782557	Accuracy: 75.33%
40	Validation loss: 0.817326	Best loss: 0.782557	Accuracy: 76.00%
41	Validation loss: 0.807987	Best loss: 0.782557	Accuracy: 70.67%
42	Validation loss: 0.838029	Best loss: 0.782557	Accuracy: 74.00%
43	Validation loss: 0.820425	Best loss: 0.782557	Accuracy: 76.00%
44	Validation loss: 0.785871	Best loss: 0.782557	Accuracy: 76.00%
45	Validation loss: 0.844337	Best loss: 0.782557	Accuracy: 78.67%
46	Validation loss: 0.764127	Best loss: 0.764127	Accuracy: 78.67%
47	Validation loss: 0.789726	Best loss: 0.764127	Accuracy: 77.33%
48	Validation loss: 0.839190	Best loss: 0.764127	Accuracy: 72.67%
49	Validation loss: 0.849353	Best loss: 0.764127	Accuracy: 75.33%
50	Validation loss: 0.869818	Best loss: 0.764127	Accuracy: 74.00%
51	Validation loss: 0.805526	Best loss: 0.764127	Accuracy: 76.67%
52	Validation loss: 0.850749	Best loss: 0.764127	Accuracy: 72.67%
53	Validation loss: 0.838693	Best loss: 0.764127	Accuracy: 71.33%
54	Validation loss: 0.791396	Best loss: 0.764127	Accuracy: 75.33%
55	Validation loss: 0.846888	Best loss: 0.764127	Accuracy: 76.00%
56	Validation loss: 0.826717	Best loss: 0.764127	Accuracy: 74.67%
57	Validation loss: 0.878286	Best loss: 0.764127	Accuracy: 70.67%
58	Validation loss: 0.878869	Best loss: 0.764127	Accuracy: 72.67%
59	Validation loss: 0.822241	Best loss: 0.764127	Accuracy: 72.67%
60	Validation loss: 0.864925	Best loss: 0.764127	Accuracy: 73.33%
61	Validation loss: 0.804545	Best loss: 0.764127	Accuracy: 73.33%
62	Validation loss: 0.891784	Best loss: 0.764127	Accuracy: 72.67%
63	Validation loss: 0.810186	Best loss: 0.764127	Accuracy: 74.00%
64	Validation loss: 0.810786	Best loss: 0.764127	Accuracy: 74.67%
65	Validation loss: 0.818044	Best loss: 0.764127	Accuracy: 74.00%
66	Validation loss: 0.853420	Best loss: 0.764127	Accuracy: 74.67%
Early stopping!
INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen
Final test accuracy: 69.10%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Still not fantastic, but much better.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="9.5.">9.5.<a class="anchor-link" href="#9.5.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Exercise: now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[151]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">unfrozen_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden[34]|new_logits&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Adam3&quot;</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">unfrozen_vars</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">two_frozen_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[152]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">max_checks_without_progress</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">four_frozen_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_four_frozen&quot;</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid2</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">save_path</span> <span class="o">=</span> <span class="n">two_frozen_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_two_frozen&quot;</span><span class="p">)</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
            <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">checks_without_progress</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">checks_without_progress</span> <span class="o">&gt;</span> <span class="n">max_checks_without_progress</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping!&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Validation loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Best loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">two_frozen_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_two_frozen&quot;</span><span class="p">)</span>
    <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test2</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final test accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc_test</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_four_frozen
0	Validation loss: 1.054859	Best loss: 1.054859	Accuracy: 74.00%
1	Validation loss: 0.812410	Best loss: 0.812410	Accuracy: 78.00%
2	Validation loss: 0.750377	Best loss: 0.750377	Accuracy: 80.67%
3	Validation loss: 0.570973	Best loss: 0.570973	Accuracy: 84.67%
4	Validation loss: 0.805442	Best loss: 0.570973	Accuracy: 79.33%
5	Validation loss: 0.920925	Best loss: 0.570973	Accuracy: 80.00%
6	Validation loss: 0.817471	Best loss: 0.570973	Accuracy: 81.33%
7	Validation loss: 0.777876	Best loss: 0.570973	Accuracy: 84.00%
8	Validation loss: 1.030498	Best loss: 0.570973	Accuracy: 74.67%
9	Validation loss: 1.074356	Best loss: 0.570973	Accuracy: 81.33%
10	Validation loss: 0.912521	Best loss: 0.570973	Accuracy: 83.33%
11	Validation loss: 1.356695	Best loss: 0.570973	Accuracy: 79.33%
12	Validation loss: 0.918798	Best loss: 0.570973	Accuracy: 82.00%
13	Validation loss: 0.971029	Best loss: 0.570973	Accuracy: 82.67%
14	Validation loss: 0.860108	Best loss: 0.570973	Accuracy: 83.33%
15	Validation loss: 1.074813	Best loss: 0.570973	Accuracy: 82.00%
16	Validation loss: 0.867760	Best loss: 0.570973	Accuracy: 84.00%
17	Validation loss: 0.858290	Best loss: 0.570973	Accuracy: 85.33%
18	Validation loss: 0.996560	Best loss: 0.570973	Accuracy: 85.33%
19	Validation loss: 1.304507	Best loss: 0.570973	Accuracy: 83.33%
20	Validation loss: 1.134808	Best loss: 0.570973	Accuracy: 80.67%
21	Validation loss: 1.189581	Best loss: 0.570973	Accuracy: 82.00%
22	Validation loss: 1.131344	Best loss: 0.570973	Accuracy: 81.33%
23	Validation loss: 1.240507	Best loss: 0.570973	Accuracy: 82.67%
Early stopping!
INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_two_frozen
Final test accuracy: 78.09%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's check what accuracy we can get by unfreezing all layers:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[153]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Adam4&quot;</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">no_frozen_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[154]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">max_checks_without_progress</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">two_frozen_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_two_frozen&quot;</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="n">loss_val</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_valid2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_valid2</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">save_path</span> <span class="o">=</span> <span class="n">no_frozen_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_no_frozen&quot;</span><span class="p">)</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
            <span class="n">checks_without_progress</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">checks_without_progress</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">checks_without_progress</span> <span class="o">&gt;</span> <span class="n">max_checks_without_progress</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping!&quot;</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="se">\t</span><span class="s2">Validation loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Best loss: </span><span class="si">{:.6f}</span><span class="se">\t</span><span class="s2">Accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">acc_val</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">no_frozen_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_5_to_9_no_frozen&quot;</span><span class="p">)</span>
    <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test2</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test2</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final test accuracy: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc_test</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_two_frozen
0	Validation loss: 0.863416	Best loss: 0.863416	Accuracy: 86.00%
1	Validation loss: 0.695079	Best loss: 0.695079	Accuracy: 90.00%
2	Validation loss: 0.402921	Best loss: 0.402921	Accuracy: 92.00%
3	Validation loss: 0.606936	Best loss: 0.402921	Accuracy: 92.00%
4	Validation loss: 0.354645	Best loss: 0.354645	Accuracy: 90.67%
5	Validation loss: 0.376935	Best loss: 0.354645	Accuracy: 90.67%
6	Validation loss: 0.593208	Best loss: 0.354645	Accuracy: 90.00%
7	Validation loss: 0.388302	Best loss: 0.354645	Accuracy: 92.67%
8	Validation loss: 0.503276	Best loss: 0.354645	Accuracy: 91.33%
9	Validation loss: 1.440716	Best loss: 0.354645	Accuracy: 80.00%
10	Validation loss: 0.464323	Best loss: 0.354645	Accuracy: 92.00%
11	Validation loss: 0.410302	Best loss: 0.354645	Accuracy: 93.33%
12	Validation loss: 1.131754	Best loss: 0.354645	Accuracy: 88.00%
13	Validation loss: 0.511544	Best loss: 0.354645	Accuracy: 92.00%
14	Validation loss: 0.402083	Best loss: 0.354645	Accuracy: 94.00%
15	Validation loss: 1.149943	Best loss: 0.354645	Accuracy: 92.00%
16	Validation loss: 0.405171	Best loss: 0.354645	Accuracy: 94.00%
17	Validation loss: 0.304346	Best loss: 0.304346	Accuracy: 94.67%
18	Validation loss: 0.386952	Best loss: 0.304346	Accuracy: 94.67%
19	Validation loss: 0.387063	Best loss: 0.304346	Accuracy: 94.67%
20	Validation loss: 0.384417	Best loss: 0.304346	Accuracy: 94.67%
21	Validation loss: 0.381116	Best loss: 0.304346	Accuracy: 94.67%
22	Validation loss: 0.379346	Best loss: 0.304346	Accuracy: 94.67%
23	Validation loss: 0.378128	Best loss: 0.304346	Accuracy: 94.67%
24	Validation loss: 0.376642	Best loss: 0.304346	Accuracy: 94.67%
25	Validation loss: 0.375432	Best loss: 0.304346	Accuracy: 94.67%
26	Validation loss: 0.374804	Best loss: 0.304346	Accuracy: 94.67%
27	Validation loss: 0.373952	Best loss: 0.304346	Accuracy: 94.67%
28	Validation loss: 0.373471	Best loss: 0.304346	Accuracy: 94.67%
29	Validation loss: 0.373027	Best loss: 0.304346	Accuracy: 94.67%
30	Validation loss: 0.373124	Best loss: 0.304346	Accuracy: 94.67%
31	Validation loss: 0.373098	Best loss: 0.304346	Accuracy: 94.67%
32	Validation loss: 0.373206	Best loss: 0.304346	Accuracy: 94.67%
33	Validation loss: 0.372812	Best loss: 0.304346	Accuracy: 94.67%
34	Validation loss: 0.373109	Best loss: 0.304346	Accuracy: 94.67%
35	Validation loss: 0.372616	Best loss: 0.304346	Accuracy: 94.67%
36	Validation loss: 0.372491	Best loss: 0.304346	Accuracy: 94.67%
37	Validation loss: 0.372270	Best loss: 0.304346	Accuracy: 94.67%
Early stopping!
INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_no_frozen
Final test accuracy: 91.34%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's compare that to a DNN trained from scratch:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[155]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn_clf_5_to_9</span> <span class="o">=</span> <span class="n">DNNClassifier</span><span class="p">(</span><span class="n">n_hidden_layers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dnn_clf_5_to_9</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train2</span><span class="p">,</span> <span class="n">y_train2</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">X_valid</span><span class="o">=</span><span class="n">X_valid2</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">=</span><span class="n">y_valid2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0	Validation loss: 0.674618	Best loss: 0.674618	Accuracy: 80.67%
1	Validation loss: 0.584845	Best loss: 0.584845	Accuracy: 88.67%
2	Validation loss: 0.647296	Best loss: 0.584845	Accuracy: 84.00%
3	Validation loss: 0.530389	Best loss: 0.530389	Accuracy: 87.33%
4	Validation loss: 0.683215	Best loss: 0.530389	Accuracy: 90.67%
5	Validation loss: 0.538040	Best loss: 0.530389	Accuracy: 89.33%
6	Validation loss: 0.670196	Best loss: 0.530389	Accuracy: 90.67%
7	Validation loss: 0.836470	Best loss: 0.530389	Accuracy: 85.33%
8	Validation loss: 0.837684	Best loss: 0.530389	Accuracy: 92.67%
9	Validation loss: 0.588950	Best loss: 0.530389	Accuracy: 88.00%
10	Validation loss: 0.643213	Best loss: 0.530389	Accuracy: 90.67%
11	Validation loss: 1.010521	Best loss: 0.530389	Accuracy: 88.00%
12	Validation loss: 0.931423	Best loss: 0.530389	Accuracy: 90.00%
13	Validation loss: 1.563524	Best loss: 0.530389	Accuracy: 88.67%
14	Validation loss: 2.340119	Best loss: 0.530389	Accuracy: 89.33%
15	Validation loss: 1.402095	Best loss: 0.530389	Accuracy: 88.00%
16	Validation loss: 1.269974	Best loss: 0.530389	Accuracy: 86.00%
17	Validation loss: 1.036325	Best loss: 0.530389	Accuracy: 89.33%
18	Validation loss: 1.578565	Best loss: 0.530389	Accuracy: 88.67%
19	Validation loss: 0.993890	Best loss: 0.530389	Accuracy: 93.33%
20	Validation loss: 0.958130	Best loss: 0.530389	Accuracy: 87.33%
21	Validation loss: 1.505322	Best loss: 0.530389	Accuracy: 88.67%
22	Validation loss: 1.378772	Best loss: 0.530389	Accuracy: 89.33%
23	Validation loss: 0.999445	Best loss: 0.530389	Accuracy: 88.00%
24	Validation loss: 2.366345	Best loss: 0.530389	Accuracy: 90.00%
Early stopping!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[155]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>DNNClassifier(activation=&lt;function elu at 0x1243639d8&gt;,
       batch_norm_momentum=None, batch_size=20, dropout_rate=None,
       initializer=&lt;tensorflow.python.ops.init_ops.VarianceScaling object at 0x117bf5828&gt;,
       learning_rate=0.01, n_hidden_layers=4, n_neurons=100,
       optimizer_class=&lt;class &#39;tensorflow.python.training.adam.AdamOptimizer&#39;&gt;,
       random_state=42)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[156]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">dnn_clf_5_to_9</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test2</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test2</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[156]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>0.8481793869574161</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Transfer learning allowed us to go from 84.8% accuracy to 91.3%. Not too bad!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="10.-Pretraining-on-an-auxiliary-task">10. Pretraining on an auxiliary task<a class="anchor-link" href="#10.-Pretraining-on-an-auxiliary-task">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="10.1.">10.1.<a class="anchor-link" href="#10.1.">&#182;</a></h3><p>Exercise: <em>Start by building two DNNs (let's call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. You should use TensorFlow's <code>concat()</code> function with <code>axis=1</code> to concatenate the outputs of both DNNs along the horizontal axis, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Warning</strong>! There was an error in the book for this exercise: there was no instruction to add a top hidden layer. Without it, the neural network generally fails to start learning. If you have the latest version of the book, this error has been fixed.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You could have two input placeholders, <code>X1</code> and <code>X2</code>, one for the images that should be fed to the first DNN, and the other for the images that should be fed to the second DNN. It would work fine. However, another option is to have a single input placeholder to hold both sets of images (each row will hold a pair of images), and use <code>tf.unstack()</code> to split this tensor into two separate tensors, like this:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[157]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span> <span class="c1"># MNIST</span>

<span class="n">reset_graph</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also need the labels placeholder. Each label will be 0 if the images represent different digits, or 1 if they represent the same digit:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[158]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's feed these inputs through two separate DNNs:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[159]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn1</span> <span class="o">=</span> <span class="n">dnn</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;DNN_A&quot;</span><span class="p">)</span>
<span class="n">dnn2</span> <span class="o">=</span> <span class="n">dnn</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;DNN_B&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And let's concatenate their outputs:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[160]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">dnn1</span><span class="p">,</span> <span class="n">dnn2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each DNN outputs 100 activations (per instance), so the shape is <code>[None, 100]</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[161]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[161]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([Dimension(None), Dimension(100)])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[162]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[162]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([Dimension(None), Dimension(100)])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And of course the concatenated outputs have a shape of <code>[None, 200]</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[163]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dnn_outputs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[163]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>TensorShape([Dimension(None), Dimension(200)])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now lets add an extra hidden layer with just 10 neurons, and the output layer, with a single neuron:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[164]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">dnn_outputs</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">elu</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">)</span>
<span class="n">y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The whole network predicts <code>1</code> if <code>y_proba &gt;= 0.5</code> (i.e. the network predicts that the images represent the same digit), or <code>0</code> otherwise. We compute instead <code>logits &gt;= 0</code>, which is equivalent but faster to compute:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[165]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's add the cost function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[166]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_as_float</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y_as_float</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can now create the training operation using an optimizer:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[167]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.95</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will want to measure our classifier's accuracy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[168]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred_correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y_pred_correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And the usual <code>init</code> and <code>saver</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[169]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="10.2.">10.2.<a class="anchor-link" href="#10.2.">&#182;</a></h3><p><em>Exercise: split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The MNIST dataset returned by TensorFlow's <code>input_data()</code> function is already split into 3 parts: a training set (55,000 instances), a validation set (5,000 instances) and a test set (10,000 instances). Let's use the first set to generate the training set composed image pairs, and we will use the second set for the second phase of the exercise (to train a regular MNIST classifier). We will use the third set as the test set for both phases.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[170]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train1</span> <span class="o">=</span> <span class="n">X_train</span>
<span class="n">y_train1</span> <span class="o">=</span> <span class="n">y_train</span>

<span class="n">X_train2</span> <span class="o">=</span> <span class="n">X_valid</span>
<span class="n">y_train2</span> <span class="o">=</span> <span class="n">y_valid</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's write a function that generates pairs of images: 50% representing the same digit, and 50% representing different digits. There are many ways to implement this. In this implementation, we first decide how many "same" pairs (i.e. pairs of images representing the same digit) we will generate, and how many "different" pairs (i.e. pairs of images representing different digits). We could just use <code>batch_size // 2</code> but we want to handle the case where it is odd (granted, that might be overkill!). Then we generate random pairs and we pick the right number of "same" pairs, then we generate the right number of "different" pairs. Finally we shuffle the batch and return it:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[171]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_batch</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">size1</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">size2</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">-</span> <span class="n">size1</span>
    <span class="k">if</span> <span class="n">size1</span> <span class="o">!=</span> <span class="n">size2</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="n">size1</span><span class="p">,</span> <span class="n">size2</span> <span class="o">=</span> <span class="n">size2</span><span class="p">,</span> <span class="n">size1</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">size1</span><span class="p">:</span>
        <span class="n">rnd_idx1</span><span class="p">,</span> <span class="n">rnd_idx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rnd_idx1</span> <span class="o">!=</span> <span class="n">rnd_idx2</span> <span class="ow">and</span> <span class="n">labels</span><span class="p">[</span><span class="n">rnd_idx1</span><span class="p">]</span> <span class="o">==</span> <span class="n">labels</span><span class="p">[</span><span class="n">rnd_idx2</span><span class="p">]:</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">images</span><span class="p">[</span><span class="n">rnd_idx1</span><span class="p">],</span> <span class="n">images</span><span class="p">[</span><span class="n">rnd_idx2</span><span class="p">]]))</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
        <span class="n">rnd_idx1</span><span class="p">,</span> <span class="n">rnd_idx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">labels</span><span class="p">[</span><span class="n">rnd_idx1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">labels</span><span class="p">[</span><span class="n">rnd_idx2</span><span class="p">]:</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">images</span><span class="p">[</span><span class="n">rnd_idx1</span><span class="p">],</span> <span class="n">images</span><span class="p">[</span><span class="n">rnd_idx2</span><span class="p">]]))</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">rnd_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)[</span><span class="n">rnd_indices</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's test it to generate a small batch of 5 image pairs:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[172]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each row in <code>X_batch</code> contains a pair of images:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[173]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[173]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>((5, 2, 784), dtype(&#39;float32&#39;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at these pairs:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[174]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_batch</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_batch</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And let's look at the labels (0 means "different", 1 means "same"):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[175]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_batch</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[175]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([[1],
       [0],
       [0],
       [1],
       [0]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Perfect!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="10.3.">10.3.<a class="anchor-link" href="#10.3.">&#182;</a></h3><p><em>Exercise: train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's generate a test set composed of many pairs of images pulled from the MNIST test set:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[176]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_test1</span><span class="p">,</span> <span class="n">y_test1</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now, let's train the model. There's really nothing special about this step, except for the fact that we need a fairly large <code>batch_size</code>, otherwise the model fails to learn anything and ends up with an accuracy of 50%:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[177]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">500</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train1</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">X_train1</span><span class="p">,</span> <span class="n">y_train1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">loss_val</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">training_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Train loss:&quot;</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test1</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test1</span><span class="p">})</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_digit_comparison_model.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Train loss: 0.69103277
0 Test accuracy: 0.542
1 Train loss: 0.6035354
2 Train loss: 0.54946035
3 Train loss: 0.47047246
4 Train loss: 0.4060757
5 Train loss: 0.38308156
5 Test accuracy: 0.824
6 Train loss: 0.39047274
7 Train loss: 0.3390794
8 Train loss: 0.3210671
9 Train loss: 0.31792685
10 Train loss: 0.24494292
10 Test accuracy: 0.8881
11 Train loss: 0.2929235
12 Train loss: 0.23225449
13 Train loss: 0.23180929
14 Train loss: 0.19877923
15 Train loss: 0.20065464
15 Test accuracy: 0.9203
16 Train loss: 0.19700499
17 Train loss: 0.18893136
18 Train loss: 0.19965452
19 Train loss: 0.24071647
20 Train loss: 0.18882024
20 Test accuracy: 0.9367
21 Train loss: 0.12419197
22 Train loss: 0.14013417
23 Train loss: 0.120789476
24 Train loss: 0.15721135
25 Train loss: 0.11507861
25 Test accuracy: 0.948
26 Train loss: 0.13891116
27 Train loss: 0.1526081
28 Train loss: 0.123436704
29 Train loss: 0.11543139
30 Train loss: 0.1140282
30 Test accuracy: 0.9507
31 Train loss: 0.11897083
32 Train loss: 0.09546645
33 Train loss: 0.082996294
34 Train loss: 0.13659164
35 Train loss: 0.0680176
35 Test accuracy: 0.9592
36 Train loss: 0.11016853
37 Train loss: 0.049499925
38 Train loss: 0.081342936
39 Train loss: 0.09441976
40 Train loss: 0.08737087
40 Test accuracy: 0.9619
41 Train loss: 0.07678531
42 Train loss: 0.065963484
43 Train loss: 0.083247386
44 Train loss: 0.07457435
45 Train loss: 0.14071603
45 Test accuracy: 0.9656
46 Train loss: 0.067260325
47 Train loss: 0.09936725
48 Train loss: 0.049633194
49 Train loss: 0.050840884
50 Train loss: 0.04415762
50 Test accuracy: 0.9685
51 Train loss: 0.052975442
52 Train loss: 0.04457429
53 Train loss: 0.09052464
54 Train loss: 0.09460125
55 Train loss: 0.036653373
55 Test accuracy: 0.9688
56 Train loss: 0.046360612
57 Train loss: 0.059152488
58 Train loss: 0.0493788
59 Train loss: 0.060345087
60 Train loss: 0.041447375
60 Test accuracy: 0.9733
61 Train loss: 0.040578153
62 Train loss: 0.057210773
63 Train loss: 0.058645356
64 Train loss: 0.042316362
65 Train loss: 0.029543107
65 Test accuracy: 0.9723
66 Train loss: 0.05906513
67 Train loss: 0.05033586
68 Train loss: 0.045772236
69 Train loss: 0.041796118
70 Train loss: 0.04738202
70 Test accuracy: 0.9743
71 Train loss: 0.019732744
72 Train loss: 0.039464083
73 Train loss: 0.04187814
74 Train loss: 0.05303406
75 Train loss: 0.052625064
75 Test accuracy: 0.9756
76 Train loss: 0.038283084
77 Train loss: 0.026332883
78 Train loss: 0.07060841
79 Train loss: 0.03239444
80 Train loss: 0.03136283
80 Test accuracy: 0.9731
81 Train loss: 0.04390848
82 Train loss: 0.015268046
83 Train loss: 0.04875638
84 Train loss: 0.029360933
85 Train loss: 0.0418443
85 Test accuracy: 0.9759
86 Train loss: 0.018274888
87 Train loss: 0.038872603
88 Train loss: 0.02969683
89 Train loss: 0.020990817
90 Train loss: 0.045234833
90 Test accuracy: 0.9769
91 Train loss: 0.039237432
92 Train loss: 0.031329047
93 Train loss: 0.033414133
94 Train loss: 0.025883088
95 Train loss: 0.019567214
95 Test accuracy: 0.9765
96 Train loss: 0.020650322
97 Train loss: 0.0339851
98 Train loss: 0.047079965
99 Train loss: 0.03125228
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All right, we reach 97.6% accuracy on this digit comparison task. That's not too bad, this model knows a thing or two about comparing handwritten digits!</p>
<p>Let's see if some of that knowledge can be useful for the regular MNIST classification task.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="10.4.">10.4.<a class="anchor-link" href="#10.4.">&#182;</a></h3><p><em>Exercise: now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's create the model, it is pretty straightforward. There are many ways to freeze the lower layers, as explained in the book. In this example, we chose to use the <code>tf.stop_gradient()</code> function. Note that we need one <code>Saver</code> to restore the pretrained DNN A, and another <code>Saver</code> to save the final model:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[178]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="n">dnn_outputs</span> <span class="o">=</span> <span class="n">dnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;DNN_A&quot;</span><span class="p">)</span>
<span class="n">frozen_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">dnn_outputs</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">dnn_outputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">)</span>
<span class="n">Y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">dnn_A_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;DNN_A&quot;</span><span class="p">)</span>
<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="p">{</span><span class="n">var</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">var</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">dnn_A_vars</span><span class="p">})</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now on to training! We first initialize all variables (including the variables in the new output layer), then we restore the pretrained DNN A. Next, we just train the model on the small MNIST dataset (containing just 5,000 images):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[179]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">restore_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_digit_comparison_model.ckpt&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:tensorflow:Restoring parameters from ./my_digit_comparison_model.ckpt
0 Test accuracy: 0.9455
10 Test accuracy: 0.9634
20 Test accuracy: 0.9659
30 Test accuracy: 0.9656
40 Test accuracy: 0.9655
50 Test accuracy: 0.9656
60 Test accuracy: 0.9655
70 Test accuracy: 0.9656
80 Test accuracy: 0.9654
90 Test accuracy: 0.9654
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well, 96.5% accuracy, that's not the best MNIST model we have trained so far, but recall that we are only using a small training set (just 500 images per digit). Let's compare this result with the same DNN trained from scratch, without using transfer learning:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[180]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reset_graph</span><span class="p">()</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>  <span class="c1"># MNIST</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="n">dnn_outputs</span> <span class="o">=</span> <span class="n">dnn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;DNN_A&quot;</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">dnn_outputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">he_init</span><span class="p">)</span>
<span class="n">Y_proba</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

<span class="n">xentropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">in_top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">dnn_A_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;DNN_A&quot;</span><span class="p">)</span>
<span class="n">restore_saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="p">{</span><span class="n">var</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">var</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">dnn_A_vars</span><span class="p">})</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[181]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">rnd_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">rnd_indices</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">rnd_idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train2</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">],</span> <span class="n">y_train2</span><span class="p">[</span><span class="n">rnd_indices</span><span class="p">]</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_test</span><span class="p">})</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="s2">&quot;Test accuracy:&quot;</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">)</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_mnist_model_final.ckpt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>0 Test accuracy: 0.8694
10 Test accuracy: 0.9276
20 Test accuracy: 0.9299
30 Test accuracy: 0.935
40 Test accuracy: 0.942
50 Test accuracy: 0.9435
60 Test accuracy: 0.9442
70 Test accuracy: 0.9447
80 Test accuracy: 0.9448
90 Test accuracy: 0.945
100 Test accuracy: 0.945
110 Test accuracy: 0.9458
120 Test accuracy: 0.9456
130 Test accuracy: 0.9458
140 Test accuracy: 0.9458
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Only 94.6% accuracy... So transfer learning helped us reduce the error rate from 5.4% to 3.5% (that's over 35% error reduction). Moreover, the model using transfer learning reached over 96% accuracy in less than 10 epochs.</p>
<p>Bottom line: transfer learning does not always work, but when it does it can make a big difference. So try it out!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
